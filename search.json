[
  {
    "objectID": "publication/index.html",
    "href": "publication/index.html",
    "title": "Chans Lecture Note",
    "section": "",
    "text": "Hierarchical Power Management Framework on Manycore Systems Using OS Migration Techniques   Chanseok Kang  M.S Thesis, 2015 \n\n\n\n\n\n\n\n\n\n Scheduling for better energy efficiency on many-core chips   Chanseok Kang, Seungyul Lee, Yong-Jun Lee, Jaejin Lee, Bernhard Egger  Workshop on Job Scheduling Strategies for Parallel Processing (JSSPP), 2015 \n\n\n\nIdentical version of M.S Thesis\n\n\n\n\n\n\n\n\n NuPow: Managing Power on NUMA Multiprocessors with Domain-Level Voltage and Frequency Control   Changmin Ahn, Seungyul Lee, Chanseok Kang, Bernhard Egger  International Conference on the Economics of Grids, Clouds, Systems, and Services (GECON), 2020"
  },
  {
    "objectID": "publication/index.html#research",
    "href": "publication/index.html#research",
    "title": "Chans Lecture Note",
    "section": "",
    "text": "Hierarchical Power Management Framework on Manycore Systems Using OS Migration Techniques   Chanseok Kang  M.S Thesis, 2015 \n\n\n\n\n\n\n\n\n\n Scheduling for better energy efficiency on many-core chips   Chanseok Kang, Seungyul Lee, Yong-Jun Lee, Jaejin Lee, Bernhard Egger  Workshop on Job Scheduling Strategies for Parallel Processing (JSSPP), 2015 \n\n\n\nIdentical version of M.S Thesis\n\n\n\n\n\n\n\n\n NuPow: Managing Power on NUMA Multiprocessors with Domain-Level Voltage and Frequency Control   Changmin Ahn, Seungyul Lee, Chanseok Kang, Bernhard Egger  International Conference on the Economics of Grids, Clouds, Systems, and Services (GECON), 2020"
  },
  {
    "objectID": "publication/index.html#translation-review",
    "href": "publication/index.html#translation-review",
    "title": "Chans Lecture Note",
    "section": "Translation & Review",
    "text": "Translation & Review\n\n\n\n\n\n\n\n\n 그로킹 심층 강화학습 (Grokking Deep Reinforcement Learning)   Miguel Morales, 강찬석(번역)  한빛미디어, 2021  22년 대한민국학술원 선정 우수학술도서 \n\n\n\n\n\n\n\n\n\n 텐서플로를 활용한 머신러닝 (Machine Learning with Tensorflow)   Nishant Shukla, 송교석(번역), 강찬석(감수)  한빛미디어, 2019"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-9.html#tldr",
    "href": "publication/GDRL/GDRL-chapter-9.html#tldr",
    "title": "Chapter 9: More Stable Value-based Methods",
    "section": "TL;DR",
    "text": "TL;DR\n\n그로킹 심층 강화학습 중 9장 내용인 “조금 더 안정적인 가치기반의 학습 방법들”에 대한 내용입니다.\n\n\n\nCode\n!pip install tqdm numpy scikit-learn pyglet setuptools && \\\n!pip install gym asciinema pandas tabulate tornado==5.* PyBullet && \\\n!pip install git+https://github.com/pybox2d/pybox2d#egg=Box2D && \\\n!pip install git+https://github.com/mimoralea/gym-bandits#egg=gym-bandits && \\\n!pip install git+https://github.com/mimoralea/gym-walk#egg=gym-walk && \\\n!pip install git+https://github.com/mimoralea/gym-aima#egg=gym-aima && \\\n!pip install gym[atari]\n!pip install torch torchvision\n\n\n\nimport warnings ; warnings.filterwarnings('ignore')\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport numpy as np\nfrom IPython.display import display\nfrom collections import namedtuple, deque\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nfrom itertools import cycle, count\nfrom textwrap import wrap\n\nimport matplotlib\nimport subprocess\nimport os.path\nimport tempfile\nimport random\nimport base64\nimport pprint\nimport glob\nimport time\nimport json\nimport sys\nimport gym\nimport io\nimport os\nimport gc\nimport platform\n\nfrom gym import wrappers\nfrom subprocess import check_output\nfrom IPython.display import HTML\n\nLEAVE_PRINT_EVERY_N_SECS = 60\nERASE_LINE = '\\x1b[2K'\nEPS = 1e-6\nRESULTS_DIR = os.path.join('.', 'gym-results')\nSEEDS = (12, 34, 56, 78, 90)\n\n%matplotlib inline\n\n\nplt.style.use('fivethirtyeight')\nparams = {\n    'figure.figsize': (15, 8),\n    'font.size': 24,\n    'legend.fontsize': 20,\n    'axes.titlesize': 28,\n    'axes.labelsize': 24,\n    'xtick.labelsize': 20,\n    'ytick.labelsize': 20\n}\npylab.rcParams.update(params)\nnp.set_printoptions(suppress=True)\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\ndef get_make_env_fn(**kargs):\n    def make_env_fn(env_name, seed=None, render=None, record=False,\n                    unwrapped=False, monitor_mode=None, \n                    inner_wrappers=None, outer_wrappers=None):\n        mdir = tempfile.mkdtemp()\n        env = None\n        if render:\n            try:\n                env = gym.make(env_name, render=render)\n            except:\n                pass\n        if env is None:\n            env = gym.make(env_name)\n        if seed is not None: env.seed(seed)\n        env = env.unwrapped if unwrapped else env\n        if inner_wrappers:\n            for wrapper in inner_wrappers:\n                env = wrapper(env)\n        env = wrappers.Monitor(\n            env, mdir, force=True, \n            mode=monitor_mode, \n            video_callable=lambda e_idx: record) if monitor_mode else env\n        if outer_wrappers:\n            for wrapper in outer_wrappers:\n                env = wrapper(env)\n        return env\n    return make_env_fn, kargs\n\n\ndef get_videos_html(env_videos, title, max_n_videos=5):\n    videos = np.array(env_videos)\n    if len(videos) == 0:\n        return\n    \n    n_videos = max(1, min(max_n_videos, len(videos)))\n    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int) if n_videos &gt; 1 else [-1,]\n    videos = videos[idxs,...]\n\n    strm = '&lt;h2&gt;{}&lt;/h2&gt;'.format(title)\n    for video_path, meta_path in videos:\n        video = io.open(video_path, 'r+b').read()\n        encoded = base64.b64encode(video)\n\n        with open(meta_path) as data_file:    \n            meta = json.load(data_file)\n\n        html_tag = \"\"\"\n        &lt;h3&gt;{0}&lt;/h3&gt;\n        &lt;video width=\"960\" height=\"540\" controls&gt;\n            &lt;source src=\"data:video/mp4;base64,{1}\" type=\"video/mp4\" /&gt;\n        &lt;/video&gt;\"\"\"\n        strm += html_tag.format('Episode ' + str(meta['episode_id']), encoded.decode('ascii'))\n    return strm\n\n\nplatform.system()\n\n'Windows'\n\n\n\ndef get_gif_html(env_videos, title, subtitle_eps=None, max_n_videos=4):\n    videos = np.array(env_videos)\n    if len(videos) == 0:\n        return\n    \n    n_videos = max(1, min(max_n_videos, len(videos)))\n    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int) if n_videos &gt; 1 else [-1,]\n    videos = videos[idxs,...]\n\n    strm = '&lt;h2&gt;{}&lt;/h2&gt;'.format(title)\n    for video_path, meta_path in videos:\n        basename = os.path.splitext(video_path)[0]\n        gif_path = basename + '.gif'\n        if not os.path.exists(gif_path):\n            if platform.system() == 'Linux':\n                ps = subprocess.Popen(\n                    ('ffmpeg', \n                     '-i', video_path, \n                     '-r', '7',\n                     '-f', 'image2pipe', \n                     '-vcodec', 'ppm',\n                     '-crf', '20',\n                     '-vf', 'scale=512:-1',\n                     '-'), \n                    stdout=subprocess.PIPE,\n                    universal_newlines=True)\n                output = subprocess.check_output(\n                    ('convert',\n                     '-coalesce',\n                     '-delay', '7',\n                     '-loop', '0',\n                     '-fuzz', '2%',\n                     '+dither',\n                     '-deconstruct',\n                     '-layers', 'Optimize',\n                     '-', gif_path), \n                    stdin=ps.stdout)\n                ps.wait()\n            else:\n                ps = subprocess.Popen('ffmpeg -i {} -r 7 -f image2pipe \\\n                                      -vcodec ppm -crf 20 -vf scale=512:-1 - | \\\n                                      convert -coalesce -delay 7 -loop 0 -fuzz 2% \\\n                                      +dither -deconstruct -layers Optimize \\\n                                      - {}'.format(video_path, gif_path), \n                                      stdin=subprocess.PIPE, \n                                      shell=True)\n                ps.wait()\n\n        gif = io.open(gif_path, 'r+b').read()\n        encoded = base64.b64encode(gif)\n            \n        with open(meta_path) as data_file:    \n            meta = json.load(data_file)\n\n        html_tag = \"\"\"\n        &lt;h3&gt;{0}&lt;/h3&gt;\n        &lt;img src=\"data:image/gif;base64,{1}\" /&gt;\"\"\"\n        prefix = 'Trial ' if subtitle_eps is None else 'Episode '\n        sufix = str(meta['episode_id'] if subtitle_eps is None \\\n                    else subtitle_eps[meta['episode_id']])\n        strm += html_tag.format(prefix + sufix, encoded.decode('ascii'))\n    return strm\n\n\nclass FCQ(nn.Module):\n    def __init__(self, \n                 input_dim, \n                 output_dim, \n                 hidden_dims=(32,32), \n                 activation_fc=F.relu):\n        super(FCQ, self).__init__()\n        self.activation_fc = activation_fc\n\n        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n\n        self.hidden_layers = nn.ModuleList()\n        for i in range(len(hidden_dims)-1):\n            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n            self.hidden_layers.append(hidden_layer)\n        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n\n        device = \"cpu\"\n        if torch.cuda.is_available():\n            device = \"cuda:0\"\n        self.device = torch.device(device)\n        self.to(self.device)\n        \n    def _format(self, state):\n        x = state\n        if not isinstance(x, torch.Tensor):\n            x = torch.tensor(x, \n                             device=self.device, \n                             dtype=torch.float32)\n            x = x.unsqueeze(0)\n        return x\n\n    def forward(self, state):\n        x = self._format(state)\n        x = self.activation_fc(self.input_layer(x))\n        for hidden_layer in self.hidden_layers:\n            x = self.activation_fc(hidden_layer(x))\n        x = self.output_layer(x)\n        return x\n    \n    def numpy_float_to_device(self, variable):\n        variable = torch.from_numpy(variable).float().to(self.device)\n        return variable\n    \n    def load(self, experiences):\n        states, actions, new_states, rewards, is_terminals = experiences\n        states = torch.from_numpy(states).float().to(self.device)\n        actions = torch.from_numpy(actions).long().to(self.device)\n        new_states = torch.from_numpy(new_states).float().to(self.device)\n        rewards = torch.from_numpy(rewards).float().to(self.device)\n        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n        return states, actions, new_states, rewards, is_terminals\n\n\nclass GreedyStrategy():\n    def __init__(self):\n        self.exploratory_action_taken = False\n\n    def select_action(self, model, state):\n        with torch.no_grad():\n            q_values = model(state).cpu().detach().data.numpy().squeeze()\n            return np.argmax(q_values)\n\n\nclass EGreedyStrategy():\n    def __init__(self, epsilon=0.1):\n        self.epsilon = epsilon\n        self.exploratory_action_taken = None\n\n    def select_action(self, model, state):\n        self.exploratory_action_taken = False\n        with torch.no_grad():\n            q_values = model(state).cpu().detach().data.numpy().squeeze()\n\n        if np.random.rand() &gt; self.epsilon:\n            action = np.argmax(q_values)\n        else: \n            action = np.random.randint(len(q_values))\n\n        self.exploratory_action_taken = action != np.argmax(q_values)\n        return action\n\n\ns = EGreedyStrategy()\nplt.plot([s.epsilon for _ in range(50000)])\nplt.title('Epsilon-Greedy epsilon value')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\nclass EGreedyLinearStrategy():\n    def __init__(self, init_epsilon=1.0, min_epsilon=0.1, decay_steps=20000):\n        self.t = 0\n        self.epsilon = init_epsilon\n        self.init_epsilon = init_epsilon\n        self.min_epsilon = min_epsilon\n        self.decay_steps = decay_steps\n        self.exploratory_action_taken = None\n        \n    def _epsilon_update(self):\n        epsilon = 1 - self.t / self.decay_steps\n        epsilon = (self.init_epsilon - self.min_epsilon) * epsilon + self.min_epsilon\n        epsilon = np.clip(epsilon, self.min_epsilon, self.init_epsilon)\n        self.t += 1\n        return epsilon\n\n    def select_action(self, model, state):\n        self.exploratory_action_taken = False\n        with torch.no_grad():\n            q_values = model(state).cpu().detach().data.numpy().squeeze()\n\n        if np.random.rand() &gt; self.epsilon:\n            action = np.argmax(q_values)\n        else: \n            action = np.random.randint(len(q_values))\n\n        self.epsilon = self._epsilon_update()\n        self.exploratory_action_taken = action != np.argmax(q_values)\n        return action\n\n\ns = EGreedyLinearStrategy()\nplt.plot([s._epsilon_update() for _ in range(50000)])\nplt.title('Epsilon-Greedy linearly decaying epsilon value')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\nclass EGreedyExpStrategy():\n    def __init__(self, init_epsilon=1.0, min_epsilon=0.1, decay_steps=20000):\n        self.epsilon = init_epsilon\n        self.init_epsilon = init_epsilon\n        self.decay_steps = decay_steps\n        self.min_epsilon = min_epsilon\n        self.epsilons = 0.01 / np.logspace(-2, 0, decay_steps, endpoint=False) - 0.01\n        self.epsilons = self.epsilons * (init_epsilon - min_epsilon) + min_epsilon\n        self.t = 0\n        self.exploratory_action_taken = None\n\n    def _epsilon_update(self):\n        self.epsilon = self.min_epsilon if self.t &gt;= self.decay_steps else self.epsilons[self.t]\n        self.t += 1\n        return self.epsilon\n\n    def select_action(self, model, state):\n        self.exploratory_action_taken = False\n        with torch.no_grad():\n            q_values = model(state).detach().cpu().data.numpy().squeeze()\n\n        if np.random.rand() &gt; self.epsilon:\n            action = np.argmax(q_values)\n        else:\n            action = np.random.randint(len(q_values))\n\n        self._epsilon_update()\n        self.exploratory_action_taken = action != np.argmax(q_values)\n        return action\n\n\ns = EGreedyExpStrategy()\nplt.plot([s._epsilon_update() for _ in range(50000)])\nplt.title('Epsilon-Greedy exponentially decaying epsilon value')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\nclass SoftMaxStrategy():\n    def __init__(self, \n                 init_temp=1.0, \n                 min_temp=0.3, \n                 exploration_ratio=0.8, \n                 max_steps=25000):\n        self.t = 0\n        self.init_temp = init_temp\n        self.exploration_ratio = exploration_ratio\n        self.min_temp = min_temp\n        self.max_steps = max_steps\n        self.exploratory_action_taken = None\n        \n    def _update_temp(self):\n        temp = 1 - self.t / (self.max_steps * self.exploration_ratio)\n        temp = (self.init_temp - self.min_temp) * temp + self.min_temp\n        temp = np.clip(temp, self.min_temp, self.init_temp)\n        self.t += 1\n        return temp\n\n    def select_action(self, model, state):\n        self.exploratory_action_taken = False\n        temp = self._update_temp()\n\n        with torch.no_grad():\n            q_values = model(state).cpu().detach().data.numpy().squeeze()\n            scaled_qs = q_values/temp\n            norm_qs = scaled_qs - scaled_qs.max()            \n            e = np.exp(norm_qs)\n            probs = e / np.sum(e)\n            assert np.isclose(probs.sum(), 1.0)\n\n        action = np.random.choice(np.arange(len(probs)), size=1, p=probs)[0]\n        self.exploratory_action_taken = action != np.argmax(q_values)\n        return action\n\n\ns = SoftMaxStrategy()\nplt.plot([s._update_temp() for _ in range(50000)])\nplt.title('SoftMax linearly decaying temperature value')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\nclass ReplayBuffer():\n    def __init__(self, \n                 max_size=10000, \n                 batch_size=64):\n        self.ss_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n        self.as_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n        self.rs_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n        self.ps_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n        self.ds_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n\n        self.max_size = max_size\n        self.batch_size = batch_size\n        self._idx = 0\n        self.size = 0\n    \n    def store(self, sample):\n        s, a, r, p, d = sample\n        self.ss_mem[self._idx] = s\n        self.as_mem[self._idx] = a\n        self.rs_mem[self._idx] = r\n        self.ps_mem[self._idx] = p\n        self.ds_mem[self._idx] = d\n        \n        self._idx += 1\n        self._idx = self._idx % self.max_size\n\n        self.size += 1\n        self.size = min(self.size, self.max_size)\n\n    def sample(self, batch_size=None):\n        if batch_size == None:\n            batch_size = self.batch_size\n\n        idxs = np.random.choice(\n            self.size, batch_size, replace=False)\n        experiences = np.vstack(self.ss_mem[idxs]), \\\n                      np.vstack(self.as_mem[idxs]), \\\n                      np.vstack(self.rs_mem[idxs]), \\\n                      np.vstack(self.ps_mem[idxs]), \\\n                      np.vstack(self.ds_mem[idxs])\n        return experiences\n\n    def __len__(self):\n        return self.size"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-9.html#dqn",
    "href": "publication/GDRL/GDRL-chapter-9.html#dqn",
    "title": "Chapter 9: More Stable Value-based Methods",
    "section": "DQN",
    "text": "DQN\n\nclass DQN():\n    def __init__(self, \n                 replay_buffer_fn, \n                 value_model_fn, \n                 value_optimizer_fn, \n                 value_optimizer_lr,\n                 training_strategy_fn,\n                 evaluation_strategy_fn,\n                 n_warmup_batches,\n                 update_target_every_steps):\n        self.replay_buffer_fn = replay_buffer_fn\n        self.value_model_fn = value_model_fn\n        self.value_optimizer_fn = value_optimizer_fn\n        self.value_optimizer_lr = value_optimizer_lr\n        self.training_strategy_fn = training_strategy_fn\n        self.evaluation_strategy_fn = evaluation_strategy_fn\n        self.n_warmup_batches = n_warmup_batches\n        self.update_target_every_steps = update_target_every_steps\n\n    def optimize_model(self, experiences):\n        states, actions, rewards, next_states, is_terminals = experiences\n        batch_size = len(is_terminals)\n        \n        max_a_q_sp = self.target_model(next_states).detach().max(1)[0].unsqueeze(1)\n        target_q_sa = rewards + (self.gamma * max_a_q_sp * (1 - is_terminals))\n        q_sa = self.online_model(states).gather(1, actions)\n\n        td_error = q_sa - target_q_sa\n        value_loss = td_error.pow(2).mul(0.5).mean()\n        self.value_optimizer.zero_grad()\n        value_loss.backward()\n        self.value_optimizer.step()\n\n    def interaction_step(self, state, env):\n        action = self.training_strategy.select_action(self.online_model, state)\n        new_state, reward, is_terminal, info = env.step(action)\n        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n        is_failure = is_terminal and not is_truncated\n        experience = (state, action, reward, new_state, float(is_failure))\n\n        self.replay_buffer.store(experience)\n        self.episode_reward[-1] += reward\n        self.episode_timestep[-1] += 1\n        self.episode_exploration[-1] += int(self.training_strategy.exploratory_action_taken)\n        return new_state, is_terminal\n    \n    def update_network(self):\n        for target, online in zip(self.target_model.parameters(), \n                                  self.online_model.parameters()):\n            target.data.copy_(online.data)\n\n    def train(self, make_env_fn, make_env_kargs, seed, gamma, \n              max_minutes, max_episodes, goal_mean_100_reward):\n        training_start, last_debug_time = time.time(), float('-inf')\n\n        self.checkpoint_dir = tempfile.mkdtemp()\n        self.make_env_fn = make_env_fn\n        self.make_env_kargs = make_env_kargs\n        self.seed = seed\n        self.gamma = gamma\n        \n        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n    \n        nS, nA = env.observation_space.shape[0], env.action_space.n\n        self.episode_timestep = []\n        self.episode_reward = []\n        self.episode_seconds = []\n        self.evaluation_scores = []        \n        self.episode_exploration = []\n        \n        self.target_model = self.value_model_fn(nS, nA)\n        self.online_model = self.value_model_fn(nS, nA)\n        self.update_network()\n\n        self.value_optimizer = self.value_optimizer_fn(self.online_model, \n                                                       self.value_optimizer_lr)\n\n        self.replay_buffer = self.replay_buffer_fn()\n        self.training_strategy = training_strategy_fn()\n        self.evaluation_strategy = evaluation_strategy_fn() \n                    \n        result = np.empty((max_episodes, 5))\n        result[:] = np.nan\n        training_time = 0\n        for episode in range(1, max_episodes + 1):\n            episode_start = time.time()\n            \n            state, is_terminal = env.reset(), False\n            self.episode_reward.append(0.0)\n            self.episode_timestep.append(0.0)\n            self.episode_exploration.append(0.0)\n\n            for step in count():\n                state, is_terminal = self.interaction_step(state, env)\n                \n                min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n                if len(self.replay_buffer) &gt; min_samples:\n                    experiences = self.replay_buffer.sample()\n                    experiences = self.online_model.load(experiences)\n                    self.optimize_model(experiences)\n                \n                if np.sum(self.episode_timestep) % self.update_target_every_steps == 0:\n                    self.update_network()\n                \n                if is_terminal:\n                    gc.collect()\n                    break\n            \n            # stats\n            episode_elapsed = time.time() - episode_start\n            self.episode_seconds.append(episode_elapsed)\n            training_time += episode_elapsed\n            evaluation_score, _ = self.evaluate(self.online_model, env)\n            self.save_checkpoint(episode-1, self.online_model)\n            \n            total_step = int(np.sum(self.episode_timestep))\n            self.evaluation_scores.append(evaluation_score)\n            \n            mean_10_reward = np.mean(self.episode_reward[-10:])\n            std_10_reward = np.std(self.episode_reward[-10:])\n            mean_100_reward = np.mean(self.episode_reward[-100:])\n            std_100_reward = np.std(self.episode_reward[-100:])\n            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n            lst_100_exp_rat = np.array(\n                self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n            mean_100_exp_rat = np.mean(lst_100_exp_rat)\n            std_100_exp_rat = np.std(lst_100_exp_rat)\n            \n            wallclock_elapsed = time.time() - training_start\n            result[episode-1] = total_step, mean_100_reward, \\\n                mean_100_eval_score, training_time, wallclock_elapsed\n            \n            reached_debug_time = time.time() - last_debug_time &gt;= LEAVE_PRINT_EVERY_N_SECS\n            reached_max_minutes = wallclock_elapsed &gt;= max_minutes * 60\n            reached_max_episodes = episode &gt;= max_episodes\n            reached_goal_mean_reward = mean_100_eval_score &gt;= goal_mean_100_reward\n            training_is_over = reached_max_minutes or \\\n                               reached_max_episodes or \\\n                               reached_goal_mean_reward\n\n            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n            debug_message = 'el {}, ep {:04}, ts {:06}, '\n            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n            debug_message = debug_message.format(\n                elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n                mean_100_eval_score, std_100_eval_score)\n            print(debug_message, end='\\r', flush=True)\n            if reached_debug_time or training_is_over:\n                print(ERASE_LINE + debug_message, flush=True)\n                last_debug_time = time.time()\n            if training_is_over:\n                if reached_max_minutes: print(u'--&gt; reached_max_minutes \\u2715')\n                if reached_max_episodes: print(u'--&gt; reached_max_episodes \\u2715')\n                if reached_goal_mean_reward: print(u'--&gt; reached_goal_mean_reward \\u2713')\n                break\n                \n        final_eval_score, score_std = self.evaluate(self.online_model, env, n_episodes=100)\n        wallclock_time = time.time() - training_start\n        print('Training complete.')\n        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n              ' {:.2f}s wall-clock time.\\n'.format(\n                  final_eval_score, score_std, training_time, wallclock_time))\n        env.close() ; del env\n        self.get_cleaned_checkpoints()\n        return result, final_eval_score, training_time, wallclock_time\n    \n    def evaluate(self, eval_policy_model, eval_env, n_episodes=1):\n        rs = []\n        for _ in range(n_episodes):\n            s, d = eval_env.reset(), False\n            rs.append(0)\n            for _ in count():\n                a = self.evaluation_strategy.select_action(eval_policy_model, s)\n                s, r, d, _ = eval_env.step(a)\n                rs[-1] += r\n                if d: break\n        return np.mean(rs), np.std(rs)\n\n    def get_cleaned_checkpoints(self, n_checkpoints=5):\n        try: \n            return self.checkpoint_paths\n        except AttributeError:\n            self.checkpoint_paths = {}\n\n        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n        last_ep = max(paths_dic.keys())\n        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n\n        for idx, path in paths_dic.items():\n            if idx in checkpoint_idxs:\n                self.checkpoint_paths[idx] = path\n            else:\n                os.unlink(path)\n\n        return self.checkpoint_paths\n\n    def demo_last(self, title='Fully-trained {} Agent', n_episodes=3, max_n_videos=3):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        last_ep = max(checkpoint_paths.keys())\n        self.online_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n\n        self.evaluate(self.online_model, env, n_episodes=n_episodes)\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def demo_progression(self, title='{} Agent progression', max_n_videos=5):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        for i in sorted(checkpoint_paths.keys()):\n            self.online_model.load_state_dict(torch.load(checkpoint_paths[i]))\n            self.evaluate(self.online_model, env, n_episodes=1)\n\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            subtitle_eps=sorted(checkpoint_paths.keys()),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def save_checkpoint(self, episode_idx, model):\n        torch.save(model.state_dict(), \n                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))\n\n\ndqn_results = []\nbest_agent, best_eval_score = None, float('-inf')\nfor seed in SEEDS:\n    environment_settings = {\n        'env_name': 'CartPole-v1',\n        'gamma': 1.00,\n        'max_minutes': 20,\n        'max_episodes': 10000,\n        'goal_mean_100_reward': 475\n    }\n    \n    value_model_fn = lambda nS, nA: FCQ(nS, nA, hidden_dims=(512,128))\n    value_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr=lr)\n    value_optimizer_lr = 0.0005\n\n    # training_strategy_fn = lambda: EGreedyStrategy(epsilon=0.5)\n    # training_strategy_fn = lambda: EGreedyLinearStrategy(init_epsilon=1.0,\n    #                                                      min_epsilon=0.3, \n    #                                                      max_steps=20000)\n    # training_strategy_fn = lambda: SoftMaxStrategy(init_temp=1.0, \n    #                                                min_temp=0.1, \n    #                                                exploration_ratio=0.8, \n    #                                                max_steps=20000)\n    training_strategy_fn = lambda: EGreedyExpStrategy(init_epsilon=1.0,  \n                                                      min_epsilon=0.3, \n                                                      decay_steps=20000)\n    evaluation_strategy_fn = lambda: GreedyStrategy()\n\n    replay_buffer_fn = lambda: ReplayBuffer(max_size=50000, batch_size=64)\n    n_warmup_batches = 5\n    update_target_every_steps = 10\n\n    env_name, gamma, max_minutes, \\\n    max_episodes, goal_mean_100_reward = environment_settings.values()\n    agent = DQN(replay_buffer_fn,\n                value_model_fn,\n                value_optimizer_fn,\n                value_optimizer_lr,\n                training_strategy_fn,\n                evaluation_strategy_fn,\n                n_warmup_batches,\n                update_target_every_steps)\n\n    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n    result, final_eval_score, training_time, wallclock_time = agent.train(\n        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n    dqn_results.append(result)\n    if final_eval_score &gt; best_eval_score:\n        best_eval_score = final_eval_score\n        best_agent = agent\ndqn_results = np.array(dqn_results)\n\nel 00:00:01, ep 0000, ts 000016, ar 10 016.0±000.0, 100 016.0±000.0, ex 100 0.4±0.0, ev 019.0±000.0\nel 00:01:02, ep 0167, ts 016307, ar 10 244.5±071.2, 100 139.5±083.8, ex 100 0.3±0.1, ev 298.9±099.1\nel 00:02:03, ep 0209, ts 034129, ar 10 454.2±106.8, 100 281.0±159.7, ex 100 0.2±0.1, ev 384.8±117.8\nel 00:03:03, ep 0245, ts 050529, ar 10 440.8±103.7, 100 388.3±147.7, ex 100 0.2±0.0, ev 458.1±085.3\nel 00:03:24, ep 0257, ts 055793, ar 10 458.9±083.1, 100 419.3±130.8, ex 100 0.2±0.0, ev 477.7±068.2\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 175.96s training time, 220.58s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000034, ar 10 034.0±000.0, 100 034.0±000.0, ex 100 0.6±0.0, ev 008.0±000.0\nel 00:01:00, ep 0161, ts 016773, ar 10 288.4±130.7, 100 149.3±118.1, ex 100 0.3±0.1, ev 290.7±113.8\nel 00:02:01, ep 0212, ts 034365, ar 10 458.2±085.1, 100 286.4±135.4, ex 100 0.2±0.1, ev 384.4±119.0\nel 00:03:03, ep 0248, ts 051154, ar 10 500.0±000.0, 100 381.8±131.7, ex 100 0.2±0.0, ev 449.1±090.9\nel 00:03:35, ep 0267, ts 059611, ar 10 430.9±148.7, 100 410.9±127.3, ex 100 0.2±0.0, ev 475.5±064.4\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 187.33s training time, 231.06s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000012, ar 10 012.0±000.0, 100 012.0±000.0, ex 100 0.6±0.0, ev 010.0±000.0\nel 00:01:00, ep 0161, ts 016947, ar 10 241.3±090.4, 100 151.3±104.1, ex 100 0.3±0.1, ev 284.7±092.7\nel 00:02:00, ep 0211, ts 034385, ar 10 421.6±156.1, 100 285.7±140.3, ex 100 0.2±0.0, ev 365.2±120.5\nel 00:03:02, ep 0247, ts 051165, ar 10 468.9±093.3, 100 373.4±150.3, ex 100 0.2±0.0, ev 432.6±111.0\nel 00:03:40, ep 0268, ts 060955, ar 10 474.5±071.6, 100 423.5±128.1, ex 100 0.2±0.0, ev 478.3±064.0\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 192.58s training time, 236.86s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000012, ar 10 012.0±000.0, 100 012.0±000.0, ex 100 0.6±0.0, ev 012.0±000.0\nel 00:01:01, ep 0179, ts 016714, ar 10 329.5±139.7, 100 142.5±105.5, ex 100 0.3±0.1, ev 250.7±097.5\nel 00:02:02, ep 0219, ts 034188, ar 10 464.7±105.9, 100 283.3±173.5, ex 100 0.2±0.1, ev 368.3±131.1\nel 00:03:04, ep 0254, ts 050476, ar 10 426.7±133.5, 100 398.1±149.5, ex 100 0.2±0.0, ev 441.3±098.8\nel 00:03:36, ep 0271, ts 058449, ar 10 470.9±087.3, 100 446.0±113.1, ex 100 0.2±0.0, ev 475.3±066.1\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 488.33±51.75 in 190.46s training time, 232.66s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000044, ar 10 044.0±000.0, 100 044.0±000.0, ex 100 0.6±0.0, ev 009.0±000.0\nel 00:01:00, ep 0158, ts 016269, ar 10 298.9±075.9, 100 148.5±117.7, ex 100 0.3±0.1, ev 301.5±114.7\nel 00:02:00, ep 0218, ts 033489, ar 10 401.9±114.2, 100 271.9±115.6, ex 100 0.2±0.0, ev 346.2±101.7\nel 00:03:00, ep 0293, ts 048735, ar 10 127.4±034.9, 100 234.6±150.4, ex 100 0.2±0.0, ev 287.6±157.1\nel 00:04:01, ep 0383, ts 063261, ar 10 192.7±172.1, 100 158.0±126.9, ex 100 0.2±0.1, ev 213.0±157.0\nel 00:05:02, ep 0495, ts 077925, ar 10 465.6±103.2, 100 103.8±163.3, ex 100 0.2±0.1, ev 150.3±202.5\nel 00:06:03, ep 0530, ts 093857, ar 10 481.7±054.9, 100 237.5±225.7, ex 100 0.2±0.1, ev 284.0±234.1\nel 00:07:04, ep 0565, ts 109795, ar 10 465.5±091.5, 100 386.5±184.9, ex 100 0.2±0.0, ev 448.1±145.2\nel 00:07:15, ep 0571, ts 112795, ar 10 500.0±000.0, 100 414.6±163.3, ex 100 0.2±0.0, ev 475.4±102.0\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 499.98±0.14 in 384.84s training time, 451.81s wall-clock time.\n\n\n\n\nbest_agent.demo_progression()\n\nDQN Agent progression\n        Episode 0\n        \n        Episode 64\n        \n        Episode 128\n        \n        Episode 192\n        \n        Episode 257\n        \n\n\n\nbest_agent.demo_last()\n\nFully-trained DQN Agent\n        Trial 0\n        \n        Trial 1\n        \n        Trial 2\n        \n\n\n\nnfq_root_dir = os.path.join(RESULTS_DIR, 'nfq')\nnfq_x = np.load(os.path.join(nfq_root_dir, 'x.npy'))\n\nnfq_max_r = np.load(os.path.join(nfq_root_dir, 'max_r.npy'))\nnfq_min_r = np.load(os.path.join(nfq_root_dir, 'min_r.npy'))\nnfq_mean_r = np.load(os.path.join(nfq_root_dir, 'mean_r.npy'))\n\nnfq_max_s = np.load(os.path.join(nfq_root_dir, 'max_s.npy'))\nnfq_min_s = np.load(os.path.join(nfq_root_dir, 'min_s.npy'))\nnfq_mean_s = np.load(os.path.join(nfq_root_dir, 'mean_s.npy'))\n\nnfq_max_t = np.load(os.path.join(nfq_root_dir, 'max_t.npy'))\nnfq_min_t = np.load(os.path.join(nfq_root_dir, 'min_t.npy'))\nnfq_mean_t = np.load(os.path.join(nfq_root_dir, 'mean_t.npy'))\n\nnfq_max_sec = np.load(os.path.join(nfq_root_dir, 'max_sec.npy'))\nnfq_min_sec = np.load(os.path.join(nfq_root_dir, 'min_sec.npy'))\nnfq_mean_sec = np.load(os.path.join(nfq_root_dir, 'mean_sec.npy'))\n\nnfq_max_rt = np.load(os.path.join(nfq_root_dir, 'max_rt.npy'))\nnfq_min_rt = np.load(os.path.join(nfq_root_dir, 'min_rt.npy'))\nnfq_mean_rt = np.load(os.path.join(nfq_root_dir, 'mean_rt.npy'))\n\n\ndqn_max_t, dqn_max_r, dqn_max_s, \\\n    dqn_max_sec, dqn_max_rt = np.max(dqn_results, axis=0).T\ndqn_min_t, dqn_min_r, dqn_min_s, \\\n    dqn_min_sec, dqn_min_rt = np.min(dqn_results, axis=0).T\ndqn_mean_t, dqn_mean_r, dqn_mean_s, \\\n    dqn_mean_sec, dqn_mean_rt = np.mean(dqn_results, axis=0).T\ndqn_x = np.arange(np.max((len(dqn_mean_s), len(nfq_mean_s))))\n\n\nfig, axs = plt.subplots(5, 1, figsize=(15,30), sharey=False, sharex=True)\n\n# NFQ\naxs[0].plot(nfq_max_r, 'y', linewidth=1)\naxs[0].plot(nfq_min_r, 'y', linewidth=1)\naxs[0].plot(nfq_mean_r, 'y', label='NFQ', linewidth=2)\naxs[0].fill_between(nfq_x, nfq_min_r, nfq_max_r, facecolor='y', alpha=0.3)\n\naxs[1].plot(nfq_max_s, 'y', linewidth=1)\naxs[1].plot(nfq_min_s, 'y', linewidth=1)\naxs[1].plot(nfq_mean_s, 'y', label='NFQ', linewidth=2)\naxs[1].fill_between(nfq_x, nfq_min_s, nfq_max_s, facecolor='y', alpha=0.3)\n\naxs[2].plot(nfq_max_t, 'y', linewidth=1)\naxs[2].plot(nfq_min_t, 'y', linewidth=1)\naxs[2].plot(nfq_mean_t, 'y', label='NFQ', linewidth=2)\naxs[2].fill_between(nfq_x, nfq_min_t, nfq_max_t, facecolor='y', alpha=0.3)\n\naxs[3].plot(nfq_max_sec, 'y', linewidth=1)\naxs[3].plot(nfq_min_sec, 'y', linewidth=1)\naxs[3].plot(nfq_mean_sec, 'y', label='NFQ', linewidth=2)\naxs[3].fill_between(nfq_x, nfq_min_sec, nfq_max_sec, facecolor='y', alpha=0.3)\n\naxs[4].plot(nfq_max_rt, 'y', linewidth=1)\naxs[4].plot(nfq_min_rt, 'y', linewidth=1)\naxs[4].plot(nfq_mean_rt, 'y', label='NFQ', linewidth=2)\naxs[4].fill_between(nfq_x, nfq_min_rt, nfq_max_rt, facecolor='y', alpha=0.3)\n\n# DQN\naxs[0].plot(dqn_max_r, 'b', linewidth=1)\naxs[0].plot(dqn_min_r, 'b', linewidth=1)\naxs[0].plot(dqn_mean_r, 'b--', label='DQN', linewidth=2)\naxs[0].fill_between(dqn_x, dqn_min_r, dqn_max_r, facecolor='b', alpha=0.3)\n\naxs[1].plot(dqn_max_s, 'b', linewidth=1)\naxs[1].plot(dqn_min_s, 'b', linewidth=1)\naxs[1].plot(dqn_mean_s, 'b--', label='DQN', linewidth=2)\naxs[1].fill_between(dqn_x, dqn_min_s, dqn_max_s, facecolor='b', alpha=0.3)\n\naxs[2].plot(dqn_max_t, 'b', linewidth=1)\naxs[2].plot(dqn_min_t, 'b', linewidth=1)\naxs[2].plot(dqn_mean_t, 'b--', label='DQN', linewidth=2)\naxs[2].fill_between(dqn_x, dqn_min_t, dqn_max_t, facecolor='b', alpha=0.3)\n\naxs[3].plot(dqn_max_sec, 'b', linewidth=1)\naxs[3].plot(dqn_min_sec, 'b', linewidth=1)\naxs[3].plot(dqn_mean_sec, 'b--', label='DQN', linewidth=2)\naxs[3].fill_between(dqn_x, dqn_min_sec, dqn_max_sec, facecolor='b', alpha=0.3)\n\naxs[4].plot(dqn_max_rt, 'b', linewidth=1)\naxs[4].plot(dqn_min_rt, 'b', linewidth=1)\naxs[4].plot(dqn_mean_rt, 'b--', label='DQN', linewidth=2)\naxs[4].fill_between(dqn_x, dqn_min_rt, dqn_max_rt, facecolor='b', alpha=0.3)\n\n# ALL\naxs[0].set_title('Moving Avg Reward (Training)')\naxs[1].set_title('Moving Avg Reward (Evaluation)')\naxs[2].set_title('Total Steps')\naxs[3].set_title('Training Time')\naxs[4].set_title('Wall-clock Time')\nplt.xlabel('Episodes')\naxs[0].legend(loc='upper left')\nplt.show()\n\n\n\n\n\ndqn_root_dir = os.path.join(RESULTS_DIR, 'dqn')\nnot os.path.exists(dqn_root_dir) and os.makedirs(dqn_root_dir)\n\nnp.save(os.path.join(dqn_root_dir, 'x'), dqn_x)\n\nnp.save(os.path.join(dqn_root_dir, 'max_r'), dqn_max_r)\nnp.save(os.path.join(dqn_root_dir, 'min_r'), dqn_min_r)\nnp.save(os.path.join(dqn_root_dir, 'mean_r'), dqn_mean_r)\n\nnp.save(os.path.join(dqn_root_dir, 'max_s'), dqn_max_s)\nnp.save(os.path.join(dqn_root_dir, 'min_s'), dqn_min_s )\nnp.save(os.path.join(dqn_root_dir, 'mean_s'), dqn_mean_s)\n\nnp.save(os.path.join(dqn_root_dir, 'max_t'), dqn_max_t)\nnp.save(os.path.join(dqn_root_dir, 'min_t'), dqn_min_t)\nnp.save(os.path.join(dqn_root_dir, 'mean_t'), dqn_mean_t)\n\nnp.save(os.path.join(dqn_root_dir, 'max_sec'), dqn_max_sec)\nnp.save(os.path.join(dqn_root_dir, 'min_sec'), dqn_min_sec)\nnp.save(os.path.join(dqn_root_dir, 'mean_sec'), dqn_mean_sec)\n\nnp.save(os.path.join(dqn_root_dir, 'max_rt'), dqn_max_rt)\nnp.save(os.path.join(dqn_root_dir, 'min_rt'), dqn_min_rt)\nnp.save(os.path.join(dqn_root_dir, 'mean_rt'), dqn_mean_rt)"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-9.html#double-dqn-ddqn",
    "href": "publication/GDRL/GDRL-chapter-9.html#double-dqn-ddqn",
    "title": "Chapter 9: More Stable Value-based Methods",
    "section": "Double DQN (DDQN)",
    "text": "Double DQN (DDQN)\n\npred = np.linspace(-100,100,500)\ntruth = np.zeros(pred.shape)\nerror = truth - pred\n\n\nse = 0.5*error**2\nae = np.abs(error)\nhe = lambda delta=1: ae - delta/2 if delta == 0 else \\\n              np.where(ae &lt;= np.repeat(delta, len(ae)), se, delta*(ae - delta/2))\n\n\nprint(np.mean(se))\nprint(torch.Tensor(error).pow(2).mul(0.5).mean())\n\n1673.3466933867735\ntensor(1673.3467)\n\n\n\nprint(np.mean(ae))\nprint(torch.Tensor(error).abs().mean())\n\n50.1002004008016\ntensor(50.1002)\n\n\n\nprint(np.mean(he(float('inf'))))\nprint(np.mean(he(0)))\n\n1673.3466933867735\n50.1002004008016\n\n\n\nplt.plot(pred, se)\nplt.title('Mean Squared Error (MSE/L2)')\nplt.show()\n\n\n\n\n\nplt.plot(pred, ae)\nplt.title('Mean Absolute Error (MAE/L1)')\nplt.show()\n\n\n\n\n\nplot1, = plt.plot(pred, he(30))\nplot2, = plt.plot(pred, he(10), ':')\nplt.title('Huber Loss')\nplt.legend([plot1,plot2],[\"Huber, δ=30\", \"Huber, δ=10\"])\nplt.show()\n\n\n\n\n\n# plot1, = plt.plot(pred, se, ':')\nplot1, = plt.plot(pred, he(float('inf')), ':')\nplot2, = plt.plot(pred, he(30), '--')\nplot3, = plt.plot(pred, he(10), '-.')\nplot4, = plt.plot(pred, he(0))\n# plot4, = plt.plot(pred, ae)\nplt.title('MAE, MSE and Huber Loss')\nplt.legend([plot1,plot2,plot3,plot4],[\"MSE/L2/Huber, δ=∞\", \"Huber, δ=30\", \"Huber, δ=10\", \"MAE/L1/Huber, δ=0\"])\nplt.show()\n\n\n\n\n\nclass DDQN():\n    def __init__(self, \n                 replay_buffer_fn, \n                 value_model_fn, \n                 value_optimizer_fn, \n                 value_optimizer_lr,\n                 max_gradient_norm,\n                 training_strategy_fn,\n                 evaluation_strategy_fn,\n                 n_warmup_batches,\n                 update_target_every_steps):\n        self.replay_buffer_fn = replay_buffer_fn\n        self.value_model_fn = value_model_fn\n        self.value_optimizer_fn = value_optimizer_fn\n        self.value_optimizer_lr = value_optimizer_lr\n        self.max_gradient_norm = max_gradient_norm\n        self.training_strategy_fn = training_strategy_fn\n        self.evaluation_strategy_fn = evaluation_strategy_fn\n        self.n_warmup_batches = n_warmup_batches\n        self.update_target_every_steps = update_target_every_steps\n\n    def optimize_model(self, experiences):\n        states, actions, rewards, next_states, is_terminals = experiences\n        batch_size = len(is_terminals)\n        \n        # argmax_a_q_sp = self.target_model(next_states).max(1)[1]\n        argmax_a_q_sp = self.online_model(next_states).max(1)[1]\n        q_sp = self.target_model(next_states).detach()\n        max_a_q_sp = q_sp[\n            np.arange(batch_size), argmax_a_q_sp].unsqueeze(1)\n        target_q_sa = rewards + (self.gamma * max_a_q_sp * (1 - is_terminals))\n        q_sa = self.online_model(states).gather(1, actions)\n\n        td_error = q_sa - target_q_sa\n        value_loss = td_error.pow(2).mul(0.5).mean()\n        self.value_optimizer.zero_grad()\n        value_loss.backward()        \n        torch.nn.utils.clip_grad_norm_(self.online_model.parameters(), \n                                       self.max_gradient_norm)\n        self.value_optimizer.step()\n\n    def interaction_step(self, state, env):\n        action = self.training_strategy.select_action(self.online_model, state)\n        new_state, reward, is_terminal, info = env.step(action)\n        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n        is_failure = is_terminal and not is_truncated\n        experience = (state, action, reward, new_state, float(is_failure))\n\n        self.replay_buffer.store(experience)\n        self.episode_reward[-1] += reward\n        self.episode_timestep[-1] += 1\n        self.episode_exploration[-1] += int(self.training_strategy.exploratory_action_taken)\n        return new_state, is_terminal\n    \n    def update_network(self):\n        for target, online in zip(self.target_model.parameters(), \n                                  self.online_model.parameters()):\n            target.data.copy_(online.data)\n\n    def train(self, make_env_fn, make_env_kargs, seed, gamma, \n              max_minutes, max_episodes, goal_mean_100_reward):\n        training_start, last_debug_time = time.time(), float('-inf')\n\n        self.checkpoint_dir = tempfile.mkdtemp()\n        self.make_env_fn = make_env_fn\n        self.make_env_kargs = make_env_kargs\n        self.seed = seed\n        self.gamma = gamma\n        \n        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n    \n        nS, nA = env.observation_space.shape[0], env.action_space.n\n        self.episode_timestep = []\n        self.episode_reward = []\n        self.episode_seconds = []\n        self.evaluation_scores = []        \n        self.episode_exploration = []\n        \n        self.target_model = self.value_model_fn(nS, nA)\n        self.online_model = self.value_model_fn(nS, nA)\n        self.update_network()\n\n        self.value_optimizer = self.value_optimizer_fn(self.online_model, \n                                                       self.value_optimizer_lr)\n\n        self.replay_buffer = self.replay_buffer_fn()\n        self.training_strategy = training_strategy_fn()\n        self.evaluation_strategy = evaluation_strategy_fn() \n                    \n        result = np.empty((max_episodes, 5))\n        result[:] = np.nan\n        training_time = 0\n        for episode in range(1, max_episodes + 1):\n            episode_start = time.time()\n            \n            state, is_terminal = env.reset(), False\n            self.episode_reward.append(0.0)\n            self.episode_timestep.append(0.0)\n            self.episode_exploration.append(0.0)\n\n            for step in count():\n                state, is_terminal = self.interaction_step(state, env)\n                \n                min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n                if len(self.replay_buffer) &gt; min_samples:\n                    experiences = self.replay_buffer.sample()\n                    experiences = self.online_model.load(experiences)\n                    self.optimize_model(experiences)\n                \n                if np.sum(self.episode_timestep) % self.update_target_every_steps == 0:\n                    self.update_network()\n\n                if is_terminal:\n                    gc.collect()\n                    break\n            \n            # stats\n            episode_elapsed = time.time() - episode_start\n            self.episode_seconds.append(episode_elapsed)\n            training_time += episode_elapsed\n            evaluation_score, _ = self.evaluate(self.online_model, env)\n            self.save_checkpoint(episode-1, self.online_model)\n            \n            total_step = int(np.sum(self.episode_timestep))\n            self.evaluation_scores.append(evaluation_score)\n            \n            mean_10_reward = np.mean(self.episode_reward[-10:])\n            std_10_reward = np.std(self.episode_reward[-10:])\n            mean_100_reward = np.mean(self.episode_reward[-100:])\n            std_100_reward = np.std(self.episode_reward[-100:])\n            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n            lst_100_exp_rat = np.array(\n                self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n            mean_100_exp_rat = np.mean(lst_100_exp_rat)\n            std_100_exp_rat = np.std(lst_100_exp_rat)\n            \n            wallclock_elapsed = time.time() - training_start\n            result[episode-1] = total_step, mean_100_reward, \\\n                mean_100_eval_score, training_time, wallclock_elapsed\n            \n            reached_debug_time = time.time() - last_debug_time &gt;= LEAVE_PRINT_EVERY_N_SECS\n            reached_max_minutes = wallclock_elapsed &gt;= max_minutes * 60\n            reached_max_episodes = episode &gt;= max_episodes\n            reached_goal_mean_reward = mean_100_eval_score &gt;= goal_mean_100_reward\n            training_is_over = reached_max_minutes or \\\n                               reached_max_episodes or \\\n                               reached_goal_mean_reward\n\n            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n            debug_message = 'el {}, ep {:04}, ts {:06}, '\n            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n            debug_message = debug_message.format(\n                elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n                mean_100_eval_score, std_100_eval_score)\n            print(debug_message, end='\\r', flush=True)\n            if reached_debug_time or training_is_over:\n                print(ERASE_LINE + debug_message, flush=True)\n                last_debug_time = time.time()\n            if training_is_over:\n                if reached_max_minutes: print(u'--&gt; reached_max_minutes \\u2715')\n                if reached_max_episodes: print(u'--&gt; reached_max_episodes \\u2715')\n                if reached_goal_mean_reward: print(u'--&gt; reached_goal_mean_reward \\u2713')\n                break\n                \n        final_eval_score, score_std = self.evaluate(self.online_model, env, n_episodes=100)\n        wallclock_time = time.time() - training_start\n        print('Training complete.')\n        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n              ' {:.2f}s wall-clock time.\\n'.format(\n                  final_eval_score, score_std, training_time, wallclock_time))\n        env.close() ; del env\n        self.get_cleaned_checkpoints()\n        return result, final_eval_score, training_time, wallclock_time\n    \n    def evaluate(self, eval_policy_model, eval_env, n_episodes=1):\n        rs = []\n        for _ in range(n_episodes):\n            s, d = eval_env.reset(), False\n            rs.append(0)\n            for _ in count():\n                a = self.evaluation_strategy.select_action(eval_policy_model, s)\n                s, r, d, _ = eval_env.step(a)\n                rs[-1] += r\n                if d: break\n        return np.mean(rs), np.std(rs)\n\n    def get_cleaned_checkpoints(self, n_checkpoints=5):\n        try: \n            return self.checkpoint_paths\n        except AttributeError:\n            self.checkpoint_paths = {}\n\n        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n        last_ep = max(paths_dic.keys())\n        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n\n        for idx, path in paths_dic.items():\n            if idx in checkpoint_idxs:\n                self.checkpoint_paths[idx] = path\n            else:\n                os.unlink(path)\n\n        return self.checkpoint_paths\n\n    def demo_last(self, title='Fully-trained {} Agent', n_episodes=3, max_n_videos=3):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        last_ep = max(checkpoint_paths.keys())\n        self.online_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n\n        self.evaluate(self.online_model, env, n_episodes=n_episodes)\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def demo_progression(self, title='{} Agent progression', max_n_videos=5):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        for i in sorted(checkpoint_paths.keys()):\n            self.online_model.load_state_dict(torch.load(checkpoint_paths[i]))\n            self.evaluate(self.online_model, env, n_episodes=1)\n\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            subtitle_eps=sorted(checkpoint_paths.keys()),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def save_checkpoint(self, episode_idx, model):\n        torch.save(model.state_dict(), \n                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))\n\n\nddqn_results = []\nddqn_agents, best_ddqn_agent_key, best_eval_score = {}, None, float('-inf')\nfor seed in SEEDS:\n    environment_settings = {\n        'env_name': 'CartPole-v1',\n        'gamma': 1.00,\n        'max_minutes': 20,\n        'max_episodes': 10000,\n        'goal_mean_100_reward': 475\n    }\n\n    value_model_fn = lambda nS, nA: FCQ(nS, nA, hidden_dims=(512,128))\n    value_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr=lr)\n    value_optimizer_lr = 0.0005\n    max_gradient_norm = float('inf')\n\n    training_strategy_fn = lambda: EGreedyExpStrategy(init_epsilon=1.0,  \n                                                      min_epsilon=0.3, \n                                                      decay_steps=20000)\n    evaluation_strategy_fn = lambda: GreedyStrategy()\n\n    replay_buffer_fn = lambda: ReplayBuffer(max_size=50000, batch_size=64)\n    n_warmup_batches = 5\n    update_target_every_steps = 10\n    \n    env_name, gamma, max_minutes, \\\n    max_episodes, goal_mean_100_reward = environment_settings.values()\n    agent = DDQN(replay_buffer_fn, \n                 value_model_fn, \n                 value_optimizer_fn, \n                 value_optimizer_lr,\n                 max_gradient_norm,\n                 training_strategy_fn,\n                 evaluation_strategy_fn,\n                 n_warmup_batches,\n                 update_target_every_steps)\n\n    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n    result, final_eval_score, training_time, wallclock_time = agent.train(\n        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n    ddqn_results.append(result)\n    ddqn_agents[seed] = agent\n    if final_eval_score &gt; best_eval_score:\n        best_eval_score = final_eval_score\n        best_ddqn_agent_key = seed\nddqn_results = np.array(ddqn_results)\n\nel 00:00:00, ep 0000, ts 000016, ar 10 016.0±000.0, 100 016.0±000.0, ex 100 0.4±0.0, ev 019.0±000.0\nel 00:01:00, ep 0144, ts 014010, ar 10 261.8±099.7, 100 127.3±107.8, ex 100 0.3±0.1, ev 307.9±117.8\nel 00:02:01, ep 0193, ts 029087, ar 10 361.6±157.5, 100 254.7±116.1, ex 100 0.2±0.1, ev 383.2±100.0\nel 00:03:02, ep 0229, ts 043657, ar 10 457.4±105.5, 100 338.5±128.9, ex 100 0.2±0.0, ev 431.9±090.2\nel 00:04:03, ep 0260, ts 057328, ar 10 480.5±039.5, 100 391.2±131.8, ex 100 0.2±0.0, ev 462.7±073.3\nel 00:04:26, ep 0272, ts 062544, ar 10 421.6±121.3, 100 405.1±132.6, ex 100 0.2±0.0, ev 476.1±060.2\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 236.34s training time, 281.77s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000034, ar 10 034.0±000.0, 100 034.0±000.0, ex 100 0.6±0.0, ev 008.0±000.0\nel 00:01:00, ep 0141, ts 014154, ar 10 237.8±082.6, 100 130.4±103.6, ex 100 0.3±0.1, ev 289.9±112.8\nel 00:02:01, ep 0201, ts 029248, ar 10 365.2±094.4, 100 239.8±093.4, ex 100 0.2±0.1, ev 347.4±109.3\nel 00:03:03, ep 0233, ts 043959, ar 10 429.6±141.5, 100 317.2±139.5, ex 100 0.2±0.0, ev 392.3±115.7\nel 00:04:03, ep 0264, ts 057512, ar 10 448.0±140.4, 100 386.8±143.5, ex 100 0.2±0.0, ev 452.9±092.4\nel 00:04:25, ep 0275, ts 062370, ar 10 435.8±137.2, 100 410.1±139.4, ex 100 0.2±0.0, ev 475.8±067.8\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 236.32s training time, 280.46s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000012, ar 10 012.0±000.0, 100 012.0±000.0, ex 100 0.6±0.0, ev 010.0±000.0\nel 00:01:01, ep 0152, ts 014310, ar 10 335.6±117.5, 100 129.8±111.8, ex 100 0.3±0.1, ev 271.8±098.0\nel 00:02:02, ep 0201, ts 029513, ar 10 322.0±089.6, 100 249.8±125.9, ex 100 0.2±0.1, ev 363.1±107.5\nel 00:03:02, ep 0233, ts 043967, ar 10 475.4±073.8, 100 351.5±117.4, ex 100 0.2±0.0, ev 427.4±090.7\nel 00:04:04, ep 0264, ts 057819, ar 10 453.7±102.4, 100 392.4±125.3, ex 100 0.2±0.0, ev 458.2±074.3\nel 00:04:38, ep 0290, ts 065214, ar 10 302.2±214.1, 100 391.6±158.9, ex 100 0.2±0.0, ev 476.8±067.6\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 247.09s training time, 295.15s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000012, ar 10 012.0±000.0, 100 012.0±000.0, ex 100 0.6±0.0, ev 012.0±000.0\nel 00:01:00, ep 0156, ts 014229, ar 10 235.7±077.3, 100 127.2±091.2, ex 100 0.3±0.1, ev 252.6±100.7\nel 00:02:01, ep 0196, ts 029542, ar 10 393.0±185.7, 100 256.4±150.6, ex 100 0.2±0.1, ev 368.9±122.4\nel 00:03:01, ep 0227, ts 044187, ar 10 475.2±074.4, 100 359.0±151.3, ex 100 0.2±0.0, ev 437.7±098.9\nel 00:03:40, ep 0246, ts 053294, ar 10 500.0±000.0, 100 414.2±130.2, ex 100 0.2±0.0, ev 476.5±062.5\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 197.22s training time, 236.05s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000044, ar 10 044.0±000.0, 100 044.0±000.0, ex 100 0.6±0.0, ev 009.0±000.0\nel 00:01:00, ep 0167, ts 014030, ar 10 220.7±060.6, 100 123.7±100.3, ex 100 0.3±0.1, ev 254.3±081.7\nel 00:02:01, ep 0217, ts 029624, ar 10 379.2±115.2, 100 249.9±130.2, ex 100 0.2±0.1, ev 328.4±116.2\nel 00:03:02, ep 0249, ts 044346, ar 10 462.3±113.1, 100 340.8±145.3, ex 100 0.2±0.0, ev 403.1±116.0\nel 00:04:04, ep 0280, ts 058662, ar 10 427.4±131.1, 100 412.4±134.3, ex 100 0.2±0.0, ev 467.7±076.9\nel 00:04:14, ep 0285, ts 060832, ar 10 441.4±118.4, 100 424.1±126.4, ex 100 0.2±0.0, ev 475.2±066.7\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 226.86s training time, 269.07s wall-clock time.\n\n\n\n\nddqn_agents[best_ddqn_agent_key].demo_progression()\n\nDDQN Agent progression\n        Episode 0\n        \n        Episode 68\n        \n        Episode 136\n        \n        Episode 204\n        \n        Episode 272\n        \n\n\n\nddqn_agents[best_ddqn_agent_key].demo_last()\n\nFully-trained DDQN Agent\n        Trial 0\n        \n        Trial 1\n        \n        Trial 2\n        \n\n\n\nddqn_max_t, ddqn_max_r, ddqn_max_s, ddqn_max_sec, ddqn_max_rt = np.max(ddqn_results, axis=0).T\nddqn_min_t, ddqn_min_r, ddqn_min_s, ddqn_min_sec, ddqn_min_rt = np.min(ddqn_results, axis=0).T\nddqn_mean_t, ddqn_mean_r, ddqn_mean_s, ddqn_mean_sec, ddqn_mean_rt = np.mean(ddqn_results, axis=0).T\nddqn_x = np.arange(np.max((len(ddqn_mean_s), len(dqn_mean_s))))\n\n\nfig, axs = plt.subplots(5, 1, figsize=(15,30), sharey=False, sharex=True)\n\n# DQN\naxs[0].plot(dqn_max_r, 'b', linewidth=1)\naxs[0].plot(dqn_min_r, 'b', linewidth=1)\naxs[0].plot(dqn_mean_r, 'b--', label='DQN', linewidth=2)\naxs[0].fill_between(dqn_x, dqn_min_r, dqn_max_r, facecolor='b', alpha=0.3)\n\naxs[1].plot(dqn_max_s, 'b', linewidth=1)\naxs[1].plot(dqn_min_s, 'b', linewidth=1)\naxs[1].plot(dqn_mean_s, 'b--', label='DQN', linewidth=2)\naxs[1].fill_between(dqn_x, dqn_min_s, dqn_max_s, facecolor='b', alpha=0.3)\n\naxs[2].plot(dqn_max_t, 'b', linewidth=1)\naxs[2].plot(dqn_min_t, 'b', linewidth=1)\naxs[2].plot(dqn_mean_t, 'b--', label='DQN', linewidth=2)\naxs[2].fill_between(dqn_x, dqn_min_t, dqn_max_t, facecolor='b', alpha=0.3)\n\naxs[3].plot(dqn_max_sec, 'b', linewidth=1)\naxs[3].plot(dqn_min_sec, 'b', linewidth=1)\naxs[3].plot(dqn_mean_sec, 'b--', label='DQN', linewidth=2)\naxs[3].fill_between(dqn_x, dqn_min_sec, dqn_max_sec, facecolor='b', alpha=0.3)\n\naxs[4].plot(dqn_max_rt, 'b', linewidth=1)\naxs[4].plot(dqn_min_rt, 'b', linewidth=1)\naxs[4].plot(dqn_mean_rt, 'b--', label='DQN', linewidth=2)\naxs[4].fill_between(dqn_x, dqn_min_rt, dqn_max_rt, facecolor='b', alpha=0.3)\n\n# DDQN\naxs[0].plot(ddqn_max_r, 'g', linewidth=1)\naxs[0].plot(ddqn_min_r, 'g', linewidth=1)\naxs[0].plot(ddqn_mean_r, 'g-.', label='DDQN', linewidth=2)\naxs[0].fill_between(ddqn_x, ddqn_min_r, ddqn_max_r, facecolor='g', alpha=0.3)\n\naxs[1].plot(ddqn_max_s, 'g', linewidth=1)\naxs[1].plot(ddqn_min_s, 'g', linewidth=1)\naxs[1].plot(ddqn_mean_s, 'g-.', label='DDQN', linewidth=2)\naxs[1].fill_between(ddqn_x, ddqn_min_s, ddqn_max_s, facecolor='g', alpha=0.3)\n\naxs[2].plot(ddqn_max_t, 'g', linewidth=1)\naxs[2].plot(ddqn_min_t, 'g', linewidth=1)\naxs[2].plot(ddqn_mean_t, 'g-.', label='DDQN', linewidth=2)\naxs[2].fill_between(ddqn_x, ddqn_min_t, ddqn_max_t, facecolor='g', alpha=0.3)\n\naxs[3].plot(ddqn_max_sec, 'g', linewidth=1)\naxs[3].plot(ddqn_min_sec, 'g', linewidth=1)\naxs[3].plot(ddqn_mean_sec, 'g-.', label='DDQN', linewidth=2)\naxs[3].fill_between(ddqn_x, ddqn_min_sec, ddqn_max_sec, facecolor='g', alpha=0.3)\n\naxs[4].plot(ddqn_max_rt, 'g', linewidth=1)\naxs[4].plot(ddqn_min_rt, 'g', linewidth=1)\naxs[4].plot(ddqn_mean_rt, 'g-.', label='DDQN', linewidth=2)\naxs[4].fill_between(ddqn_x, ddqn_min_rt, ddqn_max_rt, facecolor='g', alpha=0.3)\n\n# ALL\naxs[0].set_title('Moving Avg Reward (Training)')\naxs[1].set_title('Moving Avg Reward (Evaluation)')\naxs[2].set_title('Total Steps')\naxs[3].set_title('Training Time')\naxs[4].set_title('Wall-clock Time')\nplt.xlabel('Episodes')\naxs[0].legend(loc='upper left')\nplt.show()\n\n\n\n\n\nddqn_root_dir = os.path.join(RESULTS_DIR, 'ddqn')\nnot os.path.exists(ddqn_root_dir) and os.makedirs(ddqn_root_dir)\n\nnp.save(os.path.join(ddqn_root_dir, 'x'), ddqn_x)\n\nnp.save(os.path.join(ddqn_root_dir, 'max_r'), ddqn_max_r)\nnp.save(os.path.join(ddqn_root_dir, 'min_r'), ddqn_min_r)\nnp.save(os.path.join(ddqn_root_dir, 'mean_r'), ddqn_mean_r)\n\nnp.save(os.path.join(ddqn_root_dir, 'max_s'), ddqn_max_s)\nnp.save(os.path.join(ddqn_root_dir, 'min_s'), ddqn_min_s )\nnp.save(os.path.join(ddqn_root_dir, 'mean_s'), ddqn_mean_s)\n\nnp.save(os.path.join(ddqn_root_dir, 'max_t'), ddqn_max_t)\nnp.save(os.path.join(ddqn_root_dir, 'min_t'), ddqn_min_t)\nnp.save(os.path.join(ddqn_root_dir, 'mean_t'), ddqn_mean_t)\n\nnp.save(os.path.join(ddqn_root_dir, 'max_sec'), ddqn_max_sec)\nnp.save(os.path.join(ddqn_root_dir, 'min_sec'), ddqn_min_sec)\nnp.save(os.path.join(ddqn_root_dir, 'mean_sec'), ddqn_mean_sec)\n\nnp.save(os.path.join(ddqn_root_dir, 'max_rt'), ddqn_max_rt)\nnp.save(os.path.join(ddqn_root_dir, 'min_rt'), ddqn_min_rt)\nnp.save(os.path.join(ddqn_root_dir, 'mean_rt'), ddqn_mean_rt)\n\n\nenv = make_env_fn(**make_env_kargs, seed=123, monitor_mode='evaluation')\nstate = env.reset()\nenv.close()\ndel env\nprint(state)\n\n[ 0.02078762 -0.01301236 -0.0209893  -0.03935255]\n\n\n\nq_values = ddqn_agents[best_ddqn_agent_key].online_model(state).detach().cpu().numpy()[0]\nprint(q_values)\n\n[2134438.2 2138762.8]\n\n\n\nq_s = q_values\nv_s = q_values.mean()\na_s = q_values - q_values.mean()\n\n\nplt.bar(('Left (idx=0)','Right (idx=1)'), q_s)\nplt.xlabel('Action')\nplt.ylabel('Estimate')\nplt.title(\"Action-value function, Q(\" + str(np.round(state,2)) + \")\")\nplt.show()\n\n\n\n\n\nplt.bar('s='+str(np.round(state,2)), v_s, width=0.1)\nplt.xlabel('State')\nplt.ylabel('Estimate')\nplt.title(\"State-value function, V(\"+str(np.round(state,2))+\")\")\nplt.show()\n\n\n\n\n\nplt.bar(('Left (idx=0)','Right (idx=1)'), a_s)\nplt.xlabel('Action')\nplt.ylabel('Estimate')\nplt.title(\"Advantage function, (\" + str(np.round(state,2)) + \")\")\nplt.show()\n\n\n\n\n\nenv = make_env_fn(**make_env_kargs, seed=123, monitor_mode='evaluation')\n\nstates = []\nfor agent in ddqn_agents.values():\n    for episode in range(100):\n        state, done = env.reset(), False\n        while not done:\n            states.append(state)\n            action = agent.evaluation_strategy.select_action(agent.online_model, state)\n            state, _, done, _ = env.step(action)\nenv.close()\ndel env\n\nx = np.array(states)[:,0]\nxd = np.array(states)[:,1]\na = np.array(states)[:,2]\nad = np.array(states)[:,3]\n\n\nparts = plt.violinplot((x, xd, a, ad), \n                       vert=False, showmeans=False, showmedians=False, showextrema=False)\n\ncolors = ['red','green','yellow','blue']\nfor i, pc in enumerate(parts['bodies']):\n    pc.set_facecolor(colors[i])\n    pc.set_edgecolor(colors[i])\n    pc.set_alpha(0.5)\n\nplt.yticks(range(1,5), [\"cart position\", \"cart velocity\", \"pole angle\", \"pole velocity\"])\nplt.yticks(rotation=45)\nplt.title('Range of state-variable values for ' + str(\n    ddqn_agents[best_ddqn_agent_key].__class__.__name__))\n\nplt.show()"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-7.html#tldr",
    "href": "publication/GDRL/GDRL-chapter-7.html#tldr",
    "title": "Chapter 7: Archieving goals more effectively and efficiently",
    "section": "TL;DR",
    "text": "TL;DR\n\n그로킹 심층 강화학습 중 7장 내용인 “조금 더 효율적인 방법으로 목표에 도달하기”에 대한 내용입니다.\n\n\nNote: 실행을 위해 아래의 패키지들을 설치해주기 바랍니다.\n\n\n\nCode\n!pip install tqdm numpy scikit-learn pyglet setuptools && \\\n!pip install gym asciinema pandas tabulate tornado==5.* PyBullet && \\\n!pip install git+https://github.com/pybox2d/pybox2d#egg=Box2D && \\\n!pip install git+https://github.com/mimoralea/gym-bandits#egg=gym-bandits && \\\n!pip install git+https://github.com/mimoralea/gym-walk#egg=gym-walk && \\\n!pip install git+https://github.com/mimoralea/gym-aima#egg=gym-aima && \\\n!pip install gym[atari]"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-7.html#실행에-필요한-helper-function",
    "href": "publication/GDRL/GDRL-chapter-7.html#실행에-필요한-helper-function",
    "title": "Chapter 7: Archieving goals more effectively and efficiently",
    "section": "실행에 필요한 helper function",
    "text": "실행에 필요한 helper function\n\ndef value_iteration(P, gamma=1.0, theta=1e-10):\n    V = np.zeros(len(P), dtype=np.float64)\n    while True:\n        Q = np.zeros((len(P), len(P[0])), dtype=np.float64)\n        for s in range(len(P)):\n            for a in range(len(P[s])):\n                for prob, next_state, reward, done in P[s][a]:\n                    Q[s][a] += prob * (reward + gamma * V[next_state] * (not done))\n        if np.max(np.abs(V - np.max(Q, axis=1))) &lt; theta:\n            break\n        V = np.max(Q, axis=1)\n    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n    return Q, V, pi\n\n\ndef print_policy(pi, P, action_symbols=('&lt;', 'v', '&gt;', '^'), n_cols=4, title='정책:'):\n    print(title)\n    arrs = {k:v for k,v in enumerate(action_symbols)}\n    for s in range(len(P)):\n        a = pi(s)\n        print(\"| \", end=\"\")\n        if np.all([done for action in P[s].values() for _, _, _, done in action]):\n            print(\"\".rjust(9), end=\" \")\n        else:\n            print(str(s).zfill(2), arrs[a].rjust(6), end=\" \")\n        if (s + 1) % n_cols == 0: print(\"|\")\n\n\ndef print_state_value_function(V, P, n_cols=4, prec=3, title='상태-가치 함수:'):\n    print(title)\n    for s in range(len(P)):\n        v = V[s]\n        print(\"| \", end=\"\")\n        if np.all([done for action in P[s].values() for _, _, _, done in action]):\n            print(\"\".rjust(9), end=\" \")\n        else:\n            print(str(s).zfill(2), '{}'.format(np.round(v, prec)).rjust(6), end=\" \")\n        if (s + 1) % n_cols == 0: print(\"|\")\n\n\ndef print_action_value_function(Q, \n                                optimal_Q=None, \n                                action_symbols=('&lt;', '&gt;'), \n                                prec=3, \n                                title='행동-가치 함수:'):\n    vf_types=('',) if optimal_Q is None else ('', '*', 'er')\n    headers = ['s',] + [' '.join(i) for i in list(itertools.product(vf_types, action_symbols))]\n    print(title)\n    states = np.arange(len(Q))[..., np.newaxis]\n    arr = np.hstack((states, np.round(Q, prec)))\n    if not (optimal_Q is None):\n        arr = np.hstack((arr, np.round(optimal_Q, prec), np.round(optimal_Q-Q, prec)))\n    print(tabulate(arr, headers, tablefmt=\"fancy_grid\"))\n\n\ndef get_policy_metrics(env, gamma, pi, goal_state, optimal_Q, \n                       n_episodes=100, max_steps=200):\n    random.seed(123); np.random.seed(123) ; env.seed(123)\n    reached_goal, episode_reward, episode_regret = [], [], []\n    for _ in range(n_episodes):\n        state, done, steps = env.reset(), False, 0\n        episode_reward.append(0.0)\n        episode_regret.append(0.0)\n        while not done and steps &lt; max_steps:\n            action = pi(state)\n            regret = np.max(optimal_Q[state]) - optimal_Q[state][action]\n            episode_regret[-1] += regret\n            \n            state, reward, done, _ = env.step(action)\n            episode_reward[-1] += (gamma**steps * reward)\n            \n            steps += 1\n\n        reached_goal.append(state == goal_state)\n    results = np.array((np.sum(reached_goal)/len(reached_goal)*100, \n                        np.mean(episode_reward), \n                        np.mean(episode_regret)))\n    return results\n\n\ndef get_metrics_from_tracks(env, gamma, goal_state, optimal_Q, pi_track, coverage=0.1):\n    total_samples = len(pi_track)\n    n_samples = int(total_samples * coverage)\n    samples_e = np.linspace(0, total_samples, n_samples, endpoint=True, dtype=np.int)\n    metrics = []\n    for e, pi in enumerate(tqdm(pi_track)):\n        if e in samples_e:\n            metrics.append(get_policy_metrics(\n                env, \n                gamma=gamma, \n                pi=lambda s: pi[s], \n                goal_state=goal_state, \n                optimal_Q=optimal_Q))\n        else:\n            metrics.append(metrics[-1])\n    metrics = np.array(metrics)\n    success_rate_ma, mean_return_ma, mean_regret_ma = np.apply_along_axis(moving_average, axis=0, arr=metrics).T\n    return success_rate_ma, mean_return_ma, mean_regret_ma\n\n\ndef rmse(x, y, dp=4):\n    return np.round(np.sqrt(np.mean((x - y)**2)), dp)\n\n\ndef moving_average(a, n=100) :\n    ret = np.cumsum(a, dtype=float)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1:] / n\n\n\ndef plot_value_function(title, V_track, V_true=None, log=False, limit_value=0.05, limit_items=5):\n    np.random.seed(123)\n    per_col = 25\n    linecycler = cycle([\"-\",\"--\",\":\",\"-.\"])\n    legends = []\n\n    valid_values = np.argwhere(V_track[-1] &gt; limit_value).squeeze()\n    items_idxs = np.random.choice(valid_values, \n                                  min(len(valid_values), limit_items), \n                                  replace=False)\n    # 첫번째 참값을 뽑아냅니다.\n    if V_true is not None:\n        for i, state in enumerate(V_track.T):\n            if i not in items_idxs:\n                continue\n            if state[-1] &lt; limit_value:\n                continue\n\n            label = 'v*({})'.format(i)\n            plt.axhline(y=V_true[i], color='k', linestyle='-', linewidth=1)\n            plt.text(int(len(V_track)*1.02), V_true[i]+.01, label)\n\n    # 이에 대한 추정치를 계산합니다.\n    for i, state in enumerate(V_track.T):\n        if i not in items_idxs:\n            continue\n        if state[-1] &lt; limit_value:\n            continue\n        line_type = next(linecycler)\n        label = 'V({})'.format(i)\n        p, = plt.plot(state, line_type, label=label, linewidth=3)\n        legends.append(p)\n        \n    legends.reverse()\n\n    ls = []\n    for loc, idx in enumerate(range(0, len(legends), per_col)):\n        subset = legends[idx:idx+per_col]\n        l = plt.legend(subset, [p.get_label() for p in subset], \n                       loc='center right', bbox_to_anchor=(1.25, 0.5))\n        ls.append(l)\n    [plt.gca().add_artist(l) for l in ls[:-1]]\n    if log: plt.xscale('log')\n    plt.title(title)\n    plt.ylabel('State-value function')\n    plt.xlabel('Episodes (log scale)' if log else 'Episodes')\n    plt.show()\n\n\ndef plot_transition_model(T_track, episode = 0):\n    fig = plt.figure(figsize=(20,10))\n    ax = fig.add_subplot(111, projection='3d')\n    ax.view_init(elev=20, azim=50)\n\n    color_left = '#008fd5' # ax._get_lines.get_next_color()\n    color_right = '#fc4f30' #ax._get_lines.get_next_color()\n\n    left_prob = np.divide(T_track[episode][:,0].T, \n                          T_track[episode][:,0].sum(axis=1).T).T\n    left_prob = np.nan_to_num(left_prob, 0)\n\n    right_prob = np.divide(T_track[episode][:,1].T, \n                           T_track[episode][:,1].sum(axis=1).T).T\n    right_prob = np.nan_to_num(right_prob, 0)\n\n    for s in np.arange(9):\n        ax.bar3d(s+0.1, np.arange(9)+0.1, np.zeros(9),\n                 np.zeros(9)+0.3,\n                 np.zeros(9)+0.3, \n                 left_prob[s], \n                 color=color_left, \n                 alpha=0.75,\n                 shade=True)\n        ax.bar3d(s+0.1, np.arange(9)+0.1, left_prob[s],\n                 np.zeros(9)+0.3,\n                 np.zeros(9)+0.3, \n                 right_prob[s], \n                 color=color_right, \n                 alpha=0.75,\n                 shade=True)\n\n    ax.tick_params(axis='x', which='major', pad=10)\n    ax.tick_params(axis='y', which='major', pad=10)\n    ax.tick_params(axis='z', which='major', pad=10)\n\n    ax.xaxis.set_rotate_label(False)\n    ax.yaxis.set_rotate_label(False)\n    ax.zaxis.set_rotate_label(False)\n    ax.set_xticks(np.arange(9))\n    ax.set_yticks(np.arange(9))\n\n    plt.title('SWS learned MDP after {} episodes'.format(episode+1))\n    ax.set_xlabel('Initial\\nstate', labelpad=75, rotation=0)\n    ax.set_ylabel('Landing\\nstate', labelpad=75, rotation=0)\n    ax.set_zlabel('Transition\\nprobabilities', labelpad=75, rotation=0)\n\n    left_proxy = plt.Rectangle((0, 0), 1, 1, fc=color_left)\n    right_proxy = plt.Rectangle((0, 0), 1, 1, fc=color_right)\n\n    plt.legend((left_proxy, right_proxy), \n               ('Left', 'Right'), \n               bbox_to_anchor=(0.15, 0.9), \n               borderaxespad=0.)\n\n    ax.dist = 12\n    #plt.gcf().subplots_adjust(left=0.1, right=0.9)\n    plt.tight_layout()\n\n    plt.show()\n\n\ndef plot_model_state_sampling(planning, algo='Dyna-Q'):\n    fig = plt.figure(figsize=(20,10))\n\n    color_left = '#008fd5' # ax._get_lines.get_next_color()\n    color_right = '#fc4f30' #ax._get_lines.get_next_color()\n\n    for s in np.arange(9):\n        actions = planning[np.where(planning[:,0]==s)[0], 1]\n        left = len(actions[actions == 0])\n        right = len(actions[actions == 1])\n        plt.bar(s, right, 0.2, color=color_right)\n        plt.bar(s, left, 0.2, color=color_left, bottom=right)\n\n\n    plt.title('States samples from {}\\nlearned model of SWS environment'.format(algo))\n    plt.xticks(range(9))\n    plt.xlabel('Initial states sampled', labelpad=20)\n    plt.ylabel('Count', labelpad=50, rotation=0)\n\n    left_proxy = plt.Rectangle((0, 0), 1, 1, fc=color_left)\n    right_proxy = plt.Rectangle((0, 0), 1, 1, fc=color_right)\n\n    plt.legend((left_proxy, right_proxy), \n               ('Left', 'Right'), \n               bbox_to_anchor=(0.99, 1.1), \n               borderaxespad=0.)\n\n    #plt.gcf().subplots_adjust(left=0.1, right=0.9)\n    plt.tight_layout()\n\n    plt.show()\n\n\ndef plot_model_state_7(planning, algo='Dyna-Q'):\n    fig = plt.figure(figsize=(20,10))\n\n    color_left = '#008fd5' # ax._get_lines.get_next_color()\n    color_right = '#fc4f30' #ax._get_lines.get_next_color()\n\n\n    state_7 = planning[np.where(planning[:,0]==7)]\n    for sp in [6, 7, 8]:\n\n        actions = state_7[np.where(state_7[:,3]==sp)[0], 1]\n        left = len(actions[actions == 0])\n        right = len(actions[actions == 1])\n        plt.bar(sp, right, 0.2, color=color_right)\n        plt.bar(sp, left, 0.2, color=color_left, bottom=right)\n\n\n    plt.title('Next states samples by {}\\nin SWS environment from state 7'.format(algo))\n    plt.xticks([6,7,8])\n    plt.xlabel('Landing states', labelpad=20)\n    plt.ylabel('Count', labelpad=50, rotation=0)\n\n    left_proxy = plt.Rectangle((0, 0), 1, 1, fc=color_left)\n    right_proxy = plt.Rectangle((0, 0), 1, 1, fc=color_right)\n\n    plt.legend((left_proxy, right_proxy), \n               ('Left', 'Right'), \n               bbox_to_anchor=(0.99, 1.1), \n               borderaxespad=0.)\n\n    #plt.gcf().subplots_adjust(left=0.1, right=0.9)\n    plt.tight_layout()\n\n    plt.show()\n\n\ndef decay_schedule(init_value, min_value, decay_ratio, max_steps, log_start=-2, log_base=10):\n    decay_steps = int(max_steps * decay_ratio)\n    rem_steps = max_steps - decay_steps\n    values = np.logspace(log_start, 0, decay_steps, base=log_base, endpoint=True)[::-1]\n    values = (values - values.min()) / (values.max() - values.min())\n    values = (init_value - min_value) * values + min_value\n    values = np.pad(values, (0, rem_steps), 'edge')\n    return values"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-7.html#미끄러지는-7개의-통로",
    "href": "publication/GDRL/GDRL-chapter-7.html#미끄러지는-7개의-통로",
    "title": "Chapter 7: Archieving goals more effectively and efficiently",
    "section": "미끄러지는 7개의 통로",
    "text": "미끄러지는 7개의 통로\n\nenv = gym.make('SlipperyWalkSeven-v0')\ninit_state = env.reset()\ngoal_state = 8\ngamma = 0.99\nn_episodes = 3000\nP = env.env.P\nn_cols, svf_prec, err_prec, avf_prec=9, 4, 2, 3\naction_symbols=('&lt;', '&gt;')\nlimit_items, limit_value = 5, 0.0\ncu_limit_items, cu_limit_value, cu_episodes = 10, 0.0, 100\n\n\n알파와 입실론 스케쥴링\n\nplt.plot(decay_schedule(0.5, 0.01, 0.5, n_episodes), \n         '-', linewidth=2, \n         label='Alpha schedule')\nplt.plot(decay_schedule(1.0, 0.1, 0.9, n_episodes), \n         ':', linewidth=2, \n         label='Epsilon schedule')\nplt.legend(loc=1, ncol=1)\n\nplt.title('Alpha and epsilon schedules')\nplt.xlabel('Episodes')\nplt.ylabel('Hyperparameter values')\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\n\n이상적인 가치 함수와 정책\n\noptimal_Q, optimal_V, optimal_pi = value_iteration(P, gamma=gamma)\nprint_state_value_function(optimal_V, P, n_cols=n_cols, prec=svf_prec, title='Optimal state-value function:')\nprint()\n\nprint_action_value_function(optimal_Q, \n                            None, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Optimal action-value function:')\nprint()\nprint_policy(optimal_pi, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_op, mean_return_op, mean_regret_op = get_policy_metrics(\n    env, gamma=gamma, pi=optimal_pi, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_op, mean_return_op, mean_regret_op))\n\nOptimal state-value function:\n|           | 01 0.5637 | 02  0.763 | 03 0.8449 | 04 0.8892 | 05  0.922 | 06 0.9515 | 07 0.9806 |           |\n\nOptimal action-value function:\n╒═════╤═══════╤═══════╕\n│   s │     &lt; │     &gt; │\n╞═════╪═══════╪═══════╡\n│   0 │ 0     │ 0     │\n├─────┼───────┼───────┤\n│   1 │ 0.312 │ 0.564 │\n├─────┼───────┼───────┤\n│   2 │ 0.67  │ 0.763 │\n├─────┼───────┼───────┤\n│   3 │ 0.803 │ 0.845 │\n├─────┼───────┼───────┤\n│   4 │ 0.864 │ 0.889 │\n├─────┼───────┼───────┤\n│   5 │ 0.901 │ 0.922 │\n├─────┼───────┼───────┤\n│   6 │ 0.932 │ 0.952 │\n├─────┼───────┼───────┤\n│   7 │ 0.961 │ 0.981 │\n├─────┼───────┼───────┤\n│   8 │ 0     │ 0     │\n╘═════╧═══════╧═══════╛\n\n정책:\n|           | 01      &gt; | 02      &gt; | 03      &gt; | 04      &gt; | 05      &gt; | 06      &gt; | 07      &gt; |           |\nReaches goal 96.00%. Obtains an average return of 0.8548. Regret of 0.0000\n\n\n\n\nSARSA(\\(\\lambda\\))\n\ndef sarsa_lambda(env,\n                 gamma=1.0,\n                 init_alpha=0.5,\n                 min_alpha=0.01,\n                 alpha_decay_ratio=0.5,\n                 init_epsilon=1.0,\n                 min_epsilon=0.1,\n                 epsilon_decay_ratio=0.9,\n                 lambda_=0.5,\n                 replacing_traces=True,\n                 n_episodes=3000):\n    nS, nA = env.observation_space.n, env.action_space.n\n    pi_track = []\n    Q = np.zeros((nS, nA), dtype=np.float64)\n\n    Q_track = np.zeros((n_episodes, nS, nA), \n                       dtype=np.float64)\n    E = np.zeros((nS, nA), dtype=np.float64)\n    select_action = lambda state, Q, epsilon: \\\n        np.argmax(Q[state]) \\\n        if np.random.random() &gt; epsilon \\\n        else np.random.randint(len(Q[state]))\n    alphas = decay_schedule(\n        init_alpha, min_alpha, \n        alpha_decay_ratio, n_episodes)\n    epsilons = decay_schedule(\n        init_epsilon, min_epsilon, \n        epsilon_decay_ratio, n_episodes)\n    \n    for e in tqdm(range(n_episodes), leave=False):\n        E.fill(0)\n\n        state, done = env.reset(), False\n        action = select_action(state, Q, epsilons[e])\n        while not done:\n            next_state, reward, done, _ = env.step(action)\n            next_action = select_action(next_state, Q, epsilons[e])\n\n            td_target = reward + gamma * Q[next_state][next_action] * (not done)\n            td_error = td_target - Q[state][action]\n            if replacing_traces: E[state].fill(0)\n            E[state][action] = E[state][action] + 1\n            if replacing_traces: E.clip(0, 1, out=E)\n            Q = Q + alphas[e] * td_error * E\n            E = gamma * lambda_ * E\n            \n            state, action = next_state, next_action\n\n        Q_track[e] = Q\n        pi_track.append(np.argmax(Q, axis=1))\n\n    V = np.max(Q, axis=1)\n    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n    return Q, V, pi, Q_track, pi_track\n\n\nQ_rsls, V_rsls, Q_track_rsls = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_rsl, V_rsl, pi_rsl, Q_track_rsl, pi_track_rsl = sarsa_lambda(env, gamma=gamma, n_episodes=n_episodes)\n    Q_rsls.append(Q_rsl) ; V_rsls.append(V_rsl) ; Q_track_rsls.append(Q_track_rsl)\nQ_rsl, V_rsl, Q_track_rsl = np.mean(Q_rsls, axis=0), np.mean(V_rsls, axis=0), np.mean(Q_track_rsls, axis=0)\ndel Q_rsls ; del V_rsls ; del Q_track_rsls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_rsl, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Sarsa(λ) replacing:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_rsl - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_rsl, optimal_V)))\nprint()\nprint_action_value_function(Q_rsl, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Sarsa(λ) replacing action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_rsl, optimal_Q)))\nprint()\nprint_policy(pi_rsl, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_rsl, mean_return_rsl, mean_regret_rsl = get_policy_metrics(\n    env, gamma=gamma, pi=pi_rsl, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_rsl, mean_return_rsl, mean_regret_rsl))\n\nState-value function found by Sarsa(λ) replacing:\n|           | 01 0.4672 | 02 0.6985 | 03 0.8056 | 04 0.8656 | 05 0.9102 | 06 0.9436 | 07 0.9773 |           |\nOptimal state-value function:\n|           | 01 0.5637 | 02  0.763 | 03 0.8449 | 04 0.8892 | 05  0.922 | 06 0.9515 | 07 0.9806 |           |\nState-value function errors:\n|           | 01   -0.1 | 02  -0.06 | 03  -0.04 | 04  -0.02 | 05  -0.01 | 06  -0.01 | 07   -0.0 |           |\nState-value function RMSE: 0.0419\n\nSarsa(λ) replacing action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╕\n│   s │     &lt; │     &gt; │   * &lt; │   * &gt; │   er &lt; │   er &gt; │\n╞═════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╡\n│   0 │ 0     │ 0     │ 0     │ 0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   1 │ 0.195 │ 0.467 │ 0.312 │ 0.564 │  0.117 │  0.097 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   2 │ 0.512 │ 0.698 │ 0.67  │ 0.763 │  0.158 │  0.065 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   3 │ 0.713 │ 0.806 │ 0.803 │ 0.845 │  0.091 │  0.039 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   4 │ 0.825 │ 0.866 │ 0.864 │ 0.889 │  0.038 │  0.024 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   5 │ 0.878 │ 0.91  │ 0.901 │ 0.922 │  0.023 │  0.012 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   6 │ 0.921 │ 0.944 │ 0.932 │ 0.952 │  0.012 │  0.008 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   7 │ 0.954 │ 0.977 │ 0.961 │ 0.981 │  0.008 │  0.003 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   8 │ 0     │ 0     │ 0     │ 0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╛\nAction-value function RMSE: 0.06\n\n정책:\n|           | 01      &gt; | 02      &gt; | 03      &gt; | 04      &gt; | 05      &gt; | 06      &gt; | 07      &gt; |           |\nReaches goal 96.00%. Obtains an average return of 0.8548. Regret of 0.0000\n\n\n\nQ_asls, V_asls, Q_track_asls = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_asl, V_asl, pi_asl, Q_track_asl, pi_track_asl = sarsa_lambda(env, gamma=gamma, \n                                                                   replacing_traces=False, \n                                                                   n_episodes=n_episodes)\n    Q_asls.append(Q_asl) ; V_asls.append(V_asl) ; Q_track_asls.append(Q_track_asl)\nQ_asl, V_asl, Q_track_asl = np.mean(Q_asls, axis=0), np.mean(V_asls, axis=0), np.mean(Q_track_asls, axis=0)\ndel Q_asls ; del V_asls ; del Q_track_asls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_asl, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Sarsa(λ) accumulating:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_asl - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_asl, optimal_V)))\nprint()\nprint_action_value_function(Q_asl, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Sarsa(λ) accumulating action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_asl, optimal_Q)))\nprint()\nprint_policy(pi_asl, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_asl, mean_return_asl, mean_regret_asl = get_policy_metrics(\n    env, gamma=gamma, pi=pi_asl, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_asl, mean_return_asl, mean_regret_asl))\n\nState-value function found by Sarsa(λ) accumulating:\n|           | 01 0.4814 | 02 0.7085 | 03 0.8168 | 04 0.8683 | 05 0.9082 | 06 0.9443 | 07 0.9783 |           |\nOptimal state-value function:\n|           | 01 0.5637 | 02  0.763 | 03 0.8449 | 04 0.8892 | 05  0.922 | 06 0.9515 | 07 0.9806 |           |\nState-value function errors:\n|           | 01  -0.08 | 02  -0.05 | 03  -0.03 | 04  -0.02 | 05  -0.01 | 06  -0.01 | 07   -0.0 |           |\nState-value function RMSE: 0.0353\n\nSarsa(λ) accumulating action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╕\n│   s │     &lt; │     &gt; │   * &lt; │   * &gt; │   er &lt; │   er &gt; │\n╞═════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╡\n│   0 │ 0     │ 0     │ 0     │ 0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   1 │ 0.207 │ 0.481 │ 0.312 │ 0.564 │  0.105 │  0.082 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   2 │ 0.539 │ 0.709 │ 0.67  │ 0.763 │  0.131 │  0.055 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   3 │ 0.727 │ 0.817 │ 0.803 │ 0.845 │  0.077 │  0.028 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   4 │ 0.829 │ 0.868 │ 0.864 │ 0.889 │  0.035 │  0.021 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   5 │ 0.881 │ 0.908 │ 0.901 │ 0.922 │  0.02  │  0.014 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   6 │ 0.918 │ 0.944 │ 0.932 │ 0.952 │  0.014 │  0.007 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   7 │ 0.954 │ 0.978 │ 0.961 │ 0.981 │  0.007 │  0.002 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   8 │ 0     │ 0     │ 0     │ 0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╛\nAction-value function RMSE: 0.0511\n\n정책:\n|           | 01      &gt; | 02      &gt; | 03      &gt; | 04      &gt; | 05      &gt; | 06      &gt; | 07      &gt; |           |\nReaches goal 96.00%. Obtains an average return of 0.8548. Regret of 0.0000\n\n\n\n\n왓킨스의 Q(\\(\\lambda\\))\n\ndef q_lambda(env,\n             gamma=1.0,\n             init_alpha=0.5,\n             min_alpha=0.01,\n             alpha_decay_ratio=0.5,\n             init_epsilon=1.0,\n             min_epsilon=0.1,\n             epsilon_decay_ratio=0.9,\n             lambda_=0.5,\n             replacing_traces=True,\n             n_episodes=3000):\n    nS, nA = env.observation_space.n, env.action_space.n\n    pi_track = []\n    Q = np.zeros((nS, nA), dtype=np.float64)\n    Q_track = np.zeros((n_episodes, nS, nA), dtype=np.float64)\n    E = np.zeros((nS, nA), dtype=np.float64)\n    select_action = lambda state, Q, epsilon: \\\n        np.argmax(Q[state]) \\\n        if np.random.random() &gt; epsilon \\\n        else np.random.randint(len(Q[state]))\n    alphas = decay_schedule(\n        init_alpha, min_alpha, \n        alpha_decay_ratio, n_episodes)\n    epsilons = decay_schedule(\n        init_epsilon, min_epsilon, \n        epsilon_decay_ratio, n_episodes)\n    \n    for e in tqdm(range(n_episodes), leave=False):\n        E.fill(0)\n        state, done = env.reset(), False\n        action = select_action(state, Q, epsilons[e])\n        while not done:\n            next_state, reward, done, _ = env.step(action)\n            next_action = select_action(next_state, Q, epsilons[e])\n            \n            next_action_is_greedy = Q[next_state][next_action] == Q[next_state].max()\n\n            td_target = reward + gamma * Q[next_state].max() * (not done)\n            td_error = td_target - Q[state][action]\n            if replacing_traces: E[state].fill(0)\n            E[state][action] = E[state][action] + 1\n            if replacing_traces: E.clip(0, 1, out=E)\n            Q = Q + alphas[e] * td_error * E\n            \n            if next_action_is_greedy:\n                E = gamma * lambda_ * E\n            else:\n                E.fill(0)\n\n            state, action = next_state, next_action\n\n        Q_track[e] = Q\n        pi_track.append(np.argmax(Q, axis=1))\n\n    V = np.max(Q, axis=1)        \n    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n    return Q, V, pi, Q_track, pi_track\n\n\nQ_rqlls, V_rqlls, Q_track_rqlls = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_rqll, V_rqll, pi_rqll, Q_track_rqll, pi_track_rqll = q_lambda(env, gamma=gamma, n_episodes=n_episodes)\n    Q_rqlls.append(Q_rqll) ; V_rqlls.append(V_rqll) ; Q_track_rqlls.append(Q_track_rqll)\nQ_rqll, V_rqll, Q_track_rqll = np.mean(Q_rqlls, axis=0), np.mean(V_rqlls, axis=0), np.mean(Q_track_rqlls, axis=0)\ndel Q_rqlls ; del V_rqlls ; del Q_track_rqlls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_rqll, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Q(λ) replacing:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_rqll - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_rqll, optimal_V)))\nprint()\nprint_action_value_function(Q_rqll, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Q(λ) replacing action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_rqll, optimal_Q)))\nprint()\nprint_policy(pi_rqll, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_rqll, mean_return_rqll, mean_regret_rqll = get_policy_metrics(\n    env, gamma=gamma, pi=pi_rqll, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_rqll, mean_return_rqll, mean_regret_rqll))\n\nState-value function found by Q(λ) replacing:\n|           | 01 0.5641 | 02 0.7718 | 03 0.8443 | 04 0.8878 | 05 0.9231 | 06 0.9537 | 07 0.9817 |           |\nOptimal state-value function:\n|           | 01 0.5637 | 02  0.763 | 03 0.8449 | 04 0.8892 | 05  0.922 | 06 0.9515 | 07 0.9806 |           |\nState-value function errors:\n|           | 01    0.0 | 02   0.01 | 03   -0.0 | 04   -0.0 | 05    0.0 | 06    0.0 | 07    0.0 |           |\nState-value function RMSE: 0.0031\n\nQ(λ) replacing action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╕\n│   s │     &lt; │     &gt; │   * &lt; │   * &gt; │   er &lt; │   er &gt; │\n╞═════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╡\n│   0 │ 0     │ 0     │ 0     │ 0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   1 │ 0.286 │ 0.564 │ 0.312 │ 0.564 │  0.026 │ -0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   2 │ 0.661 │ 0.772 │ 0.67  │ 0.763 │  0.01  │ -0.009 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   3 │ 0.805 │ 0.844 │ 0.803 │ 0.845 │ -0.001 │  0.001 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   4 │ 0.865 │ 0.888 │ 0.864 │ 0.889 │ -0.001 │  0.001 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   5 │ 0.901 │ 0.923 │ 0.901 │ 0.922 │  0     │ -0.001 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   6 │ 0.933 │ 0.954 │ 0.932 │ 0.952 │ -0.001 │ -0.002 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   7 │ 0.962 │ 0.982 │ 0.961 │ 0.981 │ -0.001 │ -0.001 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   8 │ 0     │ 0     │ 0     │ 0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╛\nAction-value function RMSE: 0.007\n\n정책:\n|           | 01      &gt; | 02      &gt; | 03      &gt; | 04      &gt; | 05      &gt; | 06      &gt; | 07      &gt; |           |\nReaches goal 96.00%. Obtains an average return of 0.8548. Regret of 0.0000\n\n\n\nQ_aqlls, V_aqlls, Q_track_aqlls = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_aqll, V_aqll, pi_aqll, Q_track_aqll, pi_track_aqll = q_lambda(env, gamma=gamma, \n                                                                    replacing_traces=False,\n                                                                    n_episodes=n_episodes)\n    Q_aqlls.append(Q_aqll) ; V_aqlls.append(V_aqll) ; Q_track_aqlls.append(Q_track_aqll)\nQ_aqll, V_aqll, Q_track_aqll = np.mean(Q_aqlls, axis=0), np.mean(V_aqlls, axis=0), np.mean(Q_track_aqlls, axis=0)\ndel Q_aqlls ; del V_aqlls ; del Q_track_aqlls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_aqll, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Q(λ) accumulating:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_aqll - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_aqll, optimal_V)))\nprint()\nprint_action_value_function(Q_aqll, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Q(λ) accumulating action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_aqll, optimal_Q)))\nprint()\nprint_policy(pi_aqll, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_aqll, mean_return_aqll, mean_regret_aqll = get_policy_metrics(\n    env, gamma=gamma, pi=pi_aqll, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_aqll, mean_return_aqll, mean_regret_aqll))\n\nState-value function found by Q(λ) accumulating:\n|           | 01 0.5853 | 02 0.7684 | 03 0.8461 | 04 0.8894 | 05 0.9223 | 06  0.952 | 07 0.9803 |           |\nOptimal state-value function:\n|           | 01 0.5637 | 02  0.763 | 03 0.8449 | 04 0.8892 | 05  0.922 | 06 0.9515 | 07 0.9806 |           |\nState-value function errors:\n|           | 01   0.02 | 02   0.01 | 03    0.0 | 04    0.0 | 05    0.0 | 06    0.0 | 07   -0.0 |           |\nState-value function RMSE: 0.0074\n\nQ(λ) accumulating action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╕\n│   s │     &lt; │     &gt; │   * &lt; │   * &gt; │   er &lt; │   er &gt; │\n╞═════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╡\n│   0 │ 0     │ 0     │ 0     │ 0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   1 │ 0.329 │ 0.585 │ 0.312 │ 0.564 │ -0.017 │ -0.022 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   2 │ 0.674 │ 0.768 │ 0.67  │ 0.763 │ -0.004 │ -0.005 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   3 │ 0.802 │ 0.846 │ 0.803 │ 0.845 │  0.001 │ -0.001 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   4 │ 0.865 │ 0.889 │ 0.864 │ 0.889 │ -0.001 │ -0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   5 │ 0.902 │ 0.922 │ 0.901 │ 0.922 │ -0     │ -0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   6 │ 0.932 │ 0.952 │ 0.932 │ 0.952 │  0     │ -0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   7 │ 0.962 │ 0.98  │ 0.961 │ 0.981 │ -0.001 │  0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   8 │ 0     │ 0     │ 0     │ 0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╛\nAction-value function RMSE: 0.0068\n\n정책:\n|           | 01      &gt; | 02      &gt; | 03      &gt; | 04      &gt; | 05      &gt; | 06      &gt; | 07      &gt; |           |\nReaches goal 96.00%. Obtains an average return of 0.8548. Regret of 0.0000\n\n\n\n\nDyna-Q\n\ndef dyna_q(env,\n           gamma=1.0,\n           init_alpha=0.5,\n           min_alpha=0.01,\n           alpha_decay_ratio=0.5,\n           init_epsilon=1.0,\n           min_epsilon=0.1,\n           epsilon_decay_ratio=0.9,\n           n_planning=3,             \n           n_episodes=3000):\n    nS, nA = env.observation_space.n, env.action_space.n\n    pi_track, T_track, R_track, planning_track = [], [], [], []\n    Q = np.zeros((nS, nA), dtype=np.float64)\n    T_count = np.zeros((nS, nA, nS), dtype=np.int)\n    R_model = np.zeros((nS, nA, nS), dtype=np.float64)\n\n    Q_track = np.zeros((n_episodes, nS, nA), dtype=np.float64)\n    select_action = lambda state, Q, epsilon: \\\n        np.argmax(Q[state]) \\\n        if np.random.random() &gt; epsilon \\\n        else np.random.randint(len(Q[state]))\n    alphas = decay_schedule(\n        init_alpha, min_alpha, \n        alpha_decay_ratio, n_episodes)\n    epsilons = decay_schedule(\n        init_epsilon, min_epsilon, \n        epsilon_decay_ratio, n_episodes)\n    \n    for e in tqdm(range(n_episodes), leave=False):\n        state, done = env.reset(), False\n        while not done:\n            action = select_action(state, Q, epsilons[e])\n            next_state, reward, done, _ = env.step(action)\n\n            T_count[state][action][next_state] += 1\n            r_diff = reward - R_model[state][action][next_state]\n            R_model[state][action][next_state] += (r_diff / T_count[state][action][next_state])\n\n            td_target = reward + gamma * Q[next_state].max() * (not done)\n            td_error = td_target - Q[state][action]\n            Q[state][action] = Q[state][action] + alphas[e] * td_error\n\n            backup_next_state = next_state\n            for _ in range(n_planning):\n                if Q.sum() == 0: break\n\n                visited_states = np.where(np.sum(T_count, axis=(1, 2)) &gt; 0)[0]\n                state = np.random.choice(visited_states)\n\n                actions_taken = np.where(np.sum(T_count[state], axis=1) &gt; 0)[0]\n                action = np.random.choice(actions_taken)\n\n                probs = T_count[state][action]/T_count[state][action].sum()\n                next_state = np.random.choice(np.arange(nS), size=1, p=probs)[0]\n                reward = R_model[state][action][next_state]\n                planning_track.append((state, action, reward, next_state))\n\n                    \n                td_target = reward + gamma * Q[next_state].max()\n                td_error = td_target - Q[state][action]\n                Q[state][action] = Q[state][action] + alphas[e] * td_error\n\n            state = backup_next_state\n\n        T_track.append(T_count.copy())\n        R_track.append(R_model.copy())\n        Q_track[e] = Q        \n        pi_track.append(np.argmax(Q, axis=1))\n\n    V = np.max(Q, axis=1)\n    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n    return Q, V, pi, Q_track, pi_track, T_track, R_track, np.array(planning_track)\n\n\nQ_dqs, V_dqs, Q_track_dqs = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_dq, V_dq, pi_dq, Q_track_dq, pi_track_dq, T_track_dq, R_track_dq, planning_dq = dyna_q(\n        env, gamma=gamma, n_episodes=n_episodes)\n    Q_dqs.append(Q_dq) ; V_dqs.append(V_dq) ; Q_track_dqs.append(Q_track_dq)\nQ_dq, V_dq, Q_track_dq = np.mean(Q_dqs, axis=0), np.mean(V_dqs, axis=0), np.mean(Q_track_dqs, axis=0)\ndel Q_dqs ; del V_dqs ; del Q_track_dqs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_dq, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Dyna-Q:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_dq - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_dq, optimal_V)))\nprint()\nprint_action_value_function(Q_dq, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Dyna-Q action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_dq, optimal_Q)))\nprint()\nprint_policy(pi_dq, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_dq, mean_return_dq, mean_regret_dq = get_policy_metrics(\n    env, gamma=gamma, pi=pi_dq, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_dq, mean_return_dq, mean_regret_dq))\n\nState-value function found by Dyna-Q:\n|           | 01 0.5576 | 02 0.7725 | 03 0.8452 | 04 0.8896 | 05 0.9212 | 06 0.9515 | 07 0.9821 |           |\nOptimal state-value function:\n|           | 01 0.5637 | 02  0.763 | 03 0.8449 | 04 0.8892 | 05  0.922 | 06 0.9515 | 07 0.9806 |           |\nState-value function errors:\n|           | 01  -0.01 | 02   0.01 | 03    0.0 | 04    0.0 | 05   -0.0 | 06   -0.0 | 07    0.0 |           |\nState-value function RMSE: 0.0038\n\nDyna-Q action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╕\n│   s │     &lt; │     &gt; │   * &lt; │   * &gt; │   er &lt; │   er &gt; │\n╞═════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╡\n│   0 │ 0     │ 0     │ 0     │ 0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   1 │ 0.299 │ 0.558 │ 0.312 │ 0.564 │  0.013 │  0.006 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   2 │ 0.685 │ 0.773 │ 0.67  │ 0.763 │ -0.015 │ -0.01  │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   3 │ 0.806 │ 0.845 │ 0.803 │ 0.845 │ -0.003 │ -0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   4 │ 0.862 │ 0.89  │ 0.864 │ 0.889 │  0.001 │ -0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   5 │ 0.901 │ 0.921 │ 0.901 │ 0.922 │  0     │  0.001 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   6 │ 0.933 │ 0.951 │ 0.932 │ 0.952 │ -0.001 │  0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   7 │ 0.962 │ 0.982 │ 0.961 │ 0.981 │ -0.001 │ -0.002 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   8 │ 0     │ 0     │ 0     │ 0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╛\nAction-value function RMSE: 0.0054\n\n정책:\n|           | 01      &gt; | 02      &gt; | 03      &gt; | 04      &gt; | 05      &gt; | 06      &gt; | 07      &gt; |           |\nReaches goal 96.00%. Obtains an average return of 0.8548. Regret of 0.0000\n\n\n\nplot_transition_model(T_track_dq, episode=0)\n\n\n\n\n\nplot_transition_model(T_track_dq, episode=9)\n\n\n\n\n\nplot_transition_model(T_track_dq, episode=99)\n\n\n\n\n\nplot_transition_model(T_track_dq, episode=len(T_track_dq)-1)\n\n\n\n\n\nplot_model_state_sampling(planning_dq, algo='Dyna-Q')\n\n\n\n\n\nplot_model_state_7(planning_dq, algo='Dyna-Q')\n\n\n\n\n\n\n경로 샘플링\n\ndef trajectory_sampling(env,\n                        gamma=1.0,\n                        init_alpha=0.5,\n                        min_alpha=0.01,\n                        alpha_decay_ratio=0.5,\n                        init_epsilon=1.0,\n                        min_epsilon=0.1,\n                        epsilon_decay_ratio=0.9,\n                        max_trajectory_depth=100,\n                        planning_freq=5,\n                        greedy_planning=True,\n                        n_episodes=3000):\n    nS, nA = env.observation_space.n, env.action_space.n\n    pi_track, T_track, R_track, planning_track = [], [], [], []\n    Q = np.zeros((nS, nA), dtype=np.float64)\n    T_count = np.zeros((nS, nA, nS), dtype=np.int)\n    R_model = np.zeros((nS, nA, nS), dtype=np.float64)\n\n    Q_track = np.zeros((n_episodes, nS, nA), dtype=np.float64)\n    select_action = lambda state, Q, epsilon: \\\n        np.argmax(Q[state]) \\\n        if np.random.random() &gt; epsilon \\\n        else np.random.randint(len(Q[state]))\n    alphas = decay_schedule(\n        init_alpha, min_alpha, \n        alpha_decay_ratio, n_episodes)\n    epsilons = decay_schedule(\n        init_epsilon, min_epsilon, \n        epsilon_decay_ratio, n_episodes)\n\n    for e in tqdm(range(n_episodes), leave=False):\n        state, done = env.reset(), False\n\n        while not done:\n            action = select_action(state, Q, epsilons[e])\n            next_state, reward, done, _ = env.step(action)\n            \n            T_count[state][action][next_state] += 1\n            r_diff = reward - R_model[state][action][next_state]\n            R_model[state][action][next_state] += (r_diff / T_count[state][action][next_state])\n\n            td_target = reward + gamma * Q[next_state].max() * (not done)\n            td_error = td_target - Q[state][action]\n            Q[state][action] = Q[state][action] + alphas[e] * td_error\n            \n            backup_next_state = next_state\n\n            if e % planning_freq == 0:\n                for _ in range(max_trajectory_depth):\n                    if Q.sum() == 0: break\n\n                    action = Q[state].argmax() if greedy_planning else \\\n                        select_action(state, Q, epsilons[e])\n                    if not T_count[state][action].sum(): break\n                    probs = T_count[state][action]/T_count[state][action].sum()\n                    next_state = np.random.choice(np.arange(nS), size=1, p=probs)[0]\n                    reward = R_model[state][action][next_state]\n                    planning_track.append((state, action, reward, next_state))\n\n                    td_target = reward + gamma * Q[next_state].max()\n                    td_error = td_target - Q[state][action]\n                    Q[state][action] = Q[state][action] + alphas[e] * td_error\n\n                    state = next_state\n\n            state = backup_next_state\n\n        T_track.append(T_count.copy())\n        R_track.append(R_model.copy())\n\n        Q_track[e] = Q        \n        pi_track.append(np.argmax(Q, axis=1))\n\n    V = np.max(Q, axis=1)\n    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n    return Q, V, pi, Q_track, pi_track, T_track, R_track, np.array(planning_track)\n\n\nQ_tss, V_tss, Q_track_tss = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_ts, V_ts, pi_ts, Q_track_ts, pi_track_ts, T_track_ts, R_track_ts, planning_ts = trajectory_sampling(\n        env, gamma=gamma, n_episodes=n_episodes)\n    Q_tss.append(Q_ts) ; V_tss.append(V_ts) ; Q_track_tss.append(Q_track_ts)\nQ_ts, V_ts, Q_track_ts = np.mean(Q_tss, axis=0), np.mean(V_tss, axis=0), np.mean(Q_track_tss, axis=0)\ndel Q_tss ; del V_tss ; del Q_track_tss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_ts, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Trajectory Sampling:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_ts - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_ts, optimal_V)))\nprint()\nprint_action_value_function(Q_ts, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Trajectory Sampling action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_ts, optimal_Q)))\nprint()\nprint_policy(pi_ts, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_ts, mean_return_ts, mean_regret_ts = get_policy_metrics(\n    env, gamma=gamma, pi=pi_ts, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_ts, mean_return_ts, mean_regret_ts))\n\nState-value function found by Trajectory Sampling:\n|           | 01  0.562 | 02 0.7616 | 03 0.8434 | 04 0.8869 | 05 0.9219 | 06 0.9515 | 07  0.981 |           |\nOptimal state-value function:\n|           | 01 0.5637 | 02  0.763 | 03 0.8449 | 04 0.8892 | 05  0.922 | 06 0.9515 | 07 0.9806 |           |\nState-value function errors:\n|           | 01   -0.0 | 02   -0.0 | 03   -0.0 | 04   -0.0 | 05   -0.0 | 06   -0.0 | 07    0.0 |           |\nState-value function RMSE: 0.0012\n\nTrajectory Sampling action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╕\n│   s │     &lt; │     &gt; │   * &lt; │   * &gt; │   er &lt; │   er &gt; │\n╞═════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╡\n│   0 │ 0     │ 0     │ 0     │ 0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   1 │ 0.302 │ 0.562 │ 0.312 │ 0.564 │  0.01  │  0.002 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   2 │ 0.665 │ 0.762 │ 0.67  │ 0.763 │  0.005 │  0.001 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   3 │ 0.802 │ 0.843 │ 0.803 │ 0.845 │  0.001 │  0.002 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   4 │ 0.865 │ 0.887 │ 0.864 │ 0.889 │ -0.002 │  0.002 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   5 │ 0.903 │ 0.922 │ 0.901 │ 0.922 │ -0.001 │  0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   6 │ 0.934 │ 0.952 │ 0.932 │ 0.952 │ -0.002 │  0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   7 │ 0.962 │ 0.981 │ 0.961 │ 0.981 │ -0.001 │ -0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   8 │ 0     │ 0     │ 0     │ 0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╛\nAction-value function RMSE: 0.0029\n\n정책:\n|           | 01      &gt; | 02      &gt; | 03      &gt; | 04      &gt; | 05      &gt; | 06      &gt; | 07      &gt; |           |\nReaches goal 96.00%. Obtains an average return of 0.8548. Regret of 0.0000\n\n\n\nplot_model_state_sampling(planning_ts, algo='Trajectory Sampling')\n\n\n\n\n\nplot_model_state_7(planning_ts, algo='Trajectory Sampling')\n\n\n\n\n\n\n각 에피소드별 max(Q) 비교\n\nSARSA(\\(\\lambda\\)) 대체\n\nplot_value_function(\n    'Sarsa(λ) replacing estimates through time vs. true values', \n    np.max(Q_track_rsl, axis=2), \n    optimal_V,\n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Sarsa(λ) replacing estimates through time vs. true values (log scale)', \n    np.max(Q_track_rsl, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Sarsa(λ) replacing estimates through time (close up)', \n    np.max(Q_track_rsl, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\nSARSA(\\(\\lambda\\)) 누적\n\nplot_value_function(\n    'Sarsa(λ) accumulating estimates through time vs. true values', \n    np.max(Q_track_asl, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Sarsa(λ) accumulating estimates through time vs. true values (log scale)', \n    np.max(Q_track_asl, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Sarsa(λ) accumulating estimates through time (close up)', \n    np.max(Q_track_asl, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\nQ(\\(\\lambda\\)) 대체\n\nplot_value_function(\n    'Q(λ) replacing estimates through time vs. true values', \n    np.max(Q_track_rqll, axis=2), \n    optimal_V,\n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Q(λ) replacing estimates through time vs. true values (log scale)', \n    np.max(Q_track_rqll, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Q(λ) replacing estimates through time (close up)', \n    np.max(Q_track_rqll, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\nQ(\\(\\lambda\\)) 누적\n\nplot_value_function(\n    'Q(λ) accumulating estimates through time vs. true values', \n    np.max(Q_track_aqll, axis=2), \n    optimal_V,\n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Q(λ) accumulating estimates through time vs. true values (log scale)', \n    np.max(Q_track_aqll, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Q(λ) accumulating estimates through time (close up)', \n    np.max(Q_track_aqll, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\nDyna-Q\n\nplot_value_function(\n    'Dyna-Q estimates through time vs. true values', \n    np.max(Q_track_dq, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Dyna-Q estimates through time vs. true values (log scale)', \n    np.max(Q_track_dq, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Dyna-Q estimates through time (close up)', \n    np.max(Q_track_dq, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\n경로 샘플링\n\nplot_value_function(\n    'Trajectory Sampling estimates through time vs. true values', \n    np.max(Q_track_ts, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Trajectory Sampling estimates through time vs. true values (log scale)', \n    np.max(Q_track_ts, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Trajectory Sampling estimates through time (close up)', \n    np.max(Q_track_ts, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\n\n정책 평가 비교\n\nrsl_success_rate_ma, rsl_mean_return_ma, rsl_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_rsl)\n\n\n\n\n\nasl_success_rate_ma, asl_mean_return_ma, asl_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_asl)\n\n\n\n\n\nrqll_success_rate_ma, rqll_mean_return_ma, rqll_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_rqll)\n\n\n\n\n\naqll_success_rate_ma, aqll_mean_return_ma, aqll_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_aqll)\n\n\n\n\n\ndq_success_rate_ma, dq_mean_return_ma, dq_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_dq)\n\n\n\n\n\nts_success_rate_ma, ts_mean_return_ma, ts_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_ts)\n\n\n\n\n\nplt.axhline(y=success_rate_op, color='k', linestyle='-', linewidth=1)\nplt.text(int(len(rsl_success_rate_ma)*1.02), success_rate_op*1.01, 'π*')\n\nplt.plot(rsl_success_rate_ma, '-', linewidth=2, label='Sarsa(λ) replacing')\nplt.plot(asl_success_rate_ma, '--', linewidth=2, label='Sarsa(λ) accumulating')\nplt.plot(rqll_success_rate_ma, ':', linewidth=2, label='Q(λ) replacing')\nplt.plot(aqll_success_rate_ma, '-.', linewidth=2, label='Q(λ) accumulating')\nplt.plot(dq_success_rate_ma, '-', linewidth=2, label='Dyna-Q')\nplt.plot(ts_success_rate_ma, '--', linewidth=2, label='Trajectory Sampling')\nplt.legend(loc=4, ncol=1)\n\nplt.title('Policy success rate (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Success rate %')\nplt.ylim(-1, 101)\nplt.xticks(rotation=45)\n\n\nplt.show()\n\n\n\n\n\nplt.axhline(y=mean_return_op, color='k', linestyle='-', linewidth=1)\nplt.text(int(len(rsl_mean_return_ma)*1.02), mean_return_op*1.01, 'π*')\n\nplt.plot(rsl_mean_return_ma, '-', linewidth=2, label='Sarsa(λ) replacing')\nplt.plot(asl_mean_return_ma, '--', linewidth=2, label='Sarsa(λ) accumulating')\nplt.plot(rqll_mean_return_ma, ':', linewidth=2, label='Q(λ) replacing')\nplt.plot(aqll_mean_return_ma, '-.', linewidth=2, label='Q(λ) accumulating')\nplt.plot(dq_mean_return_ma, '-', linewidth=2, label='Dyna-Q')\nplt.plot(ts_mean_return_ma, '--', linewidth=2, label='Trajectory Sampling')\nplt.legend(loc=4, ncol=1)\n\nplt.title('Policy episode return (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Return (Gt:T)')\n\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.plot(rsl_mean_regret_ma, '-', linewidth=2, label='Sarsa(λ) replacing')\nplt.plot(asl_mean_regret_ma, '--', linewidth=2, label='Sarsa(λ) accumulating')\nplt.plot(rqll_mean_regret_ma, ':', linewidth=2, label='Q(λ) replacing')\nplt.plot(aqll_mean_regret_ma, '-.', linewidth=2, label='Q(λ) accumulating')\nplt.plot(dq_mean_regret_ma, '-', linewidth=2, label='Dyna-Q')\nplt.plot(ts_mean_regret_ma, '--', linewidth=2, label='Trajectory Sampling')\nplt.legend(loc=1, ncol=1)\n\nplt.title('Policy episode regret (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Regret (q* - Q)')\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.axhline(y=optimal_V[init_state], color='k', linestyle='-', linewidth=1)\nplt.text(int(len(Q_track_rsl)*1.05), optimal_V[init_state]+.01, 'v*({})'.format(init_state))\n\nplt.plot(moving_average(np.max(Q_track_rsl, axis=2).T[init_state]), \n         '-', linewidth=2, label='Sarsa(λ) replacing')\nplt.plot(moving_average(np.max(Q_track_asl, axis=2).T[init_state]), \n         '--', linewidth=2, label='Sarsa(λ) accumulating')\nplt.plot(moving_average(np.max(Q_track_rqll, axis=2).T[init_state]), \n         ':', linewidth=2, label='Q(λ) replacing')\nplt.plot(moving_average(np.max(Q_track_aqll, axis=2).T[init_state]), \n         '-.', linewidth=2, label='Q(λ) accumulating')\nplt.plot(moving_average(np.max(Q_track_dq, axis=2).T[init_state]), \n         '-', linewidth=2, label='Dyna-Q')\nplt.plot(moving_average(np.max(Q_track_ts, axis=2).T[init_state]), \n         '--', linewidth=2, label='Trajectory Sampling')\nplt.legend(loc=4, ncol=1)\n\nplt.title('Estimated expected return (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Estimated value of initial state V({})'.format(init_state))\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_rsl, axis=2) - optimal_V), axis=1)), \n         '-', linewidth=2, label='Sarsa(λ) replacing')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_asl, axis=2) - optimal_V), axis=1)), \n         '--', linewidth=2, label='Sarsa(λ) accumulating')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_rqll, axis=2) - optimal_V), axis=1)), \n         ':', linewidth=2, label='Q(λ) replacing')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_aqll, axis=2) - optimal_V), axis=1)), \n         '-.', linewidth=2, label='Q(λ) accumulating')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_dq, axis=2) - optimal_V), axis=1)), \n         '-', linewidth=2, label='Dyna-Q')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_ts, axis=2) - optimal_V), axis=1)), \n         '--', linewidth=2, label='Trajectory Sampling')\nplt.legend(loc=1, ncol=1)\n\nplt.title('State-value function estimation error (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Mean Absolute Error MAE(V, v*)')\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.plot(moving_average(np.mean(np.abs(Q_track_rsl - optimal_Q), axis=(1,2))), \n         '-', linewidth=2, label='Sarsa(λ) replacing')\nplt.plot(moving_average(np.mean(np.abs(Q_track_asl - optimal_Q), axis=(1,2))), \n         '--', linewidth=2, label='Sarsa(λ) accumulating')\nplt.plot(moving_average(np.mean(np.abs(Q_track_rqll - optimal_Q), axis=(1,2))), \n         ':', linewidth=2, label='Q(λ) replacing')\nplt.plot(moving_average(np.mean(np.abs(Q_track_aqll - optimal_Q), axis=(1,2))), \n         '-.', linewidth=2, label='Q(λ) accumulating')\nplt.plot(moving_average(np.mean(np.abs(Q_track_dq - optimal_Q), axis=(1,2))), \n         '-', linewidth=2, label='Dyna-Q')\nplt.plot(moving_average(np.mean(np.abs(Q_track_ts - optimal_Q), axis=(1,2))), \n         '--', linewidth=2, label='Trajectory Sampling')\nplt.legend(loc=1, ncol=1)\n\nplt.title('Action-value function estimation error (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Mean Absolute Error MAE(Q, q*)')\nplt.xticks(rotation=45)\n\nplt.show()"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-7.html#프로즌레이크-환경",
    "href": "publication/GDRL/GDRL-chapter-7.html#프로즌레이크-환경",
    "title": "Chapter 7: Archieving goals more effectively and efficiently",
    "section": "프로즌레이크 환경",
    "text": "프로즌레이크 환경\n\nenv = gym.make('FrozenLake-v0')\ninit_state = env.reset()\ngoal_state = 15\ngamma = 0.99\nn_episodes = 10000\nP = env.env.P\nn_cols, svf_prec, err_prec, avf_prec=4, 4, 2, 3\naction_symbols=('&lt;', 'v', '&gt;', '^')\nlimit_items, limit_value = 5, 0.0\ncu_limit_items, cu_limit_value, cu_episodes = 10, 0.01, 2000\n\n\n알파와 입실론 스케쥴링\n\nplt.plot(decay_schedule(0.5, 0.01, 0.5, n_episodes), \n         '-', linewidth=2, \n         label='Alpha schedule')\nplt.plot(decay_schedule(1.0, 0.1, 0.9, n_episodes), \n         ':', linewidth=2, \n         label='Epsilon schedule')\nplt.legend(loc=1, ncol=1)\n\nplt.title('Alpha and epsilon schedules')\nplt.xlabel('Episodes')\nplt.ylabel('Hyperparameter values')\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\n\n이상적인 가치 함수와 정책\n\noptimal_Q, optimal_V, optimal_pi = value_iteration(P, gamma=gamma)\nprint_state_value_function(optimal_V, P, n_cols=n_cols, prec=svf_prec, title='Optimal state-value function:')\nprint()\n\nprint_action_value_function(optimal_Q, \n                            None, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Optimal action-value function:')\nprint()\nprint_policy(optimal_pi, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_op, mean_return_op, mean_regret_op = get_policy_metrics(\n    env, gamma=gamma, pi=optimal_pi, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_op, mean_return_op, mean_regret_op))\n\nOptimal state-value function:\n| 00  0.542 | 01 0.4988 | 02 0.4707 | 03 0.4569 |\n| 04 0.5585 |           | 06 0.3583 |           |\n| 08 0.5918 | 09 0.6431 | 10 0.6152 |           |\n|           | 13 0.7417 | 14 0.8628 |           |\n\nOptimal action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╕\n│   s │     &lt; │     v │     &gt; │     ^ │\n╞═════╪═══════╪═══════╪═══════╪═══════╡\n│   0 │ 0.542 │ 0.528 │ 0.528 │ 0.522 │\n├─────┼───────┼───────┼───────┼───────┤\n│   1 │ 0.343 │ 0.334 │ 0.32  │ 0.499 │\n├─────┼───────┼───────┼───────┼───────┤\n│   2 │ 0.438 │ 0.434 │ 0.424 │ 0.471 │\n├─────┼───────┼───────┼───────┼───────┤\n│   3 │ 0.306 │ 0.306 │ 0.302 │ 0.457 │\n├─────┼───────┼───────┼───────┼───────┤\n│   4 │ 0.558 │ 0.38  │ 0.374 │ 0.363 │\n├─────┼───────┼───────┼───────┼───────┤\n│   5 │ 0     │ 0     │ 0     │ 0     │\n├─────┼───────┼───────┼───────┼───────┤\n│   6 │ 0.358 │ 0.203 │ 0.358 │ 0.155 │\n├─────┼───────┼───────┼───────┼───────┤\n│   7 │ 0     │ 0     │ 0     │ 0     │\n├─────┼───────┼───────┼───────┼───────┤\n│   8 │ 0.38  │ 0.408 │ 0.397 │ 0.592 │\n├─────┼───────┼───────┼───────┼───────┤\n│   9 │ 0.44  │ 0.643 │ 0.448 │ 0.398 │\n├─────┼───────┼───────┼───────┼───────┤\n│  10 │ 0.615 │ 0.497 │ 0.403 │ 0.33  │\n├─────┼───────┼───────┼───────┼───────┤\n│  11 │ 0     │ 0     │ 0     │ 0     │\n├─────┼───────┼───────┼───────┼───────┤\n│  12 │ 0     │ 0     │ 0     │ 0     │\n├─────┼───────┼───────┼───────┼───────┤\n│  13 │ 0.457 │ 0.53  │ 0.742 │ 0.497 │\n├─────┼───────┼───────┼───────┼───────┤\n│  14 │ 0.733 │ 0.863 │ 0.821 │ 0.781 │\n├─────┼───────┼───────┼───────┼───────┤\n│  15 │ 0     │ 0     │ 0     │ 0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╛\n\n정책:\n| 00      &lt; | 01      ^ | 02      ^ | 03      ^ |\n| 04      &lt; |           | 06      &lt; |           |\n| 08      ^ | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      v |           |\nReaches goal 74.00%. Obtains an average return of 0.5116. Regret of 0.0000\n\n\n\n\n추적을 대체하는 SARSA(\\(\\lambda\\))\n\nQ_rsls, V_rsls, Q_track_rsls = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_rsl, V_rsl, pi_rsl, Q_track_rsl, pi_track_rsl = sarsa_lambda(env, gamma=gamma, n_episodes=n_episodes)\n    Q_rsls.append(Q_rsl) ; V_rsls.append(V_rsl) ; Q_track_rsls.append(Q_track_rsl)\nQ_rsl, V_rsl, Q_track_rsl = np.mean(Q_rsls, axis=0), np.mean(V_rsls, axis=0), np.mean(Q_track_rsls, axis=0)\ndel Q_rsls ; del V_rsls ; del Q_track_rsls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_rsl, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Sarsa(λ) replacing:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_rsl - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_rsl, optimal_V)))\nprint()\nprint_action_value_function(Q_rsl, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Sarsa(λ) replacing action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_rsl, optimal_Q)))\nprint()\nprint_policy(pi_rsl, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_rsl, mean_return_rsl, mean_regret_rsl = get_policy_metrics(\n    env, gamma=gamma, pi=pi_rsl, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_rsl, mean_return_rsl, mean_regret_rsl))\n\nState-value function found by Sarsa(λ) replacing:\n| 00 0.2941 | 01 0.2414 | 02 0.2168 | 03  0.133 |\n| 04 0.3138 |           | 06 0.2152 |           |\n| 08 0.3585 | 09 0.4465 | 10 0.4496 |           |\n|           | 13 0.5839 | 14 0.7726 |           |\nOptimal state-value function:\n| 00  0.542 | 01 0.4988 | 02 0.4707 | 03 0.4569 |\n| 04 0.5585 |           | 06 0.3583 |           |\n| 08 0.5918 | 09 0.6431 | 10 0.6152 |           |\n|           | 13 0.7417 | 14 0.8628 |           |\nState-value function errors:\n| 00  -0.25 | 01  -0.26 | 02  -0.25 | 03  -0.32 |\n| 04  -0.24 |           | 06  -0.14 |           |\n| 08  -0.23 | 09   -0.2 | 10  -0.17 |           |\n|           | 13  -0.16 | 14  -0.09 |           |\nState-value function RMSE: 0.1822\n\nSarsa(λ) replacing action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╤════════╤════════╕\n│   s │     &lt; │     v │     &gt; │     ^ │   * &lt; │   * v │   * &gt; │   * ^ │   er &lt; │   er v │   er &gt; │   er ^ │\n╞═════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╪════════╪════════╡\n│   0 │ 0.294 │ 0.27  │ 0.271 │ 0.265 │ 0.542 │ 0.528 │ 0.528 │ 0.522 │  0.248 │  0.258 │  0.256 │  0.257 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   1 │ 0.118 │ 0.126 │ 0.105 │ 0.241 │ 0.343 │ 0.334 │ 0.32  │ 0.499 │  0.225 │  0.208 │  0.215 │  0.257 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   2 │ 0.217 │ 0.136 │ 0.139 │ 0.133 │ 0.438 │ 0.434 │ 0.424 │ 0.471 │  0.221 │  0.298 │  0.285 │  0.338 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   3 │ 0.045 │ 0.047 │ 0.036 │ 0.133 │ 0.306 │ 0.306 │ 0.302 │ 0.457 │  0.262 │  0.259 │  0.265 │  0.324 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   4 │ 0.314 │ 0.218 │ 0.209 │ 0.198 │ 0.558 │ 0.38  │ 0.374 │ 0.363 │  0.245 │  0.162 │  0.165 │  0.165 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   5 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   6 │ 0.162 │ 0.116 │ 0.202 │ 0.049 │ 0.358 │ 0.203 │ 0.358 │ 0.155 │  0.196 │  0.087 │  0.156 │  0.107 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   7 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   8 │ 0.214 │ 0.255 │ 0.252 │ 0.358 │ 0.38  │ 0.408 │ 0.397 │ 0.592 │  0.166 │  0.152 │  0.144 │  0.233 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   9 │ 0.291 │ 0.447 │ 0.326 │ 0.259 │ 0.44  │ 0.643 │ 0.448 │ 0.398 │  0.149 │  0.197 │  0.122 │  0.139 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  10 │ 0.45  │ 0.354 │ 0.289 │ 0.184 │ 0.615 │ 0.497 │ 0.403 │ 0.33  │  0.166 │  0.143 │  0.114 │  0.147 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  11 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  12 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  13 │ 0.304 │ 0.418 │ 0.584 │ 0.365 │ 0.457 │ 0.53  │ 0.742 │ 0.497 │  0.153 │  0.112 │  0.158 │  0.131 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  14 │ 0.557 │ 0.773 │ 0.713 │ 0.64  │ 0.733 │ 0.863 │ 0.821 │ 0.781 │  0.175 │  0.09  │  0.108 │  0.141 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  15 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╧════════╧════════╛\nAction-value function RMSE: 0.167\n\n정책:\n| 00      &lt; | 01      ^ | 02      &lt; | 03      ^ |\n| 04      &lt; |           | 06      &gt; |           |\n| 08      ^ | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      v |           |\nReaches goal 70.00%. Obtains an average return of 0.4864. Regret of 0.0156\n\n\n\n\n추적을 누적하는 SARSA(\\(\\lambda\\))\n\nQ_asls, V_asls, Q_track_asls = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_asl, V_asl, pi_asl, Q_track_asl, pi_track_asl = sarsa_lambda(env, gamma=gamma, \n                                                                   replacing_traces=False, \n                                                                   n_episodes=n_episodes)\n    Q_asls.append(Q_asl) ; V_asls.append(V_asl) ; Q_track_asls.append(Q_track_asl)\nQ_asl, V_asl, Q_track_asl = np.mean(Q_asls, axis=0), np.mean(V_asls, axis=0), np.mean(Q_track_asls, axis=0)\ndel Q_asls ; del V_asls ; del Q_track_asls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_asl, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Sarsa(λ) accumulating:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_asl - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_asl, optimal_V)))\nprint()\nprint_action_value_function(Q_asl, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Sarsa(λ) accumulating action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_asl, optimal_Q)))\nprint()\nprint_policy(pi_asl, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_asl, mean_return_asl, mean_regret_asl = get_policy_metrics(\n    env, gamma=gamma, pi=pi_asl, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_asl, mean_return_asl, mean_regret_asl))\n\nState-value function found by Sarsa(λ) accumulating:\n| 00 0.2872 | 01 0.2453 | 02 0.2138 | 03 0.1526 |\n| 04 0.3114 |           | 06 0.2142 |           |\n| 08 0.3617 | 09 0.4517 | 10 0.4699 |           |\n|           | 13 0.5917 | 14 0.7812 |           |\nOptimal state-value function:\n| 00  0.542 | 01 0.4988 | 02 0.4707 | 03 0.4569 |\n| 04 0.5585 |           | 06 0.3583 |           |\n| 08 0.5918 | 09 0.6431 | 10 0.6152 |           |\n|           | 13 0.7417 | 14 0.8628 |           |\nState-value function errors:\n| 00  -0.25 | 01  -0.25 | 02  -0.26 | 03   -0.3 |\n| 04  -0.25 |           | 06  -0.14 |           |\n| 08  -0.23 | 09  -0.19 | 10  -0.15 |           |\n|           | 13  -0.15 | 14  -0.08 |           |\nState-value function RMSE: 0.1784\n\nSarsa(λ) accumulating action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╤════════╤════════╕\n│   s │     &lt; │     v │     &gt; │     ^ │   * &lt; │   * v │   * &gt; │   * ^ │   er &lt; │   er v │   er &gt; │   er ^ │\n╞═════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╪════════╪════════╡\n│   0 │ 0.287 │ 0.274 │ 0.275 │ 0.271 │ 0.542 │ 0.528 │ 0.528 │ 0.522 │  0.255 │  0.253 │  0.253 │  0.252 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   1 │ 0.125 │ 0.127 │ 0.11  │ 0.245 │ 0.343 │ 0.334 │ 0.32  │ 0.499 │  0.218 │  0.207 │  0.21  │  0.254 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   2 │ 0.214 │ 0.153 │ 0.142 │ 0.14  │ 0.438 │ 0.434 │ 0.424 │ 0.471 │  0.224 │  0.28  │  0.282 │  0.33  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   3 │ 0.047 │ 0.048 │ 0.043 │ 0.153 │ 0.306 │ 0.306 │ 0.302 │ 0.457 │  0.26  │  0.258 │  0.259 │  0.304 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   4 │ 0.311 │ 0.213 │ 0.197 │ 0.199 │ 0.558 │ 0.38  │ 0.374 │ 0.363 │  0.247 │  0.166 │  0.177 │  0.164 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   5 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   6 │ 0.206 │ 0.125 │ 0.176 │ 0.049 │ 0.358 │ 0.203 │ 0.358 │ 0.155 │  0.153 │  0.078 │  0.182 │  0.106 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   7 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   8 │ 0.223 │ 0.264 │ 0.241 │ 0.362 │ 0.38  │ 0.408 │ 0.397 │ 0.592 │  0.156 │  0.144 │  0.155 │  0.23  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   9 │ 0.292 │ 0.452 │ 0.326 │ 0.264 │ 0.44  │ 0.643 │ 0.448 │ 0.398 │  0.148 │  0.191 │  0.122 │  0.134 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  10 │ 0.47  │ 0.35  │ 0.295 │ 0.183 │ 0.615 │ 0.497 │ 0.403 │ 0.33  │  0.145 │  0.147 │  0.108 │  0.148 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  11 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  12 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  13 │ 0.314 │ 0.424 │ 0.592 │ 0.395 │ 0.457 │ 0.53  │ 0.742 │ 0.497 │  0.143 │  0.106 │  0.15  │  0.102 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  14 │ 0.557 │ 0.781 │ 0.705 │ 0.66  │ 0.733 │ 0.863 │ 0.821 │ 0.781 │  0.175 │  0.082 │  0.116 │  0.121 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  15 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╧════════╧════════╛\nAction-value function RMSE: 0.1633\n\n정책:\n| 00      &lt; | 01      ^ | 02      &lt; | 03      ^ |\n| 04      &lt; |           | 06      &gt; |           |\n| 08      ^ | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      v |           |\nReaches goal 70.00%. Obtains an average return of 0.4864. Regret of 0.0156\n\n\n\n\n추적을 대체하는 왓킨스 Q(\\(\\lambda\\))\n\nQ_rqlls, V_rqlls, Q_track_rqlls = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_rqll, V_rqll, pi_rqll, Q_track_rqll, pi_track_rqll = q_lambda(env, gamma=gamma, n_episodes=n_episodes)\n    Q_rqlls.append(Q_rqll) ; V_rqlls.append(V_rqll) ; Q_track_rqlls.append(Q_track_rqll)\nQ_rqll, V_rqll, Q_track_rqll = np.mean(Q_rqlls, axis=0), np.mean(V_rqlls, axis=0), np.mean(Q_track_rqlls, axis=0)\ndel Q_rqlls ; del V_rqlls ; del Q_track_rqlls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_rqll, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Q(λ) replacing:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_rqll - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_rqll, optimal_V)))\nprint()\nprint_action_value_function(Q_rqll, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Q(λ) replacing action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_rqll, optimal_Q)))\nprint()\nprint_policy(pi_rqll, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_rqll, mean_return_rqll, mean_regret_rqll = get_policy_metrics(\n    env, gamma=gamma, pi=pi_rqll, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_rqll, mean_return_rqll, mean_regret_rqll))\n\nState-value function found by Q(λ) replacing:\n| 00 0.5205 | 01 0.4758 | 02 0.4481 | 03 0.4346 |\n| 04 0.5376 |           | 06 0.3382 |           |\n| 08 0.5704 | 09 0.6248 | 10 0.6006 |           |\n|           | 13 0.7198 | 14 0.8505 |           |\nOptimal state-value function:\n| 00  0.542 | 01 0.4988 | 02 0.4707 | 03 0.4569 |\n| 04 0.5585 |           | 06 0.3583 |           |\n| 08 0.5918 | 09 0.6431 | 10 0.6152 |           |\n|           | 13 0.7417 | 14 0.8628 |           |\nState-value function errors:\n| 00  -0.02 | 01  -0.02 | 02  -0.02 | 03  -0.02 |\n| 04  -0.02 |           | 06  -0.02 |           |\n| 08  -0.02 | 09  -0.02 | 10  -0.01 |           |\n|           | 13  -0.02 | 14  -0.01 |           |\nState-value function RMSE: 0.0167\n\nQ(λ) replacing action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╤════════╤════════╕\n│   s │     &lt; │     v │     &gt; │     ^ │   * &lt; │   * v │   * &gt; │   * ^ │   er &lt; │   er v │   er &gt; │   er ^ │\n╞═════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╪════════╪════════╡\n│   0 │ 0.521 │ 0.511 │ 0.512 │ 0.505 │ 0.542 │ 0.528 │ 0.528 │ 0.522 │  0.021 │  0.017 │  0.016 │  0.017 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   1 │ 0.344 │ 0.336 │ 0.305 │ 0.476 │ 0.343 │ 0.334 │ 0.32  │ 0.499 │ -0     │ -0.002 │  0.015 │  0.023 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   2 │ 0.422 │ 0.426 │ 0.415 │ 0.448 │ 0.438 │ 0.434 │ 0.424 │ 0.471 │  0.016 │  0.008 │  0.01  │  0.023 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   3 │ 0.295 │ 0.296 │ 0.286 │ 0.435 │ 0.306 │ 0.306 │ 0.302 │ 0.457 │  0.011 │  0.01  │  0.016 │  0.022 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   4 │ 0.538 │ 0.348 │ 0.357 │ 0.362 │ 0.558 │ 0.38  │ 0.374 │ 0.363 │  0.021 │  0.032 │  0.017 │  0.002 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   5 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   6 │ 0.326 │ 0.185 │ 0.318 │ 0.147 │ 0.358 │ 0.203 │ 0.358 │ 0.155 │  0.033 │  0.018 │  0.041 │  0.008 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   7 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   8 │ 0.364 │ 0.402 │ 0.38  │ 0.57  │ 0.38  │ 0.408 │ 0.397 │ 0.592 │  0.015 │  0.006 │  0.016 │  0.021 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   9 │ 0.434 │ 0.625 │ 0.439 │ 0.402 │ 0.44  │ 0.643 │ 0.448 │ 0.398 │  0.006 │  0.018 │  0.009 │ -0.004 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  10 │ 0.601 │ 0.51  │ 0.413 │ 0.344 │ 0.615 │ 0.497 │ 0.403 │ 0.33  │  0.015 │ -0.014 │ -0.01  │ -0.013 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  11 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  12 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  13 │ 0.441 │ 0.517 │ 0.72  │ 0.489 │ 0.457 │ 0.53  │ 0.742 │ 0.497 │  0.016 │  0.012 │  0.022 │  0.008 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  14 │ 0.734 │ 0.851 │ 0.824 │ 0.784 │ 0.733 │ 0.863 │ 0.821 │ 0.781 │ -0.001 │  0.012 │ -0.003 │ -0.003 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  15 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╧════════╧════════╛\nAction-value function RMSE: 0.0137\n\n정책:\n| 00      &lt; | 01      ^ | 02      ^ | 03      ^ |\n| 04      &lt; |           | 06      &lt; |           |\n| 08      ^ | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      v |           |\nReaches goal 74.00%. Obtains an average return of 0.5116. Regret of 0.0000\n\n\n\n\n추적을 누적하는 왓킨스 Q(\\(\\lambda\\))\n\nQ_aqlls, V_aqlls, Q_track_aqlls = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_aqll, V_aqll, pi_aqll, Q_track_aqll, pi_track_aqll = q_lambda(env, gamma=gamma, \n                                                                    replacing_traces=False,\n                                                                    n_episodes=n_episodes)\n    Q_aqlls.append(Q_aqll) ; V_aqlls.append(V_aqll) ; Q_track_aqlls.append(Q_track_aqll)\nQ_aqll, V_aqll, Q_track_aqll = np.mean(Q_aqlls, axis=0), np.mean(V_aqlls, axis=0), np.mean(Q_track_aqlls, axis=0)\ndel Q_aqlls ; del V_aqlls ; del Q_track_aqlls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_aqll, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Q(λ) accumulating:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_aqll - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_aqll, optimal_V)))\nprint()\nprint_action_value_function(Q_aqll, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Q(λ) accumulating action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_aqll, optimal_Q)))\nprint()\nprint_policy(pi_aqll, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_aqll, mean_return_aqll, mean_regret_aqll = get_policy_metrics(\n    env, gamma=gamma, pi=pi_aqll, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_aqll, mean_return_aqll, mean_regret_aqll))\n\nState-value function found by Q(λ) accumulating:\n| 00 0.5208 | 01 0.4669 | 02 0.4335 | 03 0.4211 |\n| 04 0.5382 |           | 06  0.321 |           |\n| 08 0.5731 | 09 0.6253 | 10 0.5811 |           |\n|           | 13 0.7373 | 14  0.862 |           |\nOptimal state-value function:\n| 00  0.542 | 01 0.4988 | 02 0.4707 | 03 0.4569 |\n| 04 0.5585 |           | 06 0.3583 |           |\n| 08 0.5918 | 09 0.6431 | 10 0.6152 |           |\n|           | 13 0.7417 | 14 0.8628 |           |\nState-value function errors:\n| 00  -0.02 | 01  -0.03 | 02  -0.04 | 03  -0.04 |\n| 04  -0.02 |           | 06  -0.04 |           |\n| 08  -0.02 | 09  -0.02 | 10  -0.03 |           |\n|           | 13   -0.0 | 14   -0.0 |           |\nState-value function RMSE: 0.0221\n\nQ(λ) accumulating action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╤════════╤════════╕\n│   s │     &lt; │     v │     &gt; │     ^ │   * &lt; │   * v │   * &gt; │   * ^ │   er &lt; │   er v │   er &gt; │   er ^ │\n╞═════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╪════════╪════════╡\n│   0 │ 0.521 │ 0.5   │ 0.497 │ 0.495 │ 0.542 │ 0.528 │ 0.528 │ 0.522 │  0.021 │  0.028 │  0.03  │  0.027 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   1 │ 0.331 │ 0.307 │ 0.302 │ 0.467 │ 0.343 │ 0.334 │ 0.32  │ 0.499 │  0.013 │  0.028 │  0.018 │  0.032 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   2 │ 0.421 │ 0.416 │ 0.41  │ 0.433 │ 0.438 │ 0.434 │ 0.424 │ 0.471 │  0.018 │  0.017 │  0.014 │  0.037 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   3 │ 0.287 │ 0.297 │ 0.294 │ 0.421 │ 0.306 │ 0.306 │ 0.302 │ 0.457 │  0.019 │  0.009 │  0.008 │  0.036 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   4 │ 0.538 │ 0.352 │ 0.348 │ 0.343 │ 0.558 │ 0.38  │ 0.374 │ 0.363 │  0.02  │  0.027 │  0.026 │  0.02  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   5 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   6 │ 0.307 │ 0.188 │ 0.318 │ 0.13  │ 0.358 │ 0.203 │ 0.358 │ 0.155 │  0.051 │  0.015 │  0.041 │  0.025 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   7 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   8 │ 0.379 │ 0.393 │ 0.387 │ 0.573 │ 0.38  │ 0.408 │ 0.397 │ 0.592 │  0.001 │  0.014 │  0.01  │  0.019 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   9 │ 0.424 │ 0.625 │ 0.419 │ 0.388 │ 0.44  │ 0.643 │ 0.448 │ 0.398 │  0.016 │  0.018 │  0.029 │  0.01  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  10 │ 0.581 │ 0.493 │ 0.396 │ 0.317 │ 0.615 │ 0.497 │ 0.403 │ 0.33  │  0.034 │  0.004 │  0.007 │  0.014 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  11 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  12 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  13 │ 0.44  │ 0.514 │ 0.737 │ 0.491 │ 0.457 │ 0.53  │ 0.742 │ 0.497 │  0.017 │  0.016 │  0.004 │  0.006 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  14 │ 0.721 │ 0.862 │ 0.805 │ 0.764 │ 0.733 │ 0.863 │ 0.821 │ 0.781 │  0.012 │  0.001 │  0.016 │  0.017 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  15 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╧════════╧════════╛\nAction-value function RMSE: 0.0183\n\n정책:\n| 00      &lt; | 01      ^ | 02      ^ | 03      ^ |\n| 04      &lt; |           | 06      &lt; |           |\n| 08      ^ | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      v |           |\nReaches goal 74.00%. Obtains an average return of 0.5116. Regret of 0.0000\n\n\n\n\nDyna-Q\n\nQ_dqs, V_dqs, Q_track_dqs = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_dq, V_dq, pi_dq, Q_track_dq, pi_track_dq, T_track_dq, R_track_dq, planning_dq = dyna_q(\n        env, gamma=gamma, n_episodes=n_episodes)\n    Q_dqs.append(Q_dq) ; V_dqs.append(V_dq) ; Q_track_dqs.append(Q_track_dq)\nQ_dq, V_dq, Q_track_dq = np.mean(Q_dqs, axis=0), np.mean(V_dqs, axis=0), np.mean(Q_track_dqs, axis=0)\ndel Q_dqs ; del V_dqs ; del Q_track_dqs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_dq, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Dyna-Q:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_dq - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_dq, optimal_V)))\nprint()\nprint_action_value_function(Q_dq, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Dyna-Q action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_dq, optimal_Q)))\nprint()\nprint_policy(pi_dq, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_dq, mean_return_dq, mean_regret_dq = get_policy_metrics(\n    env, gamma=gamma, pi=pi_dq, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_dq, mean_return_dq, mean_regret_dq))\n\nState-value function found by Dyna-Q:\n| 00 0.5278 | 01 0.4863 | 02 0.4592 | 03 0.4437 |\n| 04 0.5448 |           | 06 0.3628 |           |\n| 08  0.579 | 09 0.6339 | 10  0.604 |           |\n|           | 13 0.7354 | 14 0.8566 |           |\nOptimal state-value function:\n| 00  0.542 | 01 0.4988 | 02 0.4707 | 03 0.4569 |\n| 04 0.5585 |           | 06 0.3583 |           |\n| 08 0.5918 | 09 0.6431 | 10 0.6152 |           |\n|           | 13 0.7417 | 14 0.8628 |           |\nState-value function errors:\n| 00  -0.01 | 01  -0.01 | 02  -0.01 | 03  -0.01 |\n| 04  -0.01 |           | 06    0.0 |           |\n| 08  -0.01 | 09  -0.01 | 10  -0.01 |           |\n|           | 13  -0.01 | 14  -0.01 |           |\nState-value function RMSE: 0.0091\n\nDyna-Q action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╤════════╤════════╕\n│   s │     &lt; │     v │     &gt; │     ^ │   * &lt; │   * v │   * &gt; │   * ^ │   er &lt; │   er v │   er &gt; │   er ^ │\n╞═════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╪════════╪════════╡\n│   0 │ 0.528 │ 0.514 │ 0.514 │ 0.509 │ 0.542 │ 0.528 │ 0.528 │ 0.522 │  0.014 │  0.013 │  0.014 │  0.014 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   1 │ 0.322 │ 0.335 │ 0.314 │ 0.486 │ 0.343 │ 0.334 │ 0.32  │ 0.499 │  0.022 │ -0.001 │  0.006 │  0.013 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   2 │ 0.432 │ 0.427 │ 0.416 │ 0.459 │ 0.438 │ 0.434 │ 0.424 │ 0.471 │  0.006 │  0.007 │  0.009 │  0.012 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   3 │ 0.298 │ 0.303 │ 0.286 │ 0.444 │ 0.306 │ 0.306 │ 0.302 │ 0.457 │  0.008 │  0.003 │  0.015 │  0.013 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   4 │ 0.545 │ 0.371 │ 0.36  │ 0.361 │ 0.558 │ 0.38  │ 0.374 │ 0.363 │  0.014 │  0.009 │  0.014 │  0.002 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   5 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   6 │ 0.336 │ 0.238 │ 0.358 │ 0.134 │ 0.358 │ 0.203 │ 0.358 │ 0.155 │  0.023 │ -0.035 │  0     │  0.022 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   7 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   8 │ 0.37  │ 0.419 │ 0.366 │ 0.579 │ 0.38  │ 0.408 │ 0.397 │ 0.592 │  0.009 │ -0.011 │  0.03  │  0.013 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   9 │ 0.446 │ 0.634 │ 0.446 │ 0.401 │ 0.44  │ 0.643 │ 0.448 │ 0.398 │ -0.006 │  0.009 │  0.002 │ -0.002 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  10 │ 0.604 │ 0.488 │ 0.393 │ 0.348 │ 0.615 │ 0.497 │ 0.403 │ 0.33  │  0.011 │  0.009 │  0.01  │ -0.017 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  11 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  12 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  13 │ 0.472 │ 0.5   │ 0.735 │ 0.509 │ 0.457 │ 0.53  │ 0.742 │ 0.497 │ -0.015 │  0.029 │  0.006 │ -0.012 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  14 │ 0.718 │ 0.857 │ 0.813 │ 0.786 │ 0.733 │ 0.863 │ 0.821 │ 0.781 │  0.015 │  0.006 │  0.008 │ -0.005 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  15 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╧════════╧════════╛\nAction-value function RMSE: 0.0115\n\n정책:\n| 00      &lt; | 01      ^ | 02      ^ | 03      ^ |\n| 04      &lt; |           | 06      &gt; |           |\n| 08      ^ | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      v |           |\nReaches goal 72.00%. Obtains an average return of 0.4936. Regret of 0.0000\n\n\n\n\n경로 샘플링\n\nQ_tss, V_tss, Q_track_tss = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_ts, V_ts, pi_ts, Q_track_ts, pi_track_ts, T_track_ts, R_track_ts, planning_ts = trajectory_sampling(\n        env, gamma=gamma, n_episodes=n_episodes)\n    Q_tss.append(Q_ts) ; V_tss.append(V_ts) ; Q_track_tss.append(Q_track_ts)\nQ_ts, V_ts, Q_track_ts = np.mean(Q_tss, axis=0), np.mean(V_tss, axis=0), np.mean(Q_track_tss, axis=0)\ndel Q_tss ; del V_tss ; del Q_track_tss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_ts, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Trajectory Sampling:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_ts - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_ts, optimal_V)))\nprint()\nprint_action_value_function(Q_ts, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Trajectory Sampling action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_ts, optimal_Q)))\nprint()\nprint_policy(pi_ts, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_ts, mean_return_ts, mean_regret_ts = get_policy_metrics(\n    env, gamma=gamma, pi=pi_ts, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_ts, mean_return_ts, mean_regret_ts))\n\nState-value function found by Trajectory Sampling:\n| 00 0.5377 | 01 0.4944 | 02 0.4656 | 03 0.4507 |\n| 04 0.5536 |           | 06 0.3605 |           |\n| 08 0.5857 | 09 0.6343 | 10 0.5992 |           |\n|           | 13 0.7337 | 14 0.8616 |           |\nOptimal state-value function:\n| 00  0.542 | 01 0.4988 | 02 0.4707 | 03 0.4569 |\n| 04 0.5585 |           | 06 0.3583 |           |\n| 08 0.5918 | 09 0.6431 | 10 0.6152 |           |\n|           | 13 0.7417 | 14 0.8628 |           |\nState-value function errors:\n| 00   -0.0 | 01   -0.0 | 02  -0.01 | 03  -0.01 |\n| 04   -0.0 |           | 06    0.0 |           |\n| 08  -0.01 | 09  -0.01 | 10  -0.02 |           |\n|           | 13  -0.01 | 14   -0.0 |           |\nState-value function RMSE: 0.006\n\nTrajectory Sampling action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╤════════╤════════╕\n│   s │     &lt; │     v │     &gt; │     ^ │   * &lt; │   * v │   * &gt; │   * ^ │   er &lt; │   er v │   er &gt; │   er ^ │\n╞═════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╪════════╪════════╡\n│   0 │ 0.538 │ 0.522 │ 0.52  │ 0.515 │ 0.542 │ 0.528 │ 0.528 │ 0.522 │  0.004 │  0.006 │  0.008 │  0.007 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   1 │ 0.345 │ 0.324 │ 0.313 │ 0.494 │ 0.343 │ 0.334 │ 0.32  │ 0.499 │ -0.001 │  0.01  │  0.007 │  0.004 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   2 │ 0.432 │ 0.426 │ 0.419 │ 0.466 │ 0.438 │ 0.434 │ 0.424 │ 0.471 │  0.006 │  0.008 │  0.005 │  0.005 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   3 │ 0.3   │ 0.29  │ 0.284 │ 0.451 │ 0.306 │ 0.306 │ 0.302 │ 0.457 │  0.006 │  0.016 │  0.017 │  0.006 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   4 │ 0.554 │ 0.374 │ 0.382 │ 0.342 │ 0.558 │ 0.38  │ 0.374 │ 0.363 │  0.005 │  0.006 │ -0.008 │  0.021 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   5 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   6 │ 0.333 │ 0.175 │ 0.335 │ 0.168 │ 0.358 │ 0.203 │ 0.358 │ 0.155 │  0.025 │  0.028 │  0.023 │ -0.012 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   7 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   8 │ 0.365 │ 0.413 │ 0.404 │ 0.586 │ 0.38  │ 0.408 │ 0.397 │ 0.592 │  0.015 │ -0.006 │ -0.008 │  0.006 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   9 │ 0.426 │ 0.634 │ 0.439 │ 0.373 │ 0.44  │ 0.643 │ 0.448 │ 0.398 │  0.014 │  0.009 │  0.009 │  0.025 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  10 │ 0.599 │ 0.485 │ 0.405 │ 0.331 │ 0.615 │ 0.497 │ 0.403 │ 0.33  │  0.016 │  0.012 │ -0.002 │ -0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  11 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  12 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  13 │ 0.461 │ 0.537 │ 0.734 │ 0.501 │ 0.457 │ 0.53  │ 0.742 │ 0.497 │ -0.004 │ -0.008 │  0.008 │ -0.004 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  14 │ 0.73  │ 0.862 │ 0.813 │ 0.776 │ 0.733 │ 0.863 │ 0.821 │ 0.781 │  0.002 │  0.001 │  0.008 │  0.005 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  15 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╧════════╧════════╛\nAction-value function RMSE: 0.0095\n\n정책:\n| 00      &lt; | 01      ^ | 02      ^ | 03      ^ |\n| 04      &lt; |           | 06      &lt; |           |\n| 08      ^ | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      v |           |\nReaches goal 74.00%. Obtains an average return of 0.5116. Regret of 0.0000\n\n\n\n\n각 에피소드별 max(Q) 비교\n\nSARSA(\\(\\lambda\\)) 대체\n\nplot_value_function(\n    'Sarsa(λ) replacing estimates through time vs. true values', \n    np.max(Q_track_rsl, axis=2), \n    optimal_V,\n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Sarsa(λ) replacing estimates through time vs. true values (log scale)', \n    np.max(Q_track_rsl, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Sarsa(λ) replacing estimates through time (close up)', \n    np.max(Q_track_rsl, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\nSARSA(\\(\\lambda\\)) 누적\n\nplot_value_function(\n    'Sarsa(λ) accumulating estimates through time vs. true values', \n    np.max(Q_track_asl, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Sarsa(λ) accumulating estimates through time vs. true values (log scale)', \n    np.max(Q_track_asl, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Sarsa(λ) accumulating estimates through time (close up)', \n    np.max(Q_track_asl, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\nQ(\\(\\lambda\\)) 대체\n\nplot_value_function(\n    'Q(λ) replacing estimates through time vs. true values', \n    np.max(Q_track_rqll, axis=2), \n    optimal_V,\n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Q(λ) replacing estimates through time vs. true values (log scale)', \n    np.max(Q_track_rqll, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Q(λ) replacing estimates through time (close up)', \n    np.max(Q_track_rqll, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\nQ(\\(\\lambda\\)) 누적\n\nplot_value_function(\n    'Q(λ) accumulating estimates through time vs. true values', \n    np.max(Q_track_aqll, axis=2), \n    optimal_V,\n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Q(λ) accumulating estimates through time vs. true values (log scale)', \n    np.max(Q_track_aqll, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Q(λ) accumulating estimates through time (close up)', \n    np.max(Q_track_aqll, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\nDyna-Q\n\nplot_value_function(\n    'Dyna-Q estimates through time vs. true values', \n    np.max(Q_track_dq, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Dyna-Q estimates through time vs. true values (log scale)', \n    np.max(Q_track_dq, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Dyna-Q estimates through time (close up)', \n    np.max(Q_track_dq, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\n경로 샘플링\n\nplot_value_function(\n    'Trajectory Sampling estimates through time vs. true values', \n    np.max(Q_track_ts, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Trajectory Sampling estimates through time vs. true values (log scale)', \n    np.max(Q_track_ts, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Trajectory Sampling estimates through time (close up)', \n    np.max(Q_track_ts, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\n\n정책 평가 비교\n\nrsl_success_rate_ma, rsl_mean_return_ma, rsl_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_rsl)\n\n\n\n\n\nasl_success_rate_ma, asl_mean_return_ma, asl_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_asl)\n\n\n\n\n\nrqll_success_rate_ma, rqll_mean_return_ma, rqll_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_rqll)\n\n\n\n\n\naqll_success_rate_ma, aqll_mean_return_ma, aqll_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_aqll)\n\n\n\n\n\ndq_success_rate_ma, dq_mean_return_ma, dq_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_dq)\n\n\n\n\n\nts_success_rate_ma, ts_mean_return_ma, ts_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_ts)\n\n\n\n\n\nplt.axhline(y=success_rate_op, color='k', linestyle='-', linewidth=1)\nplt.text(int(len(rsl_success_rate_ma)*1.02), success_rate_op*1.01, 'π*')\n\nplt.plot(rsl_success_rate_ma, '-', linewidth=2, label='Sarsa(λ) replacing')\nplt.plot(asl_success_rate_ma, '--', linewidth=2, label='Sarsa(λ) accumulating')\nplt.plot(rqll_success_rate_ma, ':', linewidth=2, label='Q(λ) replacing')\nplt.plot(aqll_success_rate_ma, '-.', linewidth=2, label='Q(λ) accumulating')\nplt.plot(dq_success_rate_ma, '-', linewidth=2, label='Dyna-Q')\nplt.plot(ts_success_rate_ma, '--', linewidth=2, label='Trajectory Sampling')\nplt.legend(loc=4, ncol=1)\n\nplt.title('Policy success rate (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Success rate %')\nplt.ylim(-1, 101)\nplt.xticks(rotation=45)\n\n\nplt.show()\n\n\n\n\n\nplt.axhline(y=mean_return_op, color='k', linestyle='-', linewidth=1)\nplt.text(int(len(rsl_mean_return_ma)*1.02), mean_return_op*1.01, 'π*')\n\nplt.plot(rsl_mean_return_ma, '-', linewidth=2, label='Sarsa(λ) replacing')\nplt.plot(asl_mean_return_ma, '--', linewidth=2, label='Sarsa(λ) accumulating')\nplt.plot(rqll_mean_return_ma, ':', linewidth=2, label='Q(λ) replacing')\nplt.plot(aqll_mean_return_ma, '-.', linewidth=2, label='Q(λ) accumulating')\nplt.plot(dq_mean_return_ma, '-', linewidth=2, label='Dyna-Q')\nplt.plot(ts_mean_return_ma, '--', linewidth=2, label='Trajectory Sampling')\nplt.legend(loc=4, ncol=1)\n\nplt.title('Policy episode return (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Return (Gt:T)')\n\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.plot(rsl_mean_regret_ma, '-', linewidth=2, label='Sarsa(λ) replacing')\nplt.plot(asl_mean_regret_ma, '--', linewidth=2, label='Sarsa(λ) accumulating')\nplt.plot(rqll_mean_regret_ma, ':', linewidth=2, label='Q(λ) replacing')\nplt.plot(aqll_mean_regret_ma, '-.', linewidth=2, label='Q(λ) accumulating')\nplt.plot(dq_mean_regret_ma, '-', linewidth=2, label='Dyna-Q')\nplt.plot(ts_mean_regret_ma, '--', linewidth=2, label='Trajectory Sampling')\nplt.legend(loc=1, ncol=1)\n\nplt.title('Policy episode regret (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Regret (q* - Q)')\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.axhline(y=optimal_V[init_state], color='k', linestyle='-', linewidth=1)\nplt.text(int(len(Q_track_rsl)*1.05), optimal_V[init_state]+.01, 'v*({})'.format(init_state))\n\nplt.plot(moving_average(np.max(Q_track_rsl, axis=2).T[init_state]), \n         '-', linewidth=2, label='Sarsa(λ) replacing')\nplt.plot(moving_average(np.max(Q_track_asl, axis=2).T[init_state]), \n         '--', linewidth=2, label='Sarsa(λ) accumulating')\nplt.plot(moving_average(np.max(Q_track_rqll, axis=2).T[init_state]), \n         ':', linewidth=2, label='Q(λ) replacing')\nplt.plot(moving_average(np.max(Q_track_aqll, axis=2).T[init_state]), \n         '-.', linewidth=2, label='Q(λ) accumulating')\nplt.plot(moving_average(np.max(Q_track_dq, axis=2).T[init_state]), \n         '-', linewidth=2, label='Dyna-Q')\nplt.plot(moving_average(np.max(Q_track_ts, axis=2).T[init_state]), \n         '--', linewidth=2, label='Trajectory Sampling')\nplt.legend(loc=4, ncol=1)\n\nplt.title('Estimated expected return (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Estimated value of initial state V({})'.format(init_state))\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_rsl, axis=2) - optimal_V), axis=1)), \n         '-', linewidth=2, label='Sarsa(λ) replacing')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_asl, axis=2) - optimal_V), axis=1)), \n         '--', linewidth=2, label='Sarsa(λ) accumulating')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_rqll, axis=2) - optimal_V), axis=1)), \n         ':', linewidth=2, label='Q(λ) replacing')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_aqll, axis=2) - optimal_V), axis=1)), \n         '-.', linewidth=2, label='Q(λ) accumulating')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_dq, axis=2) - optimal_V), axis=1)), \n         '-', linewidth=2, label='Dyna-Q')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_ts, axis=2) - optimal_V), axis=1)), \n         '--', linewidth=2, label='Trajectory Sampling')\nplt.legend(loc=1, ncol=1)\n\nplt.title('State-value function estimation error (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Mean Absolute Error MAE(V, v*)')\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.plot(moving_average(np.mean(np.abs(Q_track_rsl - optimal_Q), axis=(1,2))), \n         '-', linewidth=2, label='Sarsa(λ) replacing')\nplt.plot(moving_average(np.mean(np.abs(Q_track_asl - optimal_Q), axis=(1,2))), \n         '--', linewidth=2, label='Sarsa(λ) accumulating')\nplt.plot(moving_average(np.mean(np.abs(Q_track_rqll - optimal_Q), axis=(1,2))), \n         ':', linewidth=2, label='Q(λ) replacing')\nplt.plot(moving_average(np.mean(np.abs(Q_track_aqll - optimal_Q), axis=(1,2))), \n         '-.', linewidth=2, label='Q(λ) accumulating')\nplt.plot(moving_average(np.mean(np.abs(Q_track_dq - optimal_Q), axis=(1,2))), \n         '-', linewidth=2, label='Dyna-Q')\nplt.plot(moving_average(np.mean(np.abs(Q_track_ts - optimal_Q), axis=(1,2))), \n         '--', linewidth=2, label='Trajectory Sampling')\nplt.legend(loc=1, ncol=1)\n\nplt.title('Action-value function estimation error (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Mean Absolute Error MAE(Q, q*)')\nplt.xticks(rotation=45)\n\nplt.show()"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-7.html#큰-프로즌레이크-8x8",
    "href": "publication/GDRL/GDRL-chapter-7.html#큰-프로즌레이크-8x8",
    "title": "Chapter 7: Archieving goals more effectively and efficiently",
    "section": "큰 프로즌레이크 (8x8)",
    "text": "큰 프로즌레이크 (8x8)\n\nenv = gym.make('FrozenLake8x8-v0')\ninit_state = env.reset()\ngoal_state = 63\ngamma = 0.99\nn_episodes = 30000\nP = env.env.P\nn_cols, svf_prec, err_prec, avf_prec=8, 4, 2, 3\naction_symbols=('&lt;', 'v', '&gt;', '^')\nlimit_items, limit_value = 5, 0.025\ncu_limit_items, cu_limit_value, cu_episodes = 10, 0.0, 5000\n\n\n알파와 입실론 스케쥴링\n\nplt.plot(decay_schedule(0.5, 0.01, 0.5, n_episodes), \n         '-', linewidth=2, \n         label='Alpha schedule')\nplt.plot(decay_schedule(1.0, 0.1, 0.9, n_episodes), \n         ':', linewidth=2, \n         label='Epsilon schedule')\nplt.legend(loc=1, ncol=1)\n\nplt.title('Alpha and epsilon schedules')\nplt.xlabel('Episodes')\nplt.ylabel('Hyperparameter values')\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\n\n이상적인 가치 함수와 정책\n\noptimal_Q, optimal_V, optimal_pi = value_iteration(P, gamma=gamma)\nprint_state_value_function(optimal_V, P, n_cols=n_cols, prec=svf_prec, title='Optimal state-value function:')\nprint()\n\nprint_action_value_function(optimal_Q, \n                            None, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Optimal action-value function:')\nprint()\nprint_policy(optimal_pi, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_op, mean_return_op, mean_regret_op = get_policy_metrics(\n    env, gamma=gamma, pi=optimal_pi, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_op, mean_return_op, mean_regret_op))\n\nOptimal state-value function:\n| 00 0.4146 | 01 0.4272 | 02 0.4461 | 03 0.4683 | 04 0.4924 | 05 0.5166 | 06 0.5353 | 07  0.541 |\n| 08 0.4117 | 09 0.4212 | 10 0.4375 | 11 0.4584 | 12 0.4832 | 13 0.5135 | 14 0.5458 | 15 0.5574 |\n| 16 0.3968 | 17 0.3938 | 18 0.3755 |           | 20 0.4217 | 21 0.4938 | 22 0.5612 | 23 0.5859 |\n| 24 0.3693 | 25  0.353 | 26 0.3065 | 27 0.2004 | 28 0.3008 |           | 30  0.569 | 31 0.6283 |\n| 32 0.3327 | 33 0.2914 | 34 0.1973 |           | 36 0.2893 | 37  0.362 | 38 0.5348 | 39 0.6897 |\n| 40 0.3061 |           |           | 43 0.0863 | 44 0.2139 | 45 0.2727 |           | 47  0.772 |\n| 48 0.2889 |           | 50 0.0577 | 51 0.0475 |           | 53 0.2505 |           | 55 0.8778 |\n| 56 0.2804 | 57 0.2008 | 58 0.1273 |           | 60 0.2396 | 61 0.4864 | 62 0.7371 |           |\n\nOptimal action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╕\n│   s │     &lt; │     v │     &gt; │     ^ │\n╞═════╪═══════╪═══════╪═══════╪═══════╡\n│   0 │ 0.41  │ 0.414 │ 0.414 │ 0.415 │\n├─────┼───────┼───────┼───────┼───────┤\n│   1 │ 0.417 │ 0.423 │ 0.427 │ 0.425 │\n├─────┼───────┼───────┼───────┼───────┤\n│   2 │ 0.433 │ 0.44  │ 0.446 │ 0.443 │\n├─────┼───────┼───────┼───────┼───────┤\n│   3 │ 0.453 │ 0.461 │ 0.468 │ 0.464 │\n├─────┼───────┼───────┼───────┼───────┤\n│   4 │ 0.477 │ 0.484 │ 0.492 │ 0.488 │\n├─────┼───────┼───────┼───────┼───────┤\n│   5 │ 0.502 │ 0.509 │ 0.517 │ 0.51  │\n├─────┼───────┼───────┼───────┼───────┤\n│   6 │ 0.527 │ 0.529 │ 0.535 │ 0.526 │\n├─────┼───────┼───────┼───────┼───────┤\n│   7 │ 0.539 │ 0.539 │ 0.541 │ 0.534 │\n├─────┼───────┼───────┼───────┼───────┤\n│   8 │ 0.404 │ 0.406 │ 0.407 │ 0.412 │\n├─────┼───────┼───────┼───────┼───────┤\n│   9 │ 0.407 │ 0.41  │ 0.415 │ 0.421 │\n├─────┼───────┼───────┼───────┼───────┤\n│  10 │ 0.41  │ 0.414 │ 0.422 │ 0.437 │\n├─────┼───────┼───────┼───────┼───────┤\n│  11 │ 0.299 │ 0.304 │ 0.314 │ 0.458 │\n├─────┼───────┼───────┼───────┼───────┤\n│  12 │ 0.453 │ 0.46  │ 0.471 │ 0.483 │\n├─────┼───────┼───────┼───────┼───────┤\n│  13 │ 0.493 │ 0.503 │ 0.514 │ 0.51  │\n├─────┼───────┼───────┼───────┼───────┤\n│  14 │ 0.531 │ 0.539 │ 0.546 │ 0.53  │\n├─────┼───────┼───────┼───────┼───────┤\n│  15 │ 0.552 │ 0.557 │ 0.556 │ 0.543 │\n├─────┼───────┼───────┼───────┼───────┤\n│  16 │ 0.389 │ 0.383 │ 0.388 │ 0.397 │\n├─────┼───────┼───────┼───────┼───────┤\n│  17 │ 0.386 │ 0.371 │ 0.379 │ 0.394 │\n├─────┼───────┼───────┼───────┼───────┤\n│  18 │ 0.375 │ 0.231 │ 0.246 │ 0.274 │\n├─────┼───────┼───────┼───────┼───────┤\n│  19 │ 0     │ 0     │ 0     │ 0     │\n├─────┼───────┼───────┼───────┼───────┤\n│  20 │ 0.259 │ 0.262 │ 0.422 │ 0.322 │\n├─────┼───────┼───────┼───────┼───────┤\n│  21 │ 0.309 │ 0.324 │ 0.355 │ 0.494 │\n├─────┼───────┼───────┼───────┼───────┤\n│  22 │ 0.531 │ 0.544 │ 0.561 │ 0.536 │\n├─────┼───────┼───────┼───────┼───────┤\n│  23 │ 0.576 │ 0.586 │ 0.585 │ 0.562 │\n├─────┼───────┼───────┼───────┼───────┤\n│  24 │ 0.363 │ 0.348 │ 0.357 │ 0.369 │\n├─────┼───────┼───────┼───────┼───────┤\n│  25 │ 0.348 │ 0.319 │ 0.327 │ 0.353 │\n├─────┼───────┼───────┼───────┼───────┤\n│  26 │ 0.306 │ 0.248 │ 0.255 │ 0.307 │\n├─────┼───────┼───────┼───────┼───────┤\n│  27 │ 0.101 │ 0.2   │ 0.099 │ 0.2   │\n├─────┼───────┼───────┼───────┼───────┤\n│  28 │ 0.301 │ 0.162 │ 0.235 │ 0.205 │\n├─────┼───────┼───────┼───────┼───────┤\n│  29 │ 0     │ 0     │ 0     │ 0     │\n├─────┼───────┼───────┼───────┼───────┤\n│  30 │ 0.362 │ 0.384 │ 0.569 │ 0.393 │\n├─────┼───────┼───────┼───────┼───────┤\n│  31 │ 0.609 │ 0.623 │ 0.628 │ 0.588 │\n├─────┼───────┼───────┼───────┼───────┤\n│  32 │ 0.333 │ 0.307 │ 0.319 │ 0.328 │\n├─────┼───────┼───────┼───────┼───────┤\n│  33 │ 0.226 │ 0.175 │ 0.182 │ 0.291 │\n├─────┼───────┼───────┼───────┼───────┤\n│  34 │ 0.197 │ 0.096 │ 0.101 │ 0.197 │\n├─────┼───────┼───────┼───────┼───────┤\n│  35 │ 0     │ 0     │ 0     │ 0     │\n├─────┼───────┼───────┼───────┼───────┤\n│  36 │ 0.17  │ 0.19  │ 0.289 │ 0.219 │\n├─────┼───────┼───────┼───────┼───────┤\n│  37 │ 0.185 │ 0.362 │ 0.266 │ 0.272 │\n├─────┼───────┼───────┼───────┼───────┤\n│  38 │ 0.307 │ 0.347 │ 0.415 │ 0.535 │\n├─────┼───────┼───────┼───────┼───────┤\n│  39 │ 0.639 │ 0.659 │ 0.69  │ 0.611 │\n├─────┼───────┼───────┼───────┼───────┤\n│  40 │ 0.306 │ 0.196 │ 0.205 │ 0.211 │\n├─────┼───────┼───────┼───────┼───────┤\n│  41 │ 0     │ 0     │ 0     │ 0     │\n├─────┼───────┼───────┼───────┼───────┤\n│  42 │ 0     │ 0     │ 0     │ 0     │\n├─────┼───────┼───────┼───────┼───────┤\n│  43 │ 0.016 │ 0.086 │ 0.086 │ 0.071 │\n├─────┼───────┼───────┼───────┼───────┤\n│  44 │ 0.124 │ 0.118 │ 0.185 │ 0.214 │\n├─────┼───────┼───────┼───────┼───────┤\n│  45 │ 0.273 │ 0.153 │ 0.202 │ 0.19  │\n├─────┼───────┼───────┼───────┼───────┤\n│  46 │ 0     │ 0     │ 0     │ 0     │\n├─────┼───────┼───────┼───────┼───────┤\n│  47 │ 0.517 │ 0.544 │ 0.772 │ 0.482 │\n├─────┼───────┼───────┼───────┼───────┤\n│  48 │ 0.289 │ 0.188 │ 0.194 │ 0.196 │\n├─────┼───────┼───────┼───────┼───────┤\n│  49 │ 0     │ 0     │ 0     │ 0     │\n├─────┼───────┼───────┼───────┼───────┤\n│  50 │ 0.042 │ 0.058 │ 0.058 │ 0.016 │\n├─────┼───────┼───────┼───────┼───────┤\n│  51 │ 0.048 │ 0.019 │ 0.028 │ 0.048 │\n├─────┼───────┼───────┼───────┼───────┤\n│  52 │ 0     │ 0     │ 0     │ 0     │\n├─────┼───────┼───────┼───────┼───────┤\n│  53 │ 0.251 │ 0.161 │ 0.251 │ 0.09  │\n├─────┼───────┼───────┼───────┼───────┤\n│  54 │ 0     │ 0     │ 0     │ 0     │\n├─────┼───────┼───────┼───────┼───────┤\n│  55 │ 0.588 │ 0.623 │ 0.878 │ 0.544 │\n├─────┼───────┼───────┼───────┼───────┤\n│  56 │ 0.28  │ 0.251 │ 0.254 │ 0.254 │\n├─────┼───────┼───────┼───────┼───────┤\n│  57 │ 0.159 │ 0.201 │ 0.108 │ 0.135 │\n├─────┼───────┼───────┼───────┼───────┤\n│  58 │ 0.127 │ 0.108 │ 0.061 │ 0.085 │\n├─────┼───────┼───────┼───────┼───────┤\n│  59 │ 0     │ 0     │ 0     │ 0     │\n├─────┼───────┼───────┼───────┼───────┤\n│  60 │ 0.079 │ 0.24  │ 0.24  │ 0.161 │\n├─────┼───────┼───────┼───────┼───────┤\n│  61 │ 0.322 │ 0.483 │ 0.486 │ 0.405 │\n├─────┼───────┼───────┼───────┼───────┤\n│  62 │ 0.404 │ 0.737 │ 0.577 │ 0.494 │\n├─────┼───────┼───────┼───────┼───────┤\n│  63 │ 0     │ 0     │ 0     │ 0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╛\n\n정책:\n| 00      ^ | 01      &gt; | 02      &gt; | 03      &gt; | 04      &gt; | 05      &gt; | 06      &gt; | 07      &gt; |\n| 08      ^ | 09      ^ | 10      ^ | 11      ^ | 12      ^ | 13      &gt; | 14      &gt; | 15      v |\n| 16      ^ | 17      ^ | 18      &lt; |           | 20      &gt; | 21      ^ | 22      &gt; | 23      v |\n| 24      ^ | 25      ^ | 26      ^ | 27      v | 28      &lt; |           | 30      &gt; | 31      &gt; |\n| 32      &lt; | 33      ^ | 34      &lt; |           | 36      &gt; | 37      v | 38      ^ | 39      &gt; |\n| 40      &lt; |           |           | 43      v | 44      ^ | 45      &lt; |           | 47      &gt; |\n| 48      &lt; |           | 50      v | 51      &lt; |           | 53      &lt; |           | 55      &gt; |\n| 56      &lt; | 57      v | 58      &lt; |           | 60      v | 61      &gt; | 62      v |           |\nReaches goal 81.00%. Obtains an average return of 0.3994. Regret of 0.0000\n\n\n\nSARSA(\\(\\lambda\\)) 대체\n\nQ_rsls, V_rsls, Q_track_rsls = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_rsl, V_rsl, pi_rsl, Q_track_rsl, pi_track_rsl = sarsa_lambda(env, gamma=gamma, n_episodes=n_episodes)\n    Q_rsls.append(Q_rsl) ; V_rsls.append(V_rsl) ; Q_track_rsls.append(Q_track_rsl)\nQ_rsl, V_rsl, Q_track_rsl = np.mean(Q_rsls, axis=0), np.mean(V_rsls, axis=0), np.mean(Q_track_rsls, axis=0)\ndel Q_rsls ; del V_rsls ; del Q_track_rsls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_rsl, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Sarsa(λ) replacing:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_rsl - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_rsl, optimal_V)))\nprint()\nprint_action_value_function(Q_rsl, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Sarsa(λ) replacing action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_rsl, optimal_Q)))\nprint()\nprint_policy(pi_rsl, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_rsl, mean_return_rsl, mean_regret_rsl = get_policy_metrics(\n    env, gamma=gamma, pi=pi_rsl, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_rsl, mean_return_rsl, mean_regret_rsl))\n\nState-value function found by Sarsa(λ) replacing:\n| 00 0.2416 | 01 0.2499 | 02 0.2651 | 03 0.2854 | 04 0.3088 | 05 0.3341 | 06  0.353 | 07 0.3573 |\n| 08 0.2375 | 09 0.2447 | 10 0.2578 | 11 0.2771 | 12  0.299 | 13 0.3286 | 14 0.3617 | 15 0.3746 |\n| 16 0.2115 | 17 0.2091 | 18 0.1956 |           | 20 0.2412 | 21 0.3088 | 22 0.3711 | 23 0.4023 |\n| 24 0.1504 | 25 0.1559 | 26  0.139 | 27 0.0886 | 28 0.1472 |           | 30 0.3798 | 31 0.4486 |\n| 32 0.0533 | 33 0.0487 | 34 0.0364 |           | 36 0.1404 | 37 0.2004 | 38 0.3508 | 39 0.5192 |\n| 40 0.0066 |           |           | 43 0.0341 | 44  0.099 | 45  0.136 |           | 47 0.6163 |\n| 48 0.0019 |           | 50 0.0009 | 51 0.0062 |           | 53  0.122 |           | 55 0.7921 |\n| 56 0.0005 | 57 0.0001 | 58 0.0002 |           | 60 0.0668 | 61 0.2667 | 62 0.5133 |           |\nOptimal state-value function:\n| 00 0.4146 | 01 0.4272 | 02 0.4461 | 03 0.4683 | 04 0.4924 | 05 0.5166 | 06 0.5353 | 07  0.541 |\n| 08 0.4117 | 09 0.4212 | 10 0.4375 | 11 0.4584 | 12 0.4832 | 13 0.5135 | 14 0.5458 | 15 0.5574 |\n| 16 0.3968 | 17 0.3938 | 18 0.3755 |           | 20 0.4217 | 21 0.4938 | 22 0.5612 | 23 0.5859 |\n| 24 0.3693 | 25  0.353 | 26 0.3065 | 27 0.2004 | 28 0.3008 |           | 30  0.569 | 31 0.6283 |\n| 32 0.3327 | 33 0.2914 | 34 0.1973 |           | 36 0.2893 | 37  0.362 | 38 0.5348 | 39 0.6897 |\n| 40 0.3061 |           |           | 43 0.0863 | 44 0.2139 | 45 0.2727 |           | 47  0.772 |\n| 48 0.2889 |           | 50 0.0577 | 51 0.0475 |           | 53 0.2505 |           | 55 0.8778 |\n| 56 0.2804 | 57 0.2008 | 58 0.1273 |           | 60 0.2396 | 61 0.4864 | 62 0.7371 |           |\nState-value function errors:\n| 00  -0.17 | 01  -0.18 | 02  -0.18 | 03  -0.18 | 04  -0.18 | 05  -0.18 | 06  -0.18 | 07  -0.18 |\n| 08  -0.17 | 09  -0.18 | 10  -0.18 | 11  -0.18 | 12  -0.18 | 13  -0.18 | 14  -0.18 | 15  -0.18 |\n| 16  -0.19 | 17  -0.18 | 18  -0.18 |           | 20  -0.18 | 21  -0.19 | 22  -0.19 | 23  -0.18 |\n| 24  -0.22 | 25   -0.2 | 26  -0.17 | 27  -0.11 | 28  -0.15 |           | 30  -0.19 | 31  -0.18 |\n| 32  -0.28 | 33  -0.24 | 34  -0.16 |           | 36  -0.15 | 37  -0.16 | 38  -0.18 | 39  -0.17 |\n| 40   -0.3 |           |           | 43  -0.05 | 44  -0.11 | 45  -0.14 |           | 47  -0.16 |\n| 48  -0.29 |           | 50  -0.06 | 51  -0.04 |           | 53  -0.13 |           | 55  -0.09 |\n| 56  -0.28 | 57   -0.2 | 58  -0.13 |           | 60  -0.17 | 61  -0.22 | 62  -0.22 |           |\nState-value function RMSE: 0.1666\n\nSarsa(λ) replacing action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╤════════╤════════╕\n│   s │     &lt; │     v │     &gt; │     ^ │   * &lt; │   * v │   * &gt; │   * ^ │   er &lt; │   er v │   er &gt; │   er ^ │\n╞═════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╪════════╪════════╡\n│   0 │ 0.235 │ 0.237 │ 0.237 │ 0.242 │ 0.41  │ 0.414 │ 0.414 │ 0.415 │  0.175 │  0.176 │  0.176 │  0.173 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   1 │ 0.239 │ 0.244 │ 0.25  │ 0.246 │ 0.417 │ 0.423 │ 0.427 │ 0.425 │  0.178 │  0.179 │  0.177 │  0.179 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   2 │ 0.25  │ 0.257 │ 0.265 │ 0.259 │ 0.433 │ 0.44  │ 0.446 │ 0.443 │  0.183 │  0.183 │  0.181 │  0.184 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   3 │ 0.266 │ 0.275 │ 0.284 │ 0.281 │ 0.453 │ 0.461 │ 0.468 │ 0.464 │  0.187 │  0.186 │  0.184 │  0.183 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   4 │ 0.29  │ 0.299 │ 0.309 │ 0.302 │ 0.477 │ 0.484 │ 0.492 │ 0.488 │  0.186 │  0.186 │  0.184 │  0.185 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   5 │ 0.316 │ 0.323 │ 0.334 │ 0.324 │ 0.502 │ 0.509 │ 0.517 │ 0.51  │  0.186 │  0.186 │  0.182 │  0.186 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   6 │ 0.345 │ 0.347 │ 0.353 │ 0.344 │ 0.527 │ 0.529 │ 0.535 │ 0.526 │  0.182 │  0.182 │  0.182 │  0.182 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   7 │ 0.356 │ 0.356 │ 0.355 │ 0.354 │ 0.539 │ 0.539 │ 0.541 │ 0.534 │  0.183 │  0.183 │  0.186 │  0.179 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   8 │ 0.215 │ 0.215 │ 0.217 │ 0.237 │ 0.404 │ 0.406 │ 0.407 │ 0.412 │  0.189 │  0.19  │  0.19  │  0.174 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   9 │ 0.223 │ 0.223 │ 0.227 │ 0.245 │ 0.407 │ 0.41  │ 0.415 │ 0.421 │  0.183 │  0.188 │  0.188 │  0.177 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  10 │ 0.224 │ 0.227 │ 0.233 │ 0.258 │ 0.41  │ 0.414 │ 0.422 │ 0.437 │  0.186 │  0.187 │  0.189 │  0.18  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  11 │ 0.184 │ 0.177 │ 0.19  │ 0.277 │ 0.299 │ 0.304 │ 0.314 │ 0.458 │  0.114 │  0.127 │  0.124 │  0.181 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  12 │ 0.262 │ 0.271 │ 0.278 │ 0.299 │ 0.453 │ 0.46  │ 0.471 │ 0.483 │  0.191 │  0.189 │  0.193 │  0.184 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  13 │ 0.302 │ 0.308 │ 0.327 │ 0.322 │ 0.493 │ 0.503 │ 0.514 │ 0.51  │  0.191 │  0.195 │  0.186 │  0.188 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  14 │ 0.346 │ 0.351 │ 0.362 │ 0.347 │ 0.531 │ 0.539 │ 0.546 │ 0.53  │  0.185 │  0.187 │  0.184 │  0.183 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  15 │ 0.366 │ 0.372 │ 0.369 │ 0.361 │ 0.552 │ 0.557 │ 0.556 │ 0.543 │  0.186 │  0.186 │  0.187 │  0.182 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  16 │ 0.12  │ 0.114 │ 0.118 │ 0.212 │ 0.389 │ 0.383 │ 0.388 │ 0.397 │  0.269 │  0.269 │  0.269 │  0.185 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  17 │ 0.127 │ 0.117 │ 0.121 │ 0.209 │ 0.386 │ 0.371 │ 0.379 │ 0.394 │  0.259 │  0.254 │  0.258 │  0.185 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  18 │ 0.196 │ 0.07  │ 0.079 │ 0.092 │ 0.375 │ 0.231 │ 0.246 │ 0.274 │  0.18  │  0.161 │  0.166 │  0.182 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  19 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  20 │ 0.107 │ 0.119 │ 0.241 │ 0.148 │ 0.259 │ 0.262 │ 0.422 │ 0.322 │  0.152 │  0.143 │  0.181 │  0.174 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  21 │ 0.156 │ 0.178 │ 0.204 │ 0.309 │ 0.309 │ 0.324 │ 0.355 │ 0.494 │  0.152 │  0.146 │  0.151 │  0.185 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  22 │ 0.343 │ 0.353 │ 0.371 │ 0.347 │ 0.531 │ 0.544 │ 0.561 │ 0.536 │  0.188 │  0.192 │  0.19  │  0.189 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  23 │ 0.388 │ 0.394 │ 0.398 │ 0.377 │ 0.576 │ 0.586 │ 0.585 │ 0.562 │  0.188 │  0.192 │  0.187 │  0.185 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  24 │ 0.047 │ 0.04  │ 0.049 │ 0.15  │ 0.363 │ 0.348 │ 0.357 │ 0.369 │  0.315 │  0.308 │  0.308 │  0.219 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  25 │ 0.053 │ 0.046 │ 0.051 │ 0.156 │ 0.348 │ 0.319 │ 0.327 │ 0.353 │  0.295 │  0.273 │  0.277 │  0.197 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  26 │ 0.068 │ 0.046 │ 0.056 │ 0.139 │ 0.306 │ 0.248 │ 0.255 │ 0.307 │  0.238 │  0.201 │  0.199 │  0.168 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  27 │ 0.018 │ 0.053 │ 0.021 │ 0.079 │ 0.101 │ 0.2   │ 0.099 │ 0.2   │  0.083 │  0.148 │  0.078 │  0.121 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  28 │ 0.147 │ 0.049 │ 0.081 │ 0.065 │ 0.301 │ 0.162 │ 0.235 │ 0.205 │  0.154 │  0.112 │  0.154 │  0.14  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  29 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  30 │ 0.241 │ 0.253 │ 0.38  │ 0.271 │ 0.362 │ 0.384 │ 0.569 │ 0.393 │  0.12  │  0.131 │  0.189 │  0.122 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  31 │ 0.418 │ 0.431 │ 0.449 │ 0.4   │ 0.609 │ 0.623 │ 0.628 │ 0.588 │  0.191 │  0.191 │  0.18  │  0.188 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  32 │ 0.012 │ 0.009 │ 0.013 │ 0.053 │ 0.333 │ 0.307 │ 0.319 │ 0.328 │  0.321 │  0.298 │  0.306 │  0.275 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  33 │ 0.014 │ 0.008 │ 0.01  │ 0.049 │ 0.226 │ 0.175 │ 0.182 │ 0.291 │  0.212 │  0.167 │  0.171 │  0.243 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  34 │ 0.022 │ 0.003 │ 0.006 │ 0.023 │ 0.197 │ 0.096 │ 0.101 │ 0.197 │  0.175 │  0.093 │  0.095 │  0.174 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  35 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  36 │ 0.05  │ 0.068 │ 0.14  │ 0.083 │ 0.17  │ 0.19  │ 0.289 │ 0.219 │  0.119 │  0.122 │  0.149 │  0.136 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  37 │ 0.074 │ 0.2   │ 0.145 │ 0.135 │ 0.185 │ 0.362 │ 0.266 │ 0.272 │  0.112 │  0.162 │  0.121 │  0.137 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  38 │ 0.187 │ 0.216 │ 0.285 │ 0.351 │ 0.307 │ 0.347 │ 0.415 │ 0.535 │  0.12  │  0.131 │  0.131 │  0.184 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  39 │ 0.454 │ 0.481 │ 0.519 │ 0.428 │ 0.639 │ 0.659 │ 0.69  │ 0.611 │  0.184 │  0.178 │  0.171 │  0.184 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  40 │ 0.005 │ 0.001 │ 0.002 │ 0.005 │ 0.306 │ 0.196 │ 0.205 │ 0.211 │  0.301 │  0.196 │  0.203 │  0.206 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  41 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  42 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  43 │ 0     │ 0.013 │ 0.015 │ 0.021 │ 0.016 │ 0.086 │ 0.086 │ 0.071 │  0.015 │  0.073 │  0.072 │  0.05  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  44 │ 0.033 │ 0.028 │ 0.059 │ 0.091 │ 0.124 │ 0.118 │ 0.185 │ 0.214 │  0.09  │  0.09  │  0.127 │  0.123 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  45 │ 0.136 │ 0.045 │ 0.073 │ 0.059 │ 0.273 │ 0.153 │ 0.202 │ 0.19  │  0.137 │  0.108 │  0.129 │  0.131 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  46 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  47 │ 0.413 │ 0.467 │ 0.616 │ 0.366 │ 0.517 │ 0.544 │ 0.772 │ 0.482 │  0.104 │  0.077 │  0.156 │  0.117 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  48 │ 0.001 │ 0     │ 0.001 │ 0     │ 0.289 │ 0.188 │ 0.194 │ 0.196 │  0.288 │  0.188 │  0.192 │  0.196 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  49 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  50 │ 0     │ 0     │ 0.001 │ 0     │ 0.042 │ 0.058 │ 0.058 │ 0.016 │  0.042 │  0.058 │  0.057 │  0.016 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  51 │ 0.003 │ 0     │ 0.001 │ 0.003 │ 0.048 │ 0.019 │ 0.028 │ 0.048 │  0.044 │  0.019 │  0.028 │  0.044 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  52 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  53 │ 0.067 │ 0.053 │ 0.097 │ 0.015 │ 0.251 │ 0.161 │ 0.251 │ 0.09  │  0.184 │  0.108 │  0.154 │  0.075 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  54 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  55 │ 0.519 │ 0.573 │ 0.792 │ 0.434 │ 0.588 │ 0.623 │ 0.878 │ 0.544 │  0.069 │  0.05  │  0.086 │  0.11  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  56 │ 0     │ 0     │ 0     │ 0     │ 0.28  │ 0.251 │ 0.254 │ 0.254 │  0.28  │  0.251 │  0.254 │  0.254 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  57 │ 0     │ 0     │ 0     │ 0     │ 0.159 │ 0.201 │ 0.108 │ 0.135 │  0.159 │  0.201 │  0.108 │  0.135 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  58 │ 0     │ 0     │ 0     │ 0     │ 0.127 │ 0.108 │ 0.061 │ 0.085 │  0.127 │  0.108 │  0.061 │  0.085 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  59 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  60 │ 0.003 │ 0.009 │ 0.055 │ 0.021 │ 0.079 │ 0.24  │ 0.24  │ 0.161 │  0.076 │  0.23  │  0.185 │  0.14  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  61 │ 0.04  │ 0.13  │ 0.176 │ 0.138 │ 0.322 │ 0.483 │ 0.486 │ 0.405 │  0.282 │  0.353 │  0.31  │  0.267 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  62 │ 0.043 │ 0.394 │ 0.131 │ 0.228 │ 0.404 │ 0.737 │ 0.577 │ 0.494 │  0.361 │  0.343 │  0.445 │  0.266 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  63 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╧════════╧════════╛\nAction-value function RMSE: 0.17\n\n정책:\n| 00      ^ | 01      &gt; | 02      &gt; | 03      &gt; | 04      &gt; | 05      &gt; | 06      &gt; | 07      &lt; |\n| 08      ^ | 09      ^ | 10      ^ | 11      ^ | 12      ^ | 13      ^ | 14      &gt; | 15      &gt; |\n| 16      ^ | 17      ^ | 18      &lt; |           | 20      &gt; | 21      ^ | 22      &gt; | 23      v |\n| 24      ^ | 25      ^ | 26      ^ | 27      ^ | 28      &lt; |           | 30      &gt; | 31      &gt; |\n| 32      ^ | 33      ^ | 34      ^ |           | 36      &gt; | 37      v | 38      ^ | 39      &gt; |\n| 40      ^ |           |           | 43      ^ | 44      ^ | 45      &lt; |           | 47      &gt; |\n| 48      &gt; |           | 50      &gt; | 51      ^ |           | 53      &gt; |           | 55      &gt; |\n| 56      &gt; | 57      v | 58      &lt; |           | 60      &gt; | 61      v | 62      v |           |\nReaches goal 92.00%. Obtains an average return of 0.4264. Regret of 0.0207\n\n\n\n\nSARSA(\\(\\lambda\\)) 누적\n\nQ_asls, V_asls, Q_track_asls = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_asl, V_asl, pi_asl, Q_track_asl, pi_track_asl = sarsa_lambda(env, gamma=gamma, \n                                                                   replacing_traces=False, \n                                                                   n_episodes=n_episodes)\n    Q_asls.append(Q_asl) ; V_asls.append(V_asl) ; Q_track_asls.append(Q_track_asl)\nQ_asl, V_asl, Q_track_asl = np.mean(Q_asls, axis=0), np.mean(V_asls, axis=0), np.mean(Q_track_asls, axis=0)\ndel Q_asls ; del V_asls ; del Q_track_asls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_asl, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Sarsa(λ) accumulating:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_asl - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_asl, optimal_V)))\nprint()\nprint_action_value_function(Q_asl, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Sarsa(λ) accumulating action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_asl, optimal_Q)))\nprint()\nprint_policy(pi_asl, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_asl, mean_return_asl, mean_regret_asl = get_policy_metrics(\n    env, gamma=gamma, pi=pi_asl, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_asl, mean_return_asl, mean_regret_asl))\n\nState-value function found by Sarsa(λ) accumulating:\n| 00 0.2398 | 01  0.248 | 02 0.2628 | 03 0.2829 | 04 0.3103 | 05 0.3389 | 06 0.3578 | 07 0.3628 |\n| 08 0.2364 | 09 0.2427 | 10 0.2557 | 11 0.2745 | 12 0.2989 | 13 0.3309 | 14  0.365 | 15 0.3766 |\n| 16 0.2147 | 17 0.2118 | 18 0.1967 |           | 20 0.2444 | 21 0.3088 | 22 0.3763 | 23 0.4018 |\n| 24 0.1644 | 25 0.1636 | 26 0.1449 | 27 0.0939 | 28 0.1546 |           | 30 0.3848 | 31 0.4434 |\n| 32 0.0544 | 33 0.0521 | 34 0.0354 |           | 36 0.1426 | 37 0.2024 | 38 0.3576 | 39 0.5073 |\n| 40 0.0075 |           |           | 43 0.0276 | 44 0.0905 | 45  0.123 |           | 47 0.6022 |\n| 48 0.0016 |           | 50 0.0009 | 51 0.0063 |           | 53 0.1092 |           | 55 0.7683 |\n| 56 0.0007 | 57 0.0002 | 58 0.0001 |           | 60 0.0747 | 61 0.2227 | 62 0.4008 |           |\nOptimal state-value function:\n| 00 0.4146 | 01 0.4272 | 02 0.4461 | 03 0.4683 | 04 0.4924 | 05 0.5166 | 06 0.5353 | 07  0.541 |\n| 08 0.4117 | 09 0.4212 | 10 0.4375 | 11 0.4584 | 12 0.4832 | 13 0.5135 | 14 0.5458 | 15 0.5574 |\n| 16 0.3968 | 17 0.3938 | 18 0.3755 |           | 20 0.4217 | 21 0.4938 | 22 0.5612 | 23 0.5859 |\n| 24 0.3693 | 25  0.353 | 26 0.3065 | 27 0.2004 | 28 0.3008 |           | 30  0.569 | 31 0.6283 |\n| 32 0.3327 | 33 0.2914 | 34 0.1973 |           | 36 0.2893 | 37  0.362 | 38 0.5348 | 39 0.6897 |\n| 40 0.3061 |           |           | 43 0.0863 | 44 0.2139 | 45 0.2727 |           | 47  0.772 |\n| 48 0.2889 |           | 50 0.0577 | 51 0.0475 |           | 53 0.2505 |           | 55 0.8778 |\n| 56 0.2804 | 57 0.2008 | 58 0.1273 |           | 60 0.2396 | 61 0.4864 | 62 0.7371 |           |\nState-value function errors:\n| 00  -0.17 | 01  -0.18 | 02  -0.18 | 03  -0.19 | 04  -0.18 | 05  -0.18 | 06  -0.18 | 07  -0.18 |\n| 08  -0.18 | 09  -0.18 | 10  -0.18 | 11  -0.18 | 12  -0.18 | 13  -0.18 | 14  -0.18 | 15  -0.18 |\n| 16  -0.18 | 17  -0.18 | 18  -0.18 |           | 20  -0.18 | 21  -0.19 | 22  -0.18 | 23  -0.18 |\n| 24   -0.2 | 25  -0.19 | 26  -0.16 | 27  -0.11 | 28  -0.15 |           | 30  -0.18 | 31  -0.18 |\n| 32  -0.28 | 33  -0.24 | 34  -0.16 |           | 36  -0.15 | 37  -0.16 | 38  -0.18 | 39  -0.18 |\n| 40   -0.3 |           |           | 43  -0.06 | 44  -0.12 | 45  -0.15 |           | 47  -0.17 |\n| 48  -0.29 |           | 50  -0.06 | 51  -0.04 |           | 53  -0.14 |           | 55  -0.11 |\n| 56  -0.28 | 57   -0.2 | 58  -0.13 |           | 60  -0.16 | 61  -0.26 | 62  -0.34 |           |\nState-value function RMSE: 0.1702\n\nSarsa(λ) accumulating action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╤════════╤════════╕\n│   s │     &lt; │     v │     &gt; │     ^ │   * &lt; │   * v │   * &gt; │   * ^ │   er &lt; │   er v │   er &gt; │   er ^ │\n╞═════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╪════════╪════════╡\n│   0 │ 0.234 │ 0.236 │ 0.236 │ 0.24  │ 0.41  │ 0.414 │ 0.414 │ 0.415 │  0.175 │  0.177 │  0.177 │  0.175 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   1 │ 0.238 │ 0.243 │ 0.247 │ 0.245 │ 0.417 │ 0.423 │ 0.427 │ 0.425 │  0.179 │  0.18  │  0.18  │  0.18  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   2 │ 0.249 │ 0.255 │ 0.263 │ 0.258 │ 0.433 │ 0.44  │ 0.446 │ 0.443 │  0.183 │  0.185 │  0.183 │  0.185 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   3 │ 0.266 │ 0.274 │ 0.281 │ 0.28  │ 0.453 │ 0.461 │ 0.468 │ 0.464 │  0.187 │  0.187 │  0.188 │  0.184 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   4 │ 0.288 │ 0.299 │ 0.31  │ 0.301 │ 0.477 │ 0.484 │ 0.492 │ 0.488 │  0.188 │  0.186 │  0.182 │  0.186 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   5 │ 0.315 │ 0.321 │ 0.339 │ 0.324 │ 0.502 │ 0.509 │ 0.517 │ 0.51  │  0.187 │  0.188 │  0.178 │  0.185 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   6 │ 0.345 │ 0.347 │ 0.358 │ 0.344 │ 0.527 │ 0.529 │ 0.535 │ 0.526 │  0.182 │  0.182 │  0.177 │  0.181 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   7 │ 0.359 │ 0.356 │ 0.36  │ 0.353 │ 0.539 │ 0.539 │ 0.541 │ 0.534 │  0.18  │  0.183 │  0.181 │  0.18  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   8 │ 0.22  │ 0.219 │ 0.221 │ 0.236 │ 0.404 │ 0.406 │ 0.407 │ 0.412 │  0.184 │  0.187 │  0.185 │  0.175 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   9 │ 0.225 │ 0.226 │ 0.229 │ 0.243 │ 0.407 │ 0.41  │ 0.415 │ 0.421 │  0.181 │  0.184 │  0.186 │  0.178 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  10 │ 0.225 │ 0.226 │ 0.232 │ 0.256 │ 0.41  │ 0.414 │ 0.422 │ 0.437 │  0.185 │  0.188 │  0.19  │  0.182 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  11 │ 0.178 │ 0.182 │ 0.188 │ 0.275 │ 0.299 │ 0.304 │ 0.314 │ 0.458 │  0.121 │  0.122 │  0.126 │  0.184 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  12 │ 0.26  │ 0.269 │ 0.281 │ 0.299 │ 0.453 │ 0.46  │ 0.471 │ 0.483 │  0.193 │  0.191 │  0.19  │  0.184 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  13 │ 0.301 │ 0.309 │ 0.324 │ 0.326 │ 0.493 │ 0.503 │ 0.514 │ 0.51  │  0.192 │  0.194 │  0.189 │  0.184 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  14 │ 0.348 │ 0.355 │ 0.365 │ 0.348 │ 0.531 │ 0.539 │ 0.546 │ 0.53  │  0.183 │  0.183 │  0.181 │  0.182 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  15 │ 0.369 │ 0.375 │ 0.372 │ 0.361 │ 0.552 │ 0.557 │ 0.556 │ 0.543 │  0.183 │  0.183 │  0.184 │  0.182 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  16 │ 0.128 │ 0.124 │ 0.13  │ 0.215 │ 0.389 │ 0.383 │ 0.388 │ 0.397 │  0.261 │  0.259 │  0.258 │  0.182 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  17 │ 0.133 │ 0.118 │ 0.127 │ 0.212 │ 0.386 │ 0.371 │ 0.379 │ 0.394 │  0.253 │  0.253 │  0.252 │  0.182 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  18 │ 0.197 │ 0.069 │ 0.086 │ 0.1   │ 0.375 │ 0.231 │ 0.246 │ 0.274 │  0.179 │  0.162 │  0.159 │  0.175 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  19 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  20 │ 0.115 │ 0.109 │ 0.244 │ 0.15  │ 0.259 │ 0.262 │ 0.422 │ 0.322 │  0.144 │  0.153 │  0.177 │  0.172 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  21 │ 0.153 │ 0.168 │ 0.194 │ 0.309 │ 0.309 │ 0.324 │ 0.355 │ 0.494 │  0.155 │  0.156 │  0.16  │  0.185 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  22 │ 0.345 │ 0.351 │ 0.376 │ 0.351 │ 0.531 │ 0.544 │ 0.561 │ 0.536 │  0.186 │  0.193 │  0.185 │  0.185 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  23 │ 0.393 │ 0.397 │ 0.4   │ 0.381 │ 0.576 │ 0.586 │ 0.585 │ 0.562 │  0.184 │  0.189 │  0.184 │  0.182 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  24 │ 0.053 │ 0.044 │ 0.052 │ 0.164 │ 0.363 │ 0.348 │ 0.357 │ 0.369 │  0.31  │  0.304 │  0.305 │  0.205 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  25 │ 0.059 │ 0.048 │ 0.053 │ 0.164 │ 0.348 │ 0.319 │ 0.327 │ 0.353 │  0.289 │  0.272 │  0.274 │  0.189 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  26 │ 0.066 │ 0.05  │ 0.053 │ 0.145 │ 0.306 │ 0.248 │ 0.255 │ 0.307 │  0.239 │  0.198 │  0.202 │  0.162 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  27 │ 0.023 │ 0.085 │ 0.025 │ 0.051 │ 0.101 │ 0.2   │ 0.099 │ 0.2   │  0.079 │  0.116 │  0.074 │  0.149 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  28 │ 0.155 │ 0.046 │ 0.085 │ 0.073 │ 0.301 │ 0.162 │ 0.235 │ 0.205 │  0.146 │  0.115 │  0.15  │  0.133 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  29 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  30 │ 0.237 │ 0.26  │ 0.385 │ 0.266 │ 0.362 │ 0.384 │ 0.569 │ 0.393 │  0.125 │  0.124 │  0.184 │  0.127 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  31 │ 0.419 │ 0.429 │ 0.443 │ 0.402 │ 0.609 │ 0.623 │ 0.628 │ 0.588 │  0.189 │  0.194 │  0.185 │  0.186 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  32 │ 0.015 │ 0.01  │ 0.015 │ 0.054 │ 0.333 │ 0.307 │ 0.319 │ 0.328 │  0.318 │  0.297 │  0.304 │  0.273 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  33 │ 0.012 │ 0.008 │ 0.011 │ 0.052 │ 0.226 │ 0.175 │ 0.182 │ 0.291 │  0.214 │  0.167 │  0.17  │  0.239 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  34 │ 0.031 │ 0.004 │ 0.006 │ 0.015 │ 0.197 │ 0.096 │ 0.101 │ 0.197 │  0.166 │  0.092 │  0.095 │  0.182 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  35 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  36 │ 0.054 │ 0.067 │ 0.143 │ 0.083 │ 0.17  │ 0.19  │ 0.289 │ 0.219 │  0.116 │  0.123 │  0.147 │  0.136 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  37 │ 0.071 │ 0.202 │ 0.136 │ 0.14  │ 0.185 │ 0.362 │ 0.266 │ 0.272 │  0.114 │  0.16  │  0.131 │  0.132 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  38 │ 0.177 │ 0.199 │ 0.287 │ 0.358 │ 0.307 │ 0.347 │ 0.415 │ 0.535 │  0.13  │  0.148 │  0.128 │  0.177 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  39 │ 0.462 │ 0.471 │ 0.507 │ 0.431 │ 0.639 │ 0.659 │ 0.69  │ 0.611 │  0.177 │  0.188 │  0.182 │  0.181 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  40 │ 0.005 │ 0.001 │ 0.002 │ 0.005 │ 0.306 │ 0.196 │ 0.205 │ 0.211 │  0.301 │  0.195 │  0.203 │  0.206 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  41 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  42 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  43 │ 0     │ 0.012 │ 0.022 │ 0.01  │ 0.016 │ 0.086 │ 0.086 │ 0.071 │  0.015 │  0.074 │  0.064 │  0.061 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  44 │ 0.029 │ 0.029 │ 0.056 │ 0.085 │ 0.124 │ 0.118 │ 0.185 │ 0.214 │  0.095 │  0.09  │  0.13  │  0.129 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  45 │ 0.119 │ 0.041 │ 0.079 │ 0.065 │ 0.273 │ 0.153 │ 0.202 │ 0.19  │  0.153 │  0.113 │  0.123 │  0.125 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  46 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  47 │ 0.419 │ 0.451 │ 0.602 │ 0.359 │ 0.517 │ 0.544 │ 0.772 │ 0.482 │  0.098 │  0.094 │  0.17  │  0.123 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  48 │ 0.001 │ 0     │ 0.001 │ 0.001 │ 0.289 │ 0.188 │ 0.194 │ 0.196 │  0.288 │  0.188 │  0.193 │  0.196 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  49 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  50 │ 0     │ 0     │ 0     │ 0     │ 0.042 │ 0.058 │ 0.058 │ 0.016 │  0.042 │  0.058 │  0.057 │  0.015 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  51 │ 0.003 │ 0     │ 0.003 │ 0.003 │ 0.048 │ 0.019 │ 0.028 │ 0.048 │  0.045 │  0.019 │  0.026 │  0.045 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  52 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  53 │ 0.077 │ 0.04  │ 0.083 │ 0.02  │ 0.251 │ 0.161 │ 0.251 │ 0.09  │  0.174 │  0.121 │  0.168 │  0.07  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  54 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  55 │ 0.524 │ 0.57  │ 0.768 │ 0.459 │ 0.588 │ 0.623 │ 0.878 │ 0.544 │  0.064 │  0.053 │  0.109 │  0.085 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  56 │ 0.001 │ 0     │ 0     │ 0     │ 0.28  │ 0.251 │ 0.254 │ 0.254 │  0.28  │  0.251 │  0.254 │  0.254 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  57 │ 0     │ 0     │ 0     │ 0     │ 0.159 │ 0.201 │ 0.108 │ 0.135 │  0.159 │  0.201 │  0.108 │  0.135 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  58 │ 0     │ 0     │ 0     │ 0     │ 0.127 │ 0.108 │ 0.061 │ 0.085 │  0.127 │  0.108 │  0.061 │  0.085 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  59 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  60 │ 0.007 │ 0.05  │ 0.049 │ 0.013 │ 0.079 │ 0.24  │ 0.24  │ 0.161 │  0.073 │  0.19  │  0.191 │  0.148 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  61 │ 0.046 │ 0.115 │ 0.159 │ 0.108 │ 0.322 │ 0.483 │ 0.486 │ 0.405 │  0.276 │  0.368 │  0.328 │  0.297 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  62 │ 0.056 │ 0.208 │ 0.258 │ 0.133 │ 0.404 │ 0.737 │ 0.577 │ 0.494 │  0.348 │  0.529 │  0.319 │  0.361 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  63 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╧════════╧════════╛\nAction-value function RMSE: 0.171\n\n정책:\n| 00      ^ | 01      ^ | 02      &gt; | 03      ^ | 04      &gt; | 05      &gt; | 06      &gt; | 07      &lt; |\n| 08      ^ | 09      ^ | 10      ^ | 11      ^ | 12      ^ | 13      &gt; | 14      &gt; | 15      v |\n| 16      ^ | 17      ^ | 18      &lt; |           | 20      &gt; | 21      ^ | 22      &gt; | 23      &gt; |\n| 24      ^ | 25      ^ | 26      ^ | 27      v | 28      &lt; |           | 30      &gt; | 31      &gt; |\n| 32      ^ | 33      ^ | 34      &lt; |           | 36      &gt; | 37      v | 38      ^ | 39      &gt; |\n| 40      &lt; |           |           | 43      &gt; | 44      ^ | 45      &lt; |           | 47      &gt; |\n| 48      &lt; |           | 50      v | 51      &gt; |           | 53      &lt; |           | 55      &gt; |\n| 56      &gt; | 57      &lt; | 58      ^ |           | 60      v | 61      v | 62      &gt; |           |\nReaches goal 85.00%. Obtains an average return of 0.3755. Regret of 0.0430\n\n\n\n\n왓킨스 Q(\\(\\lambda\\)) 대체\n\nQ_rqlls, V_rqlls, Q_track_rqlls = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_rqll, V_rqll, pi_rqll, Q_track_rqll, pi_track_rqll = q_lambda(env, gamma=gamma, n_episodes=n_episodes)\n    Q_rqlls.append(Q_rqll) ; V_rqlls.append(V_rqll) ; Q_track_rqlls.append(Q_track_rqll)\nQ_rqll, V_rqll, Q_track_rqll = np.mean(Q_rqlls, axis=0), np.mean(V_rqlls, axis=0), np.mean(Q_track_rqlls, axis=0)\ndel Q_rqlls ; del V_rqlls ; del Q_track_rqlls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_rqll, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Q(λ) replacing:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_rqll - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_rqll, optimal_V)))\nprint()\nprint_action_value_function(Q_rqll, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Q(λ) replacing action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_rqll, optimal_Q)))\nprint()\nprint_policy(pi_rqll, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_rqll, mean_return_rqll, mean_regret_rqll = get_policy_metrics(\n    env, gamma=gamma, pi=pi_rqll, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_rqll, mean_return_rqll, mean_regret_rqll))\n\nState-value function found by Q(λ) replacing:\n| 00 0.3966 | 01 0.4085 | 02  0.428 | 03 0.4494 | 04 0.4714 | 05 0.4951 | 06 0.5117 | 07 0.5166 |\n| 08 0.3931 | 09 0.4027 | 10 0.4195 | 11 0.4402 | 12  0.463 | 13 0.4898 | 14 0.5203 | 15 0.5309 |\n| 16 0.3733 | 17 0.3721 | 18 0.3533 |           | 20 0.3995 | 21 0.4694 | 22  0.534 | 23 0.5577 |\n| 24 0.3414 | 25 0.3258 | 26 0.2845 | 27 0.1791 | 28 0.2781 |           | 30 0.5409 | 31 0.5953 |\n| 32 0.2965 | 33 0.2623 | 34 0.1737 |           | 36 0.2607 | 37 0.3286 | 38 0.5083 | 39 0.6558 |\n| 40 0.2546 |           |           | 43 0.0707 | 44 0.1817 | 45 0.2299 |           | 47 0.7506 |\n| 48 0.2161 |           | 50 0.0149 | 51 0.0303 |           | 53 0.1888 |           | 55 0.8732 |\n| 56  0.194 | 57 0.0796 | 58  0.034 |           | 60  0.098 | 61 0.3324 | 62  0.509 |           |\nOptimal state-value function:\n| 00 0.4146 | 01 0.4272 | 02 0.4461 | 03 0.4683 | 04 0.4924 | 05 0.5166 | 06 0.5353 | 07  0.541 |\n| 08 0.4117 | 09 0.4212 | 10 0.4375 | 11 0.4584 | 12 0.4832 | 13 0.5135 | 14 0.5458 | 15 0.5574 |\n| 16 0.3968 | 17 0.3938 | 18 0.3755 |           | 20 0.4217 | 21 0.4938 | 22 0.5612 | 23 0.5859 |\n| 24 0.3693 | 25  0.353 | 26 0.3065 | 27 0.2004 | 28 0.3008 |           | 30  0.569 | 31 0.6283 |\n| 32 0.3327 | 33 0.2914 | 34 0.1973 |           | 36 0.2893 | 37  0.362 | 38 0.5348 | 39 0.6897 |\n| 40 0.3061 |           |           | 43 0.0863 | 44 0.2139 | 45 0.2727 |           | 47  0.772 |\n| 48 0.2889 |           | 50 0.0577 | 51 0.0475 |           | 53 0.2505 |           | 55 0.8778 |\n| 56 0.2804 | 57 0.2008 | 58 0.1273 |           | 60 0.2396 | 61 0.4864 | 62 0.7371 |           |\nState-value function errors:\n| 00  -0.02 | 01  -0.02 | 02  -0.02 | 03  -0.02 | 04  -0.02 | 05  -0.02 | 06  -0.02 | 07  -0.02 |\n| 08  -0.02 | 09  -0.02 | 10  -0.02 | 11  -0.02 | 12  -0.02 | 13  -0.02 | 14  -0.03 | 15  -0.03 |\n| 16  -0.02 | 17  -0.02 | 18  -0.02 |           | 20  -0.02 | 21  -0.02 | 22  -0.03 | 23  -0.03 |\n| 24  -0.03 | 25  -0.03 | 26  -0.02 | 27  -0.02 | 28  -0.02 |           | 30  -0.03 | 31  -0.03 |\n| 32  -0.04 | 33  -0.03 | 34  -0.02 |           | 36  -0.03 | 37  -0.03 | 38  -0.03 | 39  -0.03 |\n| 40  -0.05 |           |           | 43  -0.02 | 44  -0.03 | 45  -0.04 |           | 47  -0.02 |\n| 48  -0.07 |           | 50  -0.04 | 51  -0.02 |           | 53  -0.06 |           | 55   -0.0 |\n| 56  -0.09 | 57  -0.12 | 58  -0.09 |           | 60  -0.14 | 61  -0.15 | 62  -0.23 |           |\nState-value function RMSE: 0.051\n\nQ(λ) replacing action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╤════════╤════════╕\n│   s │     &lt; │     v │     &gt; │     ^ │   * &lt; │   * v │   * &gt; │   * ^ │   er &lt; │   er v │   er &gt; │   er ^ │\n╞═════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╪════════╪════════╡\n│   0 │ 0.387 │ 0.39  │ 0.393 │ 0.394 │ 0.41  │ 0.414 │ 0.414 │ 0.415 │  0.023 │  0.024 │  0.021 │  0.021 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   1 │ 0.393 │ 0.399 │ 0.408 │ 0.401 │ 0.417 │ 0.423 │ 0.427 │ 0.425 │  0.023 │  0.024 │  0.019 │  0.024 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   2 │ 0.409 │ 0.415 │ 0.428 │ 0.418 │ 0.433 │ 0.44  │ 0.446 │ 0.443 │  0.023 │  0.025 │  0.018 │  0.025 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   3 │ 0.428 │ 0.434 │ 0.449 │ 0.438 │ 0.453 │ 0.461 │ 0.468 │ 0.464 │  0.025 │  0.027 │  0.019 │  0.027 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   4 │ 0.45  │ 0.457 │ 0.471 │ 0.458 │ 0.477 │ 0.484 │ 0.492 │ 0.488 │  0.027 │  0.027 │  0.021 │  0.029 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   5 │ 0.475 │ 0.478 │ 0.495 │ 0.48  │ 0.502 │ 0.509 │ 0.517 │ 0.51  │  0.027 │  0.03  │  0.021 │  0.029 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   6 │ 0.501 │ 0.502 │ 0.512 │ 0.499 │ 0.527 │ 0.529 │ 0.535 │ 0.526 │  0.026 │  0.027 │  0.024 │  0.026 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   7 │ 0.51  │ 0.511 │ 0.516 │ 0.508 │ 0.539 │ 0.539 │ 0.541 │ 0.534 │  0.029 │  0.029 │  0.025 │  0.026 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   8 │ 0.376 │ 0.378 │ 0.378 │ 0.393 │ 0.404 │ 0.406 │ 0.407 │ 0.412 │  0.028 │  0.028 │  0.029 │  0.019 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   9 │ 0.383 │ 0.386 │ 0.39  │ 0.403 │ 0.407 │ 0.41  │ 0.415 │ 0.421 │  0.024 │  0.024 │  0.025 │  0.019 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  10 │ 0.388 │ 0.39  │ 0.397 │ 0.419 │ 0.41  │ 0.414 │ 0.422 │ 0.437 │  0.022 │  0.024 │  0.025 │  0.018 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  11 │ 0.277 │ 0.279 │ 0.294 │ 0.44  │ 0.299 │ 0.304 │ 0.314 │ 0.458 │  0.022 │  0.024 │  0.02  │  0.018 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  12 │ 0.429 │ 0.432 │ 0.441 │ 0.463 │ 0.453 │ 0.46  │ 0.471 │ 0.483 │  0.024 │  0.028 │  0.03  │  0.02  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  13 │ 0.465 │ 0.472 │ 0.485 │ 0.483 │ 0.493 │ 0.503 │ 0.514 │ 0.51  │  0.028 │  0.031 │  0.029 │  0.027 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  14 │ 0.507 │ 0.512 │ 0.52  │ 0.506 │ 0.531 │ 0.539 │ 0.546 │ 0.53  │  0.025 │  0.027 │  0.025 │  0.024 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  15 │ 0.522 │ 0.531 │ 0.525 │ 0.518 │ 0.552 │ 0.557 │ 0.556 │ 0.543 │  0.029 │  0.027 │  0.031 │  0.024 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  16 │ 0.359 │ 0.356 │ 0.359 │ 0.373 │ 0.389 │ 0.383 │ 0.388 │ 0.397 │  0.029 │  0.026 │  0.029 │  0.023 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  17 │ 0.357 │ 0.349 │ 0.354 │ 0.372 │ 0.386 │ 0.371 │ 0.379 │ 0.394 │  0.029 │  0.022 │  0.025 │  0.022 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  18 │ 0.353 │ 0.212 │ 0.236 │ 0.252 │ 0.375 │ 0.231 │ 0.246 │ 0.274 │  0.022 │  0.019 │  0.009 │  0.023 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  19 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  20 │ 0.25  │ 0.249 │ 0.399 │ 0.304 │ 0.259 │ 0.262 │ 0.422 │ 0.322 │  0.008 │  0.013 │  0.022 │  0.018 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  21 │ 0.301 │ 0.323 │ 0.33  │ 0.469 │ 0.309 │ 0.324 │ 0.355 │ 0.494 │  0.007 │  0.001 │  0.024 │  0.024 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  22 │ 0.506 │ 0.521 │ 0.534 │ 0.513 │ 0.531 │ 0.544 │ 0.561 │ 0.536 │  0.025 │  0.023 │  0.027 │  0.023 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  23 │ 0.552 │ 0.558 │ 0.553 │ 0.541 │ 0.576 │ 0.586 │ 0.585 │ 0.562 │  0.025 │  0.028 │  0.031 │  0.022 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  24 │ 0.329 │ 0.322 │ 0.327 │ 0.341 │ 0.363 │ 0.348 │ 0.357 │ 0.369 │  0.034 │  0.026 │  0.03  │  0.029 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  25 │ 0.317 │ 0.296 │ 0.302 │ 0.325 │ 0.348 │ 0.319 │ 0.327 │ 0.353 │  0.031 │  0.023 │  0.025 │  0.028 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  26 │ 0.277 │ 0.232 │ 0.242 │ 0.283 │ 0.306 │ 0.248 │ 0.255 │ 0.307 │  0.028 │  0.016 │  0.013 │  0.024 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  27 │ 0.089 │ 0.175 │ 0.084 │ 0.171 │ 0.101 │ 0.2   │ 0.099 │ 0.2   │  0.012 │  0.025 │  0.015 │  0.029 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  28 │ 0.278 │ 0.151 │ 0.209 │ 0.193 │ 0.301 │ 0.162 │ 0.235 │ 0.205 │  0.023 │  0.01  │  0.025 │  0.012 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  29 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  30 │ 0.339 │ 0.352 │ 0.541 │ 0.371 │ 0.362 │ 0.384 │ 0.569 │ 0.393 │  0.023 │  0.032 │  0.028 │  0.022 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  31 │ 0.584 │ 0.589 │ 0.595 │ 0.568 │ 0.609 │ 0.623 │ 0.628 │ 0.588 │  0.025 │  0.034 │  0.033 │  0.02  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  32 │ 0.294 │ 0.274 │ 0.282 │ 0.292 │ 0.333 │ 0.307 │ 0.319 │ 0.328 │  0.039 │  0.033 │  0.037 │  0.036 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  33 │ 0.209 │ 0.154 │ 0.175 │ 0.262 │ 0.226 │ 0.175 │ 0.182 │ 0.291 │  0.018 │  0.021 │  0.007 │  0.029 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  34 │ 0.168 │ 0.088 │ 0.092 │ 0.162 │ 0.197 │ 0.096 │ 0.101 │ 0.197 │  0.029 │  0.008 │  0.009 │  0.035 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  35 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  36 │ 0.156 │ 0.166 │ 0.261 │ 0.199 │ 0.17  │ 0.19  │ 0.289 │ 0.219 │  0.014 │  0.024 │  0.029 │  0.02  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  37 │ 0.165 │ 0.329 │ 0.244 │ 0.248 │ 0.185 │ 0.362 │ 0.266 │ 0.272 │  0.021 │  0.033 │  0.023 │  0.024 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  38 │ 0.291 │ 0.324 │ 0.401 │ 0.508 │ 0.307 │ 0.347 │ 0.415 │ 0.535 │  0.017 │  0.023 │  0.014 │  0.027 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  39 │ 0.626 │ 0.637 │ 0.656 │ 0.589 │ 0.639 │ 0.659 │ 0.69  │ 0.611 │  0.012 │  0.022 │  0.034 │  0.023 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  40 │ 0.255 │ 0.131 │ 0.163 │ 0.197 │ 0.306 │ 0.196 │ 0.205 │ 0.211 │  0.052 │  0.065 │  0.042 │  0.014 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  41 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  42 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  43 │ 0.004 │ 0.065 │ 0.046 │ 0.042 │ 0.016 │ 0.086 │ 0.086 │ 0.071 │  0.012 │  0.022 │  0.04  │  0.029 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  44 │ 0.111 │ 0.096 │ 0.154 │ 0.182 │ 0.124 │ 0.118 │ 0.185 │ 0.214 │  0.013 │  0.023 │  0.031 │  0.032 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  45 │ 0.23  │ 0.118 │ 0.16  │ 0.163 │ 0.273 │ 0.153 │ 0.202 │ 0.19  │  0.043 │  0.035 │  0.042 │  0.027 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  46 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  47 │ 0.492 │ 0.552 │ 0.751 │ 0.476 │ 0.517 │ 0.544 │ 0.772 │ 0.482 │  0.025 │ -0.008 │  0.021 │  0.006 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  48 │ 0.216 │ 0.1   │ 0.126 │ 0.133 │ 0.289 │ 0.188 │ 0.194 │ 0.196 │  0.073 │  0.088 │  0.068 │  0.063 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  49 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  50 │ 0.003 │ 0.009 │ 0.008 │ 0.001 │ 0.042 │ 0.058 │ 0.058 │ 0.016 │  0.039 │  0.049 │  0.05  │  0.015 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  51 │ 0.026 │ 0.001 │ 0.011 │ 0.008 │ 0.048 │ 0.019 │ 0.028 │ 0.048 │  0.022 │  0.018 │  0.018 │  0.039 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  52 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  53 │ 0.149 │ 0.068 │ 0.163 │ 0.057 │ 0.251 │ 0.161 │ 0.251 │ 0.09  │  0.101 │  0.092 │  0.087 │  0.033 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  54 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  55 │ 0.605 │ 0.594 │ 0.873 │ 0.556 │ 0.588 │ 0.623 │ 0.878 │ 0.544 │ -0.017 │  0.029 │  0.005 │ -0.011 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  56 │ 0.194 │ 0.109 │ 0.123 │ 0.125 │ 0.28  │ 0.251 │ 0.254 │ 0.254 │  0.086 │  0.143 │  0.131 │  0.129 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  57 │ 0.054 │ 0.064 │ 0.017 │ 0.035 │ 0.159 │ 0.201 │ 0.108 │ 0.135 │  0.105 │  0.137 │  0.092 │  0.1   │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  58 │ 0.033 │ 0.014 │ 0.004 │ 0.009 │ 0.127 │ 0.108 │ 0.061 │ 0.085 │  0.095 │  0.095 │  0.057 │  0.077 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  59 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  60 │ 0.009 │ 0.044 │ 0.082 │ 0.022 │ 0.079 │ 0.24  │ 0.24  │ 0.161 │  0.07  │  0.195 │  0.158 │  0.139 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  61 │ 0.097 │ 0.195 │ 0.286 │ 0.144 │ 0.322 │ 0.483 │ 0.486 │ 0.405 │  0.225 │  0.288 │  0.201 │  0.261 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  62 │ 0.078 │ 0.28  │ 0.296 │ 0.229 │ 0.404 │ 0.737 │ 0.577 │ 0.494 │  0.326 │  0.457 │  0.281 │  0.264 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  63 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╧════════╧════════╛\nAction-value function RMSE: 0.0656\n\n정책:\n| 00      ^ | 01      &gt; | 02      &gt; | 03      &gt; | 04      &gt; | 05      &gt; | 06      &gt; | 07      &gt; |\n| 08      ^ | 09      ^ | 10      ^ | 11      ^ | 12      ^ | 13      &gt; | 14      &gt; | 15      v |\n| 16      ^ | 17      ^ | 18      &lt; |           | 20      &gt; | 21      ^ | 22      &gt; | 23      v |\n| 24      ^ | 25      ^ | 26      ^ | 27      v | 28      &lt; |           | 30      &gt; | 31      &gt; |\n| 32      ^ | 33      ^ | 34      ^ |           | 36      &gt; | 37      v | 38      ^ | 39      &gt; |\n| 40      &lt; |           |           | 43      &gt; | 44      ^ | 45      &lt; |           | 47      &gt; |\n| 48      &lt; |           | 50      &lt; | 51      &lt; |           | 53      &gt; |           | 55      &gt; |\n| 56      &lt; | 57      &lt; | 58      v |           | 60      &gt; | 61      v | 62      v |           |\nReaches goal 82.00%. Obtains an average return of 0.4156. Regret of 0.0032\n\n\n\n\n왓킨스 Q(\\(\\lambda\\)) 누적\n\nQ_aqlls, V_aqlls, Q_track_aqlls = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_aqll, V_aqll, pi_aqll, Q_track_aqll, pi_track_aqll = q_lambda(env, gamma=gamma, \n                                                                    replacing_traces=False,\n                                                                    n_episodes=n_episodes)\n    Q_aqlls.append(Q_aqll) ; V_aqlls.append(V_aqll) ; Q_track_aqlls.append(Q_track_aqll)\nQ_aqll, V_aqll, Q_track_aqll = np.mean(Q_aqlls, axis=0), np.mean(V_aqlls, axis=0), np.mean(Q_track_aqlls, axis=0)\ndel Q_aqlls ; del V_aqlls ; del Q_track_aqlls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_aqll, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Q(λ) accumulating:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_aqll - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_aqll, optimal_V)))\nprint()\nprint_action_value_function(Q_aqll, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Q(λ) accumulating action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_aqll, optimal_Q)))\nprint()\nprint_policy(pi_aqll, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_aqll, mean_return_aqll, mean_regret_aqll = get_policy_metrics(\n    env, gamma=gamma, pi=pi_aqll, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_aqll, mean_return_aqll, mean_regret_aqll))\n\nState-value function found by Q(λ) accumulating:\n| 00 0.3924 | 01 0.4057 | 02 0.4252 | 03 0.4471 | 04 0.4706 | 05 0.4929 | 06 0.5099 | 07 0.5143 |\n| 08 0.3896 | 09 0.3995 | 10 0.4173 | 11 0.4381 | 12 0.4627 | 13 0.4907 | 14 0.5207 | 15 0.5321 |\n| 16 0.3708 | 17 0.3686 | 18 0.3491 |           | 20 0.3985 | 21 0.4725 | 22 0.5346 | 23 0.5601 |\n| 24 0.3436 | 25 0.3289 | 26 0.2823 | 27 0.1806 | 28 0.2756 |           | 30 0.5408 | 31 0.6102 |\n| 32 0.2998 | 33 0.2637 | 34 0.1776 |           | 36 0.2505 | 37  0.313 | 38 0.5038 | 39 0.6802 |\n| 40 0.2487 |           |           | 43  0.061 | 44 0.1707 | 45 0.2132 |           | 47 0.7706 |\n| 48 0.2174 |           | 50 0.0083 | 51 0.0241 |           | 53 0.1686 |           | 55 0.8835 |\n| 56 0.2001 | 57 0.0772 | 58  0.022 |           | 60 0.0605 | 61 0.3062 | 62 0.4714 |           |\nOptimal state-value function:\n| 00 0.4146 | 01 0.4272 | 02 0.4461 | 03 0.4683 | 04 0.4924 | 05 0.5166 | 06 0.5353 | 07  0.541 |\n| 08 0.4117 | 09 0.4212 | 10 0.4375 | 11 0.4584 | 12 0.4832 | 13 0.5135 | 14 0.5458 | 15 0.5574 |\n| 16 0.3968 | 17 0.3938 | 18 0.3755 |           | 20 0.4217 | 21 0.4938 | 22 0.5612 | 23 0.5859 |\n| 24 0.3693 | 25  0.353 | 26 0.3065 | 27 0.2004 | 28 0.3008 |           | 30  0.569 | 31 0.6283 |\n| 32 0.3327 | 33 0.2914 | 34 0.1973 |           | 36 0.2893 | 37  0.362 | 38 0.5348 | 39 0.6897 |\n| 40 0.3061 |           |           | 43 0.0863 | 44 0.2139 | 45 0.2727 |           | 47  0.772 |\n| 48 0.2889 |           | 50 0.0577 | 51 0.0475 |           | 53 0.2505 |           | 55 0.8778 |\n| 56 0.2804 | 57 0.2008 | 58 0.1273 |           | 60 0.2396 | 61 0.4864 | 62 0.7371 |           |\nState-value function errors:\n| 00  -0.02 | 01  -0.02 | 02  -0.02 | 03  -0.02 | 04  -0.02 | 05  -0.02 | 06  -0.03 | 07  -0.03 |\n| 08  -0.02 | 09  -0.02 | 10  -0.02 | 11  -0.02 | 12  -0.02 | 13  -0.02 | 14  -0.03 | 15  -0.03 |\n| 16  -0.03 | 17  -0.03 | 18  -0.03 |           | 20  -0.02 | 21  -0.02 | 22  -0.03 | 23  -0.03 |\n| 24  -0.03 | 25  -0.02 | 26  -0.02 | 27  -0.02 | 28  -0.03 |           | 30  -0.03 | 31  -0.02 |\n| 32  -0.03 | 33  -0.03 | 34  -0.02 |           | 36  -0.04 | 37  -0.05 | 38  -0.03 | 39  -0.01 |\n| 40  -0.06 |           |           | 43  -0.03 | 44  -0.04 | 45  -0.06 |           | 47   -0.0 |\n| 48  -0.07 |           | 50  -0.05 | 51  -0.02 |           | 53  -0.08 |           | 55   0.01 |\n| 56  -0.08 | 57  -0.12 | 58  -0.11 |           | 60  -0.18 | 61  -0.18 | 62  -0.27 |           |\nState-value function RMSE: 0.0581\n\nQ(λ) accumulating action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╤════════╤════════╕\n│   s │     &lt; │     v │     &gt; │     ^ │   * &lt; │   * v │   * &gt; │   * ^ │   er &lt; │   er v │   er &gt; │   er ^ │\n╞═════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╪════════╪════════╡\n│   0 │ 0.383 │ 0.389 │ 0.386 │ 0.39  │ 0.41  │ 0.414 │ 0.414 │ 0.415 │  0.026 │  0.024 │  0.027 │  0.025 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   1 │ 0.391 │ 0.396 │ 0.406 │ 0.398 │ 0.417 │ 0.423 │ 0.427 │ 0.425 │  0.026 │  0.027 │  0.021 │  0.027 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   2 │ 0.407 │ 0.412 │ 0.425 │ 0.413 │ 0.433 │ 0.44  │ 0.446 │ 0.443 │  0.026 │  0.028 │  0.021 │  0.03  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   3 │ 0.427 │ 0.432 │ 0.447 │ 0.433 │ 0.453 │ 0.461 │ 0.468 │ 0.464 │  0.026 │  0.029 │  0.021 │  0.031 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   4 │ 0.448 │ 0.454 │ 0.471 │ 0.456 │ 0.477 │ 0.484 │ 0.492 │ 0.488 │  0.028 │  0.031 │  0.022 │  0.032 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   5 │ 0.473 │ 0.475 │ 0.493 │ 0.478 │ 0.502 │ 0.509 │ 0.517 │ 0.51  │  0.03  │  0.034 │  0.024 │  0.031 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   6 │ 0.499 │ 0.499 │ 0.51  │ 0.496 │ 0.527 │ 0.529 │ 0.535 │ 0.526 │  0.029 │  0.03  │  0.025 │  0.029 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   7 │ 0.506 │ 0.512 │ 0.503 │ 0.501 │ 0.539 │ 0.539 │ 0.541 │ 0.534 │  0.033 │  0.027 │  0.038 │  0.032 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   8 │ 0.376 │ 0.377 │ 0.378 │ 0.39  │ 0.404 │ 0.406 │ 0.407 │ 0.412 │  0.028 │  0.029 │  0.029 │  0.022 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   9 │ 0.382 │ 0.384 │ 0.388 │ 0.399 │ 0.407 │ 0.41  │ 0.415 │ 0.421 │  0.025 │  0.026 │  0.028 │  0.022 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  10 │ 0.384 │ 0.389 │ 0.393 │ 0.417 │ 0.41  │ 0.414 │ 0.422 │ 0.437 │  0.026 │  0.025 │  0.029 │  0.02  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  11 │ 0.281 │ 0.29  │ 0.294 │ 0.438 │ 0.299 │ 0.304 │ 0.314 │ 0.458 │  0.017 │  0.014 │  0.02  │  0.02  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  12 │ 0.425 │ 0.432 │ 0.439 │ 0.463 │ 0.453 │ 0.46  │ 0.471 │ 0.483 │  0.028 │  0.027 │  0.033 │  0.021 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  13 │ 0.466 │ 0.472 │ 0.491 │ 0.474 │ 0.493 │ 0.503 │ 0.514 │ 0.51  │  0.027 │  0.03  │  0.023 │  0.036 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  14 │ 0.504 │ 0.511 │ 0.521 │ 0.501 │ 0.531 │ 0.539 │ 0.546 │ 0.53  │  0.027 │  0.028 │  0.025 │  0.029 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  15 │ 0.52  │ 0.531 │ 0.524 │ 0.513 │ 0.552 │ 0.557 │ 0.556 │ 0.543 │  0.031 │  0.026 │  0.032 │  0.029 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  16 │ 0.36  │ 0.356 │ 0.359 │ 0.371 │ 0.389 │ 0.383 │ 0.388 │ 0.397 │  0.029 │  0.027 │  0.029 │  0.026 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  17 │ 0.356 │ 0.347 │ 0.355 │ 0.369 │ 0.386 │ 0.371 │ 0.379 │ 0.394 │  0.031 │  0.024 │  0.024 │  0.025 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  18 │ 0.349 │ 0.206 │ 0.229 │ 0.266 │ 0.375 │ 0.231 │ 0.246 │ 0.274 │  0.026 │  0.025 │  0.017 │  0.008 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  19 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  20 │ 0.242 │ 0.247 │ 0.398 │ 0.304 │ 0.259 │ 0.262 │ 0.422 │ 0.322 │  0.016 │  0.015 │  0.023 │  0.018 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  21 │ 0.287 │ 0.31  │ 0.336 │ 0.473 │ 0.309 │ 0.324 │ 0.355 │ 0.494 │  0.021 │  0.014 │  0.019 │  0.021 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  22 │ 0.505 │ 0.517 │ 0.535 │ 0.513 │ 0.531 │ 0.544 │ 0.561 │ 0.536 │  0.025 │  0.027 │  0.027 │  0.024 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  23 │ 0.549 │ 0.56  │ 0.552 │ 0.539 │ 0.576 │ 0.586 │ 0.585 │ 0.562 │  0.028 │  0.026 │  0.032 │  0.023 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  24 │ 0.329 │ 0.319 │ 0.328 │ 0.344 │ 0.363 │ 0.348 │ 0.357 │ 0.369 │  0.033 │  0.029 │  0.029 │  0.026 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  25 │ 0.317 │ 0.289 │ 0.3   │ 0.329 │ 0.348 │ 0.319 │ 0.327 │ 0.353 │  0.031 │  0.03  │  0.027 │  0.024 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  26 │ 0.272 │ 0.23  │ 0.231 │ 0.28  │ 0.306 │ 0.248 │ 0.255 │ 0.307 │  0.033 │  0.018 │  0.024 │  0.027 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  27 │ 0.091 │ 0.172 │ 0.084 │ 0.17  │ 0.101 │ 0.2   │ 0.099 │ 0.2   │  0.011 │  0.029 │  0.015 │  0.031 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  28 │ 0.276 │ 0.143 │ 0.22  │ 0.185 │ 0.301 │ 0.162 │ 0.235 │ 0.205 │  0.025 │  0.018 │  0.014 │  0.02  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  29 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  30 │ 0.349 │ 0.368 │ 0.541 │ 0.373 │ 0.362 │ 0.384 │ 0.569 │ 0.393 │  0.012 │  0.015 │  0.028 │  0.019 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  31 │ 0.583 │ 0.592 │ 0.609 │ 0.565 │ 0.609 │ 0.623 │ 0.628 │ 0.588 │  0.025 │  0.031 │  0.02  │  0.023 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  32 │ 0.288 │ 0.267 │ 0.276 │ 0.299 │ 0.333 │ 0.307 │ 0.319 │ 0.328 │  0.044 │  0.039 │  0.043 │  0.029 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  33 │ 0.203 │ 0.164 │ 0.17  │ 0.264 │ 0.226 │ 0.175 │ 0.182 │ 0.291 │  0.023 │  0.011 │  0.011 │  0.028 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  34 │ 0.161 │ 0.101 │ 0.098 │ 0.167 │ 0.197 │ 0.096 │ 0.101 │ 0.197 │  0.036 │ -0.005 │  0.003 │  0.03  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  35 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  36 │ 0.154 │ 0.17  │ 0.251 │ 0.193 │ 0.17  │ 0.19  │ 0.289 │ 0.219 │  0.016 │  0.02  │  0.039 │  0.026 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  37 │ 0.154 │ 0.313 │ 0.231 │ 0.254 │ 0.185 │ 0.362 │ 0.266 │ 0.272 │  0.031 │  0.049 │  0.036 │  0.018 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  38 │ 0.294 │ 0.323 │ 0.414 │ 0.504 │ 0.307 │ 0.347 │ 0.415 │ 0.535 │  0.013 │  0.024 │  0.001 │  0.031 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  39 │ 0.62  │ 0.634 │ 0.68  │ 0.59  │ 0.639 │ 0.659 │ 0.69  │ 0.611 │  0.018 │  0.024 │  0.009 │  0.021 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  40 │ 0.249 │ 0.154 │ 0.168 │ 0.175 │ 0.306 │ 0.196 │ 0.205 │ 0.211 │  0.057 │  0.042 │  0.037 │  0.036 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  41 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  42 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  43 │ 0.005 │ 0.048 │ 0.056 │ 0.046 │ 0.016 │ 0.086 │ 0.086 │ 0.071 │  0.011 │  0.038 │  0.03  │  0.024 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  44 │ 0.107 │ 0.089 │ 0.156 │ 0.171 │ 0.124 │ 0.118 │ 0.185 │ 0.214 │  0.017 │  0.029 │  0.03  │  0.043 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  45 │ 0.212 │ 0.113 │ 0.148 │ 0.162 │ 0.273 │ 0.153 │ 0.202 │ 0.19  │  0.061 │  0.04  │  0.054 │  0.028 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  46 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  47 │ 0.498 │ 0.534 │ 0.771 │ 0.498 │ 0.517 │ 0.544 │ 0.772 │ 0.482 │  0.02  │  0.011 │  0.001 │ -0.016 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  48 │ 0.217 │ 0.114 │ 0.129 │ 0.147 │ 0.289 │ 0.188 │ 0.194 │ 0.196 │  0.072 │  0.074 │  0.065 │  0.049 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  49 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  50 │ 0.001 │ 0     │ 0.007 │ 0.001 │ 0.042 │ 0.058 │ 0.058 │ 0.016 │  0.041 │  0.057 │  0.051 │  0.015 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  51 │ 0.01  │ 0.001 │ 0.016 │ 0.008 │ 0.048 │ 0.019 │ 0.028 │ 0.048 │  0.038 │  0.018 │  0.013 │  0.04  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  52 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  53 │ 0.158 │ 0.069 │ 0.138 │ 0.066 │ 0.251 │ 0.161 │ 0.251 │ 0.09  │  0.093 │  0.091 │  0.112 │  0.024 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  54 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  55 │ 0.6   │ 0.616 │ 0.884 │ 0.528 │ 0.588 │ 0.623 │ 0.878 │ 0.544 │ -0.012 │  0.007 │ -0.006 │  0.016 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  56 │ 0.2   │ 0.116 │ 0.131 │ 0.131 │ 0.28  │ 0.251 │ 0.254 │ 0.254 │  0.08  │  0.135 │  0.123 │  0.123 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  57 │ 0.029 │ 0.058 │ 0.008 │ 0.03  │ 0.159 │ 0.201 │ 0.108 │ 0.135 │  0.129 │  0.143 │  0.1   │  0.105 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  58 │ 0.009 │ 0.014 │ 0.001 │ 0.008 │ 0.127 │ 0.108 │ 0.061 │ 0.085 │  0.118 │  0.095 │  0.06  │  0.077 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  59 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  60 │ 0.002 │ 0.028 │ 0.028 │ 0.014 │ 0.079 │ 0.24  │ 0.24  │ 0.161 │  0.077 │  0.211 │  0.212 │  0.146 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  61 │ 0.097 │ 0.146 │ 0.301 │ 0.152 │ 0.322 │ 0.483 │ 0.486 │ 0.405 │  0.225 │  0.337 │  0.186 │  0.253 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  62 │ 0.071 │ 0.397 │ 0.191 │ 0.164 │ 0.404 │ 0.737 │ 0.577 │ 0.494 │  0.332 │  0.34  │  0.386 │  0.33  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  63 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╧════════╧════════╛\nAction-value function RMSE: 0.0683\n\n정책:\n| 00      ^ | 01      &gt; | 02      &gt; | 03      &gt; | 04      &gt; | 05      &gt; | 06      &gt; | 07      v |\n| 08      ^ | 09      ^ | 10      ^ | 11      ^ | 12      ^ | 13      &gt; | 14      &gt; | 15      v |\n| 16      ^ | 17      ^ | 18      &lt; |           | 20      &gt; | 21      ^ | 22      &gt; | 23      v |\n| 24      ^ | 25      ^ | 26      ^ | 27      v | 28      &lt; |           | 30      &gt; | 31      v |\n| 32      ^ | 33      ^ | 34      &lt; |           | 36      &gt; | 37      v | 38      ^ | 39      &gt; |\n| 40      &lt; |           |           | 43      ^ | 44      ^ | 45      &lt; |           | 47      &gt; |\n| 48      &lt; |           | 50      ^ | 51      ^ |           | 53      &gt; |           | 55      &gt; |\n| 56      &lt; | 57      ^ | 58      v |           | 60      &gt; | 61      &gt; | 62      v |           |\nReaches goal 81.00%. Obtains an average return of 0.4145. Regret of 0.0444\n\n\n\n\nDyna-Q\n\nQ_dqs, V_dqs, Q_track_dqs = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_dq, V_dq, pi_dq, Q_track_dq, pi_track_dq, T_track_dq, R_track_dq, planning_dq = dyna_q(\n        env, gamma=gamma, n_episodes=n_episodes)\n    Q_dqs.append(Q_dq) ; V_dqs.append(V_dq) ; Q_track_dqs.append(Q_track_dq)\nQ_dq, V_dq, Q_track_dq = np.mean(Q_dqs, axis=0), np.mean(V_dqs, axis=0), np.mean(Q_track_dqs, axis=0)\ndel Q_dqs ; del V_dqs ; del Q_track_dqs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_dq, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Dyna-Q:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_dq - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_dq, optimal_V)))\nprint()\nprint_action_value_function(Q_dq, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Dyna-Q action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_dq, optimal_Q)))\nprint()\nprint_policy(pi_dq, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_dq, mean_return_dq, mean_regret_dq = get_policy_metrics(\n    env, gamma=gamma, pi=pi_dq, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_dq, mean_return_dq, mean_regret_dq))\n\nState-value function found by Dyna-Q:\n| 00   0.41 | 01 0.4221 | 02 0.4414 | 03  0.462 | 04 0.4856 | 05 0.5085 | 06 0.5245 | 07 0.5297 |\n| 08 0.4068 | 09 0.4164 | 10 0.4328 | 11 0.4536 | 12 0.4777 | 13 0.5048 | 14 0.5342 | 15 0.5457 |\n| 16 0.3926 | 17 0.3905 | 18 0.3749 |           | 20  0.419 | 21 0.4876 | 22 0.5497 | 23 0.5729 |\n| 24 0.3644 | 25 0.3516 | 26  0.311 | 27 0.2079 | 28 0.3035 |           | 30 0.5569 | 31 0.6136 |\n| 32 0.3292 | 33 0.2942 | 34 0.2039 |           | 36 0.2907 | 37 0.3604 | 38 0.5213 | 39 0.6729 |\n| 40 0.3028 |           |           | 43 0.0912 | 44 0.2171 | 45 0.2795 |           | 47 0.7556 |\n| 48 0.2855 |           | 50  0.063 | 51 0.0517 |           | 53 0.2658 |           | 55 0.8627 |\n| 56 0.2764 | 57 0.1989 | 58 0.1282 |           | 60  0.245 | 61 0.5248 | 62 0.7573 |           |\nOptimal state-value function:\n| 00 0.4146 | 01 0.4272 | 02 0.4461 | 03 0.4683 | 04 0.4924 | 05 0.5166 | 06 0.5353 | 07  0.541 |\n| 08 0.4117 | 09 0.4212 | 10 0.4375 | 11 0.4584 | 12 0.4832 | 13 0.5135 | 14 0.5458 | 15 0.5574 |\n| 16 0.3968 | 17 0.3938 | 18 0.3755 |           | 20 0.4217 | 21 0.4938 | 22 0.5612 | 23 0.5859 |\n| 24 0.3693 | 25  0.353 | 26 0.3065 | 27 0.2004 | 28 0.3008 |           | 30  0.569 | 31 0.6283 |\n| 32 0.3327 | 33 0.2914 | 34 0.1973 |           | 36 0.2893 | 37  0.362 | 38 0.5348 | 39 0.6897 |\n| 40 0.3061 |           |           | 43 0.0863 | 44 0.2139 | 45 0.2727 |           | 47  0.772 |\n| 48 0.2889 |           | 50 0.0577 | 51 0.0475 |           | 53 0.2505 |           | 55 0.8778 |\n| 56 0.2804 | 57 0.2008 | 58 0.1273 |           | 60 0.2396 | 61 0.4864 | 62 0.7371 |           |\nState-value function errors:\n| 00   -0.0 | 01  -0.01 | 02   -0.0 | 03  -0.01 | 04  -0.01 | 05  -0.01 | 06  -0.01 | 07  -0.01 |\n| 08   -0.0 | 09   -0.0 | 10   -0.0 | 11   -0.0 | 12  -0.01 | 13  -0.01 | 14  -0.01 | 15  -0.01 |\n| 16   -0.0 | 17   -0.0 | 18   -0.0 |           | 20   -0.0 | 21  -0.01 | 22  -0.01 | 23  -0.01 |\n| 24   -0.0 | 25   -0.0 | 26    0.0 | 27   0.01 | 28    0.0 |           | 30  -0.01 | 31  -0.01 |\n| 32   -0.0 | 33    0.0 | 34   0.01 |           | 36    0.0 | 37   -0.0 | 38  -0.01 | 39  -0.02 |\n| 40   -0.0 |           |           | 43    0.0 | 44    0.0 | 45   0.01 |           | 47  -0.02 |\n| 48   -0.0 |           | 50   0.01 | 51    0.0 |           | 53   0.02 |           | 55  -0.02 |\n| 56   -0.0 | 57   -0.0 | 58    0.0 |           | 60   0.01 | 61   0.04 | 62   0.02 |           |\nState-value function RMSE: 0.009\n\nDyna-Q action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╤════════╤════════╕\n│   s │     &lt; │     v │     &gt; │     ^ │   * &lt; │   * v │   * &gt; │   * ^ │   er &lt; │   er v │   er &gt; │   er ^ │\n╞═════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╪════════╪════════╡\n│   0 │ 0.405 │ 0.409 │ 0.409 │ 0.41  │ 0.41  │ 0.414 │ 0.414 │ 0.415 │  0.005 │  0.005 │  0.005 │  0.005 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   1 │ 0.412 │ 0.419 │ 0.422 │ 0.421 │ 0.417 │ 0.423 │ 0.427 │ 0.425 │  0.005 │  0.004 │  0.005 │  0.004 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   2 │ 0.428 │ 0.434 │ 0.441 │ 0.438 │ 0.433 │ 0.44  │ 0.446 │ 0.443 │  0.005 │  0.006 │  0.005 │  0.005 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   3 │ 0.448 │ 0.455 │ 0.462 │ 0.459 │ 0.453 │ 0.461 │ 0.468 │ 0.464 │  0.005 │  0.006 │  0.007 │  0.005 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   4 │ 0.471 │ 0.478 │ 0.486 │ 0.481 │ 0.477 │ 0.484 │ 0.492 │ 0.488 │  0.006 │  0.006 │  0.007 │  0.006 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   5 │ 0.496 │ 0.502 │ 0.508 │ 0.501 │ 0.502 │ 0.509 │ 0.517 │ 0.51  │  0.007 │  0.007 │  0.008 │  0.009 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   6 │ 0.518 │ 0.519 │ 0.525 │ 0.517 │ 0.527 │ 0.529 │ 0.535 │ 0.526 │  0.01  │  0.01  │  0.011 │  0.009 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   7 │ 0.529 │ 0.528 │ 0.529 │ 0.523 │ 0.539 │ 0.539 │ 0.541 │ 0.534 │  0.01  │  0.011 │  0.012 │  0.01  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   8 │ 0.399 │ 0.4   │ 0.403 │ 0.407 │ 0.404 │ 0.406 │ 0.407 │ 0.412 │  0.005 │  0.005 │  0.004 │  0.005 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   9 │ 0.403 │ 0.406 │ 0.41  │ 0.416 │ 0.407 │ 0.41  │ 0.415 │ 0.421 │  0.003 │  0.004 │  0.005 │  0.005 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  10 │ 0.407 │ 0.411 │ 0.418 │ 0.433 │ 0.41  │ 0.414 │ 0.422 │ 0.437 │  0.003 │  0.003 │  0.004 │  0.005 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  11 │ 0.287 │ 0.302 │ 0.31  │ 0.454 │ 0.299 │ 0.304 │ 0.314 │ 0.458 │  0.012 │  0.002 │  0.004 │  0.005 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  12 │ 0.448 │ 0.458 │ 0.465 │ 0.478 │ 0.453 │ 0.46  │ 0.471 │ 0.483 │  0.005 │  0.002 │  0.006 │  0.005 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  13 │ 0.485 │ 0.495 │ 0.505 │ 0.503 │ 0.493 │ 0.503 │ 0.514 │ 0.51  │  0.007 │  0.008 │  0.009 │  0.007 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  14 │ 0.522 │ 0.531 │ 0.534 │ 0.521 │ 0.531 │ 0.539 │ 0.546 │ 0.53  │  0.009 │  0.008 │  0.012 │  0.009 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  15 │ 0.54  │ 0.545 │ 0.545 │ 0.532 │ 0.552 │ 0.557 │ 0.556 │ 0.543 │  0.012 │  0.012 │  0.011 │  0.01  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  16 │ 0.384 │ 0.378 │ 0.384 │ 0.393 │ 0.389 │ 0.383 │ 0.388 │ 0.397 │  0.005 │  0.005 │  0.004 │  0.004 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  17 │ 0.382 │ 0.369 │ 0.378 │ 0.39  │ 0.386 │ 0.371 │ 0.379 │ 0.394 │  0.005 │  0.002 │  0.002 │  0.003 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  18 │ 0.375 │ 0.228 │ 0.248 │ 0.278 │ 0.375 │ 0.231 │ 0.246 │ 0.274 │  0.001 │  0.003 │ -0.003 │ -0.003 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  19 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  20 │ 0.268 │ 0.262 │ 0.419 │ 0.312 │ 0.259 │ 0.262 │ 0.422 │ 0.322 │ -0.01  │  0     │  0.003 │  0.01  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  21 │ 0.307 │ 0.315 │ 0.358 │ 0.488 │ 0.309 │ 0.324 │ 0.355 │ 0.494 │  0.002 │  0.009 │ -0.003 │  0.006 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  22 │ 0.522 │ 0.534 │ 0.55  │ 0.528 │ 0.531 │ 0.544 │ 0.561 │ 0.536 │  0.008 │  0.01  │  0.012 │  0.008 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  23 │ 0.566 │ 0.572 │ 0.572 │ 0.552 │ 0.576 │ 0.586 │ 0.585 │ 0.562 │  0.01  │  0.014 │  0.012 │  0.011 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  24 │ 0.359 │ 0.345 │ 0.356 │ 0.364 │ 0.363 │ 0.348 │ 0.357 │ 0.369 │  0.004 │  0.004 │  0.001 │  0.005 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  25 │ 0.347 │ 0.32  │ 0.328 │ 0.351 │ 0.348 │ 0.319 │ 0.327 │ 0.353 │  0.001 │ -0     │ -0     │  0.002 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  26 │ 0.305 │ 0.248 │ 0.26  │ 0.305 │ 0.306 │ 0.248 │ 0.255 │ 0.307 │  0     │ -0     │ -0.005 │  0.001 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  27 │ 0.095 │ 0.199 │ 0.099 │ 0.208 │ 0.101 │ 0.2   │ 0.099 │ 0.2   │  0.007 │  0.002 │  0.001 │ -0.007 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  28 │ 0.303 │ 0.16  │ 0.239 │ 0.207 │ 0.301 │ 0.162 │ 0.235 │ 0.205 │ -0.003 │  0.002 │ -0.004 │ -0.002 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  29 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  30 │ 0.362 │ 0.375 │ 0.557 │ 0.388 │ 0.362 │ 0.384 │ 0.569 │ 0.393 │  0     │  0.009 │  0.012 │  0.005 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  31 │ 0.599 │ 0.612 │ 0.612 │ 0.579 │ 0.609 │ 0.623 │ 0.628 │ 0.588 │  0.01  │  0.011 │  0.016 │  0.009 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  32 │ 0.328 │ 0.306 │ 0.317 │ 0.326 │ 0.333 │ 0.307 │ 0.319 │ 0.328 │  0.005 │  0.001 │  0.002 │  0.002 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  33 │ 0.225 │ 0.168 │ 0.18  │ 0.294 │ 0.226 │ 0.175 │ 0.182 │ 0.291 │  0.002 │  0.007 │  0.001 │ -0.003 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  34 │ 0.196 │ 0.092 │ 0.099 │ 0.196 │ 0.197 │ 0.096 │ 0.101 │ 0.197 │  0.002 │  0.005 │  0.002 │  0.002 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  35 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  36 │ 0.174 │ 0.191 │ 0.291 │ 0.213 │ 0.17  │ 0.19  │ 0.289 │ 0.219 │ -0.004 │ -0.001 │ -0.001 │  0.006 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  37 │ 0.196 │ 0.36  │ 0.256 │ 0.273 │ 0.185 │ 0.362 │ 0.266 │ 0.272 │ -0.01  │  0.002 │  0.01  │ -0.001 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  38 │ 0.308 │ 0.325 │ 0.403 │ 0.521 │ 0.307 │ 0.347 │ 0.415 │ 0.535 │ -0.001 │  0.022 │  0.012 │  0.014 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  39 │ 0.624 │ 0.64  │ 0.673 │ 0.604 │ 0.639 │ 0.659 │ 0.69  │ 0.611 │  0.015 │  0.019 │  0.017 │  0.008 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  40 │ 0.303 │ 0.207 │ 0.202 │ 0.212 │ 0.306 │ 0.196 │ 0.205 │ 0.211 │  0.003 │ -0.01  │  0.003 │ -0.002 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  41 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  42 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  43 │ 0.02  │ 0.082 │ 0.09  │ 0.069 │ 0.016 │ 0.086 │ 0.086 │ 0.071 │ -0.004 │  0.004 │ -0.003 │  0.001 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  44 │ 0.133 │ 0.13  │ 0.177 │ 0.217 │ 0.124 │ 0.118 │ 0.185 │ 0.214 │ -0.009 │ -0.012 │  0.009 │ -0.003 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  45 │ 0.28  │ 0.156 │ 0.213 │ 0.204 │ 0.273 │ 0.153 │ 0.202 │ 0.19  │ -0.007 │ -0.003 │ -0.011 │ -0.014 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  46 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  47 │ 0.51  │ 0.531 │ 0.756 │ 0.472 │ 0.517 │ 0.544 │ 0.772 │ 0.482 │  0.008 │  0.014 │  0.016 │  0.01  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  48 │ 0.285 │ 0.196 │ 0.186 │ 0.197 │ 0.289 │ 0.188 │ 0.194 │ 0.196 │  0.003 │ -0.008 │  0.008 │ -0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  49 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  50 │ 0.038 │ 0.051 │ 0.06  │ 0.016 │ 0.042 │ 0.058 │ 0.058 │ 0.016 │  0.004 │  0.006 │ -0.002 │ -0.001 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  51 │ 0.049 │ 0.023 │ 0.029 │ 0.05  │ 0.048 │ 0.019 │ 0.028 │ 0.048 │ -0.001 │ -0.004 │ -0.001 │ -0.003 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  52 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  53 │ 0.26  │ 0.213 │ 0.258 │ 0.078 │ 0.251 │ 0.161 │ 0.251 │ 0.09  │ -0.009 │ -0.052 │ -0.007 │  0.012 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  54 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  55 │ 0.588 │ 0.626 │ 0.863 │ 0.547 │ 0.588 │ 0.623 │ 0.878 │ 0.544 │ -0     │ -0.003 │  0.015 │ -0.002 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  56 │ 0.276 │ 0.246 │ 0.255 │ 0.251 │ 0.28  │ 0.251 │ 0.254 │ 0.254 │  0.004 │  0.005 │ -0     │  0.003 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  57 │ 0.149 │ 0.199 │ 0.106 │ 0.127 │ 0.159 │ 0.201 │ 0.108 │ 0.135 │  0.01  │  0.002 │  0.002 │  0.007 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  58 │ 0.124 │ 0.103 │ 0.056 │ 0.09  │ 0.127 │ 0.108 │ 0.061 │ 0.085 │  0.004 │  0.005 │  0.005 │ -0.004 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  59 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  60 │ 0.068 │ 0.216 │ 0.213 │ 0.186 │ 0.079 │ 0.24  │ 0.24  │ 0.161 │  0.011 │  0.023 │  0.027 │ -0.025 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  61 │ 0.34  │ 0.503 │ 0.515 │ 0.422 │ 0.322 │ 0.483 │ 0.486 │ 0.405 │ -0.018 │ -0.02  │ -0.028 │ -0.017 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  62 │ 0.401 │ 0.757 │ 0.497 │ 0.49  │ 0.404 │ 0.737 │ 0.577 │ 0.494 │  0.003 │ -0.02  │  0.08  │  0.004 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  63 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╧════════╧════════╛\nAction-value function RMSE: 0.0096\n\n정책:\n| 00      ^ | 01      ^ | 02      &gt; | 03      &gt; | 04      &gt; | 05      &gt; | 06      &gt; | 07      &lt; |\n| 08      ^ | 09      ^ | 10      ^ | 11      ^ | 12      ^ | 13      &gt; | 14      &gt; | 15      v |\n| 16      ^ | 17      ^ | 18      &lt; |           | 20      &gt; | 21      ^ | 22      &gt; | 23      v |\n| 24      ^ | 25      ^ | 26      ^ | 27      ^ | 28      &lt; |           | 30      &gt; | 31      v |\n| 32      &lt; | 33      ^ | 34      &lt; |           | 36      &gt; | 37      v | 38      ^ | 39      &gt; |\n| 40      &lt; |           |           | 43      &gt; | 44      ^ | 45      &lt; |           | 47      &gt; |\n| 48      &lt; |           | 50      v | 51      &lt; |           | 53      &lt; |           | 55      &gt; |\n| 56      &lt; | 57      v | 58      v |           | 60      v | 61      v | 62      v |           |\nReaches goal 77.00%. Obtains an average return of 0.3837. Regret of 0.0511\n\n\n\n\n경로 샘플링\n\nQ_tss, V_tss, Q_track_tss = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_ts, V_ts, pi_ts, Q_track_ts, pi_track_ts, T_track_ts, R_track_ts, planning_ts = trajectory_sampling(\n        env, gamma=gamma, n_episodes=n_episodes)\n    Q_tss.append(Q_ts) ; V_tss.append(V_ts) ; Q_track_tss.append(Q_track_ts)\nQ_ts, V_ts, Q_track_ts = np.mean(Q_tss, axis=0), np.mean(V_tss, axis=0), np.mean(Q_track_tss, axis=0)\ndel Q_tss ; del V_tss ; del Q_track_tss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_ts, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Trajectory Sampling:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_ts - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_ts, optimal_V)))\nprint()\nprint_action_value_function(Q_ts, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Trajectory Sampling action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_ts, optimal_Q)))\nprint()\nprint_policy(pi_ts, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_ts, mean_return_ts, mean_regret_ts = get_policy_metrics(\n    env, gamma=gamma, pi=pi_ts, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_ts, mean_return_ts, mean_regret_ts))\n\nState-value function found by Trajectory Sampling:\n| 00 0.4086 | 01 0.4223 | 02 0.4415 | 03 0.4633 | 04 0.4881 | 05 0.5137 | 06 0.5315 | 07 0.5364 |\n| 08  0.406 | 09 0.4152 | 10  0.432 | 11 0.4539 | 12 0.4792 | 13 0.5106 | 14  0.542 | 15 0.5522 |\n| 16 0.3938 | 17 0.3907 | 18 0.3746 |           | 20 0.4216 | 21 0.4925 | 22 0.5553 | 23 0.5811 |\n| 24 0.3668 | 25 0.3534 | 26 0.3077 | 27 0.1988 | 28 0.2935 |           | 30  0.564 | 31 0.6245 |\n| 32 0.3268 | 33 0.2918 | 34 0.1974 |           | 36 0.2699 | 37 0.3413 | 38 0.5286 | 39 0.6887 |\n| 40 0.3005 |           |           | 43 0.0695 | 44  0.192 | 45 0.2442 |           | 47  0.775 |\n| 48  0.283 |           | 50 0.0351 | 51 0.0344 |           | 53 0.1925 |           | 55 0.8773 |\n| 56 0.2748 | 57  0.181 | 58 0.0862 |           | 60 0.1042 | 61 0.3408 | 62 0.5623 |           |\nOptimal state-value function:\n| 00 0.4146 | 01 0.4272 | 02 0.4461 | 03 0.4683 | 04 0.4924 | 05 0.5166 | 06 0.5353 | 07  0.541 |\n| 08 0.4117 | 09 0.4212 | 10 0.4375 | 11 0.4584 | 12 0.4832 | 13 0.5135 | 14 0.5458 | 15 0.5574 |\n| 16 0.3968 | 17 0.3938 | 18 0.3755 |           | 20 0.4217 | 21 0.4938 | 22 0.5612 | 23 0.5859 |\n| 24 0.3693 | 25  0.353 | 26 0.3065 | 27 0.2004 | 28 0.3008 |           | 30  0.569 | 31 0.6283 |\n| 32 0.3327 | 33 0.2914 | 34 0.1973 |           | 36 0.2893 | 37  0.362 | 38 0.5348 | 39 0.6897 |\n| 40 0.3061 |           |           | 43 0.0863 | 44 0.2139 | 45 0.2727 |           | 47  0.772 |\n| 48 0.2889 |           | 50 0.0577 | 51 0.0475 |           | 53 0.2505 |           | 55 0.8778 |\n| 56 0.2804 | 57 0.2008 | 58 0.1273 |           | 60 0.2396 | 61 0.4864 | 62 0.7371 |           |\nState-value function errors:\n| 00  -0.01 | 01   -0.0 | 02   -0.0 | 03   -0.0 | 04   -0.0 | 05   -0.0 | 06   -0.0 | 07   -0.0 |\n| 08  -0.01 | 09  -0.01 | 10  -0.01 | 11   -0.0 | 12   -0.0 | 13   -0.0 | 14   -0.0 | 15  -0.01 |\n| 16   -0.0 | 17   -0.0 | 18   -0.0 |           | 20   -0.0 | 21   -0.0 | 22  -0.01 | 23   -0.0 |\n| 24   -0.0 | 25    0.0 | 26    0.0 | 27   -0.0 | 28  -0.01 |           | 30   -0.0 | 31   -0.0 |\n| 32  -0.01 | 33    0.0 | 34    0.0 |           | 36  -0.02 | 37  -0.02 | 38  -0.01 | 39   -0.0 |\n| 40  -0.01 |           |           | 43  -0.02 | 44  -0.02 | 45  -0.03 |           | 47    0.0 |\n| 48  -0.01 |           | 50  -0.02 | 51  -0.01 |           | 53  -0.06 |           | 55   -0.0 |\n| 56  -0.01 | 57  -0.02 | 58  -0.04 |           | 60  -0.14 | 61  -0.15 | 62  -0.17 |           |\nState-value function RMSE: 0.0352\n\nTrajectory Sampling action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╤════════╤════════╕\n│   s │     &lt; │     v │     &gt; │     ^ │   * &lt; │   * v │   * &gt; │   * ^ │   er &lt; │   er v │   er &gt; │   er ^ │\n╞═════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╪════════╪════════╡\n│   0 │ 0.405 │ 0.407 │ 0.407 │ 0.408 │ 0.41  │ 0.414 │ 0.414 │ 0.415 │  0.004 │  0.007 │  0.006 │  0.007 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   1 │ 0.413 │ 0.417 │ 0.422 │ 0.418 │ 0.417 │ 0.423 │ 0.427 │ 0.425 │  0.004 │  0.006 │  0.005 │  0.007 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   2 │ 0.429 │ 0.434 │ 0.441 │ 0.436 │ 0.433 │ 0.44  │ 0.446 │ 0.443 │  0.003 │  0.006 │  0.005 │  0.006 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   3 │ 0.449 │ 0.456 │ 0.463 │ 0.457 │ 0.453 │ 0.461 │ 0.468 │ 0.464 │  0.004 │  0.005 │  0.005 │  0.007 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   4 │ 0.473 │ 0.48  │ 0.488 │ 0.48  │ 0.477 │ 0.484 │ 0.492 │ 0.488 │  0.004 │  0.005 │  0.004 │  0.008 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   5 │ 0.498 │ 0.502 │ 0.514 │ 0.502 │ 0.502 │ 0.509 │ 0.517 │ 0.51  │  0.004 │  0.007 │  0.003 │  0.008 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   6 │ 0.519 │ 0.521 │ 0.532 │ 0.518 │ 0.527 │ 0.529 │ 0.535 │ 0.526 │  0.008 │  0.009 │  0.004 │  0.008 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   7 │ 0.526 │ 0.527 │ 0.536 │ 0.525 │ 0.539 │ 0.539 │ 0.541 │ 0.534 │  0.013 │  0.012 │  0.005 │  0.008 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   8 │ 0.4   │ 0.402 │ 0.402 │ 0.406 │ 0.404 │ 0.406 │ 0.407 │ 0.412 │  0.004 │  0.004 │  0.004 │  0.006 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   9 │ 0.404 │ 0.407 │ 0.41  │ 0.415 │ 0.407 │ 0.41  │ 0.415 │ 0.421 │  0.003 │  0.003 │  0.005 │  0.006 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  10 │ 0.408 │ 0.412 │ 0.421 │ 0.432 │ 0.41  │ 0.414 │ 0.422 │ 0.437 │  0.003 │  0.002 │  0.001 │  0.006 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  11 │ 0.294 │ 0.295 │ 0.317 │ 0.454 │ 0.299 │ 0.304 │ 0.314 │ 0.458 │  0.005 │  0.009 │ -0.003 │  0.005 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  12 │ 0.45  │ 0.455 │ 0.465 │ 0.479 │ 0.453 │ 0.46  │ 0.471 │ 0.483 │  0.003 │  0.004 │  0.006 │  0.004 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  13 │ 0.489 │ 0.497 │ 0.511 │ 0.499 │ 0.493 │ 0.503 │ 0.514 │ 0.51  │  0.004 │  0.006 │  0.003 │  0.011 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  14 │ 0.526 │ 0.529 │ 0.542 │ 0.524 │ 0.531 │ 0.539 │ 0.546 │ 0.53  │  0.005 │  0.009 │  0.004 │  0.006 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  15 │ 0.541 │ 0.552 │ 0.542 │ 0.537 │ 0.552 │ 0.557 │ 0.556 │ 0.543 │  0.011 │  0.005 │  0.014 │  0.006 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  16 │ 0.385 │ 0.38  │ 0.384 │ 0.394 │ 0.389 │ 0.383 │ 0.388 │ 0.397 │  0.003 │  0.003 │  0.004 │  0.003 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  17 │ 0.382 │ 0.37  │ 0.378 │ 0.391 │ 0.386 │ 0.371 │ 0.379 │ 0.394 │  0.004 │  0.002 │  0.002 │  0.003 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  18 │ 0.375 │ 0.228 │ 0.241 │ 0.287 │ 0.375 │ 0.231 │ 0.246 │ 0.274 │  0.001 │  0.003 │  0.004 │ -0.012 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  19 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  20 │ 0.249 │ 0.255 │ 0.422 │ 0.337 │ 0.259 │ 0.262 │ 0.422 │ 0.322 │  0.009 │  0.007 │  0     │ -0.014 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  21 │ 0.301 │ 0.315 │ 0.356 │ 0.492 │ 0.309 │ 0.324 │ 0.355 │ 0.494 │  0.007 │  0.009 │ -0.001 │  0.001 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  22 │ 0.527 │ 0.538 │ 0.555 │ 0.53  │ 0.531 │ 0.544 │ 0.561 │ 0.536 │  0.004 │  0.006 │  0.006 │  0.006 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  23 │ 0.566 │ 0.579 │ 0.569 │ 0.557 │ 0.576 │ 0.586 │ 0.585 │ 0.562 │  0.01  │  0.007 │  0.016 │  0.006 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  24 │ 0.354 │ 0.343 │ 0.351 │ 0.367 │ 0.363 │ 0.348 │ 0.357 │ 0.369 │  0.009 │  0.005 │  0.006 │  0.002 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  25 │ 0.339 │ 0.314 │ 0.323 │ 0.353 │ 0.348 │ 0.319 │ 0.327 │ 0.353 │  0.009 │  0.005 │  0.005 │ -0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  26 │ 0.294 │ 0.246 │ 0.252 │ 0.302 │ 0.306 │ 0.248 │ 0.255 │ 0.307 │  0.011 │  0.001 │  0.004 │  0.004 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  27 │ 0.096 │ 0.173 │ 0.105 │ 0.191 │ 0.101 │ 0.2   │ 0.099 │ 0.2   │  0.005 │  0.027 │ -0.006 │  0.01  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  28 │ 0.293 │ 0.149 │ 0.224 │ 0.198 │ 0.301 │ 0.162 │ 0.235 │ 0.205 │  0.007 │  0.012 │  0.011 │  0.007 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  29 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  30 │ 0.356 │ 0.391 │ 0.564 │ 0.382 │ 0.362 │ 0.384 │ 0.569 │ 0.393 │  0.006 │ -0.007 │  0.005 │  0.011 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  31 │ 0.597 │ 0.604 │ 0.624 │ 0.582 │ 0.609 │ 0.623 │ 0.628 │ 0.588 │  0.012 │  0.019 │  0.005 │  0.006 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  32 │ 0.319 │ 0.298 │ 0.308 │ 0.32  │ 0.333 │ 0.307 │ 0.319 │ 0.328 │  0.014 │  0.009 │  0.011 │  0.007 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  33 │ 0.21  │ 0.167 │ 0.159 │ 0.292 │ 0.226 │ 0.175 │ 0.182 │ 0.291 │  0.016 │  0.008 │  0.023 │ -0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  34 │ 0.173 │ 0.112 │ 0.102 │ 0.184 │ 0.197 │ 0.096 │ 0.101 │ 0.197 │  0.025 │ -0.016 │ -0.001 │  0.013 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  35 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  36 │ 0.165 │ 0.174 │ 0.27  │ 0.208 │ 0.17  │ 0.19  │ 0.289 │ 0.219 │  0.005 │  0.016 │  0.019 │  0.011 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  37 │ 0.169 │ 0.341 │ 0.257 │ 0.269 │ 0.185 │ 0.362 │ 0.266 │ 0.272 │  0.016 │  0.021 │  0.01  │  0.003 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  38 │ 0.31  │ 0.346 │ 0.419 │ 0.529 │ 0.307 │ 0.347 │ 0.415 │ 0.535 │ -0.002 │  0.001 │ -0.004 │  0.006 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  39 │ 0.636 │ 0.654 │ 0.689 │ 0.608 │ 0.639 │ 0.659 │ 0.69  │ 0.611 │  0.003 │  0.004 │  0.001 │  0.003 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  40 │ 0.3   │ 0.186 │ 0.199 │ 0.203 │ 0.306 │ 0.196 │ 0.205 │ 0.211 │  0.006 │  0.011 │  0.006 │  0.007 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  41 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  42 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  43 │ 0.01  │ 0.062 │ 0.06  │ 0.047 │ 0.016 │ 0.086 │ 0.086 │ 0.071 │  0.006 │  0.024 │  0.026 │  0.024 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  44 │ 0.113 │ 0.103 │ 0.158 │ 0.192 │ 0.124 │ 0.118 │ 0.185 │ 0.214 │  0.011 │  0.015 │  0.027 │  0.022 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  45 │ 0.244 │ 0.123 │ 0.175 │ 0.176 │ 0.273 │ 0.153 │ 0.202 │ 0.19  │  0.028 │  0.03  │  0.027 │  0.014 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  46 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  47 │ 0.499 │ 0.556 │ 0.775 │ 0.475 │ 0.517 │ 0.544 │ 0.772 │ 0.482 │  0.018 │ -0.012 │ -0.003 │  0.007 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  48 │ 0.283 │ 0.167 │ 0.188 │ 0.201 │ 0.289 │ 0.188 │ 0.194 │ 0.196 │  0.006 │  0.021 │  0.005 │ -0.005 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  49 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  50 │ 0.018 │ 0.01  │ 0.027 │ 0.003 │ 0.042 │ 0.058 │ 0.058 │ 0.016 │  0.024 │  0.048 │  0.031 │  0.013 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  51 │ 0.021 │ 0.007 │ 0.016 │ 0.029 │ 0.048 │ 0.019 │ 0.028 │ 0.048 │  0.026 │  0.012 │  0.012 │  0.019 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  52 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  53 │ 0.173 │ 0.093 │ 0.165 │ 0.075 │ 0.251 │ 0.161 │ 0.251 │ 0.09  │  0.077 │  0.068 │  0.086 │  0.015 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  54 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  55 │ 0.587 │ 0.658 │ 0.877 │ 0.543 │ 0.588 │ 0.623 │ 0.878 │ 0.544 │  0.001 │ -0.035 │  0     │  0.001 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  56 │ 0.275 │ 0.231 │ 0.23  │ 0.237 │ 0.28  │ 0.251 │ 0.254 │ 0.254 │  0.006 │  0.02  │  0.024 │  0.017 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  57 │ 0.125 │ 0.163 │ 0.076 │ 0.074 │ 0.159 │ 0.201 │ 0.108 │ 0.135 │  0.034 │  0.037 │  0.032 │  0.06  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  58 │ 0.072 │ 0.05  │ 0.02  │ 0.032 │ 0.127 │ 0.108 │ 0.061 │ 0.085 │  0.056 │  0.058 │  0.041 │  0.054 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  59 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  60 │ 0.003 │ 0.065 │ 0.056 │ 0.024 │ 0.079 │ 0.24  │ 0.24  │ 0.161 │  0.076 │  0.175 │  0.184 │  0.136 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  61 │ 0.164 │ 0.198 │ 0.318 │ 0.216 │ 0.322 │ 0.483 │ 0.486 │ 0.405 │  0.158 │  0.285 │  0.168 │  0.189 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  62 │ 0.109 │ 0.499 │ 0.19  │ 0.23  │ 0.404 │ 0.737 │ 0.577 │ 0.494 │  0.295 │  0.238 │  0.386 │  0.264 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  63 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╧════════╧════════╛\nAction-value function RMSE: 0.0517\n\n정책:\n| 00      v | 01      &gt; | 02      &gt; | 03      &gt; | 04      &gt; | 05      &gt; | 06      &gt; | 07      &gt; |\n| 08      ^ | 09      ^ | 10      ^ | 11      ^ | 12      ^ | 13      &gt; | 14      &gt; | 15      v |\n| 16      ^ | 17      ^ | 18      &lt; |           | 20      &gt; | 21      ^ | 22      &gt; | 23      v |\n| 24      ^ | 25      ^ | 26      ^ | 27      ^ | 28      &lt; |           | 30      &gt; | 31      v |\n| 32      ^ | 33      ^ | 34      &lt; |           | 36      &gt; | 37      v | 38      ^ | 39      &gt; |\n| 40      &lt; |           |           | 43      &gt; | 44      ^ | 45      &lt; |           | 47      &gt; |\n| 48      &lt; |           | 50      &gt; | 51      ^ |           | 53      &lt; |           | 55      &gt; |\n| 56      &lt; | 57      &lt; | 58      v |           | 60      v | 61      &lt; | 62      v |           |\nReaches goal 79.00%. Obtains an average return of 0.4173. Regret of 0.0514\n\n\n\n\n\n각 에피소드별 max(Q) 비교\n\nSARSA(\\(\\lambda\\)) 대체\n\nplot_value_function(\n    'Sarsa(λ) replacing estimates through time vs. true values', \n    np.max(Q_track_rsl, axis=2), \n    optimal_V,\n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Sarsa(λ) replacing estimates through time vs. true values (log scale)', \n    np.max(Q_track_rsl, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Sarsa(λ) replacing estimates through time (close up)', \n    np.max(Q_track_rsl, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\nSARSA(\\(\\lambda\\)) 누적\n\nplot_value_function(\n    'Sarsa(λ) accumulating estimates through time vs. true values', \n    np.max(Q_track_asl, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Sarsa(λ) accumulating estimates through time vs. true values (log scale)', \n    np.max(Q_track_asl, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Sarsa(λ) accumulating estimates through time (close up)', \n    np.max(Q_track_asl, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\nQ(\\(\\lambda\\)) 대체\n\nplot_value_function(\n    'Q(λ) replacing estimates through time vs. true values', \n    np.max(Q_track_rqll, axis=2), \n    optimal_V,\n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Q(λ) replacing estimates through time vs. true values (log scale)', \n    np.max(Q_track_rqll, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Q(λ) replacing estimates through time (close up)', \n    np.max(Q_track_rqll, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\nQ(\\(\\lambda\\)) 누적\n\nplot_value_function(\n    'Q(λ) accumulating estimates through time vs. true values', \n    np.max(Q_track_aqll, axis=2), \n    optimal_V,\n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Q(λ) accumulating estimates through time vs. true values (log scale)', \n    np.max(Q_track_aqll, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Q(λ) accumulating estimates through time (close up)', \n    np.max(Q_track_aqll, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\nDyna-Q\n\nplot_value_function(\n    'Dyna-Q estimates through time vs. true values', \n    np.max(Q_track_dq, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Dyna-Q estimates through time vs. true values (log scale)', \n    np.max(Q_track_dq, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Dyna-Q estimates through time (close up)', \n    np.max(Q_track_dq, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\n경로 샘플링\n\nplot_value_function(\n    'Trajectory Sampling estimates through time vs. true values', \n    np.max(Q_track_ts, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Trajectory Sampling estimates through time vs. true values (log scale)', \n    np.max(Q_track_ts, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Trajectory Sampling estimates through time (close up)', \n    np.max(Q_track_ts, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\n\n정책 평가 비교\n\nrsl_success_rate_ma, rsl_mean_return_ma, rsl_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_rsl, coverage=0.05)\n\n\n\n\n\nasl_success_rate_ma, asl_mean_return_ma, asl_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_asl, coverage=0.05)\n\n\n\n\n\nrqll_success_rate_ma, rqll_mean_return_ma, rqll_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_rqll, coverage=0.05)\n\n\n\n\n\naqll_success_rate_ma, aqll_mean_return_ma, aqll_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_aqll, coverage=0.05)\n\n\n\n\n\ndq_success_rate_ma, dq_mean_return_ma, dq_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_dq, coverage=0.05)\n\n\n\n\n\nts_success_rate_ma, ts_mean_return_ma, ts_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_ts, coverage=0.05)\n\n\n\n\n\nplt.axhline(y=success_rate_op, color='k', linestyle='-', linewidth=1)\nplt.text(int(len(rsl_success_rate_ma)*1.02), success_rate_op*1.01, 'π*')\n\nplt.plot(rsl_success_rate_ma, '-', linewidth=2, label='Sarsa(λ) replacing')\nplt.plot(asl_success_rate_ma, '--', linewidth=2, label='Sarsa(λ) accumulating')\nplt.plot(rqll_success_rate_ma, ':', linewidth=2, label='Q(λ) replacing')\nplt.legend(loc=4, ncol=1)\n\nplt.title('Policy success rate (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Success rate %')\nplt.ylim(-1, 101)\nplt.xticks(rotation=45)\n\n\nplt.show()\n\n\n\n\n\nplt.axhline(y=success_rate_op, color='k', linestyle='-', linewidth=1)\nplt.text(int(len(rsl_success_rate_ma)*1.02), success_rate_op*1.01, 'π*')\n\nplt.plot(aqll_success_rate_ma, '-.', linewidth=2, label='Q(λ) accumulating')\nplt.plot(dq_success_rate_ma, '-', linewidth=2, label='Dyna-Q')\nplt.plot(ts_success_rate_ma, '--', linewidth=2, label='Trajectory Sampling')\nplt.legend(loc=4, ncol=1)\n\nplt.title('Policy success rate (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Success rate %')\nplt.ylim(-1, 101)\nplt.xticks(rotation=45)\n\n\nplt.show()\n\n\n\n\n\nplt.axhline(y=mean_return_op, color='k', linestyle='-', linewidth=1)\nplt.text(int(len(rsl_mean_return_ma)*1.02), mean_return_op*1.01, 'π*')\n\nplt.plot(rsl_mean_return_ma, '-', linewidth=2, label='Sarsa(λ) replacing')\nplt.plot(asl_mean_return_ma, '--', linewidth=2, label='Sarsa(λ) accumulating')\nplt.plot(rqll_mean_return_ma, ':', linewidth=2, label='Q(λ) replacing')\nplt.legend(loc=4, ncol=1)\n\nplt.title('Policy episode return (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Return (Gt:T)')\n\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.axhline(y=mean_return_op, color='k', linestyle='-', linewidth=1)\nplt.text(int(len(rsl_mean_return_ma)*1.02), mean_return_op*1.01, 'π*')\n\nplt.plot(aqll_mean_return_ma, '-.', linewidth=2, label='Q(λ) accumulating')\nplt.plot(dq_mean_return_ma, '-', linewidth=2, label='Dyna-Q')\nplt.plot(ts_mean_return_ma, '--', linewidth=2, label='Trajectory Sampling')\nplt.legend(loc=4, ncol=1)\n\nplt.title('Policy episode return (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Return (Gt:T)')\n\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.plot(rsl_mean_regret_ma, '-', linewidth=2, label='Sarsa(λ) replacing')\nplt.plot(asl_mean_regret_ma, '--', linewidth=2, label='Sarsa(λ) accumulating')\nplt.plot(rqll_mean_regret_ma, ':', linewidth=2, label='Q(λ) replacing')\nplt.legend(loc=1, ncol=1)\n\nplt.title('Policy episode regret (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Regret (q* - Q)')\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.plot(aqll_mean_regret_ma, '-.', linewidth=2, label='Q(λ) accumulating')\nplt.plot(dq_mean_regret_ma, '-', linewidth=2, label='Dyna-Q')\nplt.plot(ts_mean_regret_ma, '--', linewidth=2, label='Trajectory Sampling')\nplt.legend(loc=1, ncol=1)\n\nplt.title('Policy episode regret (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Regret (q* - Q)')\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.axhline(y=optimal_V[init_state], color='k', linestyle='-', linewidth=1)\nplt.text(int(len(Q_track_rsl)*1.05), optimal_V[init_state]+.01, 'v*({})'.format(init_state))\n\nplt.plot(moving_average(np.max(Q_track_rsl, axis=2).T[init_state]), \n         '-', linewidth=2, label='Sarsa(λ) replacing')\nplt.plot(moving_average(np.max(Q_track_asl, axis=2).T[init_state]), \n         '--', linewidth=2, label='Sarsa(λ) accumulating')\nplt.plot(moving_average(np.max(Q_track_rqll, axis=2).T[init_state]), \n         ':', linewidth=2, label='Q(λ) replacing')\nplt.plot(moving_average(np.max(Q_track_aqll, axis=2).T[init_state]), \n         '-.', linewidth=2, label='Q(λ) accumulating')\nplt.plot(moving_average(np.max(Q_track_dq, axis=2).T[init_state]), \n         '-', linewidth=2, label='Dyna-Q')\nplt.plot(moving_average(np.max(Q_track_ts, axis=2).T[init_state]), \n         '--', linewidth=2, label='Trajectory Sampling')\nplt.legend(loc=4, ncol=1)\n\nplt.title('Estimated expected return (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Estimated value of initial state V({})'.format(init_state))\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_rsl, axis=2) - optimal_V), axis=1)), \n         '-', linewidth=2, label='Sarsa(λ) replacing')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_asl, axis=2) - optimal_V), axis=1)), \n         '--', linewidth=2, label='Sarsa(λ) accumulating')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_rqll, axis=2) - optimal_V), axis=1)), \n         ':', linewidth=2, label='Q(λ) replacing')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_aqll, axis=2) - optimal_V), axis=1)), \n         '-.', linewidth=2, label='Q(λ) accumulating')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_dq, axis=2) - optimal_V), axis=1)), \n         '-', linewidth=2, label='Dyna-Q')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_ts, axis=2) - optimal_V), axis=1)), \n         '--', linewidth=2, label='Trajectory Sampling')\nplt.legend(loc=1, ncol=1)\n\nplt.title('State-value function estimation error (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Mean Absolute Error MAE(V, v*)')\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.plot(moving_average(np.mean(np.abs(Q_track_rsl - optimal_Q), axis=(1,2))), \n         '-', linewidth=2, label='Sarsa(λ) replacing')\nplt.plot(moving_average(np.mean(np.abs(Q_track_asl - optimal_Q), axis=(1,2))), \n         '--', linewidth=2, label='Sarsa(λ) accumulating')\nplt.plot(moving_average(np.mean(np.abs(Q_track_rqll - optimal_Q), axis=(1,2))), \n         ':', linewidth=2, label='Q(λ) replacing')\nplt.plot(moving_average(np.mean(np.abs(Q_track_aqll - optimal_Q), axis=(1,2))), \n         '-.', linewidth=2, label='Q(λ) accumulating')\nplt.plot(moving_average(np.mean(np.abs(Q_track_dq - optimal_Q), axis=(1,2))), \n         '-', linewidth=2, label='Dyna-Q')\nplt.plot(moving_average(np.mean(np.abs(Q_track_ts - optimal_Q), axis=(1,2))), \n         '--', linewidth=2, label='Trajectory Sampling')\nplt.legend(loc=1, ncol=1)\n\nplt.title('Action-value function estimation error (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Mean Absolute Error MAE(Q, q*)')\nplt.xticks(rotation=45)\n\nplt.show()"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-5.html#tldr",
    "href": "publication/GDRL/GDRL-chapter-5.html#tldr",
    "title": "Chapter 5: Evaluating Agents behaviors",
    "section": "TL;DR",
    "text": "TL;DR\n\n그로킹 심층 강화학습 중 5장 내용인 “에이전트의 행동 평가”에 대한 내용입니다.\n\n\n\nCode\n!pip install tqdm numpy scikit-learn pyglet setuptools && \\\n!pip install gym asciinema pandas tabulate tornado==5.* PyBullet && \\\n!pip install git+https://github.com/pybox2d/pybox2d#egg=Box2D && \\\n!pip install git+https://github.com/mimoralea/gym-bandits#egg=gym-bandits && \\\n!pip install git+https://github.com/mimoralea/gym-walk#egg=gym-walk && \\\n!pip install git+https://github.com/mimoralea/gym-aima#egg=gym-aima && \\\n!pip install gym[atari]"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-5.html#활성-정책-첫-방문-mc-예측-활성-정책-모든-방문-mc-예측-td-n단계-td-tdλ",
    "href": "publication/GDRL/GDRL-chapter-5.html#활성-정책-첫-방문-mc-예측-활성-정책-모든-방문-mc-예측-td-n단계-td-tdλ",
    "title": "Chapter 5: Evaluating Agents behaviors",
    "section": "활성 정책 첫 방문 MC 예측, 활성 정책 모든 방문 MC 예측, TD, n단계 TD, TD(λ)",
    "text": "활성 정책 첫 방문 MC 예측, 활성 정책 모든 방문 MC 예측, TD, n단계 TD, TD(λ)\n\nimport warnings ; warnings.filterwarnings('ignore')\n\nimport gym, gym_walk, gym_aima\nimport numpy as np\nfrom pprint import pprint\nfrom tqdm import tqdm_notebook as tqdm\n\nfrom itertools import cycle, count\n\nimport random\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nSEEDS = (12, 34, 56, 78, 90)\n\n%matplotlib inline\n\n\nplt.style.use('fivethirtyeight')\nparams = {\n    'figure.figsize': (15, 8),\n    'font.size': 24,\n    'legend.fontsize': 20,\n    'axes.titlesize': 28,\n    'axes.labelsize': 24,\n    'xtick.labelsize': 20,\n    'ytick.labelsize': 20\n}\npylab.rcParams.update(params)\nnp.set_printoptions(suppress=True)\n\n\n실행에 필요한 helper function\n\ndef policy_evaluation(pi, P, gamma=1.0, theta=1e-10):\n    prev_V = np.zeros(len(P), dtype=np.float64)\n    while True:\n        V = np.zeros(len(P), dtype=np.float64)\n        for s in range(len(P)):\n            for prob, next_state, reward, done in P[s][pi(s)]:\n                V[s] += prob * (reward + gamma * prev_V[next_state] * (not done))\n        if np.max(np.abs(prev_V - V)) &lt; theta:\n            break\n        prev_V = V.copy()\n    return V\n\n\ndef print_policy(pi, P, action_symbols=('&lt;', 'v', '&gt;', '^'), n_cols=4, title='정책:'):\n    print(title)\n    arrs = {k:v for k,v in enumerate(action_symbols)}\n    for s in range(len(P)):\n        a = pi(s)\n        print(\"| \", end=\"\")\n        if np.all([done for action in P[s].values() for _, _, _, done in action]):\n            print(\"\".rjust(9), end=\" \")\n        else:\n            print(str(s).zfill(2), arrs[a].rjust(6), end=\" \")\n        if (s + 1) % n_cols == 0: print(\"|\")\n\n\ndef print_state_value_function(V, P, n_cols=4, prec=3, title='상태-가치 함수:'):\n    print(title)\n    for s in range(len(P)):\n        v = V[s]\n        print(\"| \", end=\"\")\n        if np.all([done for action in P[s].values() for _, _, _, done in action]):\n            print(\"\".rjust(9), end=\" \")\n        else:\n            print(str(s).zfill(2), '{}'.format(np.round(v, prec)).rjust(6), end=\" \")\n        if (s + 1) % n_cols == 0: print(\"|\")\n\n\ndef print_action_value_function(Q, \n                                optimal_Q=None, \n                                action_symbols=('&lt;', '&gt;'), \n                                prec=3, \n                                title='행동-가치 함수:'):\n    vf_types=('',) if optimal_Q is None else ('', '*', 'er')\n    headers = ['s',] + [' '.join(i) for i in list(itertools.product(vf_types, action_symbols))]\n    print(title)\n    states = np.arange(len(Q))[..., np.newaxis]\n    arr = np.hstack((states, np.round(Q, prec)))\n    if not (optimal_Q is None):\n        arr = np.hstack((arr, np.round(optimal_Q, prec), np.round(optimal_Q-Q, prec)))\n    print(tabulate(arr, headers, tablefmt=\"fancy_grid\"))\n\n\ndef probability_success(env, pi, goal_state, n_episodes=100, max_steps=200):\n    random.seed(123); np.random.seed(123) ; env.seed(123)\n    results = []\n    for _ in range(n_episodes):\n        state, done, steps = env.reset(), False, 0\n        while not done and steps &lt; max_steps:\n            state, _, done, h = env.step(pi(state))\n            steps += 1\n        results.append(state == goal_state)\n    return np.sum(results)/len(results)*100\n\n\ndef mean_return(env, gamma, pi, n_episodes=100, max_steps=200):\n    random.seed(123); np.random.seed(123) ; env.seed(123)\n    results = []\n    for _ in range(n_episodes):\n        state, done, steps = env.reset(), False, 0\n        results.append(0.0)\n        while not done and steps &lt; max_steps:\n            state, reward, done, _ = env.step(pi(state))\n            results[-1] += (gamma**steps * reward)\n            steps += 1\n    return np.mean(results)\n\n\ndef rmse(x, y, dp=4):\n    return np.round(np.sqrt(np.mean((x - y)**2)), dp)\n\n\ndef plot_value_function(title, V_track, V_true=None, log=False, limit_value=0.05, limit_items=5):\n    np.random.seed(123)\n    per_col = 25\n    linecycler = cycle([\"-\",\"--\",\":\",\"-.\"])\n    legends = []\n\n    valid_values = np.argwhere(V_track[-1] &gt; limit_value).squeeze()\n    items_idxs = np.random.choice(valid_values, \n                                  min(len(valid_values), limit_items), \n                                  replace=False)\n    # draw the true values first\n    if V_true is not None:\n        for i, state in enumerate(V_track.T):\n            if i not in items_idxs:\n                continue\n            if state[-1] &lt; limit_value:\n                continue\n\n            label = 'v({})'.format(i)\n            plt.axhline(y=V_true[i], color='k', linestyle='-', linewidth=1)\n            plt.text(int(len(V_track)*1.02), V_true[i]+.01, label)\n\n    # then the estimates\n    for i, state in enumerate(V_track.T):\n        if i not in items_idxs:\n            continue\n        if state[-1] &lt; limit_value:\n            continue\n        line_type = next(linecycler)\n        label = 'V({})'.format(i)\n        p, = plt.plot(state, line_type, label=label, linewidth=3)\n        legends.append(p)\n        \n    legends.reverse()\n\n    ls = []\n    for loc, idx in enumerate(range(0, len(legends), per_col)):\n        subset = legends[idx:idx+per_col]\n        l = plt.legend(subset, [p.get_label() for p in subset], \n                       loc='center right', bbox_to_anchor=(1.25, 0.5))\n        ls.append(l)\n    [plt.gca().add_artist(l) for l in ls[:-1]]\n    if log: plt.xscale('log')\n    plt.title(title)\n    plt.ylabel('State-value function')\n    plt.xlabel('Episodes (log scale)' if log else 'Episodes')\n    plt.show()\n\n\ndef plot_targets(targets, init_state, title):\n    x = range(len(targets[init_state]))\n    y = targets[init_state]\n    \n    label = 'v({})'.format(init_state)\n    plt.axhline(y=V_true[init_state], color='k', linestyle='-', linewidth=1)\n    plt.text(int(x[-1]*1.02), V_true[init_state]+.01, label)\n\n    plt.scatter(x, y, \n                c=np.array(targets[init_state]),\n                cmap=plt.get_cmap('viridis'),\n                alpha=0.4)\n    plt.title(title)\n\n    plt.ylabel('Target value')\n    plt.xlabel('Estimate sequence number')\n    plt.show()\n\n\ndef decay_schedule(init_value, min_value, decay_ratio, max_steps, log_start=-2, log_base=10):\n    decay_steps = int(max_steps * decay_ratio)\n    rem_steps = max_steps - decay_steps\n    values = np.logspace(log_start, 0, decay_steps, base=log_base, endpoint=True)[::-1]\n    values = (values - values.min()) / (values.max() - values.min())\n    values = (init_value - min_value) * values + min_value\n    values = np.pad(values, (0, rem_steps), 'edge')\n    return values\n\n\nplt.plot(decay_schedule(0.5, 0.01, 0.5, 500))\nplt.title('Exponentially decaying schedule (for alpha)')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n랜덤 워크 / 동일한 확률을 가지는 임의의 정책을 수행한 결정 통로\n\nenv = gym.make('RandomWalk-v0')\ninit_state = env.reset()\ngoal_state = 6\ngamma = 1.0\nn_episodes = 500\nP = env.env.P\n\nLEFT, RIGHT = range(2)\npi = lambda s: {\n    0:LEFT, 1:LEFT, 2:LEFT, 3:LEFT, 4:LEFT, 5:LEFT, 6:LEFT\n}[s]\nV_true = policy_evaluation(pi, P, gamma=gamma)\n\nprint_state_value_function(V_true, P, n_cols=7)\nprint()\nprint_policy(pi, P, action_symbols=('&lt;', '&gt;'), n_cols=7)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}.'.format(\n    probability_success(env, pi, goal_state=goal_state), \n    mean_return(env, gamma, pi)))\n\n상태-가치 함수:\n|           | 01  0.167 | 02  0.333 | 03    0.5 | 04  0.667 | 05  0.833 |           |\n\n정책:\n|           | 01      &lt; | 02      &lt; | 03      &lt; | 04      &lt; | 05      &lt; |           |\nReaches goal 53.00%. Obtains an average return of 0.5300.\n\n\n\n\n첫 방문 몬테카를로 예측\n\ndef generate_trajectory(pi, env, max_steps=200):\n    done, trajectory = False, []\n    while not done:\n        state = env.reset()\n        for t in count():\n            action = pi(state) \n            next_state, reward, done, _ = env.step(action)\n            experience = (state, action, reward, next_state, done)\n            trajectory.append(experience)\n            if done:\n                break\n            if t &gt;= max_steps - 1:\n                trajectory = []\n                break\n            state = next_state\n    return np.array(trajectory, np.object)\n\n\ndef mc_prediction(pi, \n                  env, \n                  gamma=1.0,\n                  init_alpha=0.5,\n                  min_alpha=0.01,\n                  alpha_decay_ratio=0.5,\n                  n_episodes=500, \n                  max_steps=200,\n                  first_visit=True):\n    nS = env.observation_space.n\n    discounts = np.logspace(0, \n                            max_steps, \n                            num=max_steps, \n                            base=gamma, \n                            endpoint=False)\n    alphas = decay_schedule(init_alpha, \n                            min_alpha, \n                            alpha_decay_ratio, \n                            n_episodes)\n    V = np.zeros(nS, dtype=np.float64)\n    V_track = np.zeros((n_episodes, nS), dtype=np.float64)\n    targets = {state:[] for state in range(nS)}\n\n    for e in tqdm(range(n_episodes), leave=False):\n        trajectory = generate_trajectory(pi, \n                                         env, \n                                         max_steps)\n        visited = np.zeros(nS, dtype=np.bool)\n        for t, (state, _, reward, _, _) in enumerate(trajectory):\n            if visited[state] and first_visit:\n                continue\n            visited[state] = True\n\n            n_steps = len(trajectory[t:])\n            G = np.sum(discounts[:n_steps] * trajectory[t:, 2])\n            targets[state].append(G)\n            mc_error = G - V[state]\n            V[state] = V[state] + alphas[e] * mc_error\n        V_track[e] = V\n    return V.copy(), V_track, targets\n\n\nV_fvmcs, V_track_fvmcs = [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    V_fvmc, V_track_fvmc, targets_fvmc = mc_prediction(pi, env, gamma=gamma, n_episodes=n_episodes)\n    V_fvmcs.append(V_fvmc) ; V_track_fvmcs.append(V_track_fvmc)\nV_fvmc, V_track_fvmc = np.mean(V_fvmcs, axis=0), np.mean(V_track_fvmcs, axis=0)\ndel V_fvmcs ; del V_track_fvmcs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_fvmc, P, n_cols=7)\nprint()\nprint_state_value_function(V_fvmc - V_true, P, n_cols=7, title='State-value function errors:')\nprint('RMSE:', rmse(V_fvmc, V_true))\n\n상태-가치 함수:\n|           | 01  0.172 | 02  0.338 | 03  0.509 | 04  0.671 | 05  0.822 |           |\n\nState-value function errors:\n|           | 01  0.005 | 02  0.005 | 03  0.009 | 04  0.004 | 05 -0.011 |           |\nRMSE: 0.006\n\n\n\nplot_value_function('FVMC estimates through time vs. true values', V_track_fvmc, V_true, log=False)\n\n\n\n\n\nplot_value_function('FVMC estimates through time vs. true values (log scale)', V_track_fvmc, V_true, log=True)\n\n\n\n\n\n모든 방문 몬테카를로 예측\n\nV_evmcs, V_track_evmcs = [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    V_evmc, V_track_evmc, targets_evmc = mc_prediction(pi, env, gamma=gamma, n_episodes=n_episodes, first_visit=False)\n    V_evmcs.append(V_evmc) ; V_track_evmcs.append(V_track_evmc)\nV_evmc, V_track_evmc = np.mean(V_evmcs, axis=0), np.mean(V_track_evmcs, axis=0)\ndel V_evmcs ; del V_track_evmcs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_evmc, P, n_cols=7)\nprint()\nprint_state_value_function(V_evmc - V_true, P, n_cols=7, title='State-value function errors:')\nprint('RMSE:', rmse(V_evmc, V_true))\n\n상태-가치 함수:\n|           | 01  0.179 | 02  0.377 | 03  0.549 | 04  0.692 | 05   0.83 |           |\n\nState-value function errors:\n|           | 01  0.013 | 02  0.044 | 03  0.049 | 04  0.026 | 05 -0.004 |           |\nRMSE: 0.0271\n\n\n\nplot_value_function('EVMC estimates through time vs. true values', V_track_evmc, V_true, log=False)\n\n\n\n\n\nplot_value_function('EVMC estimates through time vs. true values (log scale)', V_track_evmc, V_true, log=True)\n\n\n\n\n\n\n시간차 예측 (TD)\n\ndef td(pi, \n       env, \n       gamma=1.0,\n       init_alpha=0.5,\n       min_alpha=0.01,\n       alpha_decay_ratio=0.5,\n       n_episodes=500):\n    nS = env.observation_space.n\n    V = np.zeros(nS, dtype=np.float64)\n    V_track = np.zeros((n_episodes, nS), dtype=np.float64)\n    targets = {state:[] for state in range(nS)}\n    alphas = decay_schedule(\n        init_alpha, min_alpha,\n        alpha_decay_ratio, n_episodes)\n    for e in tqdm(range(n_episodes), leave=False):\n        state, done = env.reset(), False\n        while not done:\n            action = pi(state)\n            next_state, reward, done, _ = env.step(action)\n            td_target = reward + gamma * V[next_state] * (not done)\n            targets[state].append(td_target)\n            td_error = td_target - V[state]\n            V[state] = V[state] + alphas[e] * td_error\n            state = next_state\n        V_track[e] = V\n    return V, V_track, targets\n\n\nV_tds, V_track_tds = [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    V_td, V_track_td, targets_td = td(pi, env, gamma=gamma, n_episodes=n_episodes)\n    V_tds.append(V_td) ; V_track_tds.append(V_track_td)\nV_td, V_track_td = np.mean(V_tds, axis=0), np.mean(V_track_tds, axis=0)\ndel V_tds ; del V_track_tds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_td, P, n_cols=7)\nprint()\nprint_state_value_function(V_td - V_true, P, n_cols=7, title='State-value function errors:')\nprint('RMSE:', rmse(V_td, V_true))\n\n상태-가치 함수:\n|           | 01  0.166 | 02  0.335 | 03  0.511 | 04   0.67 | 05  0.835 |           |\n\nState-value function errors:\n|           | 01   -0.0 | 02  0.001 | 03  0.011 | 04  0.003 | 05  0.001 |           |\nRMSE: 0.0043\n\n\n\nplot_value_function('TD estimates through time vs. true values', V_track_td, V_true, log=False)\n\n\n\n\n\nplot_value_function('TD estimates through time vs. true values (log scale)', V_track_td, V_true, log=True)\n\n\n\n\n\n\n몬테카를로와 TD간의 비교\n\nplot_value_function('FVMC estimates through time (close up)', V_track_fvmc[:20], None, log=False)\n\n\n\n\n\nplot_value_function('EVMC estimates through time (close up)', V_track_evmc[:20], None, log=False)\n\n\n\n\n\nplot_value_function('TD estimates through time (close up)', V_track_td[:20], None, log=False)\n\n\n\n\n\nplot_targets(targets_fvmc, init_state, title='FVMC target sequence')\n\n\n\n\n\nplot_targets(targets_evmc, init_state, title='EVMC target sequence')\n\n\n\n\n\nplot_targets(targets_td, init_state, title='TD target sequence')\n\n\n\n\n\n\nn단계 TD\n\ndef ntd(pi, \n        env, \n        gamma=1.0,\n        init_alpha=0.5,\n        min_alpha=0.01,\n        alpha_decay_ratio=0.5,\n        n_step=3,\n        n_episodes=500):\n    nS = env.observation_space.n\n    V = np.zeros(nS, dtype=np.float64)\n    V_track = np.zeros((n_episodes, nS), dtype=np.float64)\n    discounts = np.logspace(0, n_step+1, num=n_step+1, base=gamma, endpoint=False)\n    alphas = decay_schedule(\n        init_alpha, min_alpha, \n        alpha_decay_ratio, n_episodes)\n    for e in tqdm(range(n_episodes), leave=False):\n        state, done, path = env.reset(), False, []\n        while not done or path is not None:\n            path = path[1:]\n            while not done and len(path) &lt; n_step:\n                action = pi(state)\n                next_state, reward, done, _ = env.step(action)\n                experience = (state, reward, next_state, done)\n                path.append(experience)\n                state = next_state\n                if done:\n                    break\n\n            n = len(path)\n            est_state = path[0][0]\n            rewards = np.array(path)[:,1]\n            partial_return = discounts[:n] * rewards\n            bs_val = discounts[-1] * V[next_state] * (not done)\n            ntd_target = np.sum(np.append(partial_return, bs_val))\n            ntd_error = ntd_target - V[est_state]\n            V[est_state] = V[est_state] + alphas[e] * ntd_error\n            if len(path) == 1 and path[0][3]:\n                path = None\n\n        V_track[e] = V\n    return V, V_track\n\n\nV_ntds, V_track_ntds = [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    V_ntd, V_track_ntd = ntd(pi, env, gamma=gamma, n_episodes=n_episodes)\n    V_ntds.append(V_ntd) ; V_track_ntds.append(V_track_ntd)\nV_ntd, V_track_ntd = np.mean(V_ntds, axis=0), np.mean(V_track_ntds, axis=0)\ndel V_ntds ; del V_track_ntds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_ntd, P, n_cols=7)\nprint()\nprint_state_value_function(V_ntd - V_true, P, n_cols=7, title='State-value function errors:')\nprint('RMSE:', rmse(V_ntd, V_true))\n\n상태-가치 함수:\n|           | 01  0.174 | 02  0.344 | 03  0.516 | 04  0.673 | 05  0.826 |           |\n\nState-value function errors:\n|           | 01  0.007 | 02  0.011 | 03  0.016 | 04  0.006 | 05 -0.007 |           |\nRMSE: 0.0087\n\n\n\nplot_value_function('n-step TD estimates through time vs. true values', V_track_ntd, V_true, log=False)\n\n\n\n\n\nplot_value_function('n-step TD estimates through time vs. true values (log scale)', V_track_ntd, V_true, log=True)\n\n\n\n\n\nplot_value_function('n-step TD estimates through time (close up)', V_track_ntd[:20], None, log=False)\n\n\n\n\n\n\nTD(λ)\n\ndef td_lambda(pi, \n              env, \n              gamma=1.0,\n              init_alpha=0.5,\n              min_alpha=0.01,\n              alpha_decay_ratio=0.5,\n              lambda_=0.3,\n              n_episodes=500):\n    nS = env.observation_space.n\n    V = np.zeros(nS, dtype=np.float64)\n    E = np.zeros(nS, dtype=np.float64)\n    V_track = np.zeros((n_episodes, nS), dtype=np.float64)\n    alphas = decay_schedule(\n        init_alpha, min_alpha, \n        alpha_decay_ratio, n_episodes)\n    for e in tqdm(range(n_episodes), leave=False):\n        E.fill(0)\n        state, done = env.reset(), False\n        while not done:\n            action = pi(state)\n            next_state, reward, done, _ = env.step(action)\n            td_target = reward + gamma * V[next_state] * (not done)\n            td_error = td_target - V[state]\n            E[state] = E[state] + 1\n            V = V + alphas[e] * td_error * E\n            E = gamma * lambda_ * E\n            state = next_state\n        V_track[e] = V\n    return V, V_track\n\n\nV_tdls, V_track_tdls = [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    V_tdl, V_track_tdl = td_lambda(pi, env, gamma=gamma, n_episodes=n_episodes)\n    V_tdls.append(V_tdl) ; V_track_tdls.append(V_track_tdl)\nV_tdl, V_track_tdl = np.mean(V_tdls, axis=0), np.mean(V_track_tdls, axis=0)\ndel V_tdls ; del V_track_tdls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_tdl, P, n_cols=7)\nprint()\nprint_state_value_function(V_tdl - V_true, P, n_cols=7, title='State-value function errors:')\nprint('RMSE:', rmse(V_tdl, V_true))\n\n상태-가치 함수:\n|           | 01  0.169 | 02  0.338 | 03  0.512 | 04   0.67 | 05  0.833 |           |\n\nState-value function errors:\n|           | 01  0.002 | 02  0.005 | 03  0.012 | 04  0.003 | 05   -0.0 |           |\nRMSE: 0.005\n\n\n\nplot_value_function('TD(λ) estimates through time vs. true values', V_track_tdl, V_true, log=False)\n\n\n\n\n\nplot_value_function('TD(λ) estimates through time vs. true values (log scale)', V_track_tdl, V_true, log=True)\n\n\n\n\n\nplot_value_function('TD(λ) estimates through time (close up)', V_track_tdl[:20], None, log=False)\n\n\n\n\n\n\n\nRussell & Norvig의 Gridworld 환경에서의 정책\n\nenv = gym.make('RussellNorvigGridworld-v0')\ninit_state = env.reset()\ngoal_state = 3\ngamma = 1.0\nn_episodes = 1000\nP = env.env.P\n\nLEFT, DOWN, RIGHT, UP = range(4)\npi = lambda s: {\n    0:RIGHT, 1:RIGHT, 2:RIGHT, 3:LEFT,\n    4:UP,    5:LEFT,  6:UP,    7:LEFT,\n    8:UP,    9:LEFT, 10:LEFT, 11:LEFT\n}[s]\nV_true = policy_evaluation(pi, P, gamma=gamma)\n\nprint_state_value_function(V_true, P)\nprint()\n\nprint_policy(pi, P)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}.'.format(\n    probability_success(env, pi, goal_state=goal_state), \n    mean_return(env, gamma, pi)))\n\n상태-가치 함수:\n| 00  0.812 | 01  0.868 | 02  0.918 |           |\n| 04  0.762 |           | 06   0.66 |           |\n| 08  0.705 | 09  0.655 | 10  0.611 | 11  0.388 |\n\n정책:\n| 00      &gt; | 01      &gt; | 02      &gt; |           |\n| 04      ^ |           | 06      ^ |           |\n| 08      ^ | 09      &lt; | 10      &lt; | 11      &lt; |\nReaches goal 96.00%. Obtains an average return of 0.6424.\n\n\n\nRussell & Norvig의 Gridworld 환경에서의 정책 추정 (정책을 상태-가치 함수로 변환)\n\nV_fvmcs, V_track_fvmcs = [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    V_fvmc, V_track_fvmc, targets_fvmc = mc_prediction(pi, env, gamma=gamma, n_episodes=n_episodes)\n    V_fvmcs.append(V_fvmc) ; V_track_fvmcs.append(V_track_fvmc)\nV_fvmc, V_track_fvmc = np.mean(V_fvmcs, axis=0), np.mean(V_track_fvmcs, axis=0)\ndel V_fvmcs ; del V_track_fvmcs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_fvmc, P)\nprint()\nprint_state_value_function(V_fvmc - V_true, P, title='State-value function errors:')\nprint('RMSE:', rmse(V_fvmc, V_true))\n\n상태-가치 함수:\n| 00   0.81 | 01  0.867 | 02  0.918 |           |\n| 04  0.759 |           | 06  0.669 |           |\n| 08  0.703 | 09  0.643 | 10    0.0 | 11    0.0 |\n\nState-value function errors:\n| 00 -0.001 | 01 -0.001 | 02    0.0 |           |\n| 04 -0.002 |           | 06  0.008 |           |\n| 08 -0.002 | 09 -0.012 | 10 -0.611 | 11 -0.388 |\nRMSE: 0.2091\n\n\n\nV_evmcs, V_track_evmcs = [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    V_evmc, V_track_evmc, targets_evmc = mc_prediction(pi, env, gamma=gamma, n_episodes=n_episodes, first_visit=False)\n    V_evmcs.append(V_evmc) ; V_track_evmcs.append(V_track_evmc)\nV_evmc, V_track_evmc = np.mean(V_evmcs, axis=0), np.mean(V_track_evmcs, axis=0)\ndel V_evmcs ; del V_track_evmcs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_evmc, P)\nprint()\nprint_state_value_function(V_evmc - V_true, P, title='State-value function errors:')\nprint('RMSE:', rmse(V_evmc, V_true))\n\n상태-가치 함수:\n| 00  0.809 | 01  0.867 | 02  0.918 |           |\n| 04  0.748 |           | 06   0.68 |           |\n| 08  0.696 | 09  0.636 | 10    0.0 | 11    0.0 |\n\nState-value function errors:\n| 00 -0.003 | 01   -0.0 | 02  0.001 |           |\n| 04 -0.013 |           | 06   0.02 |           |\n| 08 -0.009 | 09 -0.019 | 10 -0.611 | 11 -0.388 |\nRMSE: 0.2092\n\n\n\nV_tds, V_track_tds = [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    V_td, V_track_td, targets_td = td(pi, env, gamma=gamma, n_episodes=n_episodes)\n    V_tds.append(V_td) ; V_track_tds.append(V_track_td)\nV_td, V_track_td = np.mean(V_tds, axis=0), np.mean(V_track_tds, axis=0)\ndel V_tds ; del V_track_tds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_td, P)\nprint()\nprint_state_value_function(V_td - V_true, P, title='State-value function errors:')\nprint('RMSE:', rmse(V_td, V_true))\n\n상태-가치 함수:\n| 00  0.811 | 01  0.867 | 02  0.913 |           |\n| 04  0.761 |           | 06  0.651 |           |\n| 08  0.704 | 09  0.643 | 10    0.0 | 11    0.0 |\n\nState-value function errors:\n| 00 -0.001 | 01 -0.001 | 02 -0.005 |           |\n| 04 -0.001 |           | 06  -0.01 |           |\n| 08 -0.001 | 09 -0.012 | 10 -0.611 | 11 -0.388 |\nRMSE: 0.2091\n\n\n\nV_ntds, V_track_ntds = [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    V_ntd, V_track_ntd = ntd(pi, env, gamma=gamma, n_episodes=n_episodes)\n    V_ntds.append(V_ntd) ; V_track_ntds.append(V_track_ntd)\nV_ntd, V_track_ntd = np.mean(V_ntds, axis=0), np.mean(V_track_ntds, axis=0)\ndel V_ntds ; del V_track_ntds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_ntd, P)\nprint()\nprint_state_value_function(V_ntd - V_true, P, title='State-value function errors:')\nprint('RMSE:', rmse(V_ntd, V_true))\n\n상태-가치 함수:\n| 00  0.808 | 01  0.868 | 02  0.915 |           |\n| 04  0.763 |           | 06  0.674 |           |\n| 08  0.705 | 09  0.659 | 10    0.0 | 11    0.0 |\n\nState-value function errors:\n| 00 -0.003 | 01    0.0 | 02 -0.002 |           |\n| 04  0.001 |           | 06  0.014 |           |\n| 08    0.0 | 09  0.004 | 10 -0.611 | 11 -0.388 |\nRMSE: 0.2091\n\n\n\nV_tdls, V_track_tdls = [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    V_tdl, V_track_tdl = td_lambda(pi, env, gamma=gamma, n_episodes=n_episodes)\n    V_tdls.append(V_tdl) ; V_track_tdls.append(V_track_tdl)\nV_tdl, V_track_tdl = np.mean(V_tdls, axis=0), np.mean(V_track_tdls, axis=0)\ndel V_tdls ; del V_track_tdls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_tdl, P)\nprint()\nprint_state_value_function(V_tdl - V_true, P, title='State-value function errors:')\nprint('RMSE:', rmse(V_tdl, V_true))\n\n상태-가치 함수:\n| 00  0.811 | 01  0.867 | 02  0.915 |           |\n| 04  0.761 |           | 06  0.655 |           |\n| 08  0.706 | 09  0.651 | 10    0.0 | 11    0.0 |\n\nState-value function errors:\n| 00   -0.0 | 01 -0.001 | 02 -0.003 |           |\n| 04   -0.0 |           | 06 -0.005 |           |\n| 08  0.001 | 09 -0.004 | 10 -0.611 | 11 -0.388 |\nRMSE: 0.209\n\n\n\n\nRussell & Norvig의 Gridworld 환경에서의 상태-가치 함수 추정\n\nplot_value_function('FVMC estimates through time vs. true values', V_track_fvmc, V_true, log=False)\n\n\n\n\n\nplot_value_function('EVMC estimates through time vs. true values', V_track_evmc, V_true, log=False)\n\n\n\n\n\nplot_value_function('TD estimates through time vs. true values', V_track_td, V_true, log=False)\n\n\n\n\n\nplot_value_function('n-step TD estimates through time vs. true values', V_track_ntd, V_true, log=False)\n\n\n\n\n\nplot_value_function('TD(λ) estimates through time vs. true values', V_track_tdl, V_true, log=False)\n\n\n\n\n\n\nRussell & Norvig의 Gridworld 환경에서의 상태-가치 함수 추정 (x축에 대해서는 log scale 적용)\n\nplot_value_function('FVMC estimates through time vs. true values (log scale)', V_track_fvmc, V_true, log=True)\n\n\n\n\n\nplot_value_function('EVMC estimates through time vs. true values (log scale)', V_track_evmc, V_true, log=True)\n\n\n\n\n\nplot_value_function('TD estimates through time vs. true values (log scale)', V_track_td, V_true, log=True)\n\n\n\n\n\nplot_value_function('n-step TD estimates through time vs. true values (log scale)', V_track_ntd, V_true, log=True)\n\n\n\n\n\nplot_value_function('TD(λ) estimates through time vs. true values (log scale)', V_track_tdl, V_true, log=True)\n\n\n\n\n\n\n결과에 대한 확대\n\nplot_value_function('FVMC estimates through time (close up)', V_track_fvmc[:50], None, log=False)\n\n\n\n\n\nplot_value_function('EVMC estimates through time (close up)', V_track_evmc[:50], None, log=False)\n\n\n\n\n\nplot_value_function('TD estimates through time (close up)', V_track_td[:50], None, log=False)\n\n\n\n\n\nplot_value_function('n-step TD estimates through time (close up)', V_track_ntd[:50], None, log=False)\n\n\n\n\n\nplot_value_function('TD(λ) estimates through time (close up)', V_track_tdl[:50], None, log=False)\n\n\n\n\n\n\n몬테카를로와 TD간의 비교\n\nplot_targets(targets_fvmc, init_state, title='FVMC target sequence')\n\n\n\n\n\nplot_targets(targets_evmc, init_state, title='EVMC target sequence')\n\n\n\n\n\nplot_targets(targets_td, init_state, title='TD target sequence')\n\n\n\n\n\n\n\n프로즌레이크 환경에서의 샘플 정책\n\nenv = gym.make('FrozenLake-v0')\ninit_state = env.reset()\ngoal_state = 15\ngamma = 0.99\nn_episodes = 2500\nP = env.env.P\n\nLEFT, DOWN, RIGHT, UP = range(4)\npi = lambda s: {\n    0:LEFT,   1:UP,     2:UP,    3:UP,\n    4:LEFT,   5:LEFT,   6:LEFT,  7:LEFT,\n    8:UP,     9:DOWN,  10:LEFT, 11:LEFT,\n    12:LEFT, 13:RIGHT, 14:DOWN, 15:LEFT\n}[s]\nV_true = policy_evaluation(pi, P, gamma=gamma)\n\nprint_state_value_function(V_true, P)\nprint()\n\nprint_policy(pi, P)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}.'.format(\n    probability_success(env, pi, goal_state=goal_state), \n    mean_return(env, gamma, pi)))\n\n상태-가치 함수:\n| 00  0.542 | 01  0.499 | 02  0.471 | 03  0.457 |\n| 04  0.558 |           | 06  0.358 |           |\n| 08  0.592 | 09  0.643 | 10  0.615 |           |\n|           | 13  0.742 | 14  0.863 |           |\n\n정책:\n| 00      &lt; | 01      ^ | 02      ^ | 03      ^ |\n| 04      &lt; |           | 06      &lt; |           |\n| 08      ^ | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      v |           |\nReaches goal 74.00%. Obtains an average return of 0.5116.\n\n\n\n프로즌레이크 환경에서의 정책 추정 (정책을 상태-가치 함수로 변환)\n\nV_fvmcs, V_track_fvmcs = [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    V_fvmc, V_track_fvmc, targets_fvmc = mc_prediction(pi, env, gamma=gamma, n_episodes=n_episodes)\n    V_fvmcs.append(V_fvmc) ; V_track_fvmcs.append(V_track_fvmc)\nV_fvmc, V_track_fvmc = np.mean(V_fvmcs, axis=0), np.mean(V_track_fvmcs, axis=0)\ndel V_fvmcs ; del V_track_fvmcs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_fvmc, P)\nprint()\nprint_state_value_function(V_fvmc - V_true, P, title='State-value function errors:')\nprint('RMSE:', rmse(V_fvmc, V_true))\n\n상태-가치 함수:\n| 00  0.528 | 01  0.359 | 02  0.336 | 03  0.313 |\n| 04  0.544 |           | 06  0.296 |           |\n| 08  0.575 | 09  0.618 | 10  0.579 |           |\n|           | 13  0.712 | 14  0.841 |           |\n\nState-value function errors:\n| 00 -0.014 | 01  -0.14 | 02 -0.135 | 03 -0.143 |\n| 04 -0.014 |           | 06 -0.063 |           |\n| 08 -0.016 | 09 -0.025 | 10 -0.037 |           |\n|           | 13  -0.03 | 14 -0.022 |           |\nRMSE: 0.0643\n\n\n\nV_evmcs, V_track_evmcs = [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    V_evmc, V_track_evmc, targets_evmc = mc_prediction(pi, env, gamma=gamma, n_episodes=n_episodes, first_visit=False)\n    V_evmcs.append(V_evmc) ; V_track_evmcs.append(V_track_evmc)\nV_evmc, V_track_evmc = np.mean(V_evmcs, axis=0), np.mean(V_track_evmcs, axis=0)\ndel V_evmcs ; del V_track_evmcs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_evmc, P)\nprint()\nprint_state_value_function(V_evmc - V_true, P, title='State-value function errors:')\nprint('RMSE:', rmse(V_evmc, V_true))\n\n상태-가치 함수:\n| 00  0.456 | 01  0.323 | 02  0.278 | 03  0.277 |\n| 04   0.49 |           | 06  0.277 |           |\n| 08  0.519 | 09  0.561 | 10  0.531 |           |\n|           | 13  0.668 | 14  0.836 |           |\n\nState-value function errors:\n| 00 -0.086 | 01 -0.176 | 02 -0.192 | 03 -0.179 |\n| 04 -0.069 |           | 06 -0.082 |           |\n| 08 -0.072 | 09 -0.082 | 10 -0.084 |           |\n|           | 13 -0.074 | 14 -0.027 |           |\nRMSE: 0.095\n\n\n\nV_tds, V_track_tds = [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    V_td, V_track_td, targets_td = td(pi, env, gamma=gamma, n_episodes=n_episodes)\n    V_tds.append(V_td) ; V_track_tds.append(V_track_td)\nV_td, V_track_td = np.mean(V_tds, axis=0), np.mean(V_track_tds, axis=0)\ndel V_tds ; del V_track_tds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_td, P)\nprint()\nprint_state_value_function(V_td - V_true, P, title='State-value function errors:')\nprint('RMSE:', rmse(V_td, V_true))\n\n상태-가치 함수:\n| 00  0.508 | 01  0.451 | 02  0.417 | 03  0.401 |\n| 04  0.527 |           | 06  0.337 |           |\n| 08  0.566 | 09   0.62 | 10  0.592 |           |\n|           | 13  0.722 | 14  0.851 |           |\n\nState-value function errors:\n| 00 -0.034 | 01 -0.048 | 02 -0.054 | 03 -0.056 |\n| 04 -0.031 |           | 06 -0.021 |           |\n| 08 -0.025 | 09 -0.023 | 10 -0.023 |           |\n|           | 13  -0.02 | 14 -0.012 |           |\nRMSE: 0.0287\n\n\n\nV_ntds, V_track_ntds = [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    V_ntd, V_track_ntd = ntd(pi, env, gamma=gamma, n_episodes=n_episodes)\n    V_ntds.append(V_ntd) ; V_track_ntds.append(V_track_ntd)\nV_ntd, V_track_ntd = np.mean(V_ntds, axis=0), np.mean(V_track_ntds, axis=0)\ndel V_ntds ; del V_track_ntds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_ntd, P)\nprint()\nprint_state_value_function(V_ntd - V_true, P, title='State-value function errors:')\nprint('RMSE:', rmse(V_ntd, V_true))\n\n상태-가치 함수:\n| 00  0.517 | 01  0.453 | 02  0.419 | 03  0.402 |\n| 04  0.535 |           | 06  0.334 |           |\n| 08   0.57 | 09   0.62 | 10  0.585 |           |\n|           | 13  0.717 | 14  0.849 |           |\n\nState-value function errors:\n| 00 -0.025 | 01 -0.046 | 02 -0.051 | 03 -0.055 |\n| 04 -0.023 |           | 06 -0.024 |           |\n| 08 -0.021 | 09 -0.023 | 10  -0.03 |           |\n|           | 13 -0.025 | 14 -0.014 |           |\nRMSE: 0.0276\n\n\n\nV_tdls, V_track_tdls = [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    V_tdl, V_track_tdl = td_lambda(pi, env, gamma=gamma, n_episodes=n_episodes)\n    V_tdls.append(V_tdl) ; V_track_tdls.append(V_track_tdl)\nV_tdl, V_track_tdl = np.mean(V_tdls, axis=0), np.mean(V_track_tdls, axis=0)\ndel V_tdls ; del V_track_tdls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_tdl, P)\nprint()\nprint_state_value_function(V_tdl - V_true, P, title='State-value function errors:')\nprint('RMSE:', rmse(V_tdl, V_true))\n\n상태-가치 함수:\n| 00  0.512 | 01  0.452 | 02  0.418 | 03  0.402 |\n| 04   0.53 |           | 06  0.336 |           |\n| 08  0.569 | 09  0.621 | 10  0.591 |           |\n|           | 13  0.722 | 14  0.851 |           |\n\nState-value function errors:\n| 00  -0.03 | 01 -0.047 | 02 -0.053 | 03 -0.055 |\n| 04 -0.028 |           | 06 -0.022 |           |\n| 08 -0.023 | 09 -0.022 | 10 -0.024 |           |\n|           | 13  -0.02 | 14 -0.012 |           |\nRMSE: 0.0279\n\n\n\n\n프로즌레이크 환경에서의 상태-가치 함수 추정\n\nplot_value_function('FVMC estimates through time vs. true values', V_track_fvmc, V_true, log=False)\n\n\n\n\n\nplot_value_function('EVMC estimates through time vs. true values', V_track_evmc, V_true, log=False)\n\n\n\n\n\nplot_value_function('TD estimates through time vs. true values', V_track_td, V_true, log=False)\n\n\n\n\n\nplot_value_function('n-step TD estimates through time vs. true values', V_track_ntd, V_true, log=False)\n\n\n\n\n\nplot_value_function('TD(λ) estimates through time vs. true values', V_track_tdl, V_true, log=False)\n\n\n\n\n\n\n프로즌레이크 환경에서의 상태-가치 함수 추정 (x축을 log scale로 변환)\n\nplot_value_function('FVMC estimates through time vs. true values (log scale)', V_track_fvmc, V_true, log=True)\n\n\n\n\n\nplot_value_function('EVMC estimates through time vs. true values (log scale)', V_track_evmc, V_true, log=True)\n\n\n\n\n\nplot_value_function('TD estimates through time vs. true values (log scale)', V_track_td, V_true, log=True)\n\n\n\n\n\nplot_value_function('n-step TD estimates through time vs. true values (log scale)', V_track_ntd, V_true, log=True)\n\n\n\n\n\nplot_value_function('TD(λ) estimates through time vs. true values (log scale)', V_track_tdl, V_true, log=True)\n\n\n\n\n\n\n결과에 대한 확대\n\nplot_value_function('FVMC estimates through time (close up)', V_track_fvmc[:100], None, log=False)\n\n\n\n\n\nplot_value_function('EVMC estimates through time (close up)', V_track_evmc[:100], None, log=False)\n\n\n\n\n\nplot_value_function('TD estimates through time (close up)', V_track_td[:100], None, log=False)\n\n\n\n\n\nplot_value_function('n-step TD estimates through time (close up)', V_track_ntd[:100], None, log=False)\n\n\n\n\n\nplot_value_function('TD(λ) estimates through time (close up)', V_track_tdl[:100], None, log=False)\n\n\n\n\n\n\n몬테카를로와 TD간의 비교\n\nplot_targets(targets_fvmc, init_state, title='FVMC target sequence')\n\n\n\n\n\nplot_targets(targets_evmc, init_state, title='EVMC target sequence')\n\n\n\n\n\nplot_targets(targets_td, init_state, title='TD target sequence')"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-3.html#tldr",
    "href": "publication/GDRL/GDRL-chapter-3.html#tldr",
    "title": "Chapter 3: Balancing immediate and long-term goals",
    "section": "TL;DR",
    "text": "TL;DR\n\n그로킹 심층 강화학습 중 3장 내용인 “순간 목표와 장기 목표간의 균형”에 대한 내용입니다.\n\n\n\nCode\n!pip install tqdm numpy scikit-learn pyglet setuptools && \\\n!pip install gym asciinema pandas tabulate tornado==5.* PyBullet && \\\n!pip install git+https://github.com/pybox2d/pybox2d#egg=Box2D && \\\n!pip install git+https://github.com/mimoralea/gym-bandits#egg=gym-bandits && \\\n!pip install git+https://github.com/mimoralea/gym-walk#egg=gym-walk && \\\n!pip install git+https://github.com/mimoralea/gym-aima#egg=gym-aima && \\\n!pip install gym[atari]\n\n\n\nimport gym, gym_walk, gym_aima"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-3.html#정책-반복법-가치-반복법",
    "href": "publication/GDRL/GDRL-chapter-3.html#정책-반복법-가치-반복법",
    "title": "Chapter 3: Balancing immediate and long-term goals",
    "section": "정책 반복법, 가치 반복법",
    "text": "정책 반복법, 가치 반복법\n참고: 해당 노트북에서 사용되는 환경에 대한 정보는 아래 링크를 참고하시기 바랍니다.\n\ngym_walk\ngym_aima\n\n\nimport warnings ; warnings.filterwarnings('ignore')\n\nimport gym, gym_walk, gym_aima\nimport numpy as np\nfrom pprint import pprint\nfrom tqdm import tqdm_notebook as tqdm\n\nfrom itertools import cycle\n\nimport random\n\nnp.set_printoptions(suppress=True)\nrandom.seed(123); np.random.seed(123)\n\n\n출력을 위한 helper function\n\ndef print_policy(pi, P, action_symbols=('&lt;', 'v', '&gt;', '^'), n_cols=4, title='정책:'):\n    print(title)\n    arrs = {k:v for k,v in enumerate(action_symbols)}\n    for s in range(len(P)):\n        a = pi(s)\n        print(\"| \", end=\"\")\n        if np.all([done for action in P[s].values() for _, _, _, done in action]):\n            print(\"\".rjust(9), end=\" \")\n        else:\n            print(str(s).zfill(2), arrs[a].rjust(6), end=\" \")\n        if (s + 1) % n_cols == 0: print(\"|\")\n\n\ndef print_state_value_function(V, P, n_cols=4, prec=3, title='상태-가치 함수:'):\n    print(title)\n    for s in range(len(P)):\n        v = V[s]\n        print(\"| \", end=\"\")\n        if np.all([done for action in P[s].values() for _, _, _, done in action]):\n            print(\"\".rjust(9), end=\" \")\n        else:\n            print(str(s).zfill(2), '{}'.format(np.round(v, prec)).rjust(6), end=\" \")\n        if (s + 1) % n_cols == 0: print(\"|\")\n\n\ndef print_action_value_function(Q, \n                                optimal_Q=None, \n                                action_symbols=('&lt;', '&gt;'), \n                                prec=3, \n                                title='행동-가치 함수:'):\n    vf_types=('',) if optimal_Q is None else ('', '*', 'err')\n    headers = ['s',] + [' '.join(i) for i in list(itertools.product(vf_types, action_symbols))]\n    print(title)\n    states = np.arange(len(Q))[..., np.newaxis]\n    arr = np.hstack((states, np.round(Q, prec)))\n    if not (optimal_Q is None):\n        arr = np.hstack((arr, np.round(optimal_Q, prec), np.round(optimal_Q-Q, prec)))\n    print(tabulate(arr, headers, tablefmt=\"fancy_grid\"))\n\n\ndef probability_success(env, pi, goal_state, n_episodes=100, max_steps=200):\n    random.seed(123); np.random.seed(123) ; env.seed(123)\n    results = []\n    for _ in range(n_episodes):\n        state, done, steps = env.reset(), False, 0\n        while not done and steps &lt; max_steps:\n            state, _, done, h = env.step(pi(state))\n            steps += 1\n        results.append(state == goal_state)\n    return np.sum(results)/len(results)\n\n\ndef mean_return(env, pi, n_episodes=100, max_steps=200):\n    random.seed(123); np.random.seed(123) ; env.seed(123)\n    results = []\n    for _ in range(n_episodes):\n        state, done, steps = env.reset(), False, 0\n        results.append(0.0)\n        while not done and steps &lt; max_steps:\n            state, reward, done, _ = env.step(pi(state))\n            results[-1] += reward\n            steps += 1\n    return np.mean(results)\n\n\n\nSlippery Walk Five MDP and sample policy\n\nenv = gym.make('SlipperyWalkFive-v0')\nP = env.env.P\ninit_state = env.reset()\ngoal_state = 6\n\nLEFT, RIGHT = range(2)\npi = lambda s: {\n    0:LEFT, 1:LEFT, 2:LEFT, 3:LEFT, 4:LEFT, 5:LEFT, 6:LEFT\n}[s]\nprint_policy(pi, P, action_symbols=('&lt;', '&gt;'), n_cols=7)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, pi, goal_state=goal_state)*100, \n    mean_return(env, pi)))\n\n정책:\n|           | 01      &lt; | 02      &lt; | 03      &lt; | 04      &lt; | 05      &lt; |           |\nReaches goal 7.00%. Obtains an average undiscounted return of 0.0700.\n\n\n\n정책 평가법\n\ndef policy_evaluation(pi, P, gamma=1.0, theta=1e-10):\n    prev_V = np.zeros(len(P), dtype=np.float64)\n    while True:\n        V = np.zeros(len(P), dtype=np.float64)\n        for s in range(len(P)):\n            for prob, next_state, reward, done in P[s][pi(s)]:\n                V[s] += prob * (reward + gamma * prev_V[next_state] * (not done))\n        if np.max(np.abs(prev_V - V)) &lt; theta:\n            break\n        prev_V = V.copy()\n    return V\n\n\nV = policy_evaluation(pi, P)\nprint_state_value_function(V, P, n_cols=7, prec=5)\n\n상태-가치 함수:\n|           | 01 0.00275 | 02 0.01099 | 03 0.03571 | 04 0.10989 | 05 0.33242 |           |\n\n\n\n\n정책 개선법\n\ndef policy_improvement(V, P, gamma=1.0):\n    Q = np.zeros((len(P), len(P[0])), dtype=np.float64)\n    for s in range(len(P)):\n        for a in range(len(P[s])):\n            for prob, next_state, reward, done in P[s][a]:\n                Q[s][a] += prob * (reward + gamma * V[next_state] * (not done))\n    new_pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n    return new_pi\n\n\nimproved_pi = policy_improvement(V, P)\nprint_policy(improved_pi, P, action_symbols=('&lt;', '&gt;'), n_cols=7)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, improved_pi, goal_state=goal_state)*100, \n    mean_return(env, improved_pi)))\n\n정책:\n|           | 01      &gt; | 02      &gt; | 03      &gt; | 04      &gt; | 05      &gt; |           |\nReaches goal 93.00%. Obtains an average undiscounted return of 0.9300.\n\n\n\n# how about we evaluate the improved policy?\nimproved_V = policy_evaluation(improved_pi, P)\nprint_state_value_function(improved_V, P, n_cols=7, prec=5)\n\n상태-가치 함수:\n|           | 01 0.66758 | 02 0.89011 | 03 0.96429 | 04 0.98901 | 05 0.99725 |           |\n\n\n\n# can we improved the improved policy?\nimproved_improved_pi = policy_improvement(improved_V, P)\nprint_policy(improved_improved_pi, P, action_symbols=('&lt;', '&gt;'), n_cols=7)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, improved_improved_pi, goal_state=goal_state)*100, \n    mean_return(env, improved_improved_pi)))\n\n정책:\n|           | 01      &gt; | 02      &gt; | 03      &gt; | 04      &gt; | 05      &gt; |           |\nReaches goal 93.00%. Obtains an average undiscounted return of 0.9300.\n\n\n\n# it is the same policy\n# if we evaluate again, we can see there is nothing to improve \n# that also means we reached the optimal policy\nimproved_improved_V = policy_evaluation(improved_improved_pi, P)\nprint_state_value_function(improved_improved_V, P, n_cols=7, prec=5)\n\n상태-가치 함수:\n|           | 01 0.66758 | 02 0.89011 | 03 0.96429 | 04 0.98901 | 05 0.99725 |           |\n\n\n\n# state-value function didn't improve, then we reach the optimal policy\nassert np.all(improved_V == improved_improved_V)\n\n\n\n\n정책 반복법\n\ndef policy_iteration(P, gamma=1.0, theta=1e-10):\n    random_actions = np.random.choice(tuple(P[0].keys()), len(P))\n    pi = lambda s: {s:a for s, a in enumerate(random_actions)}[s]\n    while True:\n        old_pi = {s:pi(s) for s in range(len(P))}\n        V = policy_evaluation(pi, P, gamma, theta)\n        pi = policy_improvement(V, P, gamma)\n        if old_pi == {s:pi(s) for s in range(len(P))}:\n            break\n    return V, pi\n\n\noptimal_V, optimal_pi = policy_iteration(P)\nprint('Optimal policy and state-value function (PI):')\nprint_policy(optimal_pi, P, action_symbols=('&lt;', '&gt;'), n_cols=7)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, optimal_pi, goal_state=goal_state)*100, \n    mean_return(env, optimal_pi)))\nprint()\nprint_state_value_function(optimal_V, P, n_cols=7, prec=5)\n\nOptimal policy and state-value function (PI):\n정책:\n|           | 01      &gt; | 02      &gt; | 03      &gt; | 04      &gt; | 05      &gt; |           |\nReaches goal 93.00%. Obtains an average undiscounted return of 0.9300.\n\n상태-가치 함수:\n|           | 01 0.66758 | 02 0.89011 | 03 0.96429 | 04 0.98901 | 05 0.99725 |           |\n\n\n\nassert np.all(improved_V == optimal_V)\n\n\n\nFrozen Lake MDP and sample policies\n\nenv = gym.make('FrozenLake-v0')\nP = env.env.P\ninit_state = env.reset()\ngoal_state = 15\n\nLEFT, DOWN, RIGHT, UP = range(4)\nrandom_pi = lambda s: {\n    0:RIGHT, 1:LEFT, 2:DOWN, 3:UP,\n    4:LEFT, 5:LEFT, 6:RIGHT, 7:LEFT,\n    8:UP, 9:DOWN, 10:UP, 11:LEFT,\n    12:LEFT, 13:RIGHT, 14:DOWN, 15:LEFT\n}[s]\nprint_policy(random_pi, P)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, random_pi, goal_state=goal_state)*100, \n    mean_return(env, random_pi)))\n\n정책:\n| 00      &gt; | 01      &lt; | 02      v | 03      ^ |\n| 04      &lt; |           | 06      &gt; |           |\n| 08      ^ | 09      v | 10      ^ |           |\n|           | 13      &gt; | 14      v |           |\nReaches goal 12.00%. Obtains an average undiscounted return of 0.1200.\n\n\n\ngo_get_pi = lambda s: {\n    0:RIGHT, 1:RIGHT, 2:DOWN, 3:LEFT,\n    4:DOWN, 5:LEFT, 6:DOWN, 7:LEFT,\n    8:RIGHT, 9:RIGHT, 10:DOWN, 11:LEFT,\n    12:LEFT, 13:RIGHT, 14:RIGHT, 15:LEFT\n}[s]\nprint_policy(go_get_pi, P)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, go_get_pi, goal_state=goal_state)*100, \n    mean_return(env, go_get_pi)))\n\n정책:\n| 00      &gt; | 01      &gt; | 02      v | 03      &lt; |\n| 04      v |           | 06      v |           |\n| 08      &gt; | 09      &gt; | 10      v |           |\n|           | 13      &gt; | 14      &gt; |           |\nReaches goal 5.00%. Obtains an average undiscounted return of 0.0500.\n\n\n\ncareful_pi = lambda s: {\n    0:LEFT, 1:UP, 2:UP, 3:UP,\n    4:LEFT, 5:LEFT, 6:UP, 7:LEFT,\n    8:UP, 9:DOWN, 10:LEFT, 11:LEFT,\n    12:LEFT, 13:RIGHT, 14:RIGHT, 15:LEFT\n}[s]\nprint_policy(careful_pi, P)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, careful_pi, goal_state=goal_state)*100, \n    mean_return(env, careful_pi)))\n\n정책:\n| 00      &lt; | 01      ^ | 02      ^ | 03      ^ |\n| 04      &lt; |           | 06      ^ |           |\n| 08      ^ | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      &gt; |           |\nReaches goal 52.00%. Obtains an average undiscounted return of 0.5200.\n\n\n\n정책 평가법\n\nV = policy_evaluation(careful_pi, P, gamma=0.99)\nprint_state_value_function(V, P, prec=4)\n\n상태-가치 함수:\n| 00 0.4079 | 01 0.3754 | 02 0.3543 | 03 0.3438 |\n| 04 0.4203 |           | 06 0.1169 |           |\n| 08 0.4454 | 09  0.484 | 10 0.4328 |           |\n|           | 13 0.5884 | 14 0.7107 |           |\n\n\n\n\n정책 개선법\n\ncareful_plus_pi = policy_improvement(V, P, gamma=0.99)\nprint_policy(careful_plus_pi, P)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, careful_plus_pi, goal_state=goal_state)*100, \n    mean_return(env, careful_plus_pi)))\n\n정책:\n| 00      &lt; | 01      ^ | 02      ^ | 03      ^ |\n| 04      &lt; |           | 06      &lt; |           |\n| 08      ^ | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      v |           |\nReaches goal 74.00%. Obtains an average undiscounted return of 0.7400.\n\n\n\nnew_V = policy_evaluation(careful_plus_pi, P, gamma=0.99)\nprint_state_value_function(new_V, P, prec=4)\n\n상태-가치 함수:\n| 00  0.542 | 01 0.4988 | 02 0.4707 | 03 0.4569 |\n| 04 0.5585 |           | 06 0.3583 |           |\n| 08 0.5918 | 09 0.6431 | 10 0.6152 |           |\n|           | 13 0.7417 | 14 0.8628 |           |\n\n\n\nprint_state_value_function(new_V - V, P, prec=4)\n\n상태-가치 함수:\n| 00 0.1341 | 01 0.1234 | 02 0.1164 | 03  0.113 |\n| 04 0.1381 |           | 06 0.2414 |           |\n| 08 0.1464 | 09 0.1591 | 10 0.1824 |           |\n|           | 13 0.1533 | 14 0.1521 |           |\n\n\n\n\nAlternating between evaluation and improvement\n\nadversarial_pi = lambda s: {\n    0:UP, 1:UP, 2:UP, 3:UP,\n    4:UP, 5:LEFT, 6:UP, 7:LEFT,\n    8:LEFT, 9:LEFT, 10:LEFT, 11:LEFT,\n    12:LEFT, 13:LEFT, 14:LEFT, 15:LEFT\n}[s]\nprint_policy(adversarial_pi, P)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, adversarial_pi, goal_state=goal_state)*100, \n    mean_return(env, adversarial_pi)))\n\n정책:\n| 00      ^ | 01      ^ | 02      ^ | 03      ^ |\n| 04      ^ |           | 06      ^ |           |\n| 08      &lt; | 09      &lt; | 10      &lt; |           |\n|           | 13      &lt; | 14      &lt; |           |\nReaches goal 0.00%. Obtains an average undiscounted return of 0.0000.\n\n\n\nV = policy_evaluation(adversarial_pi, P, gamma=0.99)\nprint_state_value_function(V, P, prec=2)\n\n상태-가치 함수:\n| 00    0.0 | 01    0.0 | 02    0.0 | 03    0.0 |\n| 04    0.0 |           | 06    0.0 |           |\n| 08    0.0 | 09    0.0 | 10    0.0 |           |\n|           | 13    0.0 | 14    0.0 |           |\n\n\n\ni_pi = policy_improvement(V, P, gamma=0.99)\nprint_policy(i_pi, P)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, i_pi, goal_state=goal_state)*100, \n    mean_return(env, i_pi)))\n\n정책:\n| 00      &lt; | 01      &lt; | 02      &lt; | 03      &lt; |\n| 04      &lt; |           | 06      &lt; |           |\n| 08      &lt; | 09      &lt; | 10      &lt; |           |\n|           | 13      &lt; | 14      v |           |\nReaches goal 0.00%. Obtains an average undiscounted return of 0.0000.\n\n\n\ni_V = policy_evaluation(i_pi, P, gamma=0.99)\nprint_state_value_function(i_V, P, prec=2)\n\n상태-가치 함수:\n| 00    0.0 | 01    0.0 | 02   0.04 | 03   0.02 |\n| 04    0.0 |           | 06   0.07 |           |\n| 08    0.0 | 09    0.0 | 10   0.19 |           |\n|           | 13    0.0 | 14    0.5 |           |\n\n\n\nii_pi = policy_improvement(i_V, P, gamma=0.99)\nprint_policy(ii_pi, P)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, ii_pi, goal_state=goal_state)*100, \n    mean_return(env, ii_pi)))\n\n정책:\n| 00      &lt; | 01      v | 02      &gt; | 03      ^ |\n| 04      &lt; |           | 06      &lt; |           |\n| 08      &lt; | 09      v | 10      &lt; |           |\n|           | 13      v | 14      &gt; |           |\nReaches goal 0.00%. Obtains an average undiscounted return of 0.0000.\n\n\n\nii_V = policy_evaluation(ii_pi, P, gamma=0.99)\nprint_state_value_function(ii_V, P, prec=2)\n\n상태-가치 함수:\n| 00    0.0 | 01   0.05 | 02   0.16 | 03   0.15 |\n| 04    0.0 |           | 06   0.17 |           |\n| 08    0.0 | 09   0.22 | 10   0.35 |           |\n|           | 13   0.33 | 14   0.67 |           |\n\n\n\niii_pi = policy_improvement(ii_V, P, gamma=0.99)\nprint_policy(iii_pi, P)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, iii_pi, goal_state=goal_state)*100, \n    mean_return(env, iii_pi)))\n\n정책:\n| 00      v | 01      &gt; | 02      &gt; | 03      ^ |\n| 04      &lt; |           | 06      &lt; |           |\n| 08      v | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      &gt; |           |\nReaches goal 20.00%. Obtains an average undiscounted return of 0.2000.\n\n\n\niii_V = policy_evaluation(iii_pi, P, gamma=0.99)\nprint_state_value_function(iii_V, P, prec=2)\n\n상태-가치 함수:\n| 00   0.12 | 01   0.09 | 02   0.19 | 03   0.19 |\n| 04   0.15 |           | 06    0.2 |           |\n| 08   0.19 | 09   0.38 | 10   0.43 |           |\n|           | 13   0.53 | 14   0.71 |           |\n\n\n\niiii_pi = policy_improvement(iii_V, P, gamma=0.99)\nprint_policy(iiii_pi, P)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, iiii_pi, goal_state=goal_state)*100, \n    mean_return(env, iiii_pi)))\n\n정책:\n| 00      &lt; | 01      ^ | 02      &gt; | 03      ^ |\n| 04      &lt; |           | 06      &lt; |           |\n| 08      ^ | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      v |           |\nReaches goal 73.00%. Obtains an average undiscounted return of 0.7300.\n\n\n\niiii_V = policy_evaluation(iiii_pi, P, gamma=0.99)\nprint_state_value_function(iiii_V, P, prec=2)\n\n상태-가치 함수:\n| 00   0.52 | 01   0.38 | 02   0.26 | 03   0.25 |\n| 04   0.54 |           | 06   0.28 |           |\n| 08   0.57 | 09   0.62 | 10   0.58 |           |\n|           | 13   0.72 | 14   0.85 |           |\n\n\n\niiiii_pi = policy_improvement(iiii_V, P, gamma=0.99)\nprint_policy(iiiii_pi, P)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, iiiii_pi, goal_state=goal_state)*100, \n    mean_return(env, iiiii_pi)))\n\n정책:\n| 00      &lt; | 01      ^ | 02      &lt; | 03      ^ |\n| 04      &lt; |           | 06      &lt; |           |\n| 08      ^ | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      v |           |\nReaches goal 74.00%. Obtains an average undiscounted return of 0.7400.\n\n\n\niiiii_V = policy_evaluation(iiiii_pi, P, gamma=0.99)\nprint_state_value_function(iiiii_V, P, prec=2)\n\n상태-가치 함수:\n| 00   0.53 | 01   0.45 | 02   0.38 | 03   0.37 |\n| 04   0.55 |           | 06   0.32 |           |\n| 08   0.58 | 09   0.63 | 10    0.6 |           |\n|           | 13   0.73 | 14   0.86 |           |\n\n\n\niiiiii_pi = policy_improvement(iiiii_V, P, gamma=0.99)\nprint_policy(iiiiii_pi, P)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, iiiiii_pi, goal_state=goal_state)*100, \n    mean_return(env, iiiiii_pi)))\n\n정책:\n| 00      &lt; | 01      ^ | 02      ^ | 03      ^ |\n| 04      &lt; |           | 06      &lt; |           |\n| 08      ^ | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      v |           |\nReaches goal 74.00%. Obtains an average undiscounted return of 0.7400.\n\n\n\niiiiii_V = policy_evaluation(iiiiii_pi, P, gamma=0.99)\nprint_state_value_function(iiiiii_V, P, prec=2)\n\n상태-가치 함수:\n| 00   0.54 | 01    0.5 | 02   0.47 | 03   0.46 |\n| 04   0.56 |           | 06   0.36 |           |\n| 08   0.59 | 09   0.64 | 10   0.62 |           |\n|           | 13   0.74 | 14   0.86 |           |\n\n\n\niiiiiii_pi = policy_improvement(iiiiii_V, P, gamma=0.99)\nprint_policy(iiiiiii_pi, P)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, iiiiiii_pi, goal_state=goal_state)*100, \n    mean_return(env, iiiiiii_pi)))\n\n정책:\n| 00      &lt; | 01      ^ | 02      ^ | 03      ^ |\n| 04      &lt; |           | 06      &lt; |           |\n| 08      ^ | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      v |           |\nReaches goal 74.00%. Obtains an average undiscounted return of 0.7400.\n\n\n\n\n정책 반복법\n\nV_best_p, pi_best_p = policy_iteration(P, gamma=0.99)\nprint_state_value_function(V_best_p, P, prec=4)\nprint()\nprint('Optimal policy and state-value function (PI):')\nprint_policy(pi_best_p, P)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, pi_best_p, goal_state=goal_state)*100, \n    mean_return(env, pi_best_p)))\n\n상태-가치 함수:\n| 00  0.542 | 01 0.4988 | 02 0.4707 | 03 0.4569 |\n| 04 0.5585 |           | 06 0.3583 |           |\n| 08 0.5918 | 09 0.6431 | 10 0.6152 |           |\n|           | 13 0.7417 | 14 0.8628 |           |\n\nOptimal policy and state-value function (PI):\n정책:\n| 00      &lt; | 01      ^ | 02      ^ | 03      ^ |\n| 04      &lt; |           | 06      &lt; |           |\n| 08      ^ | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      v |           |\nReaches goal 74.00%. Obtains an average undiscounted return of 0.7400.\n\n\n\n\n\nSlippery Walk Five\n\nenv = gym.make('SlipperyWalkFive-v0')\ninit_state = env.reset()\ngoal_state = 6\nP = env.env.P\n\n\n가치 반복법\n\ndef value_iteration(P, gamma=1.0, theta=1e-10):\n    V = np.zeros(len(P), dtype=np.float64)\n    while True:\n        Q = np.zeros((len(P), len(P[0])), dtype=np.float64)\n        for s in range(len(P)):\n            for a in range(len(P[s])):\n                for prob, next_state, reward, done in P[s][a]:\n                    Q[s][a] += prob * (reward + gamma * V[next_state] * (not done))\n        if np.max(np.abs(V - np.max(Q, axis=1))) &lt; theta:\n            break\n        V = np.max(Q, axis=1)\n    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n    return V, pi\n\n\noptimal_V, optimal_pi = value_iteration(P)\nprint('Optimal policy and state-value function (PI):')\nprint_policy(optimal_pi, P, action_symbols=('&lt;', '&gt;'), n_cols=7)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, optimal_pi, goal_state=goal_state)*100, \n    mean_return(env, optimal_pi)))\nprint()\nprint_state_value_function(optimal_V, P, n_cols=7, prec=5)\n# |            | 01   0.668 | 02   0.890 | 03   0.964 | 04   0.989 | 05   0.997 |            |\n\nOptimal policy and state-value function (PI):\n정책:\n|           | 01      &gt; | 02      &gt; | 03      &gt; | 04      &gt; | 05      &gt; |           |\nReaches goal 93.00%. Obtains an average undiscounted return of 0.9300.\n\n상태-가치 함수:\n|           | 01 0.66758 | 02 0.89011 | 03 0.96429 | 04 0.98901 | 05 0.99725 |           |\n\n\n\n\n\nFrozen Lake MDP\n\nenv = gym.make('FrozenLake-v0')\ninit_state = env.reset()\ngoal_state = 15\nP = env.env.P\n\n\nV_best_v, pi_best_v = value_iteration(P, gamma=0.99)\nprint('Optimal policy and state-value function (VI):')\nprint_policy(pi_best_v, P)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, pi_best_v, goal_state=goal_state)*100, \n    mean_return(env, pi_best_v)))\nprint()\nprint_state_value_function(V_best_v, P, prec=4)\n\nOptimal policy and state-value function (VI):\n정책:\n| 00      &lt; | 01      ^ | 02      ^ | 03      ^ |\n| 04      &lt; |           | 06      &lt; |           |\n| 08      ^ | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      v |           |\nReaches goal 74.00%. Obtains an average undiscounted return of 0.7400.\n\n상태-가치 함수:\n| 00  0.542 | 01 0.4988 | 02 0.4707 | 03 0.4569 |\n| 04 0.5585 |           | 06 0.3583 |           |\n| 08 0.5918 | 09 0.6431 | 10 0.6152 |           |\n|           | 13 0.7417 | 14 0.8628 |           |\n\n\n\nprint('For comparison, optimal policy and state-value function (PI):')\nprint_policy(pi_best_p, P)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, pi_best_p, goal_state=goal_state)*100, \n    mean_return(env, pi_best_p)))\nprint()\nprint_state_value_function(V_best_p, P)\n\nFor comparison, optimal policy and state-value function (PI):\n정책:\n| 00      &lt; | 01      ^ | 02      ^ | 03      ^ |\n| 04      &lt; |           | 06      &lt; |           |\n| 08      ^ | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      v |           |\nReaches goal 74.00%. Obtains an average undiscounted return of 0.7400.\n\n상태-가치 함수:\n| 00  0.542 | 01  0.499 | 02  0.471 | 03  0.457 |\n| 04  0.558 |           | 06  0.358 |           |\n| 08  0.592 | 09  0.643 | 10  0.615 |           |\n|           | 13  0.742 | 14  0.863 |           |\n\n\n\n\nChanging the Frozen Lake environment MDP\n\nenv = gym.make('FrozenLake-v0')\nP = env.env.P\n\n# change reward function\nreward_goal, reward_holes, reward_others = 1, -1, -0.01\ngoal, hole = 15, [5, 7, 11, 12]\nfor s in range(len(P)):\n    for a in range(len(P[s])):\n        for t in range(len(P[s][a])):\n            values = list(P[s][a][t])\n            if values[1] == goal:\n                values[2] = reward_goal\n                values[3] = False\n            elif values[1] in hole:\n                values[2] = reward_holes\n                values[3] = False\n            else:\n                values[2] = reward_others\n                values[3] = False\n            if s in hole or s == goal:\n                values[2] = 0\n                values[3] = True\n            P[s][a][t] = tuple(values)\n\n# change transition function\nprob_action, prob_drift_one, prob_drift_two = 0.8, 0.1, 0.1\nfor s in range(len(P)):\n    for a in range(len(P[s])):\n        for t in range(len(P[s][a])):\n            if P[s][a][t][0] == 1.0:\n                continue\n            values = list(P[s][a][t])\n            if t == 0:\n                values[0] = prob_drift_one\n            elif t == 1:\n                values[0] = prob_action\n            elif t == 2:\n                values[0] = prob_drift_two\n            P[s][a][t] = tuple(values)\n\nenv.env.P = P\n\n\nV_best, pi_best = policy_iteration(env.env.P, gamma=0.99)\nprint('Optimal policy and state-value function (PI):')\nprint_policy(pi_best, P)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, pi_best, goal_state=goal_state)*100, \n    mean_return(env, pi_best)))\nprint()\nprint_state_value_function(V_best, P)\n\nOptimal policy and state-value function (PI):\n정책:\n| 00      v | 01      ^ | 02      v | 03      ^ |\n| 04      &lt; |           | 06      v |           |\n| 08      &gt; | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      &gt; |           |\nReaches goal 78.00%. Obtains an average undiscounted return of 0.3657.\n\n상태-가치 함수:\n| 00  0.433 | 01  0.353 | 02  0.409 | 03   0.28 |\n| 04  0.461 |           | 06   0.45 |           |\n| 08  0.636 | 09  0.884 | 10  0.831 |           |\n|           | 13  0.945 | 14  0.977 |           |\n\n\n\nV_best, pi_best = value_iteration(env.env.P, gamma=0.99)\nprint('Optimal policy and state-value function (PI):')\nprint_policy(pi_best, P)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, pi_best, goal_state=goal_state)*100, \n    mean_return(env, pi_best)))\nprint()\nprint_state_value_function(V_best, P)\n\nOptimal policy and state-value function (PI):\n정책:\n| 00      v | 01      ^ | 02      v | 03      ^ |\n| 04      &lt; |           | 06      v |           |\n| 08      &gt; | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      &gt; |           |\nReaches goal 78.00%. Obtains an average undiscounted return of 0.3657.\n\n상태-가치 함수:\n| 00  0.433 | 01  0.353 | 02  0.409 | 03   0.28 |\n| 04  0.461 |           | 06   0.45 |           |\n| 08  0.636 | 09  0.884 | 10  0.831 |           |\n|           | 13  0.945 | 14  0.977 |           |\n\n\n\n\nRussell & Norvig’s Gridworld\n\nenv = gym.make('RussellNorvigGridworld-v0')\ninit_state = env.reset()\ngoal_state = 3\nP = env.env.P\n\n\nV_best_p, pi_best = policy_iteration(P)\nprint('Optimal policy and state-value function (PI):')\nprint_policy(pi_best, P)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, pi_best, goal_state=goal_state)*100, \n    mean_return(env, pi_best)))\nprint()\nprint_state_value_function(V_best_p, P)\n\nOptimal policy and state-value function (PI):\n정책:\n| 00      &gt; | 01      &gt; | 02      &gt; |           |\n| 04      ^ |           | 06      ^ |           |\n| 08      ^ | 09      &lt; | 10      &lt; | 11      &lt; |\nReaches goal 96.00%. Obtains an average undiscounted return of 0.6424.\n\n상태-가치 함수:\n| 00  0.812 | 01  0.868 | 02  0.918 |           |\n| 04  0.762 |           | 06   0.66 |           |\n| 08  0.705 | 09  0.655 | 10  0.611 | 11  0.388 |\n\n\n\nV_best_v, pi_best = value_iteration(P)\nprint('Optimal policy and state-value function (PI):')\nprint_policy(pi_best, P)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, pi_best, goal_state=goal_state)*100, \n    mean_return(env, pi_best)))\nprint()\nprint_state_value_function(V_best_v, P)\n\nOptimal policy and state-value function (PI):\n정책:\n| 00      &gt; | 01      &gt; | 02      &gt; |           |\n| 04      ^ |           | 06      ^ |           |\n| 08      ^ | 09      &lt; | 10      &lt; | 11      &lt; |\nReaches goal 96.00%. Obtains an average undiscounted return of 0.6424.\n\n상태-가치 함수:\n| 00  0.812 | 01  0.868 | 02  0.918 |           |\n| 04  0.762 |           | 06   0.66 |           |\n| 08  0.705 | 09  0.655 | 10  0.611 | 11  0.388 |\n\n\n\nLEFT, DOWN, RIGHT, UP = range(4)\npi = lambda s: {\n    0:RIGHT, 1:RIGHT, 2:RIGHT, 3:LEFT,\n    4:UP, 5:LEFT, 6:UP, 7:LEFT,\n    8:UP, 9:LEFT, 10:LEFT, 11:LEFT\n}[s]\nprint('Re-construct optimal policy:')\nprint_policy(pi, P)\nprint('Reaches goal {:.2f}%. Obtains an average undiscounted return of {:.4f}.'.format(\n    probability_success(env, pi, goal_state=goal_state)*100, \n    mean_return(env, pi)))\n\nRe-construct optimal policy:\n정책:\n| 00      &gt; | 01      &gt; | 02      &gt; |           |\n| 04      ^ |           | 06      ^ |           |\n| 08      ^ | 09      &lt; | 10      &lt; | 11      &lt; |\nReaches goal 96.00%. Obtains an average undiscounted return of 0.6424.\n\n\n\nV = policy_evaluation(pi, P)\nprint('Evaluate optimal policy:')\nprint_state_value_function(V, P)\n\nEvaluate optimal policy:\n상태-가치 함수:\n| 00  0.812 | 01  0.868 | 02  0.918 |           |\n| 04  0.762 |           | 06   0.66 |           |\n| 08  0.705 | 09  0.655 | 10  0.611 | 11  0.388 |\n\n\n\npi = policy_improvement(V, P)\nprint('Improve optimal policy (nothing to improve -- it is the same policy, because it is optimal):')\nprint_policy(pi, P)\n\nImprove optimal policy (nothing to improve -- it is the same policy, because it is optimal):\n정책:\n| 00      &gt; | 01      &gt; | 02      &gt; |           |\n| 04      ^ |           | 06      ^ |           |\n| 08      ^ | 09      &lt; | 10      &lt; | 11      &lt; |\n\n\n\nprint('There are no differences, nothing to improve on the optimal policy and state-value function:')\nprint(np.abs(V_best_p - V))\nprint(np.abs(V_best_v - V))\n\nThere are no differences, nothing to improve on the optimal policy and state-value function:\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-12.html#tldr",
    "href": "publication/GDRL/GDRL-chapter-12.html#tldr",
    "title": "Chapter 12: Advanced Actor-Critic Methods",
    "section": "TL;DR",
    "text": "TL;DR\n\n그로킹 심층 강화학습 중 12장 내용인 “조금 더 발전된 액터-크리틱 학습 방법들”에 대한 내용입니다.\n\n\n\nCode\n!pip install tqdm numpy scikit-learn pyglet setuptools \n!pip install gym asciinema pandas tabulate tornado==5.* PyBullet \n!pip install git+https://github.com/pybox2d/pybox2d#egg=Box2D \n!pip install git+https://github.com/mimoralea/gym-bandits#egg=gym-bandits \n!pip install git+https://github.com/mimoralea/gym-walk#egg=gym-walk \n!pip install git+https://github.com/mimoralea/gym-aima#egg=gym-aima \n!pip install gym[atari]\n!pip install torch torchvision\n\n\n\nimport warnings ; warnings.filterwarnings('ignore')\nimport os\nos.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\nos.environ['OMP_NUM_THREADS'] = '1'\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.multiprocessing as mp\nimport threading\nfrom torch.distributions import Normal\n\nimport numpy as np\nfrom IPython.display import display\nfrom collections import namedtuple, deque\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nfrom itertools import cycle, count\nfrom textwrap import wrap\n\nimport pybullet_envs\nimport matplotlib\nimport subprocess\nimport os.path\nimport tempfile\nimport random\nimport base64\nimport pprint\nimport glob\nimport time\nimport json\nimport sys\nimport gym\nimport io\nimport os\nimport gc\nimport platform\n\nfrom gym import wrappers\nfrom skimage.transform import resize\nfrom skimage.color import rgb2gray\nfrom subprocess import check_output\nfrom IPython.display import display, HTML\n\nLEAVE_PRINT_EVERY_N_SECS = 300\nERASE_LINE = '\\x1b[2K'\nEPS = 1e-6\nRESULTS_DIR = os.path.join('.', 'gym-results')\nSEEDS = (12, 34, 56, 78, 90)\n\n%matplotlib inline\n\n\nplt.style.use('fivethirtyeight')\nparams = {\n    'figure.figsize': (15, 8),\n    'font.size': 24,\n    'legend.fontsize': 20,\n    'axes.titlesize': 28,\n    'axes.labelsize': 24,\n    'xtick.labelsize': 20,\n    'ytick.labelsize': 20\n}\npylab.rcParams.update(params)\nnp.set_printoptions(suppress=True)\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\ndef get_make_env_fn(**kargs):\n    def make_env_fn(env_name, seed=None, render=None, record=False,\n                    unwrapped=False, monitor_mode=None, \n                    inner_wrappers=None, outer_wrappers=None):\n        mdir = tempfile.mkdtemp()\n        env = None\n        if render:\n            try:\n                env = gym.make(env_name, render=render)\n            except:\n                pass\n        if env is None:\n            env = gym.make(env_name)\n        if seed is not None: env.seed(seed)\n        env = env.unwrapped if unwrapped else env\n        if inner_wrappers:\n            for wrapper in inner_wrappers:\n                env = wrapper(env)\n        env = wrappers.Monitor(\n            env, mdir, force=True, \n            mode=monitor_mode, \n            video_callable=lambda e_idx: record) if monitor_mode else env\n        if outer_wrappers:\n            for wrapper in outer_wrappers:\n                env = wrapper(env)\n        return env\n    return make_env_fn, kargs\n\n\ndef get_videos_html(env_videos, title, max_n_videos=5):\n    videos = np.array(env_videos)\n    if len(videos) == 0:\n        return\n    \n    n_videos = max(1, min(max_n_videos, len(videos)))\n    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int) if n_videos &gt; 1 else [-1,]\n    videos = videos[idxs,...]\n\n    strm = '&lt;h2&gt;{}&lt;/h2&gt;'.format(title)\n    for video_path, meta_path in videos:\n        video = io.open(video_path, 'r+b').read()\n        encoded = base64.b64encode(video)\n\n        with open(meta_path) as data_file:    \n            meta = json.load(data_file)\n\n        html_tag = \"\"\"\n        &lt;h3&gt;{0}&lt;/h3&gt;\n        &lt;video width=\"960\" height=\"540\" controls&gt;\n            &lt;source src=\"data:video/mp4;base64,{1}\" type=\"video/mp4\" /&gt;\n        &lt;/video&gt;\"\"\"\n        strm += html_tag.format('Episode ' + str(meta['episode_id']), encoded.decode('ascii'))\n    return strm\n\n\nplatform.system()\n\n'Linux'\n\n\n\ndef get_gif_html(env_videos, title, subtitle_eps=None, max_n_videos=4):\n    videos = np.array(env_videos)\n    if len(videos) == 0:\n        return\n    \n    n_videos = max(1, min(max_n_videos, len(videos)))\n    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int) if n_videos &gt; 1 else [-1,]\n    videos = videos[idxs,...]\n\n    strm = '&lt;h2&gt;{}&lt;/h2&gt;'.format(title)\n    for video_path, meta_path in videos:\n        basename = os.path.splitext(video_path)[0]\n        gif_path = basename + '.gif'\n        if not os.path.exists(gif_path):\n            if platform.system() == 'Linux':\n                ps = subprocess.Popen(\n                    ('ffmpeg', \n                     '-i', video_path, \n                     '-r', '7',\n                     '-f', 'image2pipe', \n                     '-vcodec', 'ppm',\n                     '-crf', '20',\n                     '-vf', 'scale=512:-1',\n                     '-'), \n                    stdout=subprocess.PIPE,\n                    universal_newlines=True)\n                output = subprocess.check_output(\n                    ('convert',\n                     '-coalesce',\n                     '-delay', '7',\n                     '-loop', '0',\n                     '-fuzz', '2%',\n                     '+dither',\n                     '-deconstruct',\n                     '-layers', 'Optimize',\n                     '-', gif_path), \n                    stdin=ps.stdout)\n                ps.wait()\n            else:\n                ps = subprocess.Popen('ffmpeg -i {} -r 7 -f image2pipe \\\n                                      -vcodec ppm -crf 20 -vf scale=512:-1 - | \\\n                                      convert -coalesce -delay 7 -loop 0 -fuzz 2% \\\n                                      +dither -deconstruct -layers Optimize \\\n                                      - {}'.format(video_path, gif_path), \n                                      stdin=subprocess.PIPE, \n                                      shell=True)\n                ps.wait()\n\n        gif = io.open(gif_path, 'r+b').read()\n        encoded = base64.b64encode(gif)\n            \n        with open(meta_path) as data_file:    \n            meta = json.load(data_file)\n\n        html_tag = \"\"\"\n        &lt;h3&gt;{0}&lt;/h3&gt;\n        &lt;img src=\"data:image/gif;base64,{1}\" /&gt;\"\"\"\n        prefix = 'Trial ' if subtitle_eps is None else 'Episode '\n        sufix = str(meta['episode_id'] if subtitle_eps is None \\\n                    else subtitle_eps[meta['episode_id']])\n        strm += html_tag.format(prefix + sufix, encoded.decode('ascii'))\n    return strm\n\n\nclass RenderUint8(gym.Wrapper):\n    def __init__(self, env):\n        super().__init__(env)\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n    def render(self, mode='rgb_array'):\n        frame = self.env.render(mode=mode)\n        return frame.astype(np.uint8)"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-12.html#deep-determinisic-policy-gradient-ddpg",
    "href": "publication/GDRL/GDRL-chapter-12.html#deep-determinisic-policy-gradient-ddpg",
    "title": "Chapter 12: Advanced Actor-Critic Methods",
    "section": "Deep Determinisic Policy Gradient (DDPG)",
    "text": "Deep Determinisic Policy Gradient (DDPG)\n\nclass FCQV(nn.Module):\n    def __init__(self, \n                 input_dim, \n                 output_dim, \n                 hidden_dims=(32,32), \n                 activation_fc=F.relu):\n        super(FCQV, self).__init__()\n        self.activation_fc = activation_fc\n\n        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n        self.hidden_layers = nn.ModuleList()\n        for i in range(len(hidden_dims)-1):\n            in_dim = hidden_dims[i]\n            if i == 0: \n                in_dim += output_dim\n            hidden_layer = nn.Linear(in_dim, hidden_dims[i+1])\n            self.hidden_layers.append(hidden_layer)\n        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n\n        device = \"cpu\"\n        if torch.cuda.is_available():\n            device = \"cuda:0\"\n        self.device = torch.device(device)\n        self.to(self.device)\n    \n    def _format(self, state, action):\n        x, u = state, action\n        if not isinstance(x, torch.Tensor):\n            x = torch.tensor(x, \n                             device=self.device, \n                             dtype=torch.float32)\n            x = x.unsqueeze(0)\n        if not isinstance(u, torch.Tensor):\n            u = torch.tensor(u, \n                             device=self.device, \n                             dtype=torch.float32)\n            u = u.unsqueeze(0)\n        return x, u\n\n    def forward(self, state, action):\n        x, u = self._format(state, action)\n        x = self.activation_fc(self.input_layer(x))\n        for i, hidden_layer in enumerate(self.hidden_layers):\n            if i == 0:\n                x = torch.cat((x, u), dim=1)\n            x = self.activation_fc(hidden_layer(x))\n        return self.output_layer(x)\n    \n    def load(self, experiences):\n        states, actions, new_states, rewards, is_terminals = experiences\n        states = torch.from_numpy(states).float().to(self.device)\n        actions = torch.from_numpy(actions).float().to(self.device)\n        new_states = torch.from_numpy(new_states).float().to(self.device)\n        rewards = torch.from_numpy(rewards).float().to(self.device)\n        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n        return states, actions, new_states, rewards, is_terminals\n\n\nclass FCDP(nn.Module):\n    def __init__(self, \n                 input_dim,\n                 action_bounds,\n                 hidden_dims=(32,32), \n                 activation_fc=F.relu,\n                 out_activation_fc=F.tanh):\n        super(FCDP, self).__init__()\n        self.activation_fc = activation_fc\n        self.out_activation_fc = out_activation_fc\n        self.env_min, self.env_max = action_bounds\n\n        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n        self.hidden_layers = nn.ModuleList()\n        for i in range(len(hidden_dims)-1):\n            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n            self.hidden_layers.append(hidden_layer)\n        self.output_layer = nn.Linear(hidden_dims[-1], len(self.env_max))\n\n        device = \"cpu\"\n        if torch.cuda.is_available():\n            device = \"cuda:0\"\n        self.device = torch.device(device)\n        self.to(self.device)\n        \n        self.env_min = torch.tensor(self.env_min,\n                                    device=self.device, \n                                    dtype=torch.float32)\n\n        self.env_max = torch.tensor(self.env_max,\n                                    device=self.device, \n                                    dtype=torch.float32)\n        \n        self.nn_min = self.out_activation_fc(\n            torch.Tensor([float('-inf')])).to(self.device)\n        self.nn_max = self.out_activation_fc(\n            torch.Tensor([float('inf')])).to(self.device)\n        self.rescale_fn = lambda x: (x - self.nn_min) * (self.env_max - self.env_min) / \\\n                                    (self.nn_max - self.nn_min) + self.env_min\n\n    def _format(self, state):\n        x = state\n        if not isinstance(x, torch.Tensor):\n            x = torch.tensor(x, \n                             device=self.device, \n                             dtype=torch.float32)\n            x = x.unsqueeze(0)\n        return x\n\n    def forward(self, state):\n        x = self._format(state)\n        x = self.activation_fc(self.input_layer(x))\n        for hidden_layer in self.hidden_layers:\n            x = self.activation_fc(hidden_layer(x))\n        x = self.output_layer(x)\n        x = self.out_activation_fc(x)\n        return self.rescale_fn(x)\n\n\nclass ReplayBuffer():\n    def __init__(self, \n                 max_size=10000, \n                 batch_size=64):\n        self.ss_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n        self.as_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n        self.rs_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n        self.ps_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n        self.ds_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n\n        self.max_size = max_size\n        self.batch_size = batch_size\n        self._idx = 0\n        self.size = 0\n    \n    def store(self, sample):\n        s, a, r, p, d = sample\n        self.ss_mem[self._idx] = s\n        self.as_mem[self._idx] = a\n        self.rs_mem[self._idx] = r\n        self.ps_mem[self._idx] = p\n        self.ds_mem[self._idx] = d\n        \n        self._idx += 1\n        self._idx = self._idx % self.max_size\n\n        self.size += 1\n        self.size = min(self.size, self.max_size)\n\n    def sample(self, batch_size=None):\n        if batch_size == None:\n            batch_size = self.batch_size\n\n        idxs = np.random.choice(\n            self.size, batch_size, replace=False)\n        experiences = np.vstack(self.ss_mem[idxs]), \\\n                      np.vstack(self.as_mem[idxs]), \\\n                      np.vstack(self.rs_mem[idxs]), \\\n                      np.vstack(self.ps_mem[idxs]), \\\n                      np.vstack(self.ds_mem[idxs])\n        return experiences\n\n    def __len__(self):\n        return self.size\n\n\nclass GreedyStrategy():\n    def __init__(self, bounds):\n        self.low, self.high = bounds\n        self.ratio_noise_injected = 0\n\n    def select_action(self, model, state):\n        with torch.no_grad():\n            greedy_action = model(state).cpu().detach().data.numpy().squeeze()\n\n        action = np.clip(greedy_action, self.low, self.high)\n        return np.reshape(action, self.high.shape)\n\n\nclass NormalNoiseStrategy():\n    def __init__(self, bounds, exploration_noise_ratio=0.1):\n        self.low, self.high = bounds\n        self.exploration_noise_ratio = exploration_noise_ratio\n        self.ratio_noise_injected = 0\n\n    def select_action(self, model, state, max_exploration=False):\n        if max_exploration:\n            noise_scale = self.high\n        else:\n            noise_scale = self.exploration_noise_ratio * self.high\n\n        with torch.no_grad():\n            greedy_action = model(state).cpu().detach().data.numpy().squeeze()\n\n        noise = np.random.normal(loc=0, scale=noise_scale, size=len(self.high))\n        noisy_action = greedy_action + noise\n        action = np.clip(noisy_action, self.low, self.high)\n        \n        self.ratio_noise_injected = np.mean(abs((greedy_action - action)/(self.high - self.low)))\n        return action\n\n\nclass DDPG():\n    def __init__(self, \n                 replay_buffer_fn,\n                 policy_model_fn, \n                 policy_max_grad_norm, \n                 policy_optimizer_fn, \n                 policy_optimizer_lr,\n                 value_model_fn, \n                 value_max_grad_norm, \n                 value_optimizer_fn, \n                 value_optimizer_lr, \n                 training_strategy_fn,\n                 evaluation_strategy_fn,\n                 n_warmup_batches,\n                 update_target_every_steps,\n                 tau):\n        self.replay_buffer_fn = replay_buffer_fn\n\n        self.policy_model_fn = policy_model_fn\n        self.policy_max_grad_norm = policy_max_grad_norm\n        self.policy_optimizer_fn = policy_optimizer_fn\n        self.policy_optimizer_lr = policy_optimizer_lr\n        \n        self.value_model_fn = value_model_fn\n        self.value_max_grad_norm = value_max_grad_norm\n        self.value_optimizer_fn = value_optimizer_fn\n        self.value_optimizer_lr = value_optimizer_lr\n\n        self.training_strategy_fn = training_strategy_fn\n        self.evaluation_strategy_fn = evaluation_strategy_fn\n\n        self.n_warmup_batches = n_warmup_batches\n        self.update_target_every_steps = update_target_every_steps\n        self.tau = tau\n\n    def optimize_model(self, experiences):\n        states, actions, rewards, next_states, is_terminals = experiences\n        batch_size = len(is_terminals)\n\n        argmax_a_q_sp = self.target_policy_model(next_states)\n        max_a_q_sp = self.target_value_model(next_states, argmax_a_q_sp)\n        target_q_sa = rewards + self.gamma * max_a_q_sp * (1 - is_terminals)\n        q_sa = self.online_value_model(states, actions)\n        td_error = q_sa - target_q_sa.detach()\n        value_loss = td_error.pow(2).mul(0.5).mean()\n        self.value_optimizer.zero_grad()\n        value_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.online_value_model.parameters(), \n                                       self.value_max_grad_norm)\n        self.value_optimizer.step()\n\n        argmax_a_q_s = self.online_policy_model(states)\n        max_a_q_s = self.online_value_model(states, argmax_a_q_s)\n        policy_loss = -max_a_q_s.mean()\n        self.policy_optimizer.zero_grad()\n        policy_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.online_policy_model.parameters(), \n                                       self.policy_max_grad_norm)        \n        self.policy_optimizer.step()\n\n    def interaction_step(self, state, env):\n        min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n        action = self.training_strategy.select_action(self.online_policy_model, \n                                                      state, \n                                                      len(self.replay_buffer) &lt; min_samples)\n        new_state, reward, is_terminal, info = env.step(action)\n        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n        is_failure = is_terminal and not is_truncated\n        experience = (state, action, reward, new_state, float(is_failure))\n        self.replay_buffer.store(experience)\n        self.episode_reward[-1] += reward\n        self.episode_timestep[-1] += 1\n        self.episode_exploration[-1] += self.training_strategy.ratio_noise_injected\n        return new_state, is_terminal\n    \n    def update_networks(self, tau=None):\n        tau = self.tau if tau is None else tau\n        for target, online in zip(self.target_value_model.parameters(), \n                                  self.online_value_model.parameters()):\n            target_ratio = (1.0 - self.tau) * target.data\n            online_ratio = self.tau * online.data\n            mixed_weights = target_ratio + online_ratio\n            target.data.copy_(mixed_weights)\n\n        for target, online in zip(self.target_policy_model.parameters(), \n                                  self.online_policy_model.parameters()):\n            target_ratio = (1.0 - self.tau) * target.data\n            online_ratio = self.tau * online.data\n            mixed_weights = target_ratio + online_ratio\n            target.data.copy_(mixed_weights)\n\n    def train(self, make_env_fn, make_env_kargs, seed, gamma, \n              max_minutes, max_episodes, goal_mean_100_reward):\n        training_start, last_debug_time = time.time(), float('-inf')\n\n        self.checkpoint_dir = tempfile.mkdtemp()\n        self.make_env_fn = make_env_fn\n        self.make_env_kargs = make_env_kargs\n        self.seed = seed\n        self.gamma = gamma\n        \n        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n    \n        nS, nA = env.observation_space.shape[0], env.action_space.shape[0]\n        action_bounds = env.action_space.low, env.action_space.high\n        self.episode_timestep = []\n        self.episode_reward = []\n        self.episode_seconds = []\n        self.evaluation_scores = []        \n        self.episode_exploration = []\n        \n        self.target_value_model = self.value_model_fn(nS, nA)\n        self.online_value_model = self.value_model_fn(nS, nA)\n        self.target_policy_model = self.policy_model_fn(nS, action_bounds)\n        self.online_policy_model = self.policy_model_fn(nS, action_bounds)\n        self.update_networks(tau=1.0)\n        self.value_optimizer = self.value_optimizer_fn(self.online_value_model, \n                                                       self.value_optimizer_lr)        \n        self.policy_optimizer = self.policy_optimizer_fn(self.online_policy_model, \n                                                         self.policy_optimizer_lr)\n\n        self.replay_buffer = self.replay_buffer_fn()\n        self.training_strategy = training_strategy_fn(action_bounds)\n        self.evaluation_strategy = evaluation_strategy_fn(action_bounds)\n                    \n        result = np.empty((max_episodes, 5))\n        result[:] = np.nan\n        training_time = 0\n        for episode in range(1, max_episodes + 1):\n            episode_start = time.time()\n            \n            state, is_terminal = env.reset(), False\n            self.episode_reward.append(0.0)\n            self.episode_timestep.append(0.0)\n            self.episode_exploration.append(0.0)\n\n            for step in count():\n                state, is_terminal = self.interaction_step(state, env)\n\n                min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n                if len(self.replay_buffer) &gt; min_samples:\n                    experiences = self.replay_buffer.sample()\n                    experiences = self.online_value_model.load(experiences)\n                    self.optimize_model(experiences)\n\n                if np.sum(self.episode_timestep) % self.update_target_every_steps == 0:\n                    self.update_networks()\n\n                if is_terminal:\n                    gc.collect()\n                    break\n            \n            # stats\n            episode_elapsed = time.time() - episode_start\n            self.episode_seconds.append(episode_elapsed)\n            training_time += episode_elapsed\n            evaluation_score, _ = self.evaluate(self.online_policy_model, env)\n            self.save_checkpoint(episode-1, self.online_policy_model)\n\n            total_step = int(np.sum(self.episode_timestep))\n            self.evaluation_scores.append(evaluation_score)\n            \n            mean_10_reward = np.mean(self.episode_reward[-10:])\n            std_10_reward = np.std(self.episode_reward[-10:])\n            mean_100_reward = np.mean(self.episode_reward[-100:])\n            std_100_reward = np.std(self.episode_reward[-100:])\n            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n            lst_100_exp_rat = np.array(\n                self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n            mean_100_exp_rat = np.mean(lst_100_exp_rat)\n            std_100_exp_rat = np.std(lst_100_exp_rat)\n            \n            wallclock_elapsed = time.time() - training_start\n            result[episode-1] = total_step, mean_100_reward, \\\n                mean_100_eval_score, training_time, wallclock_elapsed\n            \n            reached_debug_time = time.time() - last_debug_time &gt;= LEAVE_PRINT_EVERY_N_SECS\n            reached_max_minutes = wallclock_elapsed &gt;= max_minutes * 60\n            reached_max_episodes = episode &gt;= max_episodes\n            reached_goal_mean_reward = mean_100_eval_score &gt;= goal_mean_100_reward\n            training_is_over = reached_max_minutes or \\\n                               reached_max_episodes or \\\n                               reached_goal_mean_reward\n            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n            debug_message = 'el {}, ep {:04}, ts {:07}, '\n            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n            debug_message = debug_message.format(\n                elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n                mean_100_eval_score, std_100_eval_score)\n            print(debug_message, end='\\r', flush=True)\n            if reached_debug_time or training_is_over:\n                print(ERASE_LINE + debug_message, flush=True)\n                last_debug_time = time.time()\n            if training_is_over:\n                if reached_max_minutes: print(u'--&gt; reached_max_minutes \\u2715')\n                if reached_max_episodes: print(u'--&gt; reached_max_episodes \\u2715')\n                if reached_goal_mean_reward: print(u'--&gt; reached_goal_mean_reward \\u2713')\n                break\n                \n        final_eval_score, score_std = self.evaluate(self.online_policy_model, env, n_episodes=100)\n        wallclock_time = time.time() - training_start\n        print('Training complete.')\n        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n              ' {:.2f}s wall-clock time.\\n'.format(\n                  final_eval_score, score_std, training_time, wallclock_time))\n        env.close() ; del env\n        self.get_cleaned_checkpoints()\n        return result, final_eval_score, training_time, wallclock_time\n    \n    def evaluate(self, eval_policy_model, eval_env, n_episodes=1):\n        rs = []\n        for _ in range(n_episodes):\n            s, d = eval_env.reset(), False\n            rs.append(0)\n            for _ in count():\n                a = self.evaluation_strategy.select_action(eval_policy_model, s)\n                s, r, d, _ = eval_env.step(a)\n                rs[-1] += r\n                if d: break\n        return np.mean(rs), np.std(rs)\n\n    def get_cleaned_checkpoints(self, n_checkpoints=4):\n        try: \n            return self.checkpoint_paths\n        except AttributeError:\n            self.checkpoint_paths = {}\n\n        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n        last_ep = max(paths_dic.keys())\n        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n\n        for idx, path in paths_dic.items():\n            if idx in checkpoint_idxs:\n                self.checkpoint_paths[idx] = path\n            else:\n                os.unlink(path)\n\n        return self.checkpoint_paths\n\n    def demo_last(self, title='Fully-trained {} Agent', n_episodes=2, max_n_videos=2):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        last_ep = max(checkpoint_paths.keys())\n        self.online_policy_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n\n        self.evaluate(self.online_policy_model, env, n_episodes=n_episodes)\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def demo_progression(self, title='{} Agent progression', max_n_videos=4):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        for i in sorted(checkpoint_paths.keys()):\n            self.online_policy_model.load_state_dict(torch.load(checkpoint_paths[i]))\n            self.evaluate(self.online_policy_model, env, n_episodes=1)\n\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            subtitle_eps=sorted(checkpoint_paths.keys()),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def save_checkpoint(self, episode_idx, model):\n        torch.save(model.state_dict(), \n                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))\n\n\nddpg_results = []\nbest_agent, best_eval_score = None, float('-inf')\nfor seed in SEEDS:\n    environment_settings = {\n        'env_name': 'Pendulum-v0',\n        'gamma': 0.99,\n        'max_minutes': 20,\n        'max_episodes': 500,\n        'goal_mean_100_reward': -150\n    }\n\n    policy_model_fn = lambda nS, bounds: FCDP(nS, bounds, hidden_dims=(256,256))\n    policy_max_grad_norm = float('inf')\n    policy_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n    policy_optimizer_lr = 0.0003\n\n    value_model_fn = lambda nS, nA: FCQV(nS, nA, hidden_dims=(256,256))\n    value_max_grad_norm = float('inf')\n    value_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n    value_optimizer_lr = 0.0003\n\n    training_strategy_fn = lambda bounds: NormalNoiseStrategy(bounds, exploration_noise_ratio=0.1)\n    evaluation_strategy_fn = lambda bounds: GreedyStrategy(bounds)\n\n    replay_buffer_fn = lambda: ReplayBuffer(max_size=100000, batch_size=256)\n    n_warmup_batches = 5\n    update_target_every_steps = 1\n    tau = 0.005\n    \n    env_name, gamma, max_minutes, \\\n    max_episodes, goal_mean_100_reward = environment_settings.values()\n\n    agent = DDPG(replay_buffer_fn,\n                 policy_model_fn, \n                 policy_max_grad_norm, \n                 policy_optimizer_fn, \n                 policy_optimizer_lr,\n                 value_model_fn, \n                 value_max_grad_norm, \n                 value_optimizer_fn, \n                 value_optimizer_lr, \n                 training_strategy_fn,\n                 evaluation_strategy_fn,\n                 n_warmup_batches,\n                 update_target_every_steps,\n                 tau)\n\n    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n    result, final_eval_score, training_time, wallclock_time = agent.train(\n        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n    ddpg_results.append(result)\n    if final_eval_score &gt; best_eval_score:\n        best_eval_score = final_eval_score\n        best_agent = agent\nddpg_results = np.array(ddpg_results)\n\nel 00:00:03, ep 0000, ts 0000200, ar 10 -1296.7±000.0, 100 -1296.7±000.0, ex 100 0.3±0.0, ev -1391.7±000.0\nel 00:03:04, ep 0116, ts 0023400, ar 10 -146.8±071.2, 100 -173.0±167.8, ex 100 0.0±0.0, ev -147.4±097.8\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score -127.46±79.44 in 171.06s training time, 193.92s wall-clock time.\n\nel 00:00:00, ep 0000, ts 0000200, ar 10 -1373.7±000.0, 100 -1373.7±000.0, ex 100 0.3±0.0, ev -1743.3±000.0\nel 00:03:11, ep 0121, ts 0024400, ar 10 -150.2±113.4, 100 -141.0±091.1, ex 100 0.0±0.0, ev -149.1±087.6\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score -148.51±80.65 in 179.45s training time, 200.79s wall-clock time.\n\nel 00:00:00, ep 0000, ts 0000200, ar 10 -1364.0±000.0, 100 -1364.0±000.0, ex 100 0.3±0.0, ev -1923.0±000.0\nel 00:05:01, ep 0182, ts 0036600, ar 10 -123.3±094.1, 100 -155.8±096.3, ex 100 0.0±0.0, ev -159.2±083.3\nel 00:08:15, ep 0286, ts 0057400, ar 10 -181.7±058.9, 100 -155.6±090.6, ex 100 0.0±0.0, ev -149.5±089.4\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score -134.81±80.61 in 466.72s training time, 504.67s wall-clock time.\n\nel 00:00:00, ep 0000, ts 0000200, ar 10 -1219.9±000.0, 100 -1219.9±000.0, ex 100 0.3±0.0, ev -1435.3±000.0\nel 00:03:14, ep 0121, ts 0024400, ar 10 -114.9±064.8, 100 -155.1±104.9, ex 100 0.0±0.0, ev -148.1±086.7\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score -149.87±79.87 in 182.66s training time, 203.56s wall-clock time.\n\nel 00:00:00, ep 0000, ts 0000200, ar 10 -1021.2±000.0, 100 -1021.2±000.0, ex 100 0.3±0.0, ev -1577.6±000.0\nel 00:03:02, ep 0117, ts 0023600, ar 10 -134.0±084.3, 100 -197.0±244.7, ex 100 0.0±0.0, ev -149.6±087.5\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score -145.64±86.48 in 171.49s training time, 192.05s wall-clock time.\n\n\n\n\nbest_agent.demo_progression()\n\nDDPG Agent progression\n        Episode 0\n        \n        Episode 38\n        \n        Episode 77\n        \n        Episode 116\n        \n\n\n\nbest_agent.demo_last()\n\nFully-trained DDPG Agent\n        Trial 0\n        \n        Trial 1\n        \n\n\n\nddpg_max_t, ddpg_max_r, ddpg_max_s, \\\nddpg_max_sec, ddpg_max_rt = np.max(ddpg_results, axis=0).T\nddpg_min_t, ddpg_min_r, ddpg_min_s, \\\nddpg_min_sec, ddpg_min_rt = np.min(ddpg_results, axis=0).T\nddpg_mean_t, ddpg_mean_r, ddpg_mean_s, \\\nddpg_mean_sec, ddpg_mean_rt = np.mean(ddpg_results, axis=0).T\nddpg_x = np.arange(len(ddpg_mean_s))\n\n\nfig, axs = plt.subplots(2, 1, figsize=(15,10), sharey=False, sharex=True)\n\n# DDPG\naxs[0].plot(ddpg_max_r, 'r', linewidth=1)\naxs[0].plot(ddpg_min_r, 'r', linewidth=1)\naxs[0].plot(ddpg_mean_r, 'r:', label='DDPG', linewidth=2)\naxs[0].fill_between(\n    ddpg_x, ddpg_min_r, ddpg_max_r, facecolor='r', alpha=0.3)\n\naxs[1].plot(ddpg_max_s, 'r', linewidth=1)\naxs[1].plot(ddpg_min_s, 'r', linewidth=1)\naxs[1].plot(ddpg_mean_s, 'r:', label='DDPG', linewidth=2)\naxs[1].fill_between(\n    ddpg_x, ddpg_min_s, ddpg_max_s, facecolor='r', alpha=0.3)\n\n# ALL\naxs[0].set_title('Moving Avg Reward (Training)')\naxs[1].set_title('Moving Avg Reward (Evaluation)')\nplt.xlabel('Episodes')\naxs[0].legend(loc='upper left')\nplt.show()\n\n\n\n\n\nfig, axs = plt.subplots(3, 1, figsize=(15,15), sharey=False, sharex=True)\n\n# DDPG\naxs[0].plot(ddpg_max_t, 'r', linewidth=1)\naxs[0].plot(ddpg_min_t, 'r', linewidth=1)\naxs[0].plot(ddpg_mean_t, 'r:', label='DDPG', linewidth=2)\naxs[0].fill_between(\n    ddpg_x, ddpg_min_t, ddpg_max_t, facecolor='r', alpha=0.3)\n\naxs[1].plot(ddpg_max_sec, 'r', linewidth=1)\naxs[1].plot(ddpg_min_sec, 'r', linewidth=1)\naxs[1].plot(ddpg_mean_sec, 'r:', label='DDPG', linewidth=2)\naxs[1].fill_between(\n    ddpg_x, ddpg_min_sec, ddpg_max_sec, facecolor='r', alpha=0.3)\n\naxs[2].plot(ddpg_max_rt, 'r', linewidth=1)\naxs[2].plot(ddpg_min_rt, 'r', linewidth=1)\naxs[2].plot(ddpg_mean_rt, 'r:', label='DDPG', linewidth=2)\naxs[2].fill_between(\n    ddpg_x, ddpg_min_rt, ddpg_max_rt, facecolor='r', alpha=0.3)\n\n# ALL\naxs[0].set_title('Total Steps')\naxs[1].set_title('Training Time')\naxs[2].set_title('Wall-clock Time')\nplt.xlabel('Episodes')\naxs[0].legend(loc='upper left')\nplt.show()\n\n\n\n\n\nddpg_root_dir = os.path.join(RESULTS_DIR, 'ddpg')\nnot os.path.exists(ddpg_root_dir) and os.makedirs(ddpg_root_dir)\n\nnp.save(os.path.join(ddpg_root_dir, 'x'), ddpg_x)\n\nnp.save(os.path.join(ddpg_root_dir, 'max_r'), ddpg_max_r)\nnp.save(os.path.join(ddpg_root_dir, 'min_r'), ddpg_min_r)\nnp.save(os.path.join(ddpg_root_dir, 'mean_r'), ddpg_mean_r)\n\nnp.save(os.path.join(ddpg_root_dir, 'max_s'), ddpg_max_s)\nnp.save(os.path.join(ddpg_root_dir, 'min_s'), ddpg_min_s )\nnp.save(os.path.join(ddpg_root_dir, 'mean_s'), ddpg_mean_s)\n\nnp.save(os.path.join(ddpg_root_dir, 'max_t'), ddpg_max_t)\nnp.save(os.path.join(ddpg_root_dir, 'min_t'), ddpg_min_t)\nnp.save(os.path.join(ddpg_root_dir, 'mean_t'), ddpg_mean_t)\n\nnp.save(os.path.join(ddpg_root_dir, 'max_sec'), ddpg_max_sec)\nnp.save(os.path.join(ddpg_root_dir, 'min_sec'), ddpg_min_sec)\nnp.save(os.path.join(ddpg_root_dir, 'mean_sec'), ddpg_mean_sec)\n\nnp.save(os.path.join(ddpg_root_dir, 'max_rt'), ddpg_max_rt)\nnp.save(os.path.join(ddpg_root_dir, 'min_rt'), ddpg_min_rt)\nnp.save(os.path.join(ddpg_root_dir, 'mean_rt'), ddpg_mean_rt)"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-12.html#twin-delayed-ddpg-td3",
    "href": "publication/GDRL/GDRL-chapter-12.html#twin-delayed-ddpg-td3",
    "title": "Chapter 12: Advanced Actor-Critic Methods",
    "section": "Twin Delayed DDPG (TD3)",
    "text": "Twin Delayed DDPG (TD3)\n\nclass NormalNoiseDecayStrategy():\n    def __init__(self, bounds, init_noise_ratio=0.5, min_noise_ratio=0.1, decay_steps=10000):\n        self.t = 0\n        self.low, self.high = bounds\n        self.noise_ratio = init_noise_ratio\n        self.init_noise_ratio = init_noise_ratio\n        self.min_noise_ratio = min_noise_ratio\n        self.decay_steps = decay_steps\n        self.ratio_noise_injected = 0\n\n    def _noise_ratio_update(self):\n        noise_ratio = 1 - self.t / self.decay_steps\n        noise_ratio = (self.init_noise_ratio - self.min_noise_ratio) * noise_ratio + self.min_noise_ratio\n        noise_ratio = np.clip(noise_ratio, self.min_noise_ratio, self.init_noise_ratio)\n        self.t += 1\n        return noise_ratio\n\n    def select_action(self, model, state, max_exploration=False):\n        if max_exploration:\n            noise_scale = self.high\n        else:\n            noise_scale = self.noise_ratio * self.high\n\n        with torch.no_grad():\n            greedy_action = model(state).cpu().detach().data.numpy().squeeze()\n\n        noise = np.random.normal(loc=0, scale=noise_scale, size=len(self.high))\n        noisy_action = greedy_action + noise\n        action = np.clip(noisy_action, self.low, self.high)\n\n        self.noise_ratio = self._noise_ratio_update()\n        self.ratio_noise_injected = np.mean(abs((greedy_action - action)/(self.high - self.low)))\n        return action\n\n\ns = NormalNoiseDecayStrategy(([-2],[2]))\nplt.plot([s._noise_ratio_update() for _ in range(50000)])\nplt.title('Normal Noise Linear ratio')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\nclass FCTQV(nn.Module):\n    def __init__(self, \n                 input_dim, \n                 output_dim, \n                 hidden_dims=(32,32), \n                 activation_fc=F.relu):\n        super(FCTQV, self).__init__()\n        self.activation_fc = activation_fc\n\n        self.input_layer_a = nn.Linear(input_dim + output_dim, hidden_dims[0])\n        self.input_layer_b = nn.Linear(input_dim + output_dim, hidden_dims[0])\n\n        self.hidden_layers_a = nn.ModuleList()\n        self.hidden_layers_b = nn.ModuleList()\n        for i in range(len(hidden_dims)-1):\n            hidden_layer_a = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n            self.hidden_layers_a.append(hidden_layer_a)\n\n            hidden_layer_b = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n            self.hidden_layers_b.append(hidden_layer_b)\n\n        self.output_layer_a = nn.Linear(hidden_dims[-1], 1)\n        self.output_layer_b = nn.Linear(hidden_dims[-1], 1)\n\n        device = \"cpu\"\n        if torch.cuda.is_available():\n            device = \"cuda:0\"\n        self.device = torch.device(device)\n        self.to(self.device)\n\n    def _format(self, state, action):\n        x, u = state, action\n        if not isinstance(x, torch.Tensor):\n            x = torch.tensor(x, \n                             device=self.device, \n                             dtype=torch.float32)\n            x = x.unsqueeze(0)\n        if not isinstance(u, torch.Tensor):\n            u = torch.tensor(u, \n                             device=self.device, \n                             dtype=torch.float32)\n            u = u.unsqueeze(0)\n        return x, u\n\n    def forward(self, state, action):\n        x, u = self._format(state, action)\n        x = torch.cat((x, u), dim=1)\n        xa = self.activation_fc(self.input_layer_a(x))\n        xb = self.activation_fc(self.input_layer_b(x))\n        for hidden_layer_a, hidden_layer_b in zip(self.hidden_layers_a, self.hidden_layers_b):\n            xa = self.activation_fc(hidden_layer_a(xa))\n            xb = self.activation_fc(hidden_layer_b(xb))\n        xa = self.output_layer_a(xa)\n        xb = self.output_layer_b(xb)\n        return xa, xb\n    \n    def Qa(self, state, action):\n        x, u = self._format(state, action)\n        x = torch.cat((x, u), dim=1)\n        xa = self.activation_fc(self.input_layer_a(x))\n        for hidden_layer_a in self.hidden_layers_a:\n            xa = self.activation_fc(hidden_layer_a(xa))\n        return self.output_layer_a(xa)\n    \n    def load(self, experiences):\n        states, actions, new_states, rewards, is_terminals = experiences\n        states = torch.from_numpy(states).float().to(self.device)\n        actions = torch.from_numpy(actions).float().to(self.device)\n        new_states = torch.from_numpy(new_states).float().to(self.device)\n        rewards = torch.from_numpy(rewards).float().to(self.device)\n        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n        return states, actions, new_states, rewards, is_terminals\n\n\nclass TD3():\n    def __init__(self, \n                 replay_buffer_fn,\n                 policy_model_fn, \n                 policy_max_grad_norm, \n                 policy_optimizer_fn, \n                 policy_optimizer_lr,\n                 value_model_fn, \n                 value_max_grad_norm, \n                 value_optimizer_fn, \n                 value_optimizer_lr, \n                 training_strategy_fn,\n                 evaluation_strategy_fn,\n                 n_warmup_batches,\n                 update_value_target_every_steps,\n                 update_policy_target_every_steps,\n                 train_policy_every_steps,\n                 tau,\n                 policy_noise_ratio,\n                 policy_noise_clip_ratio):\n        self.replay_buffer_fn = replay_buffer_fn\n\n        self.policy_model_fn = policy_model_fn\n        self.policy_max_grad_norm = policy_max_grad_norm\n        self.policy_optimizer_fn = policy_optimizer_fn\n        self.policy_optimizer_lr = policy_optimizer_lr\n        \n        self.value_model_fn = value_model_fn\n        self.value_max_grad_norm = value_max_grad_norm\n        self.value_optimizer_fn = value_optimizer_fn\n        self.value_optimizer_lr = value_optimizer_lr\n\n        self.training_strategy_fn = training_strategy_fn\n        self.evaluation_strategy_fn = evaluation_strategy_fn\n\n        self.n_warmup_batches = n_warmup_batches\n        self.update_value_target_every_steps = update_value_target_every_steps\n        self.update_policy_target_every_steps = update_policy_target_every_steps\n        self.train_policy_every_steps = train_policy_every_steps\n        \n        self.tau = tau\n        self.policy_noise_ratio = policy_noise_ratio\n        self.policy_noise_clip_ratio = policy_noise_clip_ratio\n\n    def optimize_model(self, experiences):\n        states, actions, rewards, next_states, is_terminals = experiences\n        batch_size = len(is_terminals)\n\n        with torch.no_grad():\n            a_ran = self.target_policy_model.env_max - self.target_policy_model.env_min\n            a_noise = torch.randn_like(actions) * self.policy_noise_ratio * a_ran\n            n_min = self.target_policy_model.env_min * self.policy_noise_clip_ratio\n            n_max = self.target_policy_model.env_max * self.policy_noise_clip_ratio            \n            a_noise = torch.max(torch.min(a_noise, n_max), n_min)\n\n            argmax_a_q_sp = self.target_policy_model(next_states)\n            noisy_argmax_a_q_sp = argmax_a_q_sp + a_noise\n            noisy_argmax_a_q_sp = torch.max(torch.min(noisy_argmax_a_q_sp, \n                                                      self.target_policy_model.env_max),\n                                            self.target_policy_model.env_min)\n\n            max_a_q_sp_a, max_a_q_sp_b = self.target_value_model(next_states, noisy_argmax_a_q_sp)\n            max_a_q_sp = torch.min(max_a_q_sp_a, max_a_q_sp_b)\n\n            target_q_sa = rewards + self.gamma * max_a_q_sp * (1 - is_terminals)\n\n        q_sa_a, q_sa_b = self.online_value_model(states, actions)\n        td_error_a = q_sa_a - target_q_sa\n        td_error_b = q_sa_b - target_q_sa\n\n        value_loss = td_error_a.pow(2).mul(0.5).mean() + td_error_b.pow(2).mul(0.5).mean()\n        self.value_optimizer.zero_grad()\n        value_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.online_value_model.parameters(), \n                                       self.value_max_grad_norm)\n        self.value_optimizer.step()\n\n        if np.sum(self.episode_timestep) % self.train_policy_every_steps == 0:\n            argmax_a_q_s = self.online_policy_model(states)\n            max_a_q_s = self.online_value_model.Qa(states, argmax_a_q_s)\n\n            policy_loss = -max_a_q_s.mean()\n            self.policy_optimizer.zero_grad()\n            policy_loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.online_policy_model.parameters(), \n                                           self.policy_max_grad_norm)        \n            self.policy_optimizer.step()\n\n    def interaction_step(self, state, env):\n        min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n        action = self.training_strategy.select_action(self.online_policy_model, \n                                                      state, \n                                                      len(self.replay_buffer) &lt; min_samples)\n        new_state, reward, is_terminal, info = env.step(action)\n        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n        is_failure = is_terminal and not is_truncated\n        experience = (state, action, reward, new_state, float(is_failure))\n\n        self.replay_buffer.store(experience)\n        self.episode_reward[-1] += reward\n        self.episode_timestep[-1] += 1\n        self.episode_exploration[-1] += self.training_strategy.ratio_noise_injected\n        return new_state, is_terminal\n\n    def update_value_network(self, tau=None):\n        tau = self.tau if tau is None else tau\n        for target, online in zip(self.target_value_model.parameters(), \n                                  self.online_value_model.parameters()):\n            target_ratio = (1.0 - self.tau) * target.data\n            online_ratio = self.tau * online.data\n            mixed_weights = target_ratio + online_ratio\n            target.data.copy_(mixed_weights)\n\n    def update_policy_network(self, tau=None):\n        tau = self.tau if tau is None else tau\n        for target, online in zip(self.target_policy_model.parameters(), \n                                  self.online_policy_model.parameters()):\n            target_ratio = (1.0 - self.tau) * target.data\n            online_ratio = self.tau * online.data\n            mixed_weights = target_ratio + online_ratio\n            target.data.copy_(mixed_weights)\n\n    def train(self, make_env_fn, make_env_kargs, seed, gamma, \n              max_minutes, max_episodes, goal_mean_100_reward):\n        training_start, last_debug_time = time.time(), float('-inf')\n\n        self.checkpoint_dir = tempfile.mkdtemp()\n        self.make_env_fn = make_env_fn\n        self.make_env_kargs = make_env_kargs\n        self.seed = seed\n        self.gamma = gamma\n        \n        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n    \n        nS, nA = env.observation_space.shape[0], env.action_space.shape[0]\n        action_bounds = env.action_space.low, env.action_space.high\n        self.episode_timestep = []\n        self.episode_reward = []\n        self.episode_seconds = []\n        self.evaluation_scores = []        \n        self.episode_exploration = []\n        \n        self.target_value_model = self.value_model_fn(nS, nA)\n        self.online_value_model = self.value_model_fn(nS, nA)\n        self.update_value_network(tau=1.0)\n\n        self.target_policy_model = self.policy_model_fn(nS, action_bounds)\n        self.online_policy_model = self.policy_model_fn(nS, action_bounds)\n        self.update_policy_network(tau=1.0)\n\n        self.value_optimizer = self.value_optimizer_fn(self.online_value_model, \n                                                       self.value_optimizer_lr)        \n        self.policy_optimizer = self.policy_optimizer_fn(self.online_policy_model, \n                                                         self.policy_optimizer_lr)\n\n        self.replay_buffer = self.replay_buffer_fn()\n        self.training_strategy = training_strategy_fn(action_bounds)\n        self.evaluation_strategy = evaluation_strategy_fn(action_bounds)\n                    \n        result = np.empty((max_episodes, 5))\n        result[:] = np.nan\n        training_time = 0\n        for episode in range(1, max_episodes + 1):\n            episode_start = time.time()\n            \n            state, is_terminal = env.reset(), False\n            self.episode_reward.append(0.0)\n            self.episode_timestep.append(0.0)\n            self.episode_exploration.append(0.0)\n\n            for step in count():\n                state, is_terminal = self.interaction_step(state, env)\n\n                min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n                if len(self.replay_buffer) &gt; min_samples:\n                    experiences = self.replay_buffer.sample()\n                    experiences = self.online_value_model.load(experiences)\n                    self.optimize_model(experiences)\n\n                if np.sum(self.episode_timestep) % self.update_value_target_every_steps == 0:\n                    self.update_value_network()\n\n                if np.sum(self.episode_timestep) % self.update_policy_target_every_steps == 0:\n                    self.update_policy_network()\n\n                if is_terminal:\n                    gc.collect()\n                    break\n            \n            # stats\n            episode_elapsed = time.time() - episode_start\n            self.episode_seconds.append(episode_elapsed)\n            training_time += episode_elapsed\n            evaluation_score, _ = self.evaluate(self.online_policy_model, env)\n            self.save_checkpoint(episode-1, self.online_policy_model)\n\n            total_step = int(np.sum(self.episode_timestep))\n            self.evaluation_scores.append(evaluation_score)\n            \n            mean_10_reward = np.mean(self.episode_reward[-10:])\n            std_10_reward = np.std(self.episode_reward[-10:])\n            mean_100_reward = np.mean(self.episode_reward[-100:])\n            std_100_reward = np.std(self.episode_reward[-100:])\n            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n            lst_100_exp_rat = np.array(\n                self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n            mean_100_exp_rat = np.mean(lst_100_exp_rat)\n            std_100_exp_rat = np.std(lst_100_exp_rat)\n            \n            wallclock_elapsed = time.time() - training_start\n            result[episode-1] = total_step, mean_100_reward, \\\n                mean_100_eval_score, training_time, wallclock_elapsed\n            \n            reached_debug_time = time.time() - last_debug_time &gt;= LEAVE_PRINT_EVERY_N_SECS\n            reached_max_minutes = wallclock_elapsed &gt;= max_minutes * 60\n            reached_max_episodes = episode &gt;= max_episodes\n            reached_goal_mean_reward = mean_100_eval_score &gt;= goal_mean_100_reward\n            training_is_over = reached_max_minutes or \\\n                               reached_max_episodes or \\\n                               reached_goal_mean_reward\n            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n            debug_message = 'el {}, ep {:04}, ts {:07}, '\n            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n            debug_message = debug_message.format(\n                elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n                mean_100_eval_score, std_100_eval_score)\n            print(debug_message, end='\\r', flush=True)\n            if reached_debug_time or training_is_over:\n                print(ERASE_LINE + debug_message, flush=True)\n                last_debug_time = time.time()\n            if training_is_over:\n                if reached_max_minutes: print(u'--&gt; reached_max_minutes \\u2715')\n                if reached_max_episodes: print(u'--&gt; reached_max_episodes \\u2715')\n                if reached_goal_mean_reward: print(u'--&gt; reached_goal_mean_reward \\u2713')\n                break\n                \n        final_eval_score, score_std = self.evaluate(self.online_policy_model, env, n_episodes=100)\n        wallclock_time = time.time() - training_start\n        print('Training complete.')\n        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n              ' {:.2f}s wall-clock time.\\n'.format(\n                  final_eval_score, score_std, training_time, wallclock_time))\n        env.close() ; del env\n        self.get_cleaned_checkpoints()\n        return result, final_eval_score, training_time, wallclock_time\n    \n    def evaluate(self, eval_policy_model, eval_env, n_episodes=1):\n        rs = []\n        for _ in range(n_episodes):\n            s, d = eval_env.reset(), False\n            rs.append(0)\n            for _ in count():\n                a = self.evaluation_strategy.select_action(eval_policy_model, s)\n                s, r, d, _ = eval_env.step(a)\n                rs[-1] += r\n                if d: break\n        return np.mean(rs), np.std(rs)\n\n    def get_cleaned_checkpoints(self, n_checkpoints=4):\n        try: \n            return self.checkpoint_paths\n        except AttributeError:\n            self.checkpoint_paths = {}\n\n        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n        last_ep = max(paths_dic.keys())\n        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n\n        for idx, path in paths_dic.items():\n            if idx in checkpoint_idxs:\n                self.checkpoint_paths[idx] = path\n            else:\n                os.unlink(path)\n\n        return self.checkpoint_paths\n\n    def demo_last(self, title='Fully-trained {} Agent', n_episodes=2, max_n_videos=2):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        last_ep = max(checkpoint_paths.keys())\n        self.online_policy_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n\n        self.evaluate(self.online_policy_model, env, n_episodes=n_episodes)\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def demo_progression(self, title='{} Agent progression', max_n_videos=4):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        for i in sorted(checkpoint_paths.keys()):\n            self.online_policy_model.load_state_dict(torch.load(checkpoint_paths[i]))\n            self.evaluate(self.online_policy_model, env, n_episodes=1)\n\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            subtitle_eps=sorted(checkpoint_paths.keys()),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def save_checkpoint(self, episode_idx, model):\n        torch.save(model.state_dict(), \n                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))\n\n\ntd3_results = []\nbest_agent, best_eval_score = None, float('-inf')\nfor seed in SEEDS:\n    environment_settings = {\n        'env_name': 'HopperBulletEnv-v0',\n        'gamma': 0.99,\n        'max_minutes': 300,\n        'max_episodes': 10000,\n        'goal_mean_100_reward': 1500\n    }\n    \n    policy_model_fn = lambda nS, bounds: FCDP(nS, bounds, hidden_dims=(256,256))\n    policy_max_grad_norm = float('inf')\n    policy_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n    policy_optimizer_lr = 0.0003\n\n    value_model_fn = lambda nS, nA: FCTQV(nS, nA, hidden_dims=(256,256))\n    value_max_grad_norm = float('inf')\n    value_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n    value_optimizer_lr = 0.0003\n\n    training_strategy_fn = lambda bounds: NormalNoiseDecayStrategy(bounds,\n                                                                   init_noise_ratio=0.5,\n                                                                   min_noise_ratio=0.1,\n                                                                   decay_steps=200000)\n    evaluation_strategy_fn = lambda bounds: GreedyStrategy(bounds)\n\n    replay_buffer_fn = lambda: ReplayBuffer(max_size=1000000, batch_size=256)\n    n_warmup_batches = 5\n    update_value_target_every_steps = 2\n    update_policy_target_every_steps = 2\n    train_policy_every_steps = 2\n    policy_noise_ratio = 0.1\n    policy_noise_clip_ratio = 0.5\n    tau = 0.005\n\n    env_name, gamma, max_minutes, \\\n    max_episodes, goal_mean_100_reward = environment_settings.values()\n\n    agent = TD3(replay_buffer_fn,\n                policy_model_fn, \n                policy_max_grad_norm, \n                policy_optimizer_fn, \n                policy_optimizer_lr,\n                value_model_fn,\n                value_max_grad_norm, \n                value_optimizer_fn, \n                value_optimizer_lr, \n                training_strategy_fn,\n                evaluation_strategy_fn,\n                n_warmup_batches,\n                update_value_target_every_steps,\n                update_policy_target_every_steps,\n                train_policy_every_steps,\n                tau,\n                policy_noise_ratio,\n                policy_noise_clip_ratio)\n\n    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name, inner_wrappers=[RenderUint8])\n    result, final_eval_score, training_time, wallclock_time = agent.train(\n        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n    td3_results.append(result)\n    if final_eval_score &gt; best_eval_score:\n        best_eval_score = final_eval_score\n        best_agent = agent\ntd3_results = np.array(td3_results)\n\nel 00:00:00, ep 0000, ts 0000009, ar 10 011.6±000.0, 100 011.6±000.0, ex 100 0.4±0.0, ev 035.5±000.0\nel 00:05:01, ep 0969, ts 0028719, ar 10 125.0±054.3, 100 100.8±032.9, ex 100 0.2±0.0, ev 132.1±011.6\nel 00:10:01, ep 1256, ts 0054776, ar 10 253.8±203.5, 100 141.1±117.7, ex 100 0.1±0.0, ev 593.6±520.4\nel 00:15:02, ep 1377, ts 0077554, ar 10 361.6±486.4, 100 328.1±313.6, ex 100 0.1±0.0, ev 1486.1±740.8\nel 00:15:29, ep 1386, ts 0079488, ar 10 510.4±523.0, 100 340.6±323.8, ex 100 0.1±0.0, ev 1503.1±741.9\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 2001.97±4.70 in 762.88s training time, 1011.77s wall-clock time.\n\nel 00:00:00, ep 0000, ts 0000009, ar 10 015.6±000.0, 100 015.6±000.0, ex 100 0.3±0.0, ev 035.0±000.0\nel 00:05:00, ep 0920, ts 0024315, ar 10 073.5±039.6, 100 093.9±049.0, ex 100 0.2±0.0, ev 448.3±359.1\nel 00:10:01, ep 1284, ts 0052580, ar 10 105.8±067.3, 100 106.5±049.2, ex 100 0.1±0.0, ev 165.8±015.5\nel 00:15:03, ep 1464, ts 0072347, ar 10 200.2±102.0, 100 126.5±093.5, ex 100 0.1±0.0, ev 810.8±309.7\nel 00:20:03, ep 1586, ts 0095490, ar 10 292.9±236.3, 100 222.9±136.8, ex 100 0.1±0.0, ev 775.6±333.1\nel 00:25:09, ep 1646, ts 0120802, ar 10 704.0±455.1, 100 400.1±319.3, ex 100 0.1±0.0, ev 1148.3±221.7\nel 00:30:20, ep 1682, ts 0147529, ar 10 1007.1±516.1, 100 672.4±460.3, ex 100 0.1±0.0, ev 1294.7±183.9\nel 00:35:23, ep 1709, ts 0173563, ar 10 1660.4±241.3, 100 998.7±556.1, ex 100 0.1±0.0, ev 1443.9±244.5\nel 00:37:16, ep 1719, ts 0183067, ar 10 1634.0±250.5, 100 1101.3±563.0, ex 100 0.1±0.0, ev 1507.8±234.6\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 1804.55±26.61 in 1847.36s training time, 2315.94s wall-clock time.\n\nel 00:00:00, ep 0000, ts 0000005, ar 10 015.9±000.0, 100 015.9±000.0, ex 100 0.3±0.0, ev 026.7±000.0\nel 00:05:00, ep 0998, ts 0028622, ar 10 097.6±021.4, 100 087.8±026.1, ex 100 0.2±0.0, ev 129.1±009.7\nel 00:10:00, ep 1272, ts 0055064, ar 10 150.0±051.5, 100 127.3±074.0, ex 100 0.1±0.0, ev 438.0±321.5\nel 00:15:01, ep 1452, ts 0078642, ar 10 346.0±307.3, 100 169.2±183.8, ex 100 0.1±0.0, ev 567.9±442.6\nel 00:20:04, ep 1544, ts 0105696, ar 10 698.1±366.4, 100 322.6±274.7, ex 100 0.1±0.0, ev 704.8±422.6\nel 00:25:05, ep 1579, ts 0133662, ar 10 1164.0±255.9, 100 578.5±430.5, ex 100 0.1±0.0, ev 854.2±463.2\nel 00:30:16, ep 1612, ts 0161420, ar 10 1643.3±101.3, 100 901.6±510.7, ex 100 0.1±0.0, ev 1168.8±398.6\nel 00:35:19, ep 1639, ts 0187612, ar 10 1602.7±418.5, 100 1226.1±483.1, ex 100 0.1±0.0, ev 1417.9±285.4\nel 00:38:22, ep 1654, ts 0202612, ar 10 1814.9±025.8, 100 1368.9±469.9, ex 100 0.1±0.0, ev 1505.5±293.2\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 1926.78±6.72 in 1992.84s training time, 2382.96s wall-clock time.\n\nel 00:00:00, ep 0000, ts 0000007, ar 10 020.5±000.0, 100 020.5±000.0, ex 100 0.3±0.0, ev 027.9±000.0\nel 00:05:00, ep 0911, ts 0028420, ar 10 095.9±025.1, 100 109.1±027.2, ex 100 0.2±0.0, ev 186.8±184.8\nel 00:10:04, ep 1170, ts 0056369, ar 10 227.9±188.5, 100 161.3±092.8, ex 100 0.1±0.0, ev 469.0±319.5\nel 00:15:06, ep 1279, ts 0079067, ar 10 297.9±187.7, 100 190.6±145.4, ex 100 0.1±0.0, ev 945.9±038.7\nel 00:20:07, ep 1360, ts 0103119, ar 10 448.1±294.0, 100 295.1±249.0, ex 100 0.1±0.0, ev 1045.4±143.4\nel 00:25:14, ep 1413, ts 0129683, ar 10 549.3±288.3, 100 498.0±400.9, ex 100 0.1±0.0, ev 1227.8±276.0\nel 00:30:20, ep 1449, ts 0156138, ar 10 1381.2±394.4, 100 802.3±520.1, ex 100 0.1±0.0, ev 1395.3±326.8\nel 00:34:19, ep 1477, ts 0176218, ar 10 1475.3±383.1, 100 984.8±571.1, ex 100 0.1±0.0, ev 1503.9±410.0\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 1795.06±5.44 in 1720.39s training time, 2138.96s wall-clock time.\n\nel 00:00:00, ep 0000, ts 0000009, ar 10 012.1±000.0, 100 012.1±000.0, ex 100 0.4±0.0, ev 030.4±000.0\nel 00:05:00, ep 0909, ts 0029569, ar 10 106.4±032.2, 100 114.7±029.2, ex 100 0.2±0.0, ev 145.2±013.7\nel 00:10:01, ep 1230, ts 0059057, ar 10 096.9±075.1, 100 130.9±068.8, ex 100 0.1±0.0, ev 266.4±116.8\nel 00:15:02, ep 1483, ts 0083431, ar 10 209.6±112.4, 100 149.1±082.1, ex 100 0.1±0.0, ev 943.8±615.3\nel 00:20:03, ep 1594, ts 0107010, ar 10 314.4±238.6, 100 266.6±215.3, ex 100 0.1±0.0, ev 1067.2±532.0\nel 00:24:44, ep 1669, ts 0129049, ar 10 621.0±454.3, 100 392.4±367.0, ex 100 0.1±0.0, ev 1510.6±624.5\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 1849.30±11.54 in 1227.30s training time, 1566.15s wall-clock time.\n\n\n\n\nbest_agent.demo_progression()\n\n\nTD3 Agent progression\n        Episode 0\n        \n        Episode 462\n        \n        Episode 924\n        \n        Episode 1386\n        \n\n\n\nbest_agent.demo_last()\n\n\nFully-trained TD3 Agent\n        Trial 0\n        \n        Trial 1\n        \n\n\n\ntd3_max_t, td3_max_r, td3_max_s, \\\ntd3_max_sec, td3_max_rt = np.max(td3_results, axis=0).T\ntd3_min_t, td3_min_r, td3_min_s, \\\ntd3_min_sec, td3_min_rt = np.min(td3_results, axis=0).T\ntd3_mean_t, td3_mean_r, td3_mean_s, \\\ntd3_mean_sec, td3_mean_rt = np.mean(td3_results, axis=0).T\ntd3_x = np.arange(len(td3_mean_s))\n\n\nfig, axs = plt.subplots(2, 1, figsize=(15,10), sharey=False, sharex=True)\n\n# TD3\naxs[0].plot(td3_max_r, 'b', linewidth=1)\naxs[0].plot(td3_min_r, 'b', linewidth=1)\naxs[0].plot(td3_mean_r, 'b:', label='TD3', linewidth=2)\naxs[0].fill_between(\n    td3_x, td3_min_r, td3_max_r, facecolor='b', alpha=0.3)\n\naxs[1].plot(td3_max_s, 'b', linewidth=1)\naxs[1].plot(td3_min_s, 'b', linewidth=1)\naxs[1].plot(td3_mean_s, 'b:', label='TD3', linewidth=2)\naxs[1].fill_between(\n    td3_x, td3_min_s, td3_max_s, facecolor='b', alpha=0.3)\n\n# ALL\naxs[0].set_title('Moving Avg Reward (Training)')\naxs[1].set_title('Moving Avg Reward (Evaluation)')\nplt.xlabel('Episodes')\naxs[0].legend(loc='upper left')\nplt.show()\n\n\n\n\n\nfig, axs = plt.subplots(3, 1, figsize=(15,15), sharey=False, sharex=True)\n\n# TD3\naxs[0].plot(td3_max_t, 'b', linewidth=1)\naxs[0].plot(td3_min_t, 'b', linewidth=1)\naxs[0].plot(td3_mean_t, 'b:', label='TD3', linewidth=2)\naxs[0].fill_between(\n    td3_x, td3_min_t, td3_max_t, facecolor='b', alpha=0.3)\n\naxs[1].plot(td3_max_sec, 'b', linewidth=1)\naxs[1].plot(td3_min_sec, 'b', linewidth=1)\naxs[1].plot(td3_mean_sec, 'b:', label='TD3', linewidth=2)\naxs[1].fill_between(\n    td3_x, td3_min_sec, td3_max_sec, facecolor='b', alpha=0.3)\n\naxs[2].plot(td3_max_rt, 'b', linewidth=1)\naxs[2].plot(td3_min_rt, 'b', linewidth=1)\naxs[2].plot(td3_mean_rt, 'b:', label='TD3', linewidth=2)\naxs[2].fill_between(\n    td3_x, td3_min_rt, td3_max_rt, facecolor='b', alpha=0.3)\n\n# ALL\naxs[0].set_title('Total Steps')\naxs[1].set_title('Training Time')\naxs[2].set_title('Wall-clock Time')\nplt.xlabel('Episodes')\naxs[0].legend(loc='upper left')\nplt.show()\n\n\n\n\n\ntd3_root_dir = os.path.join(RESULTS_DIR, 'td3')\nnot os.path.exists(td3_root_dir) and os.makedirs(td3_root_dir)\n\nnp.save(os.path.join(td3_root_dir, 'x'), td3_x)\n\nnp.save(os.path.join(td3_root_dir, 'max_r'), td3_max_r)\nnp.save(os.path.join(td3_root_dir, 'min_r'), td3_min_r)\nnp.save(os.path.join(td3_root_dir, 'mean_r'), td3_mean_r)\n\nnp.save(os.path.join(td3_root_dir, 'max_s'), td3_max_s)\nnp.save(os.path.join(td3_root_dir, 'min_s'), td3_min_s )\nnp.save(os.path.join(td3_root_dir, 'mean_s'), td3_mean_s)\n\nnp.save(os.path.join(td3_root_dir, 'max_t'), td3_max_t)\nnp.save(os.path.join(td3_root_dir, 'min_t'), td3_min_t)\nnp.save(os.path.join(td3_root_dir, 'mean_t'), td3_mean_t)\n\nnp.save(os.path.join(td3_root_dir, 'max_sec'), td3_max_sec)\nnp.save(os.path.join(td3_root_dir, 'min_sec'), td3_min_sec)\nnp.save(os.path.join(td3_root_dir, 'mean_sec'), td3_mean_sec)\n\nnp.save(os.path.join(td3_root_dir, 'max_rt'), td3_max_rt)\nnp.save(os.path.join(td3_root_dir, 'min_rt'), td3_min_rt)\nnp.save(os.path.join(td3_root_dir, 'mean_rt'), td3_mean_rt)"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-12.html#soft-actor-critic-sac",
    "href": "publication/GDRL/GDRL-chapter-12.html#soft-actor-critic-sac",
    "title": "Chapter 12: Advanced Actor-Critic Methods",
    "section": "Soft Actor-Critic (SAC)",
    "text": "Soft Actor-Critic (SAC)\n\nclass FCQSA(nn.Module):\n    def __init__(self,\n                 input_dim, \n                 output_dim, \n                 hidden_dims=(32,32), \n                 activation_fc=F.relu):\n        super(FCQSA, self).__init__()\n        self.activation_fc = activation_fc\n\n        self.input_layer = nn.Linear(input_dim + output_dim, hidden_dims[0])\n        self.hidden_layers = nn.ModuleList()\n        for i in range(len(hidden_dims)-1):\n            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n            self.hidden_layers.append(hidden_layer)\n        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n\n        device = \"cpu\"\n        if torch.cuda.is_available():\n            device = \"cuda:0\"\n        self.device = torch.device(device)\n        self.to(self.device)\n\n    def _format(self, state, action):\n        x, u = state, action\n        if not isinstance(x, torch.Tensor):\n            x = torch.tensor(x, \n                             device=self.device, \n                             dtype=torch.float32)\n            x = x.unsqueeze(0)\n        if not isinstance(u, torch.Tensor):\n            u = torch.tensor(u, \n                             device=self.device, \n                             dtype=torch.float32)\n            u = u.unsqueeze(0)\n        return x, u\n\n    def forward(self, state, action):\n        x, u = self._format(state, action)\n        x = self.activation_fc(self.input_layer(torch.cat((x, u), dim=1)))\n        for i, hidden_layer in enumerate(self.hidden_layers):\n            x = self.activation_fc(hidden_layer(x))\n        x = self.output_layer(x)\n        return x\n    \n    def load(self, experiences):\n        states, actions, new_states, rewards, is_terminals = experiences\n        states = torch.from_numpy(states).float().to(self.device)\n        actions = torch.from_numpy(actions).float().to(self.device)\n        new_states = torch.from_numpy(new_states).float().to(self.device)\n        rewards = torch.from_numpy(rewards).float().to(self.device)\n        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n        return states, actions, new_states, rewards, is_terminals\n\n\nclass FCGP(nn.Module):\n    def __init__(self, \n                 input_dim, \n                 action_bounds,\n                 log_std_min=-20, \n                 log_std_max=2,\n                 hidden_dims=(32,32), \n                 activation_fc=F.relu,\n                 entropy_lr=0.001):\n        super(FCGP, self).__init__()\n        self.activation_fc = activation_fc\n        self.env_min, self.env_max = action_bounds\n        \n        self.log_std_min = log_std_min\n        self.log_std_max = log_std_max\n        \n        self.input_layer = nn.Linear(input_dim, \n                                     hidden_dims[0])\n        \n        self.hidden_layers = nn.ModuleList()\n        for i in range(len(hidden_dims)-1):\n            hidden_layer = nn.Linear(\n                hidden_dims[i], hidden_dims[i+1])\n            self.hidden_layers.append(hidden_layer)\n\n        self.output_layer_mean = nn.Linear(hidden_dims[-1], len(self.env_max))\n        self.output_layer_log_std = nn.Linear(hidden_dims[-1], len(self.env_max))\n\n        device = \"cpu\"\n        if torch.cuda.is_available():\n            device = \"cuda:0\"\n        self.device = torch.device(device)\n        self.to(self.device)\n\n        self.env_min = torch.tensor(self.env_min,\n                                    device=self.device, \n                                    dtype=torch.float32)\n\n        self.env_max = torch.tensor(self.env_max,\n                                    device=self.device, \n                                    dtype=torch.float32)\n        \n        self.nn_min = F.tanh(torch.Tensor([float('-inf')])).to(self.device)\n        self.nn_max = F.tanh(torch.Tensor([float('inf')])).to(self.device)\n        self.rescale_fn = lambda x: (x - self.nn_min) * (self.env_max - self.env_min) / \\\n                                    (self.nn_max - self.nn_min) + self.env_min\n\n        self.target_entropy = -np.prod(self.env_max.shape)\n        self.logalpha = torch.zeros(1, requires_grad=True, device=self.device)\n        self.alpha_optimizer = optim.Adam([self.logalpha], lr=entropy_lr)\n\n    def _format(self, state):\n        x = state\n        if not isinstance(x, torch.Tensor):\n            x = torch.tensor(x, \n                             device=self.device, \n                             dtype=torch.float32)\n            x = x.unsqueeze(0)\n        return x\n\n    def forward(self, state):\n        x = self._format(state)\n        x = self.activation_fc(self.input_layer(x))\n        for hidden_layer in self.hidden_layers:\n            x = self.activation_fc(hidden_layer(x))\n        x_mean = self.output_layer_mean(x)\n        x_log_std = self.output_layer_log_std(x)\n        x_log_std = torch.clamp(x_log_std, \n                                self.log_std_min, \n                                self.log_std_max)\n        return x_mean, x_log_std\n\n    def full_pass(self, state, epsilon=1e-6):\n        mean, log_std = self.forward(state)\n\n        pi_s = Normal(mean, log_std.exp())\n        pre_tanh_action = pi_s.rsample()\n        tanh_action = torch.tanh(pre_tanh_action)\n        action = self.rescale_fn(tanh_action)\n\n        log_prob = pi_s.log_prob(pre_tanh_action) - torch.log(\n            (1 - tanh_action.pow(2)).clamp(0, 1) + epsilon)\n        log_prob = log_prob.sum(dim=1, keepdim=True)\n\n        return action, log_prob, self.rescale_fn(torch.tanh(mean))\n\n    def _update_exploration_ratio(self, greedy_action, action_taken):\n        env_min, env_max = self.env_min.cpu().numpy(), self.env_max.cpu().numpy()\n        self.exploration_ratio = np.mean(abs((greedy_action - action_taken)/(env_max - env_min)))\n\n    def _get_actions(self, state):\n        mean, log_std = self.forward(state)\n\n        action = self.rescale_fn(torch.tanh(Normal(mean, log_std.exp()).sample()))\n        greedy_action = self.rescale_fn(torch.tanh(mean))\n        random_action = np.random.uniform(low=self.env_min.cpu().numpy(),\n                                          high=self.env_max.cpu().numpy())\n\n        action_shape = self.env_max.cpu().numpy().shape\n        action = action.detach().cpu().numpy().reshape(action_shape)\n        greedy_action = greedy_action.detach().cpu().numpy().reshape(action_shape)\n        random_action = random_action.reshape(action_shape)\n\n        return action, greedy_action, random_action\n\n    def select_random_action(self, state):\n        action, greedy_action, random_action = self._get_actions(state)\n        self._update_exploration_ratio(greedy_action, random_action)\n        return random_action\n\n    def select_greedy_action(self, state):\n        action, greedy_action, random_action = self._get_actions(state)\n        self._update_exploration_ratio(greedy_action, greedy_action)\n        return greedy_action\n\n    def select_action(self, state):\n        action, greedy_action, random_action = self._get_actions(state)\n        self._update_exploration_ratio(greedy_action, action)\n        return action\n\n\nclass SAC():\n    def __init__(self, \n                 replay_buffer_fn,\n                 policy_model_fn, \n                 policy_max_grad_norm, \n                 policy_optimizer_fn, \n                 policy_optimizer_lr,\n                 value_model_fn,\n                 value_max_grad_norm, \n                 value_optimizer_fn, \n                 value_optimizer_lr,\n                 n_warmup_batches,\n                 update_target_every_steps,\n                 tau):\n        self.replay_buffer_fn = replay_buffer_fn\n\n        self.policy_model_fn = policy_model_fn\n        self.policy_max_grad_norm = policy_max_grad_norm\n        self.policy_optimizer_fn = policy_optimizer_fn\n        self.policy_optimizer_lr = policy_optimizer_lr\n\n        self.value_model_fn = value_model_fn\n        self.value_max_grad_norm = value_max_grad_norm\n        self.value_optimizer_fn = value_optimizer_fn\n        self.value_optimizer_lr = value_optimizer_lr\n\n        self.n_warmup_batches = n_warmup_batches\n        self.update_target_every_steps = update_target_every_steps\n\n        self.tau = tau\n\n    def optimize_model(self, experiences):\n        states, actions, rewards, next_states, is_terminals = experiences\n        batch_size = len(is_terminals)\n\n        # policy loss\n        current_actions, logpi_s, _ = self.policy_model.full_pass(states)\n\n        target_alpha = (logpi_s + self.policy_model.target_entropy).detach()\n        alpha_loss = -(self.policy_model.logalpha * target_alpha).mean()\n\n        self.policy_model.alpha_optimizer.zero_grad()\n        alpha_loss.backward()\n        self.policy_model.alpha_optimizer.step()\n        alpha = self.policy_model.logalpha.exp()\n\n        # Note: For the people who use pytorch &gt;= 1.5.0,\n        # Rearrange the code for avoiding inplace operation\n        # See the details: https://discuss.pytorch.org/t/solved-pytorch1-5-runtimeerror-one-of-the-variables-needed-for-gradient-computation-has-been-modified-by-an-inplace-operation/90256/7\n        \n        current_q_sa_a = self.online_value_model_a(states, current_actions)\n        current_q_sa_b = self.online_value_model_b(states, current_actions)\n        current_q_sa = torch.min(current_q_sa_a, current_q_sa_b)\n        policy_loss = (alpha * logpi_s - current_q_sa).mean()\n\n        self.policy_optimizer.zero_grad()\n        policy_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), \n                                       self.policy_max_grad_norm)        \n        self.policy_optimizer.step()\n\n        # Q loss\n        ap, logpi_sp, _ = self.policy_model.full_pass(next_states)\n        q_spap_a = self.target_value_model_a(next_states, ap)\n        q_spap_b = self.target_value_model_b(next_states, ap)\n        q_spap = torch.min(q_spap_a, q_spap_b) - alpha * logpi_sp\n        target_q_sa = (rewards + self.gamma * q_spap * (1 - is_terminals)).detach()\n\n        q_sa_a = self.online_value_model_a(states, actions)\n        q_sa_b = self.online_value_model_b(states, actions)\n        qa_loss = (q_sa_a - target_q_sa).pow(2).mul(0.5).mean()\n        \n        self.value_optimizer_a.zero_grad()\n        qa_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.online_value_model_a.parameters(), \n                                       self.value_max_grad_norm)\n        self.value_optimizer_a.step()\n        \n        qb_loss = (q_sa_b - target_q_sa).pow(2).mul(0.5).mean()\n    \n        self.value_optimizer_b.zero_grad()\n        qb_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.online_value_model_b.parameters(),\n                                       self.value_max_grad_norm)\n        self.value_optimizer_b.step()\n\n    def interaction_step(self, state, env):\n        min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n        if len(self.replay_buffer) &lt; min_samples:\n            action = self.policy_model.select_random_action(state)\n        else:\n            action = self.policy_model.select_action(state)\n\n        new_state, reward, is_terminal, info = env.step(action)\n        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n        is_failure = is_terminal and not is_truncated\n        experience = (state, action, reward, new_state, float(is_failure))\n\n        self.replay_buffer.store(experience)\n        self.episode_reward[-1] += reward\n        self.episode_timestep[-1] += 1\n        self.episode_exploration[-1] += self.policy_model.exploration_ratio\n        return new_state, is_terminal\n\n    def update_value_networks(self, tau=None):\n        tau = self.tau if tau is None else tau\n        for target, online in zip(self.target_value_model_a.parameters(), \n                                  self.online_value_model_a.parameters()):\n            target_ratio = (1.0 - self.tau) * target.data\n            online_ratio = self.tau * online.data\n            mixed_weights = target_ratio + online_ratio\n            target.data.copy_(mixed_weights)\n\n        for target, online in zip(self.target_value_model_b.parameters(), \n                                  self.online_value_model_b.parameters()):\n            target_ratio = (1.0 - self.tau) * target.data\n            online_ratio = self.tau * online.data\n            mixed_weights = target_ratio + online_ratio\n            target.data.copy_(mixed_weights)\n\n    def train(self, make_env_fn, make_env_kargs, seed, gamma, \n              max_minutes, max_episodes, goal_mean_100_reward):\n        training_start, last_debug_time = time.time(), float('-inf')\n\n        self.checkpoint_dir = tempfile.mkdtemp()\n        self.make_env_fn = make_env_fn\n        self.make_env_kargs = make_env_kargs\n        self.seed = seed\n        self.gamma = gamma\n        \n        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n    \n        nS, nA = env.observation_space.shape[0], env.action_space.shape[0]\n        action_bounds = env.action_space.low, env.action_space.high\n        self.episode_timestep = []\n        self.episode_reward = []\n        self.episode_seconds = []\n        self.evaluation_scores = []        \n        self.episode_exploration = []\n\n        self.target_value_model_a = self.value_model_fn(nS, nA)\n        self.online_value_model_a = self.value_model_fn(nS, nA)\n        self.target_value_model_b = self.value_model_fn(nS, nA)\n        self.online_value_model_b = self.value_model_fn(nS, nA)\n        self.update_value_networks(tau=1.0)\n\n        self.policy_model = self.policy_model_fn(nS, action_bounds)\n\n        self.value_optimizer_a = self.value_optimizer_fn(self.online_value_model_a,\n                                                         self.value_optimizer_lr)\n        self.value_optimizer_b = self.value_optimizer_fn(self.online_value_model_b,\n                                                         self.value_optimizer_lr)\n        self.policy_optimizer = self.policy_optimizer_fn(self.policy_model,\n                                                         self.policy_optimizer_lr)\n\n        self.replay_buffer = self.replay_buffer_fn()\n                    \n        result = np.empty((max_episodes, 5))\n        result[:] = np.nan\n        training_time = 0\n        for episode in range(1, max_episodes + 1):\n            episode_start = time.time()\n            \n            state, is_terminal = env.reset(), False\n            self.episode_reward.append(0.0)\n            self.episode_timestep.append(0.0)\n            self.episode_exploration.append(0.0)\n\n            for step in count():\n                state, is_terminal = self.interaction_step(state, env)\n\n                min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n                if len(self.replay_buffer) &gt; min_samples:\n                    experiences = self.replay_buffer.sample()\n                    experiences = self.online_value_model_a.load(experiences)\n                    self.optimize_model(experiences)\n\n                if np.sum(self.episode_timestep) % self.update_target_every_steps == 0:\n                    self.update_value_networks()\n\n                if is_terminal:\n                    gc.collect()\n                    break\n\n            # stats\n            episode_elapsed = time.time() - episode_start\n            self.episode_seconds.append(episode_elapsed)\n            training_time += episode_elapsed\n            evaluation_score, _ = self.evaluate(self.policy_model, env)\n            self.save_checkpoint(episode-1, self.policy_model)\n\n            total_step = int(np.sum(self.episode_timestep))\n            self.evaluation_scores.append(evaluation_score)\n            \n            mean_10_reward = np.mean(self.episode_reward[-10:])\n            std_10_reward = np.std(self.episode_reward[-10:])\n            mean_100_reward = np.mean(self.episode_reward[-100:])\n            std_100_reward = np.std(self.episode_reward[-100:])\n            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n            lst_100_exp_rat = np.array(\n                self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n            mean_100_exp_rat = np.mean(lst_100_exp_rat)\n            std_100_exp_rat = np.std(lst_100_exp_rat)\n            \n            wallclock_elapsed = time.time() - training_start\n            result[episode-1] = total_step, mean_100_reward, \\\n                mean_100_eval_score, training_time, wallclock_elapsed\n            \n            reached_debug_time = time.time() - last_debug_time &gt;= LEAVE_PRINT_EVERY_N_SECS\n            reached_max_minutes = wallclock_elapsed &gt;= max_minutes * 60\n            reached_max_episodes = episode &gt;= max_episodes\n            reached_goal_mean_reward = mean_100_eval_score &gt;= goal_mean_100_reward\n            training_is_over = reached_max_minutes or \\\n                               reached_max_episodes or \\\n                               reached_goal_mean_reward\n            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n            debug_message = 'el {}, ep {:04}, ts {:07}, '\n            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n            debug_message = debug_message.format(\n                elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n                mean_100_eval_score, std_100_eval_score)\n            print(debug_message, end='\\r', flush=True)\n            if reached_debug_time or training_is_over:\n                print(ERASE_LINE + debug_message, flush=True)\n                last_debug_time = time.time()\n            if training_is_over:\n                if reached_max_minutes: print(u'--&gt; reached_max_minutes \\u2715')\n                if reached_max_episodes: print(u'--&gt; reached_max_episodes \\u2715')\n                if reached_goal_mean_reward: print(u'--&gt; reached_goal_mean_reward \\u2713')\n                break\n                \n        final_eval_score, score_std = self.evaluate(self.policy_model, env, n_episodes=100)\n        wallclock_time = time.time() - training_start\n        print('Training complete.')\n        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n              ' {:.2f}s wall-clock time.\\n'.format(\n                  final_eval_score, score_std, training_time, wallclock_time))\n        env.close() ; del env\n        self.get_cleaned_checkpoints()\n        return result, final_eval_score, training_time, wallclock_time\n    \n    def evaluate(self, eval_policy_model, eval_env, n_episodes=1):\n        rs = []\n        for _ in range(n_episodes):\n            s, d = eval_env.reset(), False\n            rs.append(0)\n            for _ in count():\n                a = eval_policy_model.select_greedy_action(s)\n                s, r, d, _ = eval_env.step(a)\n                rs[-1] += r\n                if d: break\n        return np.mean(rs), np.std(rs)\n\n    def get_cleaned_checkpoints(self, n_checkpoints=4):\n        try: \n            return self.checkpoint_paths\n        except AttributeError:\n            self.checkpoint_paths = {}\n\n        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n        last_ep = max(paths_dic.keys())\n        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n\n        for idx, path in paths_dic.items():\n            if idx in checkpoint_idxs:\n                self.checkpoint_paths[idx] = path\n            else:\n                os.unlink(path)\n\n        return self.checkpoint_paths\n\n    def demo_last(self, title='Fully-trained {} Agent', n_episodes=2, max_n_videos=2):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        last_ep = max(checkpoint_paths.keys())\n        self.policy_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n\n        self.evaluate(self.policy_model, env, n_episodes=n_episodes)\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def demo_progression(self, title='{} Agent progression', max_n_videos=4):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        for i in sorted(checkpoint_paths.keys()):\n            self.policy_model.load_state_dict(torch.load(checkpoint_paths[i]))\n            self.evaluate(self.policy_model, env, n_episodes=1)\n\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            subtitle_eps=sorted(checkpoint_paths.keys()),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def save_checkpoint(self, episode_idx, model):\n        torch.save(model.state_dict(), \n                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))\n\n\nsac_results = []\nbest_agent, best_eval_score = None, float('-inf')\nfor seed in SEEDS:\n    environment_settings = {\n        'env_name': 'HalfCheetahBulletEnv-v0',\n        'gamma': 0.99,\n        'max_minutes': 300,\n        'max_episodes': 10000,\n        'goal_mean_100_reward': 2000\n    }\n\n    policy_model_fn = lambda nS, bounds: FCGP(nS, bounds, hidden_dims=(256,256))\n    policy_max_grad_norm = float('inf')\n    policy_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n    policy_optimizer_lr = 0.0005\n\n    value_model_fn = lambda nS, nA: FCQSA(nS, nA, hidden_dims=(256,256))\n    value_max_grad_norm = float('inf')\n    value_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n    value_optimizer_lr = 0.0007\n\n    replay_buffer_fn = lambda: ReplayBuffer(max_size=1000000, batch_size=256)\n    n_warmup_batches = 10\n    update_target_every_steps = 1\n    tau = 0.005\n\n    env_name, gamma, max_minutes, \\\n    max_episodes, goal_mean_100_reward = environment_settings.values()\n                \n    agent = SAC(replay_buffer_fn,\n                policy_model_fn, \n                policy_max_grad_norm,\n                policy_optimizer_fn, \n                policy_optimizer_lr,\n                value_model_fn,\n                value_max_grad_norm, \n                value_optimizer_fn, \n                value_optimizer_lr, \n                n_warmup_batches,\n                update_target_every_steps,\n                tau)\n\n    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name, inner_wrappers=[RenderUint8])\n    result, final_eval_score, training_time, wallclock_time = agent.train(\n        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n    sac_results.append(result)\n    if final_eval_score &gt; best_eval_score:\n        best_eval_score = final_eval_score\n        best_agent = agent\nsac_results = np.array(sac_results)\n\nel 00:00:04, ep 0000, ts 0001000, ar 10 -1318.2±000.0, 100 -1318.2±000.0, ex 100 0.3±0.0, ev -1350.8±000.0\nel 00:05:15, ep 0021, ts 0022000, ar 10 -603.1±418.3, 100 -731.2±430.5, ex 100 0.1±0.1, ev -725.3±578.6\nel 00:10:21, ep 0040, ts 0041000, ar 10 358.8±112.7, 100 -289.5±634.9, ex 100 0.1±0.1, ev -236.2±738.3\nel 00:15:35, ep 0059, ts 0060000, ar 10 666.7±048.4, 100 -03.7±673.4, ex 100 0.1±0.0, ev 036.7±731.9\nel 00:20:36, ep 0077, ts 0078000, ar 10 643.7±130.5, 100 146.1±652.8, ex 100 0.1±0.0, ev 178.5±693.8\nel 00:25:44, ep 0095, ts 0096000, ar 10 774.6±123.4, 100 261.3±637.2, ex 100 0.1±0.0, ev 234.9±689.2\nel 00:30:59, ep 0113, ts 0114000, ar 10 933.4±109.2, 100 529.9±457.2, ex 100 0.1±0.0, ev 503.3±458.2\nel 00:35:59, ep 0130, ts 0131000, ar 10 1091.4±127.5, 100 749.5±227.9, ex 100 0.1±0.0, ev 671.9±395.6\nel 00:41:02, ep 0147, ts 0148000, ar 10 1272.4±107.5, 100 893.6±246.4, ex 100 0.1±0.0, ev 817.2±399.6\nel 00:46:13, ep 0164, ts 0165000, ar 10 1383.1±027.2, 100 1015.9±274.4, ex 100 0.1±0.0, ev 939.8±445.1\nel 00:51:29, ep 0181, ts 0182000, ar 10 1510.7±029.3, 100 1157.0±261.4, ex 100 0.1±0.0, ev 1106.7±415.3\nel 00:56:43, ep 0198, ts 0199000, ar 10 1581.8±067.6, 100 1290.9±238.7, ex 100 0.1±0.0, ev 1282.0±317.2\nel 01:02:02, ep 0215, ts 0216000, ar 10 1680.7±011.1, 100 1415.5±204.6, ex 100 0.1±0.0, ev 1441.8±239.8\nel 01:07:09, ep 0231, ts 0232000, ar 10 1681.1±021.6, 100 1511.3±154.2, ex 100 0.1±0.0, ev 1539.6±200.5\nel 01:12:24, ep 0247, ts 0248000, ar 10 1708.7±036.0, 100 1578.8±124.4, ex 100 0.1±0.0, ev 1618.0±157.1\nel 01:17:37, ep 0263, ts 0264000, ar 10 1754.3±029.3, 100 1637.4±098.5, ex 100 0.1±0.0, ev 1684.2±101.5\nel 01:22:56, ep 0279, ts 0280000, ar 10 1793.8±023.8, 100 1687.6±076.1, ex 100 0.1±0.0, ev 1730.3±078.1\nel 01:28:01, ep 0294, ts 0295000, ar 10 1807.3±014.7, 100 1723.8±063.0, ex 100 0.1±0.0, ev 1767.9±064.8\nel 01:33:10, ep 0309, ts 0310000, ar 10 1851.2±014.3, 100 1754.9±063.5, ex 100 0.1±0.0, ev 1795.9±064.5\nel 01:38:24, ep 0324, ts 0325000, ar 10 1839.0±023.0, 100 1779.3±060.6, ex 100 0.1±0.0, ev 1821.5±064.5\nel 01:43:44, ep 0339, ts 0340000, ar 10 1866.2±025.7, 100 1805.2±053.3, ex 100 0.1±0.0, ev 1848.7±060.2\nel 01:48:47, ep 0353, ts 0354000, ar 10 1886.5±014.4, 100 1828.4±045.5, ex 100 0.1±0.0, ev 1870.6±051.8\nel 01:53:53, ep 0367, ts 0368000, ar 10 1891.6±016.5, 100 1846.9±040.0, ex 100 0.1±0.0, ev 1890.9±039.4\nel 01:59:03, ep 0381, ts 0382000, ar 10 1910.8±014.9, 100 1863.7±037.8, ex 100 0.1±0.0, ev 1906.4±033.8\nel 02:04:17, ep 0395, ts 0396000, ar 10 1919.1±020.8, 100 1878.8±033.7, ex 100 0.1±0.0, ev 1917.6±030.1\nel 02:09:34, ep 0409, ts 0410000, ar 10 1927.6±018.7, 100 1889.8±035.1, ex 100 0.1±0.0, ev 1929.5±030.6\nel 02:14:55, ep 0423, ts 0424000, ar 10 1932.1±024.8, 100 1902.8±031.1, ex 100 0.1±0.0, ev 1940.0±032.2\nel 02:19:56, ep 0436, ts 0437000, ar 10 1957.9±015.8, 100 1915.7±028.6, ex 100 0.1±0.0, ev 1950.3±032.9\nel 02:25:03, ep 0449, ts 0450000, ar 10 1964.1±025.8, 100 1926.2±030.1, ex 100 0.1±0.0, ev 1963.5±038.5\nel 02:30:12, ep 0462, ts 0463000, ar 10 1976.5±023.2, 100 1937.4±031.5, ex 100 0.1±0.0, ev 1977.7±044.2\nel 02:35:28, ep 0475, ts 0476000, ar 10 1993.4±014.6, 100 1949.2±034.5, ex 100 0.1±0.0, ev 1991.9±044.7\nel 02:39:07, ep 0484, ts 0485000, ar 10 2000.6±029.4, 100 1957.4±036.5, ex 100 0.1±0.0, ev 2001.1±044.4\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 2066.40±17.54 in 8717.20s training time, 9713.86s wall-clock time.\n\nel 00:00:04, ep 0000, ts 0001000, ar 10 -1145.0±000.0, 100 -1145.0±000.0, ex 100 0.2±0.0, ev -1373.2±000.0\nel 00:05:09, ep 0021, ts 0022000, ar 10 -91.7±439.2, 100 -484.0±606.1, ex 100 0.1±0.1, ev -514.2±791.3\nel 00:10:09, ep 0040, ts 0041000, ar 10 536.2±090.0, 100 -96.6±635.2, ex 100 0.1±0.1, ev -189.5±786.2\nel 00:15:16, ep 0059, ts 0060000, ar 10 613.8±114.6, 100 112.8±612.2, ex 100 0.1±0.0, ev 060.5±748.5\nel 00:20:30, ep 0078, ts 0079000, ar 10 735.4±030.2, 100 252.6±590.4, ex 100 0.1±0.0, ev 224.4±714.7\nel 00:25:35, ep 0096, ts 0097000, ar 10 753.5±043.0, 100 339.1±564.2, ex 100 0.1±0.0, ev 297.0±679.7\nel 00:30:44, ep 0114, ts 0115000, ar 10 832.5±047.8, 100 587.6±269.1, ex 100 0.1±0.0, ev 538.4±431.9\nel 00:35:56, ep 0132, ts 0133000, ar 10 921.5±059.2, 100 724.5±151.2, ex 100 0.1±0.0, ev 696.2±290.4\nel 00:40:59, ep 0149, ts 0150000, ar 10 1090.0±160.7, 100 808.0±165.1, ex 100 0.1±0.0, ev 823.4±282.5\nel 00:46:08, ep 0166, ts 0167000, ar 10 1507.6±174.0, 100 950.0±274.3, ex 100 0.1±0.0, ev 955.4±369.1\nel 00:51:21, ep 0183, ts 0184000, ar 10 1641.1±174.7, 100 1108.5±351.1, ex 100 0.1±0.0, ev 1115.1±438.4\nel 00:56:40, ep 0200, ts 0201000, ar 10 1769.7±019.7, 100 1271.8±376.1, ex 100 0.0±0.0, ev 1310.1±401.9\nel 01:01:44, ep 0216, ts 0217000, ar 10 1817.7±028.1, 100 1428.4±362.9, ex 100 0.0±0.0, ev 1484.5±344.2\nel 01:06:52, ep 0232, ts 0233000, ar 10 1857.9±013.5, 100 1578.6±306.5, ex 100 0.0±0.0, ev 1629.4±282.6\nel 01:12:03, ep 0248, ts 0249000, ar 10 1897.5±018.8, 100 1718.4±193.8, ex 100 0.0±0.0, ev 1753.7±172.4\nel 01:17:20, ep 0264, ts 0265000, ar 10 1923.0±011.4, 100 1799.5±131.0, ex 100 0.0±0.0, ev 1826.4±124.1\nel 01:22:23, ep 0279, ts 0280000, ar 10 1940.9±018.9, 100 1842.0±122.7, ex 100 0.0±0.0, ev 1878.4±081.7\nel 01:27:31, ep 0294, ts 0295000, ar 10 1966.3±008.8, 100 1888.4±060.3, ex 100 0.0±0.0, ev 1914.9±067.4\nel 01:32:45, ep 0309, ts 0310000, ar 10 1980.1±013.1, 100 1916.7±051.0, ex 100 0.0±0.0, ev 1945.8±056.7\nel 01:38:06, ep 0324, ts 0325000, ar 10 1980.7±023.8, 100 1939.5±041.6, ex 100 0.0±0.0, ev 1967.2±050.3\nel 01:43:09, ep 0338, ts 0339000, ar 10 2002.1±010.8, 100 1956.8±035.8, ex 100 0.0±0.0, ev 1985.9±047.2\nel 01:46:48, ep 0348, ts 0349000, ar 10 2017.3±016.8, 100 1968.8±033.8, ex 100 0.0±0.0, ev 2000.2±045.9\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 2087.20±10.27 in 5812.29s training time, 6575.22s wall-clock time.\n\nel 00:00:04, ep 0000, ts 0001000, ar 10 -1300.8±000.0, 100 -1300.8±000.0, ex 100 0.3±0.0, ev -1149.8±000.0\nel 00:05:07, ep 0021, ts 0022000, ar 10 -150.1±440.0, 100 -636.4±577.6, ex 100 0.1±0.1, ev -579.2±740.8\nel 00:10:22, ep 0041, ts 0042000, ar 10 543.6±077.7, 100 -105.0±704.9, ex 100 0.1±0.1, ev -125.5±777.6\nel 00:15:28, ep 0060, ts 0061000, ar 10 674.8±069.3, 100 132.2±683.9, ex 100 0.1±0.0, ev 111.7±751.5\nel 00:20:40, ep 0079, ts 0080000, ar 10 749.4±055.6, 100 276.0±651.5, ex 100 0.1±0.0, ev 253.2±705.3\nel 00:25:44, ep 0097, ts 0098000, ar 10 981.0±055.2, 100 388.4±636.9, ex 100 0.1±0.0, ev 373.4±688.5\nel 00:30:53, ep 0115, ts 0116000, ar 10 1140.7±052.3, 100 706.8±341.2, ex 100 0.1±0.0, ev 713.5±400.5\nel 00:36:07, ep 0133, ts 0134000, ar 10 1394.7±087.9, 100 913.4±285.7, ex 100 0.1±0.0, ev 931.4±360.9\nel 00:41:09, ep 0150, ts 0151000, ar 10 1513.7±018.9, 100 1070.1±312.5, ex 100 0.1±0.0, ev 1117.5±357.5\nel 00:46:16, ep 0167, ts 0168000, ar 10 1587.8±020.7, 100 1219.7±307.0, ex 100 0.1±0.0, ev 1280.6±349.1\nel 00:51:29, ep 0184, ts 0185000, ar 10 1621.1±026.0, 100 1368.4±243.2, ex 100 0.0±0.0, ev 1445.1±274.5\nel 00:56:46, ep 0201, ts 0202000, ar 10 1645.3±019.3, 100 1481.6±175.7, ex 100 0.0±0.0, ev 1560.8±193.5\nel 01:01:50, ep 0217, ts 0218000, ar 10 1678.8±017.5, 100 1565.4±103.3, ex 100 0.0±0.0, ev 1648.2±102.6\nel 01:06:57, ep 0233, ts 0234000, ar 10 1770.4±025.6, 100 1625.3±080.2, ex 100 0.0±0.0, ev 1702.0±081.7\nel 01:12:08, ep 0249, ts 0250000, ar 10 1859.0±041.5, 100 1679.8±097.3, ex 100 0.0±0.0, ev 1754.0±099.5\nel 01:17:24, ep 0265, ts 0266000, ar 10 -459.7±1551.0, 100 1495.8±821.2, ex 100 0.0±0.0, ev 1586.0±800.1\nel 01:22:25, ep 0280, ts 0281000, ar 10 2040.9±031.2, 100 1557.6±843.1, ex 100 0.0±0.0, ev 1650.5±824.2\nel 01:27:30, ep 0295, ts 0296000, ar 10 2086.4±040.3, 100 1623.3±863.2, ex 100 0.0±0.0, ev 1719.5±845.1\nel 01:32:41, ep 0310, ts 0311000, ar 10 2148.3±027.8, 100 1696.8±883.3, ex 100 0.0±0.0, ev 1793.8±862.6\nel 01:37:57, ep 0325, ts 0326000, ar 10 2225.7±021.4, 100 1773.3±902.8, ex 100 0.0±0.0, ev 1869.6±880.0\nel 01:43:18, ep 0340, ts 0341000, ar 10 2277.7±026.2, 100 1843.2±920.3, ex 100 0.0±0.0, ev 1941.0±895.7\nel 01:48:23, ep 0354, ts 0355000, ar 10 2291.2±019.3, 100 1903.1±933.2, ex 100 0.0±0.0, ev 1999.4±908.0\nel 01:48:45, ep 0355, ts 0356000, ar 10 2293.5±020.3, 100 1906.6±934.1, ex 100 0.0±0.0, ev 2003.5±908.8\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 2385.41±22.77 in 5918.36s training time, 6689.62s wall-clock time.\n\nel 00:00:04, ep 0000, ts 0001000, ar 10 -1284.4±000.0, 100 -1284.4±000.0, ex 100 0.3±0.0, ev -1131.7±000.0\nel 00:05:09, ep 0021, ts 0022000, ar 10 -413.3±478.5, 100 -837.6±516.3, ex 100 0.1±0.1, ev -878.6±731.0\nel 00:10:24, ep 0041, ts 0042000, ar 10 355.1±250.5, 100 -234.9±751.6, ex 100 0.1±0.1, ev -337.9±859.7\nel 00:15:31, ep 0060, ts 0061000, ar 10 591.1±135.5, 100 006.9±728.5, ex 100 0.1±0.0, ev -98.3±818.5\nel 00:20:45, ep 0079, ts 0080000, ar 10 718.8±263.7, 100 166.3±704.4, ex 100 0.1±0.0, ev 059.4±801.1\nel 00:25:48, ep 0097, ts 0098000, ar 10 806.6±220.9, 100 299.6±703.4, ex 100 0.1±0.0, ev 203.9±789.8\nel 00:30:56, ep 0115, ts 0116000, ar 10 1321.9±089.7, 100 690.0±448.3, ex 100 0.1±0.0, ev 577.0±595.8\nel 00:36:10, ep 0133, ts 0134000, ar 10 1548.7±043.6, 100 921.8±434.2, ex 100 0.1±0.0, ev 852.0±555.8\nel 00:41:11, ep 0150, ts 0151000, ar 10 1672.9±027.7, 100 1131.8±419.9, ex 100 0.1±0.0, ev 1069.5±554.6\nel 00:46:22, ep 0167, ts 0168000, ar 10 1647.6±119.7, 100 1313.8±377.1, ex 100 0.1±0.0, ev 1295.1±436.1\nel 00:51:29, ep 0183, ts 0184000, ar 10 1860.8±031.1, 100 1479.2±320.1, ex 100 0.1±0.0, ev 1468.5±372.0\nel 00:56:35, ep 0199, ts 0200000, ar 10 1946.0±022.7, 100 1644.1±217.1, ex 100 0.1±0.0, ev 1651.9±287.2\nel 01:01:45, ep 0215, ts 0216000, ar 10 1987.4±028.3, 100 1754.0±180.7, ex 100 0.1±0.0, ev 1782.2±220.0\nel 01:06:59, ep 0231, ts 0232000, ar 10 2022.6±007.8, 100 1837.1±160.3, ex 100 0.1±0.0, ev 1865.5±192.4\nel 01:12:13, ep 0247, ts 0248000, ar 10 2075.2±025.0, 100 1907.0±148.8, ex 100 0.1±0.0, ev 1934.5±189.2\nel 01:17:15, ep 0262, ts 0263000, ar 10 2102.4±011.9, 100 1966.8±132.9, ex 100 0.1±0.0, ev 2001.4±166.3\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 2163.22±7.46 in 4184.84s training time, 4805.54s wall-clock time.\n\nel 00:00:04, ep 0000, ts 0001000, ar 10 -1296.7±000.0, 100 -1296.7±000.0, ex 100 0.2±0.0, ev -1345.5±000.0\nel 00:05:18, ep 0021, ts 0022000, ar 10 309.4±271.4, 100 -302.8±756.6, ex 100 0.1±0.1, ev -467.2±892.7\nel 00:10:22, ep 0040, ts 0041000, ar 10 430.0±314.2, 100 035.3±688.7, ex 100 0.1±0.1, ev -34.3±864.9\nel 00:15:37, ep 0059, ts 0060000, ar 10 685.1±038.3, 100 191.7±653.4, ex 100 0.1±0.0, ev 178.2±781.3\nel 00:20:40, ep 0077, ts 0078000, ar 10 766.4±065.6, 100 303.0±617.6, ex 100 0.1±0.0, ev 318.6±733.1\nel 00:25:52, ep 0095, ts 0096000, ar 10 974.1±152.0, 100 415.3±607.2, ex 100 0.1±0.0, ev 436.4±707.5\nel 00:31:07, ep 0113, ts 0114000, ar 10 1090.9±100.8, 100 688.8±354.9, ex 100 0.1±0.0, ev 754.9±360.5\nel 00:36:11, ep 0130, ts 0131000, ar 10 1245.7±028.8, 100 831.1±365.5, ex 100 0.1±0.0, ev 912.5±253.7\nel 00:41:16, ep 0147, ts 0148000, ar 10 1325.3±027.3, 100 990.5±276.0, ex 100 0.1±0.0, ev 1032.8±253.8\nel 00:46:32, ep 0164, ts 0165000, ar 10 1383.6±033.9, 100 1119.9±231.8, ex 100 0.0±0.0, ev 1164.0±236.4\nel 00:51:33, ep 0180, ts 0181000, ar 10 1422.6±026.9, 100 1224.8±187.1, ex 100 0.0±0.0, ev 1268.4±203.8\nel 00:56:40, ep 0196, ts 0197000, ar 10 1511.3±038.2, 100 1315.0±147.5, ex 100 0.0±0.0, ev 1335.2±186.5\nel 01:01:51, ep 0212, ts 0213000, ar 10 1654.8±026.8, 100 1398.8±138.3, ex 100 0.0±0.0, ev 1423.7±191.1\nel 01:06:52, ep 0227, ts 0228000, ar 10 1770.5±030.1, 100 1479.0±163.9, ex 100 0.0±0.0, ev 1507.2±212.7\nel 01:12:12, ep 0243, ts 0244000, ar 10 1834.0±047.8, 100 1564.7±181.9, ex 100 0.0±0.0, ev 1608.4±235.9\nel 01:17:17, ep 0258, ts 0259000, ar 10 1877.0±054.4, 100 1640.9±188.5, ex 100 0.0±0.0, ev 1691.4±255.7\nel 01:22:20, ep 0273, ts 0274000, ar 10 1913.0±032.0, 100 1717.4±178.1, ex 100 0.0±0.0, ev 1769.1±253.5\nel 01:27:31, ep 0288, ts 0289000, ar 10 2001.4±011.1, 100 1797.4±160.6, ex 100 0.0±0.0, ev 1855.7±244.0\nel 01:32:51, ep 0303, ts 0304000, ar 10 2040.8±045.1, 100 1874.6±119.3, ex 100 0.0±0.0, ev 1956.3±132.8\nel 01:36:43, ep 0314, ts 0315000, ar 10 2080.2±015.5, 100 1920.1±108.8, ex 100 0.0±0.0, ev 2002.3±123.1\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 2180.44±7.19 in 5246.61s training time, 5979.55s wall-clock time.\n\n\n\n\nbest_agent.demo_progression()\n\n\nSAC Agent progression\n        Episode 0\n        \n        Episode 118\n        \n        Episode 236\n        \n        Episode 355\n        \n\n\n\nbest_agent.demo_last()\n\nFully-trained SAC Agent\n        Trial 0\n        \n        Trial 1\n        \n\n\n\nsac_max_t, sac_max_r, sac_max_s, \\\nsac_max_sec, sac_max_rt = np.max(sac_results, axis=0).T\nsac_min_t, sac_min_r, sac_min_s, \\\nsac_min_sec, sac_min_rt = np.min(sac_results, axis=0).T\nsac_mean_t, sac_mean_r, sac_mean_s, \\\nsac_mean_sec, sac_mean_rt = np.mean(sac_results, axis=0).T\nsac_x = np.arange(len(sac_mean_s))\n\n\nfig, axs = plt.subplots(2, 1, figsize=(15,10), sharey=False, sharex=True)\n\n# SAC\naxs[0].plot(sac_max_r, 'g', linewidth=1)\naxs[0].plot(sac_min_r, 'g', linewidth=1)\naxs[0].plot(sac_mean_r, 'g:', label='SAC', linewidth=2)\naxs[0].fill_between(\n    sac_x, sac_min_r, sac_max_r, facecolor='g', alpha=0.3)\n\naxs[1].plot(sac_max_s, 'g', linewidth=1)\naxs[1].plot(sac_min_s, 'g', linewidth=1)\naxs[1].plot(sac_mean_s, 'g:', label='SAC', linewidth=2)\naxs[1].fill_between(\n    sac_x, sac_min_s, sac_max_s, facecolor='g', alpha=0.3)\n\n# ALL\naxs[0].set_title('Moving Avg Reward (Training)')\naxs[1].set_title('Moving Avg Reward (Evaluation)')\nplt.xlabel('Episodes')\naxs[0].legend(loc='upper left')\nplt.show()\n\n\n\n\n\nfig, axs = plt.subplots(3, 1, figsize=(15,15), sharey=False, sharex=True)\n\n# SAC\naxs[0].plot(sac_max_t, 'g', linewidth=1)\naxs[0].plot(sac_min_t, 'g', linewidth=1)\naxs[0].plot(sac_mean_t, 'g:', label='SAC', linewidth=2)\naxs[0].fill_between(\n    sac_x, sac_min_t, sac_max_t, facecolor='g', alpha=0.3)\n\naxs[1].plot(sac_max_sec, 'g', linewidth=1)\naxs[1].plot(sac_min_sec, 'g', linewidth=1)\naxs[1].plot(sac_mean_sec, 'g:', label='SAC', linewidth=2)\naxs[1].fill_between(\n    sac_x, sac_min_sec, sac_max_sec, facecolor='g', alpha=0.3)\n\naxs[2].plot(sac_max_rt, 'g', linewidth=1)\naxs[2].plot(sac_min_rt, 'g', linewidth=1)\naxs[2].plot(sac_mean_rt, 'g:', label='SAC', linewidth=2)\naxs[2].fill_between(\n    sac_x, sac_min_rt, sac_max_rt, facecolor='g', alpha=0.3)\n\n# ALL\naxs[0].set_title('Total Steps')\naxs[1].set_title('Training Time')\naxs[2].set_title('Wall-clock Time')\nplt.xlabel('Episodes')\naxs[0].legend(loc='upper left')\nplt.show()\n\n\n\n\n\nfig, axs = plt.subplots(3, 1, figsize=(15,15), sharey=False, sharex=True)\n\n# SAC\naxs[0].plot(sac_max_t, 'g', linewidth=1)\naxs[0].plot(sac_min_t, 'g', linewidth=1)\naxs[0].plot(sac_mean_t, 'g:', label='SAC', linewidth=2)\naxs[0].fill_between(\n    sac_x, sac_min_t, sac_max_t, facecolor='g', alpha=0.3)\n\naxs[1].plot(sac_max_sec, 'g', linewidth=1)\naxs[1].plot(sac_min_sec, 'g', linewidth=1)\naxs[1].plot(sac_mean_sec, 'g:', label='SAC', linewidth=2)\naxs[1].fill_between(\n    sac_x, sac_min_sec, sac_max_sec, facecolor='g', alpha=0.3)\n\naxs[2].plot(sac_max_rt, 'g', linewidth=1)\naxs[2].plot(sac_min_rt, 'g', linewidth=1)\naxs[2].plot(sac_mean_rt, 'g:', label='SAC', linewidth=2)\naxs[2].fill_between(\n    sac_x, sac_min_rt, sac_max_rt, facecolor='g', alpha=0.3)\n\n# ALL\naxs[0].set_title('Total Steps')\naxs[1].set_title('Training Time')\naxs[2].set_title('Wall-clock Time')\nplt.xlabel('Episodes')\naxs[0].legend(loc='upper left')\nplt.show()"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-12.html#proximal-policy-optimization-ppo",
    "href": "publication/GDRL/GDRL-chapter-12.html#proximal-policy-optimization-ppo",
    "title": "Chapter 12: Advanced Actor-Critic Methods",
    "section": "Proximal Policy Optimization (PPO)",
    "text": "Proximal Policy Optimization (PPO)\n\nclass MultiprocessEnv(object):\n    def __init__(self, make_env_fn, make_env_kargs, seed, n_workers):\n        self.make_env_fn = make_env_fn\n        self.make_env_kargs = make_env_kargs\n        self.seed = seed\n        self.n_workers = n_workers\n        self.pipes = [mp.Pipe() for rank in range(self.n_workers)]\n        self.workers = [\n            mp.Process(\n                target=self.work, \n                args=(rank, self.pipes[rank][1])) for rank in range(self.n_workers)]\n        [w.start() for w in self.workers]\n        self.dones = {rank:False for rank in range(self.n_workers)}\n\n    def reset(self, ranks=None, **kwargs):\n        if not (ranks is None):\n            [self.send_msg(('reset', {}), rank) for rank in ranks]            \n            return np.stack([parent_end.recv() for rank, (parent_end, _) in enumerate(self.pipes) if rank in ranks])\n\n        self.broadcast_msg(('reset', kwargs))\n        return np.stack([parent_end.recv() for parent_end, _ in self.pipes])\n\n    def step(self, actions):\n        assert len(actions) == self.n_workers\n        [self.send_msg(\n            ('step', {'action':actions[rank]}), \n            rank) for rank in range(self.n_workers)]\n        results = []\n        for rank in range(self.n_workers):\n            parent_end, _ = self.pipes[rank]\n            o, r, d, i = parent_end.recv()\n            results.append((o,\n                            float(r),\n                            float(d),\n                            i))\n        return [np.stack(block).squeeze() for block in np.array(results).T]\n\n    def close(self, **kwargs):\n        self.broadcast_msg(('close', kwargs))\n        [w.join() for w in self.workers]\n    \n    def work(self, rank, worker_end):\n        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed+rank)\n        while True:\n            cmd, kwargs = worker_end.recv()\n            if cmd == 'reset':\n                worker_end.send(env.reset(**kwargs))\n            elif cmd == 'step':\n                worker_end.send(env.step(**kwargs))\n            elif cmd == '_past_limit':\n                worker_end.send(env._elapsed_steps &gt;= env._max_episode_steps)\n            else:\n                # including close command \n                env.close(**kwargs) ; del env ; worker_end.close()\n                break\n\n    def send_msg(self, msg, rank):\n        parent_end, _ = self.pipes[rank]\n        parent_end.send(msg)\n\n    def broadcast_msg(self, msg):    \n        [parent_end.send(msg) for parent_end, _ in self.pipes]\n\n\nclass EpisodeBuffer():\n    def __init__(self,\n                 state_dim,\n                 gamma,\n                 tau,\n                 n_workers,\n                 max_episodes,\n                 max_episode_steps):\n        \n        assert max_episodes &gt;= n_workers\n\n        self.state_dim = state_dim\n        self.gamma = gamma\n        self.tau = tau\n        self.n_workers = n_workers\n        self.max_episodes = max_episodes\n        self.max_episode_steps = max_episode_steps\n\n        self._truncated_fn = np.vectorize(lambda x: 'TimeLimit.truncated' in x and x['TimeLimit.truncated'])\n        self.discounts = np.logspace(\n            0, max_episode_steps+1, num=max_episode_steps+1, base=gamma, endpoint=False, dtype=np.float128)\n        self.tau_discounts = np.logspace(\n            0, max_episode_steps+1, num=max_episode_steps+1, base=gamma*tau, endpoint=False, dtype=np.float128)\n\n        device = 'cpu'\n        if torch.cuda.is_available():\n            device = 'cuda:0'\n        self.device = torch.device(device)\n\n        self.clear()\n\n    def clear(self):\n        self.states_mem = np.empty(\n            shape=np.concatenate(((self.max_episodes, self.max_episode_steps), self.state_dim)), dtype=np.float64)\n#         self.states_mem[:] = np.nan\n\n        self.actions_mem = np.empty(shape=(self.max_episodes, self.max_episode_steps), dtype=np.uint8)\n#         self.actions_mem[:] = np.nan\n\n        self.returns_mem = np.empty(shape=(self.max_episodes,self.max_episode_steps), dtype=np.float32)\n#         self.returns_mem[:] = np.nan\n\n        self.gaes_mem = np.empty(shape=(self.max_episodes, self.max_episode_steps), dtype=np.float32)\n#         self.gaes_mem[:] = np.nan\n\n        self.logpas_mem = np.empty(shape=(self.max_episodes, self.max_episode_steps), dtype=np.float32)\n#         self.logpas_mem[:] = np.nan\n\n        self.episode_steps = np.zeros(shape=(self.max_episodes), dtype=np.uint16)\n        self.episode_reward = np.zeros(shape=(self.max_episodes), dtype=np.float32)\n        self.episode_exploration = np.zeros(shape=(self.max_episodes), dtype=np.float32)\n        self.episode_seconds = np.zeros(shape=(self.max_episodes), dtype=np.float64)\n\n        self.current_ep_idxs = np.arange(n_workers, dtype=np.uint16)\n        gc.collect()\n\n\n    def fill(self, envs, policy_model, value_model):\n        states = envs.reset()\n\n        worker_rewards = np.zeros(shape=(n_workers, self.max_episode_steps), dtype=np.float32)\n        worker_exploratory = np.zeros(shape=(n_workers, self.max_episode_steps), dtype=np.bool)\n        worker_steps = np.zeros(shape=(n_workers), dtype=np.uint16)\n        worker_seconds = np.array([time.time(),] * n_workers, dtype=np.float64)\n\n        buffer_full = False\n        while not buffer_full and len(self.episode_steps[self.episode_steps &gt; 0]) &lt; self.max_episodes/2:\n            with torch.no_grad():\n                actions, logpas, are_exploratory = policy_model.np_pass(states)\n                values = value_model(states)\n\n            next_states, rewards, terminals, infos = envs.step(actions)\n            self.states_mem[self.current_ep_idxs, worker_steps] = states\n            self.actions_mem[self.current_ep_idxs, worker_steps] = actions\n            self.logpas_mem[self.current_ep_idxs, worker_steps] = logpas\n\n            worker_exploratory[np.arange(self.n_workers), worker_steps] = are_exploratory\n            worker_rewards[np.arange(self.n_workers), worker_steps] = rewards\n\n            for w_idx in range(self.n_workers):\n                if worker_steps[w_idx] + 1 == self.max_episode_steps:\n                    terminals[w_idx] = 1\n                    infos[w_idx]['TimeLimit.truncated'] = True\n\n            if terminals.sum():\n                idx_terminals = np.flatnonzero(terminals)\n                next_values = np.zeros(shape=(n_workers))\n                truncated = self._truncated_fn(infos)\n                if truncated.sum():\n                    idx_truncated = np.flatnonzero(truncated)\n                    with torch.no_grad():\n                        next_values[idx_truncated] = value_model(\n                            next_states[idx_truncated]).cpu().numpy()\n\n            states = next_states\n            worker_steps += 1\n\n            if terminals.sum():\n                new_states = envs.reset(ranks=idx_terminals)\n                states[idx_terminals] = new_states\n\n                for w_idx in range(self.n_workers):\n                    if w_idx not in idx_terminals:\n                        continue\n\n                    e_idx = self.current_ep_idxs[w_idx]\n                    T = worker_steps[w_idx]\n                    self.episode_steps[e_idx] = T\n                    self.episode_reward[e_idx] = worker_rewards[w_idx, :T].sum()\n                    self.episode_exploration[e_idx] = worker_exploratory[w_idx, :T].mean()\n                    self.episode_seconds[e_idx] = time.time() - worker_seconds[w_idx]\n\n                    ep_rewards = np.concatenate(\n                        (worker_rewards[w_idx, :T], [next_values[w_idx]]))\n                    ep_discounts = self.discounts[:T+1]\n                    ep_returns = np.array(\n                        [np.sum(ep_discounts[:T+1-t] * ep_rewards[t:]) for t in range(T)])\n                    self.returns_mem[e_idx, :T] = ep_returns\n\n                    ep_states = self.states_mem[e_idx, :T]\n                    with torch.no_grad():\n                        ep_values = torch.cat((value_model(ep_states),\n                                               torch.tensor([next_values[w_idx]],\n                                                            device=value_model.device,\n                                                            dtype=torch.float32)))\n                    np_ep_values = ep_values.view(-1).cpu().numpy()\n                    ep_tau_discounts = self.tau_discounts[:T]\n                    deltas = ep_rewards[:-1] + self.gamma * np_ep_values[1:] - np_ep_values[:-1]\n                    gaes = np.array(\n                        [np.sum(self.tau_discounts[:T-t] * deltas[t:]) for t in range(T)])\n                    self.gaes_mem[e_idx, :T] = gaes\n\n                    worker_exploratory[w_idx, :] = 0\n                    worker_rewards[w_idx, :] = 0\n                    worker_steps[w_idx] = 0\n                    worker_seconds[w_idx] = time.time()\n\n                    new_ep_id = max(self.current_ep_idxs) + 1\n                    if new_ep_id &gt;= self.max_episodes:\n                        buffer_full = True\n                        break\n\n                    self.current_ep_idxs[w_idx] = new_ep_id\n\n        ep_idxs = self.episode_steps &gt; 0\n        ep_t = self.episode_steps[ep_idxs]\n\n        self.states_mem = [row[:ep_t[i]] for i, row in enumerate(self.states_mem[ep_idxs])]\n        self.states_mem = np.concatenate(self.states_mem)\n        self.actions_mem = [row[:ep_t[i]] for i, row in enumerate(self.actions_mem[ep_idxs])]\n        self.actions_mem = np.concatenate(self.actions_mem)\n        self.returns_mem = [row[:ep_t[i]] for i, row in enumerate(self.returns_mem[ep_idxs])]\n        self.returns_mem = torch.tensor(np.concatenate(self.returns_mem), \n                                        device=value_model.device)\n        self.gaes_mem = [row[:ep_t[i]] for i, row in enumerate(self.gaes_mem[ep_idxs])]\n        self.gaes_mem = torch.tensor(np.concatenate(self.gaes_mem), \n                                     device=value_model.device)\n        self.logpas_mem = [row[:ep_t[i]] for i, row in enumerate(self.logpas_mem[ep_idxs])]\n        self.logpas_mem = torch.tensor(np.concatenate(self.logpas_mem), \n                                       device=value_model.device)\n\n        ep_r = self.episode_reward[ep_idxs]\n        ep_x = self.episode_exploration[ep_idxs]\n        ep_s = self.episode_seconds[ep_idxs]\n        return ep_t, ep_r, ep_x, ep_s\n\n    def get_stacks(self):\n        return (self.states_mem, self.actions_mem, \n                self.returns_mem, self.gaes_mem, self.logpas_mem)\n\n    def __len__(self):\n        return self.episode_steps[self.episode_steps &gt; 0].sum()\n\n\nclass FCCA(nn.Module):\n    def __init__(self,\n                 input_dim, \n                 output_dim,\n                 hidden_dims=(32,32), \n                 activation_fc=F.relu):\n        super(FCCA, self).__init__()\n        self.activation_fc = activation_fc\n\n        self.input_layer = nn.Linear(input_dim[0], hidden_dims[0])\n        self.hidden_layers = nn.ModuleList()\n        for i in range(len(hidden_dims)-1):\n            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n            self.hidden_layers.append(hidden_layer)\n        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n\n        device = \"cpu\"\n        if torch.cuda.is_available():\n            device = \"cuda:0\"\n        self.device = torch.device(device)\n        self.to(self.device)\n        \n    def _format(self, states):\n        x = states\n        if not isinstance(x, torch.Tensor):\n            x = torch.tensor(x, device=self.device, dtype=torch.float32)\n            if len(x.size()) == 1:\n                x = x.unsqueeze(0)\n        return x\n\n    def forward(self, states):\n        x = self._format(states)\n        x = self.activation_fc(self.input_layer(x))\n        for hidden_layer in self.hidden_layers:\n            x = self.activation_fc(hidden_layer(x))\n        return self.output_layer(x)\n\n    def np_pass(self, states):\n        logits = self.forward(states)\n        np_logits = logits.detach().cpu().numpy()\n        dist = torch.distributions.Categorical(logits=logits)\n        actions = dist.sample()\n        np_actions = actions.detach().cpu().numpy()\n        logpas = dist.log_prob(actions)\n        np_logpas = logpas.detach().cpu().numpy()\n        is_exploratory = np_actions != np.argmax(np_logits, axis=1)\n        return np_actions, np_logpas, is_exploratory\n    \n    def select_action(self, states):\n        logits = self.forward(states)\n        dist = torch.distributions.Categorical(logits=logits)\n        action = dist.sample()\n        return action.detach().cpu().item()\n    \n    def get_predictions(self, states, actions):\n        states, actions = self._format(states), self._format(actions)\n        logits = self.forward(states)\n        dist = torch.distributions.Categorical(logits=logits)\n        logpas = dist.log_prob(actions)\n        entropies = dist.entropy()\n        return logpas, entropies\n    \n    def select_greedy_action(self, states):\n        logits = self.forward(states)\n        return np.argmax(logits.detach().squeeze().cpu().numpy())\n\n\nclass FCV(nn.Module):\n    def __init__(self,\n                 input_dim,\n                 hidden_dims=(32,32), \n                 activation_fc=F.relu):\n        super(FCV, self).__init__()\n        self.activation_fc = activation_fc\n\n        self.input_layer = nn.Linear(input_dim[0], hidden_dims[0])\n        self.hidden_layers = nn.ModuleList()\n        for i in range(len(hidden_dims)-1):\n            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n            self.hidden_layers.append(hidden_layer)\n        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n\n        device = \"cpu\"\n        if torch.cuda.is_available():\n            device = \"cuda:0\"\n        self.device = torch.device(device)\n        self.to(self.device)\n        \n    def _format(self, states):\n        x = states\n        if not isinstance(x, torch.Tensor):\n            x = torch.tensor(x, device=self.device, dtype=torch.float32)\n            if len(x.size()) == 1:\n                x = x.unsqueeze(0)\n        return x\n\n    def forward(self, states):\n        x = self._format(states)\n        x = self.activation_fc(self.input_layer(x))\n        for hidden_layer in self.hidden_layers:\n            x = self.activation_fc(hidden_layer(x))\n        return self.output_layer(x).squeeze()\n\n\nclass PPO():\n    def __init__(self, \n                 policy_model_fn, \n                 policy_model_max_grad_norm,\n                 policy_optimizer_fn,\n                 policy_optimizer_lr,\n                 policy_optimization_epochs,\n                 policy_sample_ratio,\n                 policy_clip_range,\n                 policy_stopping_kl,\n                 value_model_fn, \n                 value_model_max_grad_norm,\n                 value_optimizer_fn,\n                 value_optimizer_lr,\n                 value_optimization_epochs,\n                 value_sample_ratio,\n                 value_clip_range,\n                 value_stopping_mse,\n                 episode_buffer_fn,\n                 max_buffer_episodes,\n                 max_buffer_episode_steps,\n                 entropy_loss_weight,\n                 tau,\n                 n_workers):\n        assert n_workers &gt; 1\n        assert max_buffer_episodes &gt;= n_workers\n\n        self.policy_model_fn = policy_model_fn\n        self.policy_model_max_grad_norm = policy_model_max_grad_norm\n        self.policy_optimizer_fn = policy_optimizer_fn\n        self.policy_optimizer_lr = policy_optimizer_lr\n        self.policy_optimization_epochs = policy_optimization_epochs\n        self.policy_sample_ratio = policy_sample_ratio\n        self.policy_clip_range = policy_clip_range\n        self.policy_stopping_kl = policy_stopping_kl\n\n        self.value_model_fn = value_model_fn\n        self.value_model_max_grad_norm = value_model_max_grad_norm\n        self.value_optimizer_fn = value_optimizer_fn\n        self.value_optimizer_lr = value_optimizer_lr\n        self.value_optimization_epochs = value_optimization_epochs\n        self.value_sample_ratio = value_sample_ratio\n        self.value_clip_range = value_clip_range\n        self.value_stopping_mse = value_stopping_mse\n\n        self.episode_buffer_fn = episode_buffer_fn\n        self.max_buffer_episodes = max_buffer_episodes\n        self.max_buffer_episode_steps = max_buffer_episode_steps\n\n        self.entropy_loss_weight = entropy_loss_weight\n        self.tau = tau\n        self.n_workers = n_workers\n\n    def optimize_model(self):\n        states, actions, returns, gaes, logpas = self.episode_buffer.get_stacks()\n        values = self.value_model(states).detach()\n        gaes = (gaes - gaes.mean()) / (gaes.std() + EPS)\n        n_samples = len(actions)\n        \n        for _ in range(self.policy_optimization_epochs):\n            batch_size = int(self.policy_sample_ratio * n_samples)\n            batch_idxs = np.random.choice(n_samples, batch_size, replace=False)\n            states_batch = states[batch_idxs]\n            actions_batch = actions[batch_idxs]\n            gaes_batch = gaes[batch_idxs]\n            logpas_batch = logpas[batch_idxs]\n\n            logpas_pred, entropies_pred = self.policy_model.get_predictions(states_batch,\n                                                                            actions_batch)\n\n            ratios = (logpas_pred - logpas_batch).exp()\n            pi_obj = gaes_batch * ratios\n            pi_obj_clipped = gaes_batch * ratios.clamp(1.0 - self.policy_clip_range,\n                                                       1.0 + self.policy_clip_range)\n            policy_loss = -torch.min(pi_obj, pi_obj_clipped).mean()\n            entropy_loss = -entropies_pred.mean() * self.entropy_loss_weight\n\n            self.policy_optimizer.zero_grad()\n            (policy_loss + entropy_loss).backward()\n            torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), \n                                           self.policy_model_max_grad_norm)\n            self.policy_optimizer.step()\n            \n            with torch.no_grad():\n                logpas_pred_all, _ = self.policy_model.get_predictions(states, actions)\n                kl = (logpas - logpas_pred_all).mean()\n                if kl.item() &gt; self.policy_stopping_kl:\n                    break\n\n        for _ in range(self.value_optimization_epochs):\n            batch_size = int(self.value_sample_ratio * n_samples)\n            batch_idxs = np.random.choice(n_samples, batch_size, replace=False)\n            states_batch = states[batch_idxs]\n            returns_batch = returns[batch_idxs]\n            values_batch = values[batch_idxs]\n\n            values_pred = self.value_model(states_batch)\n            values_pred_clipped = values_batch + (values_pred - values_batch).clamp(-self.value_clip_range, \n                                                                                    self.value_clip_range)\n            v_loss = (returns_batch - values_pred).pow(2)\n            v_loss_clipped = (returns_batch - values_pred_clipped).pow(2)\n            value_loss = torch.max(v_loss, v_loss_clipped).mul(0.5).mean()\n\n            self.value_optimizer.zero_grad()\n            value_loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.value_model.parameters(), \n                                           self.value_model_max_grad_norm)\n            self.value_optimizer.step()\n\n            with torch.no_grad():\n                values_pred_all = self.value_model(states)\n                mse = (values - values_pred_all).pow(2).mul(0.5).mean()\n                if mse.item() &gt; self.value_stopping_mse:\n                    break\n\n    def train(self, make_envs_fn, make_env_fn, make_env_kargs, seed, gamma, \n              max_minutes, max_episodes, goal_mean_100_reward):\n        training_start, last_debug_time = time.time(), float('-inf')\n\n        self.checkpoint_dir = tempfile.mkdtemp()\n        self.make_envs_fn = make_envs_fn\n        self.make_env_fn = make_env_fn\n        self.make_env_kargs = make_env_kargs\n        self.seed = seed\n        self.gamma = gamma\n        \n        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n        envs = self.make_envs_fn(make_env_fn, make_env_kargs, self.seed, self.n_workers)\n        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n    \n        nS, nA = env.observation_space.shape, env.action_space.n\n        self.episode_timestep, self.episode_reward = [], []\n        self.episode_seconds, self.episode_exploration = [], []\n        self.evaluation_scores = []\n\n        self.policy_model = self.policy_model_fn(nS, nA)\n        self.policy_optimizer = self.policy_optimizer_fn(self.policy_model, self.policy_optimizer_lr)\n\n        self.value_model = self.value_model_fn(nS)\n        self.value_optimizer = self.value_optimizer_fn(self.value_model, self.value_optimizer_lr)\n\n        self.episode_buffer = self.episode_buffer_fn(nS, self.gamma, self.tau,\n                                                     self.n_workers, \n                                                     self.max_buffer_episodes,\n                                                     self.max_buffer_episode_steps)\n\n        result = np.empty((max_episodes, 5))\n        result[:] = np.nan\n        training_time = 0\n        episode = 0\n\n        # collect n_steps rollout\n        while True:\n            episode_timestep, episode_reward, episode_exploration, \\\n            episode_seconds = self.episode_buffer.fill(envs, self.policy_model, self.value_model)\n            \n            n_ep_batch = len(episode_timestep)\n            self.episode_timestep.extend(episode_timestep)\n            self.episode_reward.extend(episode_reward)\n            self.episode_exploration.extend(episode_exploration)\n            self.episode_seconds.extend(episode_seconds)\n            self.optimize_model()\n            self.episode_buffer.clear()\n\n            # stats\n            evaluation_score, _ = self.evaluate(self.policy_model, env)\n            self.evaluation_scores.extend([evaluation_score,] * n_ep_batch)\n            for e in range(episode, episode + n_ep_batch):\n                self.save_checkpoint(e, self.policy_model)\n            training_time += episode_seconds.sum()\n\n            mean_10_reward = np.mean(self.episode_reward[-10:])\n            std_10_reward = np.std(self.episode_reward[-10:])\n            mean_100_reward = np.mean(self.episode_reward[-100:])\n            std_100_reward = np.std(self.episode_reward[-100:])\n            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n            mean_100_exp_rat = np.mean(self.episode_exploration[-100:])\n            std_100_exp_rat = np.std(self.episode_exploration[-100:])\n            \n            total_step = int(np.sum(self.episode_timestep))\n            wallclock_elapsed = time.time() - training_start\n            result[episode:episode+n_ep_batch] = total_step, mean_100_reward, \\\n                mean_100_eval_score, training_time, wallclock_elapsed\n\n            episode += n_ep_batch\n\n            # debug stuff\n            reached_debug_time = time.time() - last_debug_time &gt;= LEAVE_PRINT_EVERY_N_SECS\n            reached_max_minutes = wallclock_elapsed &gt;= max_minutes * 60            \n            reached_max_episodes = episode + self.max_buffer_episodes &gt;= max_episodes\n            reached_goal_mean_reward = mean_100_eval_score &gt;= goal_mean_100_reward\n            training_is_over = reached_max_minutes or \\\n                               reached_max_episodes or \\\n                               reached_goal_mean_reward\n            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n            debug_message = 'el {}, ep {:04}, ts {:07}, '\n            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n            debug_message = debug_message.format(\n                elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n                mean_100_eval_score, std_100_eval_score)\n            print(debug_message, end='\\r', flush=True)\n            if reached_debug_time or training_is_over:\n                print(ERASE_LINE + debug_message, flush=True)\n                last_debug_time = time.time()\n            if training_is_over:\n                if reached_max_minutes: print(u'--&gt; reached_max_minutes \\u2715')\n                if reached_max_episodes: print(u'--&gt; reached_max_episodes \\u2715')\n                if reached_goal_mean_reward: print(u'--&gt; reached_goal_mean_reward \\u2713')\n                break\n\n        final_eval_score, score_std = self.evaluate(self.policy_model, env, n_episodes=100)\n        wallclock_time = time.time() - training_start\n        print('Training complete.')\n        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n              ' {:.2f}s wall-clock time.\\n'.format(\n                  final_eval_score, score_std, training_time, wallclock_time))\n        env.close() ; del env\n        envs.close() ; del envs\n        self.get_cleaned_checkpoints()\n        return result, final_eval_score, training_time, wallclock_time\n\n    def evaluate(self, eval_model, eval_env, n_episodes=1, greedy=True):\n        rs = []\n        for _ in range(n_episodes):\n            s, d = eval_env.reset(), False\n            rs.append(0)\n            for _ in count():\n                if greedy:\n                    a = eval_model.select_greedy_action(s)\n                else: \n                    a = eval_model.select_action(s)\n                s, r, d, _ = eval_env.step(a)\n                rs[-1] += r\n                if d: break\n        return np.mean(rs), np.std(rs)\n\n    def get_cleaned_checkpoints(self, n_checkpoints=4):\n        try: \n            return self.checkpoint_paths\n        except AttributeError:\n            self.checkpoint_paths = {}\n\n        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n        last_ep = max(paths_dic.keys())\n        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n\n        for idx, path in paths_dic.items():\n            if idx in checkpoint_idxs:\n                self.checkpoint_paths[idx] = path\n            else:\n                os.unlink(path)\n\n        return self.checkpoint_paths\n\n    def demo_last(self, title='Fully-trained {} Agent', n_episodes=2, max_n_videos=2):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        last_ep = max(checkpoint_paths.keys())\n        self.policy_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n\n        self.evaluate(self.policy_model, env, n_episodes=n_episodes)\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def demo_progression(self, title='{} Agent progression', max_n_videos=4):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        for i in sorted(checkpoint_paths.keys()):\n            self.policy_model.load_state_dict(torch.load(checkpoint_paths[i]))\n            self.evaluate(self.policy_model, env, n_episodes=1)\n\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            subtitle_eps=sorted(checkpoint_paths.keys()),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def save_checkpoint(self, episode_idx, model):\n        torch.save(model.state_dict(),\n                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))\n\n\nppo_results = []\nbest_agent, best_eval_score = None, float('-inf')\nfor seed in SEEDS:\n    environment_settings = {\n        'env_name': 'LunarLander-v2',\n        'gamma': 0.99,\n        'max_minutes': 20,\n        'max_episodes': 2000,\n        'goal_mean_100_reward': 200\n    }\n\n    policy_model_fn = lambda nS, nA: FCCA(nS, nA, hidden_dims=(256,256))\n    policy_model_max_grad_norm = float('inf')\n    policy_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n    policy_optimizer_lr = 0.0003\n    policy_optimization_epochs = 80\n    policy_sample_ratio = 0.8\n    policy_clip_range = 0.1\n    policy_stopping_kl = 0.02\n\n    value_model_fn = lambda nS: FCV(nS, hidden_dims=(256,256))\n    value_model_max_grad_norm = float('inf')\n    value_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n    value_optimizer_lr = 0.0005\n    value_optimization_epochs = 80\n    value_sample_ratio = 0.8\n    value_clip_range = float('inf')\n    value_stopping_mse = 25\n\n    episode_buffer_fn = lambda sd, g, t, nw, me, mes: EpisodeBuffer(sd, g, t, nw, me, mes)\n    max_buffer_episodes = 16\n    max_buffer_episode_steps = 1000\n    \n    entropy_loss_weight = 0.01\n    tau = 0.97\n    n_workers = 8\n\n    env_name, gamma, max_minutes, \\\n    max_episodes, goal_mean_100_reward = environment_settings.values()\n    agent = PPO(policy_model_fn, \n                policy_model_max_grad_norm,\n                policy_optimizer_fn,\n                policy_optimizer_lr,\n                policy_optimization_epochs,\n                policy_sample_ratio,\n                policy_clip_range,\n                policy_stopping_kl,\n                value_model_fn, \n                value_model_max_grad_norm,\n                value_optimizer_fn,\n                value_optimizer_lr,\n                value_optimization_epochs,\n                value_sample_ratio,\n                value_clip_range,\n                value_stopping_mse,\n                episode_buffer_fn,\n                max_buffer_episodes,\n                max_buffer_episode_steps,\n                entropy_loss_weight,\n                tau,\n                n_workers)\n\n    make_envs_fn = lambda mef, mea, s, n: MultiprocessEnv(mef, mea, s, n)\n    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n    result, final_eval_score, training_time, wallclock_time = agent.train(make_envs_fn,\n                                                                          make_env_fn,\n                                                                          make_env_kargs,\n                                                                          seed,\n                                                                          gamma,\n                                                                          max_minutes,\n                                                                          max_episodes,\n                                                                          goal_mean_100_reward)\n    ppo_results.append(result)\n    if final_eval_score &gt; best_eval_score:\n        best_eval_score = final_eval_score\n        best_agent = agent\nppo_results = np.array(ppo_results)\n\nel 00:00:00, ep 0007, ts 0000708, ar 10 -154.5±109.6, 100 -154.5±109.6, ex 100 0.7±0.0, ev -334.7±000.0\nel 00:05:03, ep 0746, ts 0507272, ar 10 163.1±088.2, 100 143.7±071.9, ex 100 0.4±0.1, ev 177.7±056.1\nel 00:05:35, ep 0868, ts 0554669, ar 10 219.0±071.0, 100 208.4±086.0, ex 100 0.3±0.0, ev 207.4±068.9\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 192.03±86.54 in 1477.29s training time, 364.20s wall-clock time.\n\nel 00:00:00, ep 0007, ts 0000682, ar 10 -213.5±149.9, 100 -213.5±149.9, ex 100 0.7±0.0, ev -462.6±000.0\nel 00:05:03, ep 0706, ts 0454640, ar 10 078.5±051.8, 100 086.5±044.9, ex 100 0.4±0.0, ev 055.5±089.1\nel 00:07:10, ep 1039, ts 0649454, ar 10 206.7±077.8, 100 206.6±070.0, ex 100 0.3±0.0, ev 200.3±027.4\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 130.77±143.57 in 1746.69s training time, 481.33s wall-clock time.\n\nel 00:00:00, ep 0007, ts 0000762, ar 10 -160.4±095.1, 100 -160.4±095.1, ex 100 0.7±0.0, ev -535.0±000.0\nel 00:05:02, ep 0675, ts 0437478, ar 10 076.7±038.3, 100 066.0±048.3, ex 100 0.4±0.0, ev 047.6±078.0\nel 00:08:31, ep 1108, ts 0833990, ar 10 125.8±051.0, 100 132.6±047.9, ex 100 0.4±0.0, ev 213.4±040.3\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 203.66±57.11 in 2403.71s training time, 550.79s wall-clock time.\n\nel 00:00:00, ep 0007, ts 0000712, ar 10 -191.3±105.2, 100 -191.3±105.2, ex 100 0.7±0.0, ev -804.0±000.0\nel 00:05:03, ep 0681, ts 0479985, ar 10 129.1±018.3, 100 101.1±040.1, ex 100 0.4±0.0, ev 172.9±037.0\nel 00:06:07, ep 0828, ts 0603415, ar 10 219.8±080.5, 100 141.8±077.2, ex 100 0.3±0.0, ev 201.6±079.1\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 179.62±87.25 in 1737.70s training time, 416.92s wall-clock time.\n\nel 00:00:00, ep 0007, ts 0000656, ar 10 -129.6±075.8, 100 -129.6±075.8, ex 100 0.7±0.0, ev -78.6±000.0\nel 00:05:03, ep 0664, ts 0482399, ar 10 084.9±057.2, 100 074.9±046.0, ex 100 0.4±0.1, ev 094.9±067.0\nel 00:07:52, ep 1040, ts 0820930, ar 10 136.2±056.5, 100 132.9±056.8, ex 100 0.4±0.1, ev 214.6±043.6\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 200.91±75.14 in 2334.44s training time, 508.83s wall-clock time.\n\n\n\n\nbest_agent.demo_progression()\n\nPPO Agent progression\n        Episode 0\n        \n        Episode 369\n        \n        Episode 738\n        \n        Episode 1108\n        \n\n\n\nbest_agent.demo_last()\n\nFully-trained PPO Agent\n        Trial 0\n        \n        Trial 1\n        \n\n\n\nppo_max_t, ppo_max_r, ppo_max_s, \\\nppo_max_sec, ppo_max_rt = np.max(ppo_results, axis=0).T\nppo_min_t, ppo_min_r, ppo_min_s, \\\nppo_min_sec, ppo_min_rt = np.min(ppo_results, axis=0).T\nppo_mean_t, ppo_mean_r, ppo_mean_s, \\\nppo_mean_sec, ppo_mean_rt = np.mean(ppo_results, axis=0).T\nppo_x = np.arange(len(ppo_mean_s))\n\n\nfig, axs = plt.subplots(2, 1, figsize=(15,10), sharey=False, sharex=True)\n\n# PPO\naxs[0].plot(ppo_max_r, 'k', linewidth=1)\naxs[0].plot(ppo_min_r, 'k', linewidth=1)\naxs[0].plot(ppo_mean_r, 'k:', label='PPO', linewidth=2)\naxs[0].fill_between(\n    ppo_x, ppo_min_r, ppo_max_r, facecolor='k', alpha=0.3)\n\naxs[1].plot(ppo_max_s, 'k', linewidth=1)\naxs[1].plot(ppo_min_s, 'k', linewidth=1)\naxs[1].plot(ppo_mean_s, 'k:', label='PPO', linewidth=2)\naxs[1].fill_between(\n    ppo_x, ppo_min_s, ppo_max_s, facecolor='k', alpha=0.3)\n\n# ALL\naxs[0].set_title('Moving Avg Reward (Training)')\naxs[1].set_title('Moving Avg Reward (Evaluation)')\nplt.xlabel('Episodes')\naxs[0].legend(loc='upper left')\nplt.show()\n\n\n\n\n\nfig, axs = plt.subplots(3, 1, figsize=(15,15), sharey=False, sharex=True)\n\n# PPO\naxs[0].plot(ppo_max_t, 'k', linewidth=1)\naxs[0].plot(ppo_min_t, 'k', linewidth=1)\naxs[0].plot(ppo_mean_t, 'k:', label='PPO', linewidth=2)\naxs[0].fill_between(\n    ppo_x, ppo_min_t, ppo_max_t, facecolor='k', alpha=0.3)\n\naxs[1].plot(ppo_max_sec, 'k', linewidth=1)\naxs[1].plot(ppo_min_sec, 'k', linewidth=1)\naxs[1].plot(ppo_mean_sec, 'k:', label='PPO', linewidth=2)\naxs[1].fill_between(\n    ppo_x, ppo_min_sec, ppo_max_sec, facecolor='k', alpha=0.3)\n\naxs[2].plot(ppo_max_rt, 'k', linewidth=1)\naxs[2].plot(ppo_min_rt, 'k', linewidth=1)\naxs[2].plot(ppo_mean_rt, 'k:', label='PPO', linewidth=2)\naxs[2].fill_between(\n    ppo_x, ppo_min_rt, ppo_max_rt, facecolor='k', alpha=0.3)\n\n# ALL\naxs[0].set_title('Total Steps')\naxs[1].set_title('Training Time')\naxs[2].set_title('Wall-clock Time')\nplt.xlabel('Episodes')\naxs[0].legend(loc='upper left')\nplt.show()\n\n\n\n\n\nppo_root_dir = os.path.join(RESULTS_DIR, 'ppo')\nnot os.path.exists(ppo_root_dir) and os.makedirs(ppo_root_dir)\n\nnp.save(os.path.join(ppo_root_dir, 'x'), ppo_x)\n\nnp.save(os.path.join(ppo_root_dir, 'max_r'), ppo_max_r)\nnp.save(os.path.join(ppo_root_dir, 'min_r'), ppo_min_r)\nnp.save(os.path.join(ppo_root_dir, 'mean_r'), ppo_mean_r)\n\nnp.save(os.path.join(ppo_root_dir, 'max_s'), ppo_max_s)\nnp.save(os.path.join(ppo_root_dir, 'min_s'), ppo_min_s )\nnp.save(os.path.join(ppo_root_dir, 'mean_s'), ppo_mean_s)\n\nnp.save(os.path.join(ppo_root_dir, 'max_t'), ppo_max_t)\nnp.save(os.path.join(ppo_root_dir, 'min_t'), ppo_min_t)\nnp.save(os.path.join(ppo_root_dir, 'mean_t'), ppo_mean_t)\n\nnp.save(os.path.join(ppo_root_dir, 'max_sec'), ppo_max_sec)\nnp.save(os.path.join(ppo_root_dir, 'min_sec'), ppo_min_sec)\nnp.save(os.path.join(ppo_root_dir, 'mean_sec'), ppo_mean_sec)\n\nnp.save(os.path.join(ppo_root_dir, 'max_rt'), ppo_max_rt)\nnp.save(os.path.join(ppo_root_dir, 'min_rt'), ppo_min_rt)\nnp.save(os.path.join(ppo_root_dir, 'mean_rt'), ppo_mean_rt)"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-10.html#tldr",
    "href": "publication/GDRL/GDRL-chapter-10.html#tldr",
    "title": "Chapter 10: Sample-Efficient Value-Based Methods",
    "section": "TL;DR",
    "text": "TL;DR\n\n그로킹 심층 강화학습 중 10장 내용인 “샘플 효율적인 가치기반의 학습 방법들”에 대한 내용입니다.\n\n\n\nCode\n!pip install tqdm numpy scikit-learn pyglet setuptools && \\\n!pip install gym asciinema pandas tabulate tornado==5.* PyBullet && \\\n!pip install git+https://github.com/pybox2d/pybox2d#egg=Box2D && \\\n!pip install git+https://github.com/mimoralea/gym-bandits#egg=gym-bandits && \\\n!pip install git+https://github.com/mimoralea/gym-walk#egg=gym-walk && \\\n!pip install git+https://github.com/mimoralea/gym-aima#egg=gym-aima && \\\n!pip install gym[atari]\n!pip install torch torchvision\n\n\n\nimport warnings ; warnings.filterwarnings('ignore')\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport numpy as np\nfrom IPython.display import display\nfrom collections import namedtuple, deque\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nfrom itertools import cycle, count\nfrom textwrap import wrap\n\nimport matplotlib\nimport subprocess\nimport os.path\nimport tempfile\nimport random\nimport base64\nimport pprint\nimport glob\nimport time\nimport json\nimport sys\nimport gym\nimport io\nimport os\nimport gc\nimport platform\n\nfrom gym import wrappers\nfrom subprocess import check_output\nfrom IPython.display import HTML\n\nLEAVE_PRINT_EVERY_N_SECS = 60\nERASE_LINE = '\\x1b[2K'\nEPS = 1e-6\nRESULTS_DIR = os.path.join('.', 'gym-results')\nSEEDS = (12, 34, 56, 78, 90)\n\n%matplotlib inline\n\n\nplt.style.use('fivethirtyeight')\nparams = {\n    'figure.figsize': (15, 8),\n    'font.size': 24,\n    'legend.fontsize': 20,\n    'axes.titlesize': 28,\n    'axes.labelsize': 24,\n    'xtick.labelsize': 20,\n    'ytick.labelsize': 20\n}\npylab.rcParams.update(params)\nnp.set_printoptions(suppress=True)\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\ndef get_make_env_fn(**kargs):\n    def make_env_fn(env_name, seed=None, render=None, record=False,\n                    unwrapped=False, monitor_mode=None, \n                    inner_wrappers=None, outer_wrappers=None):\n        mdir = tempfile.mkdtemp()\n        env = None\n        if render:\n            try:\n                env = gym.make(env_name, render=render)\n            except:\n                pass\n        if env is None:\n            env = gym.make(env_name)\n        if seed is not None: env.seed(seed)\n        env = env.unwrapped if unwrapped else env\n        if inner_wrappers:\n            for wrapper in inner_wrappers:\n                env = wrapper(env)\n        env = wrappers.Monitor(\n            env, mdir, force=True, \n            mode=monitor_mode, \n            video_callable=lambda e_idx: record) if monitor_mode else env\n        if outer_wrappers:\n            for wrapper in outer_wrappers:\n                env = wrapper(env)\n        return env\n    return make_env_fn, kargs\n\n\ndef get_videos_html(env_videos, title, max_n_videos=5):\n    videos = np.array(env_videos)\n    if len(videos) == 0:\n        return\n    \n    n_videos = max(1, min(max_n_videos, len(videos)))\n    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int) if n_videos &gt; 1 else [-1,]\n    videos = videos[idxs,...]\n\n    strm = '&lt;h2&gt;{}&lt;/h2&gt;'.format(title)\n    for video_path, meta_path in videos:\n        video = io.open(video_path, 'r+b').read()\n        encoded = base64.b64encode(video)\n\n        with open(meta_path) as data_file:    \n            meta = json.load(data_file)\n\n        html_tag = \"\"\"\n        &lt;h3&gt;{0}&lt;/h3&gt;\n        &lt;video width=\"960\" height=\"540\" controls&gt;\n            &lt;source src=\"data:video/mp4;base64,{1}\" type=\"video/mp4\" /&gt;\n        &lt;/video&gt;\"\"\"\n        strm += html_tag.format('Episode ' + str(meta['episode_id']), encoded.decode('ascii'))\n    return strm\n\n\nplatform.system()\n\n'Windows'\n\n\n\ndef get_gif_html(env_videos, title, subtitle_eps=None, max_n_videos=4):\n    videos = np.array(env_videos)\n    if len(videos) == 0:\n        return\n    \n    n_videos = max(1, min(max_n_videos, len(videos)))\n    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int) if n_videos &gt; 1 else [-1,]\n    videos = videos[idxs,...]\n\n    strm = '&lt;h2&gt;{}&lt;/h2&gt;'.format(title)\n    for video_path, meta_path in videos:\n        basename = os.path.splitext(video_path)[0]\n        gif_path = basename + '.gif'\n        if not os.path.exists(gif_path):\n            if platform.system() == 'Linux':\n                ps = subprocess.Popen(\n                    ('ffmpeg', \n                     '-i', video_path, \n                     '-r', '7',\n                     '-f', 'image2pipe', \n                     '-vcodec', 'ppm',\n                     '-crf', '20',\n                     '-vf', 'scale=512:-1',\n                     '-'), \n                    stdout=subprocess.PIPE,\n                    universal_newlines=True)\n                output = subprocess.check_output(\n                    ('convert',\n                     '-coalesce',\n                     '-delay', '7',\n                     '-loop', '0',\n                     '-fuzz', '2%',\n                     '+dither',\n                     '-deconstruct',\n                     '-layers', 'Optimize',\n                     '-', gif_path), \n                    stdin=ps.stdout)\n                ps.wait()\n            else:\n                ps = subprocess.Popen('ffmpeg -i {} -r 7 -f image2pipe \\\n                                      -vcodec ppm -crf 20 -vf scale=512:-1 - | \\\n                                      convert -coalesce -delay 7 -loop 0 -fuzz 2% \\\n                                      +dither -deconstruct -layers Optimize \\\n                                      - {}'.format(video_path, gif_path), \n                                      stdin=subprocess.PIPE, \n                                      shell=True)\n                ps.wait()\n\n        gif = io.open(gif_path, 'r+b').read()\n        encoded = base64.b64encode(gif)\n            \n        with open(meta_path) as data_file:    \n            meta = json.load(data_file)\n\n        html_tag = \"\"\"\n        &lt;h3&gt;{0}&lt;/h3&gt;\n        &lt;img src=\"data:image/gif;base64,{1}\" /&gt;\"\"\"\n        prefix = 'Trial ' if subtitle_eps is None else 'Episode '\n        sufix = str(meta['episode_id'] if subtitle_eps is None \\\n                    else subtitle_eps[meta['episode_id']])\n        strm += html_tag.format(prefix + sufix, encoded.decode('ascii'))\n    return strm"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-10.html#dueling-ddqn",
    "href": "publication/GDRL/GDRL-chapter-10.html#dueling-ddqn",
    "title": "Chapter 10: Sample-Efficient Value-Based Methods",
    "section": "Dueling DDQN",
    "text": "Dueling DDQN\n\nclass FCQ(nn.Module):\n    def __init__(self, \n                 input_dim, \n                 output_dim, \n                 hidden_dims=(32,32), \n                 activation_fc=F.relu):\n        super(FCQ, self).__init__()\n        self.activation_fc = activation_fc\n\n        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n\n        self.hidden_layers = nn.ModuleList()\n        for i in range(len(hidden_dims)-1):\n            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n            self.hidden_layers.append(hidden_layer)\n        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n\n        device = \"cpu\"\n        if torch.cuda.is_available():\n            device = \"cuda:0\"\n        self.device = torch.device(device)\n        self.to(self.device)\n        \n    def _format(self, state):\n        x = state\n        if not isinstance(x, torch.Tensor):\n            x = torch.tensor(x, \n                             device=self.device, \n                             dtype=torch.float32)\n            x = x.unsqueeze(0)\n        return x\n\n    def forward(self, state):\n        x = self._format(state)\n        x = self.activation_fc(self.input_layer(x))\n        for hidden_layer in self.hidden_layers:\n            x = self.activation_fc(hidden_layer(x))\n        x = self.output_layer(x)\n        return x\n    \n    def numpy_float_to_device(self, variable):\n        variable = torch.from_numpy(variable).float().to(self.device)\n        return variable\n    \n    def load(self, experiences):\n        states, actions, new_states, rewards, is_terminals = experiences\n        states = torch.from_numpy(states).float().to(self.device)\n        actions = torch.from_numpy(actions).long().to(self.device)\n        new_states = torch.from_numpy(new_states).float().to(self.device)\n        rewards = torch.from_numpy(rewards).float().to(self.device)\n        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n        return states, actions, new_states, rewards, is_terminals\n\n\nclass GreedyStrategy():\n    def __init__(self):\n        self.exploratory_action_taken = False\n\n    def select_action(self, model, state):\n        with torch.no_grad():\n            q_values = model(state).cpu().detach().data.numpy().squeeze()\n            return np.argmax(q_values)\n\n\nclass EGreedyStrategy():\n    def __init__(self, epsilon=0.1):\n        self.epsilon = epsilon\n        self.exploratory_action_taken = None\n\n    def select_action(self, model, state):\n        self.exploratory_action_taken = False\n        with torch.no_grad():\n            q_values = model(state).cpu().detach().data.numpy().squeeze()\n\n        if np.random.rand() &gt; self.epsilon:\n            action = np.argmax(q_values)\n        else: \n            action = np.random.randint(len(q_values))\n\n        self.exploratory_action_taken = action != np.argmax(q_values)\n        return action\n\n\nclass EGreedyLinearStrategy():\n    def __init__(self, init_epsilon=1.0, min_epsilon=0.1, max_steps=20000):\n        self.t = 0\n        self.epsilon = init_epsilon\n        self.init_epsilon = init_epsilon\n        self.min_epsilon = min_epsilon\n        self.max_steps = max_steps\n        self.exploratory_action_taken = None\n        \n    def _epsilon_update(self):\n        epsilon = 1 - self.t / self.max_steps\n        epsilon = (self.init_epsilon - self.min_epsilon) * epsilon + self.min_epsilon\n        epsilon = np.clip(epsilon, self.min_epsilon, self.init_epsilon)\n        self.t += 1\n        return epsilon\n\n    def select_action(self, model, state):\n        self.exploratory_action_taken = False\n        with torch.no_grad():\n            q_values = model(state).cpu().detach().data.numpy().ipynb_checkpoints/squeeze()\n\n        if np.random.rand() &gt; self.epsilon:\n            action = np.argmax(q_values)\n        else: \n            action = np.random.randint(len(q_values))\n\n        self.epsilon = self._epsilon_update()\n        self.exploratory_action_taken = action != np.argmax(q_values)\n        return action\n\n\nclass EGreedyExpStrategy():\n    def __init__(self, init_epsilon=1.0, min_epsilon=0.1, decay_steps=20000):\n        self.epsilon = init_epsilon\n        self.init_epsilon = init_epsilon\n        self.decay_steps = decay_steps\n        self.min_epsilon = min_epsilon\n        self.epsilons = 0.01 / np.logspace(-2, 0, decay_steps, endpoint=False) - 0.01\n        self.epsilons = self.epsilons * (init_epsilon - min_epsilon) + min_epsilon\n        self.t = 0\n        self.exploratory_action_taken = None\n\n    def _epsilon_update(self):\n        self.epsilon = self.min_epsilon if self.t &gt;= self.decay_steps else self.epsilons[self.t]\n        self.t += 1\n        return self.epsilon\n\n    def select_action(self, model, state):\n        self.exploratory_action_taken = False\n        with torch.no_grad():\n            q_values = model(state).detach().cpu().data.numpy().squeeze()\n\n        if np.random.rand() &gt; self.epsilon:\n            action = np.argmax(q_values)\n        else:\n            action = np.random.randint(len(q_values))\n\n        self._epsilon_update()\n        self.exploratory_action_taken = action != np.argmax(q_values)\n        return action\n\n\nclass SoftMaxStrategy():\n    def __init__(self, \n                 init_temp=1.0, \n                 min_temp=0.3, \n                 exploration_ratio=0.8, \n                 max_steps=25000):\n        self.t = 0\n        self.init_temp = init_temp\n        self.exploration_ratio = exploration_ratio\n        self.min_temp = min_temp\n        self.max_steps = max_steps\n        self.exploratory_action_taken = None\n        \n    def _update_temp(self):\n        temp = 1 - self.t / (self.max_steps * self.exploration_ratio)\n        temp = (self.init_temp - self.min_temp) * temp + self.min_temp\n        temp = np.clip(temp, self.min_temp, self.init_temp)\n        self.t += 1\n        return temp\n\n    def select_action(self, model, state):\n        self.exploratory_action_taken = False\n        temp = self._update_temp()\n\n        with torch.no_grad():\n            q_values = model(state).cpu().detach().data.numpy().squeeze()\n            scaled_qs = q_values/temp\n            norm_qs = scaled_qs - scaled_qs.max()            \n            e = np.exp(norm_qs)\n            probs = e / np.sum(e)\n            assert np.isclose(probs.sum(), 1.0)\n\n        action = np.random.choice(np.arange(len(probs)), size=1, p=probs)[0]\n        self.exploratory_action_taken = action != np.argmax(q_values)\n        return action\n\n\nclass ReplayBuffer():\n    def __init__(self, \n                 max_size=10000, \n                 batch_size=64):\n        self.ss_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n        self.as_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n        self.rs_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n        self.ps_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n        self.ds_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n\n        self.max_size = max_size\n        self.batch_size = batch_size\n        self._idx = 0\n        self.size = 0\n    \n    def store(self, sample):\n        s, a, r, p, d = sample\n        self.ss_mem[self._idx] = s\n        self.as_mem[self._idx] = a\n        self.rs_mem[self._idx] = r\n        self.ps_mem[self._idx] = p\n        self.ds_mem[self._idx] = d\n        \n        self._idx += 1\n        self._idx = self._idx % self.max_size\n\n        self.size += 1\n        self.size = min(self.size, self.max_size)\n\n    def sample(self, batch_size=None):\n        if batch_size == None:\n            batch_size = self.batch_size\n\n        idxs = np.random.choice(\n            self.size, batch_size, replace=False)\n        experiences = np.vstack(self.ss_mem[idxs]), \\\n                      np.vstack(self.as_mem[idxs]), \\\n                      np.vstack(self.rs_mem[idxs]), \\\n                      np.vstack(self.ps_mem[idxs]), \\\n                      np.vstack(self.ds_mem[idxs])\n        return experiences\n\n    def __len__(self):\n        return self.size\n\n\nclass FCDuelingQ(nn.Module):\n    def __init__(self, \n                 input_dim, \n                 output_dim, \n                 hidden_dims=(32,32), \n                 activation_fc=F.relu):\n        super(FCDuelingQ, self).__init__()\n        self.activation_fc = activation_fc\n\n        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n        self.hidden_layers = nn.ModuleList()\n        for i in range(len(hidden_dims)-1):\n            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n            self.hidden_layers.append(hidden_layer)\n        self.output_value = nn.Linear(hidden_dims[-1], 1)\n        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n\n        device = \"cpu\"\n        if torch.cuda.is_available():\n            device = \"cuda:0\"\n        self.device = torch.device(device)\n        self.to(self.device)\n        \n    def _format(self, state):\n        x = state\n        if not isinstance(x, torch.Tensor):\n            x = torch.tensor(x, \n                             device=self.device, \n                             dtype=torch.float32)\n            x = x.unsqueeze(0)      \n        return x\n\n    def forward(self, state):\n        x = self._format(state)\n        x = self.activation_fc(self.input_layer(x))\n        for hidden_layer in self.hidden_layers:\n            x = self.activation_fc(hidden_layer(x))\n        a = self.output_layer(x)\n        v = self.output_value(x).expand_as(a)\n        q = v + a - a.mean(1, keepdim=True).expand_as(a)\n        return q\n\n    def numpy_float_to_device(self, variable):\n        variable = torch.from_numpy(variable).float().to(self.device)\n        return variable\n\n    def load(self, experiences):\n        states, actions, new_states, rewards, is_terminals = experiences\n        states = torch.from_numpy(states).float().to(self.device)\n        actions = torch.from_numpy(actions).long().to(self.device)\n        new_states = torch.from_numpy(new_states).float().to(self.device)\n        rewards = torch.from_numpy(rewards).float().to(self.device)\n        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n        return states, actions, new_states, rewards, is_terminals\n\n\nclass DuelingDDQN():\n    def __init__(self, \n                 replay_buffer_fn, \n                 value_model_fn, \n                 value_optimizer_fn, \n                 value_optimizer_lr,\n                 max_gradient_norm,\n                 training_strategy_fn,\n                 evaluation_strategy_fn,\n                 n_warmup_batches,\n                 update_target_every_steps,\n                 tau):\n        self.replay_buffer_fn = replay_buffer_fn\n        self.value_model_fn = value_model_fn\n        self.value_optimizer_fn = value_optimizer_fn\n        self.value_optimizer_lr = value_optimizer_lr\n        self.max_gradient_norm = max_gradient_norm\n        self.training_strategy_fn = training_strategy_fn\n        self.evaluation_strategy_fn = evaluation_strategy_fn\n        self.n_warmup_batches = n_warmup_batches\n        self.update_target_every_steps = update_target_every_steps\n        self.tau = tau\n\n    def optimize_model(self, experiences):\n        states, actions, rewards, next_states, is_terminals = experiences\n        batch_size = len(is_terminals)\n\n        argmax_a_q_sp = self.online_model(next_states).max(1)[1]\n        q_sp = self.target_model(next_states).detach()\n        max_a_q_sp = q_sp[\n            np.arange(batch_size), argmax_a_q_sp].unsqueeze(1)\n        target_q_sa = rewards + (self.gamma * max_a_q_sp * (1 - is_terminals))\n        q_sa = self.online_model(states).gather(1, actions)\n\n        td_error = q_sa - target_q_sa\n        value_loss = td_error.pow(2).mul(0.5).mean()\n        self.value_optimizer.zero_grad()\n        value_loss.backward()        \n        torch.nn.utils.clip_grad_norm_(self.online_model.parameters(), \n                                       self.max_gradient_norm)\n        self.value_optimizer.step()\n\n    def interaction_step(self, state, env):\n        action = self.training_strategy.select_action(self.online_model, state)\n        new_state, reward, is_terminal, info = env.step(action)\n        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n        is_failure = is_terminal and not is_truncated\n        experience = (state, action, reward, new_state, float(is_failure))\n        self.replay_buffer.store(experience)\n        self.episode_reward[-1] += reward\n        self.episode_timestep[-1] += 1\n        self.episode_exploration[-1] += int(self.training_strategy.exploratory_action_taken)\n        return new_state, is_terminal\n    \n    def update_network(self, tau=None):\n        tau = self.tau if tau is None else tau\n        for target, online in zip(self.target_model.parameters(), \n                                  self.online_model.parameters()):\n            target_ratio = (1.0 - self.tau) * target.data\n            online_ratio = self.tau * online.data\n            mixed_weights = target_ratio + online_ratio\n            target.data.copy_(mixed_weights)\n\n    def train(self, make_env_fn, make_env_kargs, seed, gamma, \n              max_minutes, max_episodes, goal_mean_100_reward):\n        training_start, last_debug_time = time.time(), float('-inf')\n\n        self.checkpoint_dir = tempfile.mkdtemp()\n        self.make_env_fn = make_env_fn\n        self.make_env_kargs = make_env_kargs\n        self.seed = seed\n        self.gamma = gamma\n        \n        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n    \n        nS, nA = env.observation_space.shape[0], env.action_space.n\n        self.episode_timestep = []\n        self.episode_reward = []\n        self.episode_seconds = []\n        self.evaluation_scores = []        \n        self.episode_exploration = []\n        \n        self.target_model = self.value_model_fn(nS, nA)\n        self.online_model = self.value_model_fn(nS, nA)\n        self.update_network(tau=1.0)\n\n        self.value_optimizer = self.value_optimizer_fn(self.online_model, \n                                                       self.value_optimizer_lr)\n\n        self.replay_buffer = self.replay_buffer_fn()\n        self.training_strategy = training_strategy_fn()\n        self.evaluation_strategy = evaluation_strategy_fn() \n                    \n        result = np.empty((max_episodes, 5))\n        result[:] = np.nan\n        training_time = 0\n        for episode in range(1, max_episodes + 1):\n            episode_start = time.time()\n            \n            state, is_terminal = env.reset(), False\n            self.episode_reward.append(0.0)\n            self.episode_timestep.append(0.0)\n            self.episode_exploration.append(0.0)\n\n            for step in count():\n                state, is_terminal = self.interaction_step(state, env)\n                \n                min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n                if len(self.replay_buffer) &gt; min_samples:\n                    experiences = self.replay_buffer.sample()\n                    experiences = self.online_model.load(experiences)\n                    self.optimize_model(experiences)\n                \n                if np.sum(self.episode_timestep) % self.update_target_every_steps == 0:\n                    self.update_network()\n\n                if is_terminal:\n                    gc.collect()\n                    break\n            \n            # stats\n            episode_elapsed = time.time() - episode_start\n            self.episode_seconds.append(episode_elapsed)\n            training_time += episode_elapsed\n            evaluation_score, _ = self.evaluate(self.online_model, env)\n            self.save_checkpoint(episode-1, self.online_model)\n            \n            total_step = int(np.sum(self.episode_timestep))\n            self.evaluation_scores.append(evaluation_score)\n            \n            mean_10_reward = np.mean(self.episode_reward[-10:])\n            std_10_reward = np.std(self.episode_reward[-10:])\n            mean_100_reward = np.mean(self.episode_reward[-100:])\n            std_100_reward = np.std(self.episode_reward[-100:])\n            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n            lst_100_exp_rat = np.array(\n                self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n            mean_100_exp_rat = np.mean(lst_100_exp_rat)\n            std_100_exp_rat = np.std(lst_100_exp_rat)\n            \n            wallclock_elapsed = time.time() - training_start\n            result[episode-1] = total_step, mean_100_reward, \\\n                mean_100_eval_score, training_time, wallclock_elapsed\n            \n            reached_debug_time = time.time() - last_debug_time &gt;= LEAVE_PRINT_EVERY_N_SECS\n            reached_max_minutes = wallclock_elapsed &gt;= max_minutes * 60\n            reached_max_episodes = episode &gt;= max_episodes\n            reached_goal_mean_reward = mean_100_eval_score &gt;= goal_mean_100_reward\n            training_is_over = reached_max_minutes or \\\n                               reached_max_episodes or \\\n                               reached_goal_mean_reward\n\n            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n            debug_message = 'el {}, ep {:04}, ts {:06}, '\n            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n            debug_message = debug_message.format(\n                elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n                mean_100_eval_score, std_100_eval_score)\n            print(debug_message, end='\\r', flush=True)\n            if reached_debug_time or training_is_over:\n                print(ERASE_LINE + debug_message, flush=True)\n                last_debug_time = time.time()\n            if training_is_over:\n                if reached_max_minutes: print(u'--&gt; reached_max_minutes \\u2715')\n                if reached_max_episodes: print(u'--&gt; reached_max_episodes \\u2715')\n                if reached_goal_mean_reward: print(u'--&gt; reached_goal_mean_reward \\u2713')\n                break\n                \n        final_eval_score, score_std = self.evaluate(self.online_model, env, n_episodes=100)\n        wallclock_time = time.time() - training_start\n        print('Training complete.')\n        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n              ' {:.2f}s wall-clock time.\\n'.format(\n                  final_eval_score, score_std, training_time, wallclock_time))\n        env.close() ; del env\n        self.get_cleaned_checkpoints()\n        return result, final_eval_score, training_time, wallclock_time\n    \n    def evaluate(self, eval_policy_model, eval_env, n_episodes=1):\n        rs = []\n        for _ in range(n_episodes):\n            s, d = eval_env.reset(), False\n            rs.append(0)\n            for _ in count():\n                a = self.evaluation_strategy.select_action(eval_policy_model, s)\n                s, r, d, _ = eval_env.step(a)\n                rs[-1] += r\n                if d: break\n        return np.mean(rs), np.std(rs)\n\n    def get_cleaned_checkpoints(self, n_checkpoints=5):\n        try: \n            return self.checkpoint_paths\n        except AttributeError:\n            self.checkpoint_paths = {}\n\n        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n        last_ep = max(paths_dic.keys())\n        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n\n        for idx, path in paths_dic.items():\n            if idx in checkpoint_idxs:\n                self.checkpoint_paths[idx] = path\n            else:\n                os.unlink(path)\n\n        return self.checkpoint_paths\n\n    def demo_last(self, title='Fully-trained {} Agent', n_episodes=3, max_n_videos=3):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        last_ep = max(checkpoint_paths.keys())\n        self.online_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n\n        self.evaluate(self.online_model, env, n_episodes=n_episodes)\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def demo_progression(self, title='{} Agent progression', max_n_videos=5):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        for i in sorted(checkpoint_paths.keys()):\n            self.online_model.load_state_dict(torch.load(checkpoint_paths[i]))\n            self.evaluate(self.online_model, env, n_episodes=1)\n\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            subtitle_eps=sorted(checkpoint_paths.keys()),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def save_checkpoint(self, episode_idx, model):\n        torch.save(model.state_dict(), \n                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))\n\n\ndueling_ddqn_results = []\ndueling_ddqn_agents, best_dueling_ddqn_agent_key, best_eval_score = {}, None, float('-inf')\nfor seed in SEEDS:\n    environment_settings = {\n        'env_name': 'CartPole-v1',\n        'gamma': 1.00,\n        'max_minutes': 20,\n        'max_episodes': 10000,\n        'goal_mean_100_reward': 475\n    }\n    \n    # value_model_fn = lambda nS, nA: FCQ(nS, nA, hidden_dims=(512,128))\n    value_model_fn = lambda nS, nA: FCDuelingQ(nS, nA, hidden_dims=(512,128))\n    value_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr=lr)\n    value_optimizer_lr = 0.0005\n    max_gradient_norm = float('inf')\n\n    training_strategy_fn = lambda: EGreedyExpStrategy(init_epsilon=1.0,  \n                                                      min_epsilon=0.3, \n                                                      decay_steps=20000)\n    evaluation_strategy_fn = lambda: GreedyStrategy()\n\n    replay_buffer_fn = lambda: ReplayBuffer(max_size=50000, batch_size=64)\n    n_warmup_batches = 5\n    update_target_every_steps = 1\n    tau = 0.1\n\n    env_name, gamma, max_minutes, \\\n    max_episodes, goal_mean_100_reward = environment_settings.values()\n    agent = DuelingDDQN(replay_buffer_fn,\n                        value_model_fn,\n                        value_optimizer_fn,\n                        value_optimizer_lr,\n                        max_gradient_norm,\n                        training_strategy_fn,\n                        evaluation_strategy_fn,\n                        n_warmup_batches,\n                        update_target_every_steps,\n                        tau)\n\n    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n    result, final_eval_score, training_time, wallclock_time = agent.train(\n        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n    dueling_ddqn_results.append(result)\n    dueling_ddqn_agents[seed] = agent\n    if final_eval_score &gt; best_eval_score:\n        best_eval_score = final_eval_score\n        best_dueling_ddqn_agent_key = seed\ndueling_ddqn_results = np.array(dueling_ddqn_results)\n\nel 00:00:01, ep 0000, ts 000016, ar 10 016.0±000.0, 100 016.0±000.0, ex 100 0.3±0.0, ev 009.0±000.0\nel 00:01:02, ep 0131, ts 010190, ar 10 189.8±108.5, 100 092.9±081.8, ex 100 0.3±0.1, ev 324.9±096.6\nel 00:02:03, ep 0169, ts 022591, ar 10 458.9±101.1, 100 198.1±148.2, ex 100 0.2±0.1, ev 345.4±103.2\nel 00:03:05, ep 0195, ts 035136, ar 10 454.5±089.6, 100 304.9±166.0, ex 100 0.2±0.1, ev 393.3±111.0\nel 00:04:07, ep 0222, ts 046773, ar 10 459.3±081.5, 100 382.7±147.9, ex 100 0.2±0.0, ev 441.4±097.0\nel 00:04:51, ep 0240, ts 055065, ar 10 453.6±070.4, 100 427.2±120.0, ex 100 0.2±0.0, ev 476.1±063.8\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 253.77s training time, 311.37s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000034, ar 10 034.0±000.0, 100 034.0±000.0, ex 100 0.6±0.0, ev 008.0±000.0\nel 00:01:00, ep 0147, ts 010996, ar 10 184.9±072.3, 100 096.4±085.5, ex 100 0.3±0.1, ev 253.6±099.4\nel 00:02:01, ep 0195, ts 023123, ar 10 356.0±103.3, 100 196.7±106.2, ex 100 0.2±0.1, ev 316.2±113.4\nel 00:03:01, ep 0220, ts 035234, ar 10 500.0±000.0, 100 296.7±138.3, ex 100 0.2±0.0, ev 377.4±119.4\nel 00:04:03, ep 0245, ts 047149, ar 10 449.1±104.8, 100 365.6±139.9, ex 100 0.2±0.0, ev 424.7±110.8\nel 00:05:04, ep 0268, ts 058543, ar 10 489.4±031.8, 100 431.8±114.2, ex 100 0.2±0.0, ev 471.2±071.3\nel 00:05:12, ep 0271, ts 060043, ar 10 489.4±031.8, 100 439.6±109.5, ex 100 0.2±0.0, ev 477.1±063.8\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 277.92s training time, 332.44s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000012, ar 10 012.0±000.0, 100 012.0±000.0, ex 100 0.4±0.0, ev 009.0±000.0\nel 00:01:00, ep 0149, ts 011372, ar 10 231.7±040.8, 100 100.6±085.6, ex 100 0.3±0.1, ev 224.6±095.8\nel 00:02:01, ep 0186, ts 023906, ar 10 455.9±082.6, 100 214.8±147.6, ex 100 0.2±0.1, ev 309.1±132.2\nel 00:03:01, ep 0214, ts 035806, ar 10 494.5±016.5, 100 308.9±158.8, ex 100 0.2±0.1, ev 395.0±124.4\nel 00:04:03, ep 0241, ts 047454, ar 10 398.6±156.7, 100 379.9±146.5, ex 100 0.2±0.0, ev 449.2±103.6\nel 00:04:34, ep 0254, ts 053255, ar 10 456.3±131.1, 100 410.7±138.4, ex 100 0.2±0.0, ev 475.5±078.4\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 243.35s training time, 294.46s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000012, ar 10 012.0±000.0, 100 012.0±000.0, ex 100 0.2±0.0, ev 011.0±000.0\nel 00:01:00, ep 0129, ts 011756, ar 10 233.4±107.3, 100 110.2±091.5, ex 100 0.3±0.1, ev 267.4±102.5\nel 00:02:01, ep 0172, ts 024414, ar 10 349.2±138.0, 100 217.6±123.4, ex 100 0.2±0.1, ev 347.5±100.8\nel 00:03:03, ep 0199, ts 036949, ar 10 500.0±000.0, 100 314.9±145.4, ex 100 0.2±0.0, ev 403.5±104.8\nel 00:04:04, ep 0225, ts 048902, ar 10 497.8±006.6, 100 381.8±140.5, ex 100 0.2±0.0, ev 450.1±087.1\nel 00:04:38, ep 0239, ts 055370, ar 10 446.8±108.6, 100 407.1±135.9, ex 100 0.2±0.0, ev 475.0±062.6\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 247.27s training time, 298.50s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000039, ar 10 039.0±000.0, 100 039.0±000.0, ex 100 0.4±0.0, ev 038.0±000.0\nel 00:01:00, ep 0142, ts 011535, ar 10 184.4±054.1, 100 102.8±083.3, ex 100 0.3±0.1, ev 255.2±095.3\nel 00:02:02, ep 0180, ts 024393, ar 10 485.7±042.9, 100 215.3±148.5, ex 100 0.2±0.1, ev 322.0±119.7\nel 00:03:03, ep 0206, ts 036765, ar 10 457.3±085.5, 100 313.3±162.8, ex 100 0.2±0.1, ev 386.2±124.2\nel 00:04:05, ep 0232, ts 048735, ar 10 439.3±113.1, 100 390.4±143.2, ex 100 0.2±0.0, ev 436.3±105.9\nel 00:04:52, ep 0253, ts 057655, ar 10 422.1±155.8, 100 440.0±113.5, ex 100 0.2±0.0, ev 477.1±071.1\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 259.97s training time, 311.73s wall-clock time.\n\n\n\n\ndueling_ddqn_agents[best_dueling_ddqn_agent_key].demo_progression()\n\nDuelingDDQN Agent progression\n        Episode 0\n        \n        Episode 60\n        \n        Episode 120\n        \n        Episode 180\n        \n        Episode 240\n        \n\n\n\ndueling_ddqn_agents[best_dueling_ddqn_agent_key].demo_last()\n\nFully-trained DuelingDDQN Agent\n        Trial 0\n        \n        Trial 1\n        \n        Trial 2\n        \n\n\n\nddqn_root_dir = os.path.join(RESULTS_DIR, 'ddqn')\nddqn_x = np.load(os.path.join(ddqn_root_dir, 'x.npy'))\n\nddqn_max_r = np.load(os.path.join(ddqn_root_dir, 'max_r.npy'))\nddqn_min_r = np.load(os.path.join(ddqn_root_dir, 'min_r.npy'))\nddqn_mean_r = np.load(os.path.join(ddqn_root_dir, 'mean_r.npy'))\n\nddqn_max_s = np.load(os.path.join(ddqn_root_dir, 'max_s.npy'))\nddqn_min_s = np.load(os.path.join(ddqn_root_dir, 'min_s.npy'))\nddqn_mean_s = np.load(os.path.join(ddqn_root_dir, 'mean_s.npy'))\n\nddqn_max_t = np.load(os.path.join(ddqn_root_dir, 'max_t.npy'))\nddqn_min_t = np.load(os.path.join(ddqn_root_dir, 'min_t.npy'))\nddqn_mean_t = np.load(os.path.join(ddqn_root_dir, 'mean_t.npy'))\n\nddqn_max_sec = np.load(os.path.join(ddqn_root_dir, 'max_sec.npy'))\nddqn_min_sec = np.load(os.path.join(ddqn_root_dir, 'min_sec.npy'))\nddqn_mean_sec = np.load(os.path.join(ddqn_root_dir, 'mean_sec.npy'))\n\nddqn_max_rt = np.load(os.path.join(ddqn_root_dir, 'max_rt.npy'))\nddqn_min_rt = np.load(os.path.join(ddqn_root_dir, 'min_rt.npy'))\nddqn_mean_rt = np.load(os.path.join(ddqn_root_dir, 'mean_rt.npy'))\n\n\ndueling_ddqn_max_t, dueling_ddqn_max_r, dueling_ddqn_max_s, \\\ndueling_ddqn_max_sec, dueling_ddqn_max_rt = np.max(dueling_ddqn_results, axis=0).T\ndueling_ddqn_min_t, dueling_ddqn_min_r, dueling_ddqn_min_s, \\\ndueling_ddqn_min_sec, dueling_ddqn_min_rt = np.min(dueling_ddqn_results, axis=0).T\ndueling_ddqn_mean_t, dueling_ddqn_mean_r, dueling_ddqn_mean_s, \\\ndueling_ddqn_mean_sec, dueling_ddqn_mean_rt = np.mean(dueling_ddqn_results, axis=0).T\ndueling_ddqn_x = np.arange(np.max(\n    (len(dueling_ddqn_mean_s), len(ddqn_mean_s))))\n\n\nfig, axs = plt.subplots(5, 1, figsize=(15,30), sharey=False, sharex=True)\n\n# DDQN\naxs[0].plot(ddqn_max_r, 'g', linewidth=1)\naxs[0].plot(ddqn_min_r, 'g', linewidth=1)\naxs[0].plot(ddqn_mean_r, 'g-.', label='DDQN', linewidth=2)\naxs[0].fill_between(ddqn_x, ddqn_min_r, ddqn_max_r, facecolor='g', alpha=0.3)\n\naxs[1].plot(ddqn_max_s, 'g', linewidth=1)\naxs[1].plot(ddqn_min_s, 'g', linewidth=1)\naxs[1].plot(ddqn_mean_s, 'g-.', label='DDQN', linewidth=2)\naxs[1].fill_between(ddqn_x, ddqn_min_s, ddqn_max_s, facecolor='g', alpha=0.3)\n\naxs[2].plot(ddqn_max_t, 'g', linewidth=1)\naxs[2].plot(ddqn_min_t, 'g', linewidth=1)\naxs[2].plot(ddqn_mean_t, 'g-.', label='DDQN', linewidth=2)\naxs[2].fill_between(ddqn_x, ddqn_min_t, ddqn_max_t, facecolor='g', alpha=0.3)\n\naxs[3].plot(ddqn_max_sec, 'g', linewidth=1)\naxs[3].plot(ddqn_min_sec, 'g', linewidth=1)\naxs[3].plot(ddqn_mean_sec, 'g-.', label='DDQN', linewidth=2)\naxs[3].fill_between(ddqn_x, ddqn_min_sec, ddqn_max_sec, facecolor='g', alpha=0.3)\n\naxs[4].plot(ddqn_max_rt, 'g', linewidth=1)\naxs[4].plot(ddqn_min_rt, 'g', linewidth=1)\naxs[4].plot(ddqn_mean_rt, 'g-.', label='DDQN', linewidth=2)\naxs[4].fill_between(ddqn_x, ddqn_min_rt, ddqn_max_rt, facecolor='g', alpha=0.3)\n\n# Dueling DDQN\naxs[0].plot(dueling_ddqn_max_r, 'r', linewidth=1)\naxs[0].plot(dueling_ddqn_min_r, 'r', linewidth=1)\naxs[0].plot(dueling_ddqn_mean_r, 'r:', label='Dueling DDQN', linewidth=2)\naxs[0].fill_between(\n    dueling_ddqn_x, dueling_ddqn_min_r, dueling_ddqn_max_r, facecolor='r', alpha=0.3)\n\naxs[1].plot(dueling_ddqn_max_s, 'r', linewidth=1)\naxs[1].plot(dueling_ddqn_min_s, 'r', linewidth=1)\naxs[1].plot(dueling_ddqn_mean_s, 'r:', label='Dueling DDQN', linewidth=2)\naxs[1].fill_between(\n    dueling_ddqn_x, dueling_ddqn_min_s, dueling_ddqn_max_s, facecolor='r', alpha=0.3)\n\naxs[2].plot(dueling_ddqn_max_t, 'r', linewidth=1)\naxs[2].plot(dueling_ddqn_min_t, 'r', linewidth=1)\naxs[2].plot(dueling_ddqn_mean_t, 'r:', label='Dueling DDQN', linewidth=2)\naxs[2].fill_between(\n    dueling_ddqn_x, dueling_ddqn_min_t, dueling_ddqn_max_t, facecolor='r', alpha=0.3)\n\naxs[3].plot(dueling_ddqn_max_sec, 'r', linewidth=1)\naxs[3].plot(dueling_ddqn_min_sec, 'r', linewidth=1)\naxs[3].plot(dueling_ddqn_mean_sec, 'r:', label='Dueling DDQN', linewidth=2)\naxs[3].fill_between(\n    dueling_ddqn_x, dueling_ddqn_min_sec, dueling_ddqn_max_sec, facecolor='r', alpha=0.3)\n\naxs[4].plot(dueling_ddqn_max_rt, 'r', linewidth=1)\naxs[4].plot(dueling_ddqn_min_rt, 'r', linewidth=1)\naxs[4].plot(dueling_ddqn_mean_rt, 'r:', label='Dueling DDQN', linewidth=2)\naxs[4].fill_between(\n    dueling_ddqn_x, dueling_ddqn_min_rt, dueling_ddqn_max_rt, facecolor='r', alpha=0.3)\n\n# ALL\naxs[0].set_title('Moving Avg Reward (Training)')\naxs[1].set_title('Moving Avg Reward (Evaluation)')\naxs[2].set_title('Total Steps')\naxs[3].set_title('Training Time')\naxs[4].set_title('Wall-clock Time')\nplt.xlabel('Episodes')\naxs[0].legend(loc='upper left')\nplt.show()\n\n\n\n\n\ndueling_ddqn_root_dir = os.path.join(RESULTS_DIR, 'dueling_ddqn')\nnot os.path.exists(dueling_ddqn_root_dir) and os.makedirs(dueling_ddqn_root_dir)\n\nnp.save(os.path.join(dueling_ddqn_root_dir, 'x'), dueling_ddqn_x)\n\nnp.save(os.path.join(dueling_ddqn_root_dir, 'max_r'), dueling_ddqn_max_r)\nnp.save(os.path.join(dueling_ddqn_root_dir, 'min_r'), dueling_ddqn_min_r)\nnp.save(os.path.join(dueling_ddqn_root_dir, 'mean_r'), dueling_ddqn_mean_r)\n\nnp.save(os.path.join(dueling_ddqn_root_dir, 'max_s'), dueling_ddqn_max_s)\nnp.save(os.path.join(dueling_ddqn_root_dir, 'min_s'), dueling_ddqn_min_s )\nnp.save(os.path.join(dueling_ddqn_root_dir, 'mean_s'), dueling_ddqn_mean_s)\n\nnp.save(os.path.join(dueling_ddqn_root_dir, 'max_t'), dueling_ddqn_max_t)\nnp.save(os.path.join(dueling_ddqn_root_dir, 'min_t'), dueling_ddqn_min_t)\nnp.save(os.path.join(dueling_ddqn_root_dir, 'mean_t'), dueling_ddqn_mean_t)\n\nnp.save(os.path.join(dueling_ddqn_root_dir, 'max_sec'), dueling_ddqn_max_sec)\nnp.save(os.path.join(dueling_ddqn_root_dir, 'min_sec'), dueling_ddqn_min_sec)\nnp.save(os.path.join(dueling_ddqn_root_dir, 'mean_sec'), dueling_ddqn_mean_sec)\n\nnp.save(os.path.join(dueling_ddqn_root_dir, 'max_rt'), dueling_ddqn_max_rt)\nnp.save(os.path.join(dueling_ddqn_root_dir, 'min_rt'), dueling_ddqn_min_rt)\nnp.save(os.path.join(dueling_ddqn_root_dir, 'mean_rt'), dueling_ddqn_mean_rt)\n\n\nenv = make_env_fn(**make_env_kargs, seed=123, monitor_mode='evaluation')\nstate = env.reset()\nimg = env.render(mode='rgb_array')\nenv.close()\ndel env\nprint(state)\n\n[ 0.02078762 -0.01301236 -0.0209893  -0.03935255]\n\n\n\nplt.imshow(img)\nplt.axis('off')\nplt.title(\"State s=\" + str(np.round(state,2)))\nplt.show()\n\n\n\n\n\nq_values = dueling_ddqn_agents[best_dueling_ddqn_agent_key].online_model(state).detach().cpu().numpy()[0]\nprint(q_values)\n\n[1850956.5 1843644. ]\n\n\n\nq_s = q_values\nv_s = q_values.mean()\na_s = q_values - q_values.mean()\n\n\nplt.bar(('Left (idx=0)','Right (idx=1)'), q_s)\nplt.xlabel('Action')\nplt.ylabel('Estimate')\nplt.title(\"Action-value function, Q(\" + str(np.round(state,2)) + \")\")\nplt.show()\n\n\n\n\n\nplt.bar('s='+str(np.round(state,2)), v_s, width=0.1)\nplt.xlabel('State')\nplt.ylabel('Estimate')\nplt.title(\"State-value function, V(\"+str(np.round(state,2))+\")\")\nplt.show()\n\n\n\n\n\nplt.bar(('Left (idx=0)','Right (idx=1)'), a_s)\nplt.xlabel('Action')\nplt.ylabel('Estimate')\nplt.title(\"Advantage function, (\" + str(np.round(state,2)) + \")\")\nplt.show()\n\n\n\n\n\nenv = make_env_fn(**make_env_kargs, seed=123, monitor_mode='evaluation')\n\nstate, states, imgs, t = env.reset(), [], [], False\nwhile not t:\n    states.append(state)\n    state, r, t, _ = env.step(0)\n    imgs.append(env.render(mode='rgb_array'))\n\nenv.close()\ndel env\n\n\nstates[-2]\n\narray([-0.09048686, -1.57504301,  0.13510693,  2.34025535])\n\n\n\nplt.imshow(imgs[-2])\nplt.axis('off')\nplt.title(\"State s=\" + str(np.round(state,2)))\nplt.show()\n\n\n\n\n\nq_values = dueling_ddqn_agents[best_dueling_ddqn_agent_key].online_model(state).detach().cpu().numpy()[0]\nprint(q_values)\n\n[683447.06 838859.2 ]\n\n\n\nq_s = q_values\nv_s = q_values.mean()\na_s = q_values - q_values.mean()\n\n\nplt.bar(('Left (idx=0)','Right (idx=1)'), q_s)\nplt.xlabel('Action')\nplt.ylabel('Estimate')\nplt.title(\"Action-value function, Q(\" + str(np.round(state,2)) + \")\")\nplt.show()\n\n\n\n\n\nplt.bar('s='+str(np.round(state,2)), v_s, width=0.1)\nplt.xlabel('State')\nplt.ylabel('Estimate')\nplt.title(\"State-value function, V(\"+str(np.round(state,2))+\")\")\nplt.show()\n\n\n\n\n\nplt.bar(('Left (idx=0)','Right (idx=1)'), a_s)\nplt.xlabel('Action')\nplt.ylabel('Estimate')\nplt.title(\"Advantage function, (\" + str(np.round(state,2)) + \")\")\nplt.show()\n\n\n\n\n\nenv = make_env_fn(**make_env_kargs, seed=123, monitor_mode='evaluation')\n\nstates = []\nfor agent in dueling_ddqn_agents.values():\n    for episode in range(100):\n        state, done = env.reset(), False\n        while not done:\n            states.append(state)\n            action = agent.evaluation_strategy.select_action(agent.online_model, state)\n            state, _, done, _ = env.step(action)\nenv.close()\ndel env\n\nx = np.array(states)[:,0]\nxd = np.array(states)[:,1]\na = np.array(states)[:,2]\nad = np.array(states)[:,3]\n\n\nparts = plt.violinplot((x, xd, a, ad), \n                       vert=False, showmeans=False, showmedians=False, showextrema=False)\n\ncolors = ['red','green','yellow','blue']\nfor i, pc in enumerate(parts['bodies']):\n    pc.set_facecolor(colors[i])\n    pc.set_edgecolor(colors[i])\n    pc.set_alpha(0.5)\n\nplt.yticks(range(1,5), [\"cart position\", \"cart velocity\", \"pole angle\", \"pole velocity\"])\nplt.yticks(rotation=45)\nplt.title('Range of state-variable values for ' + str(\n    dueling_ddqn_agents[best_dueling_ddqn_agent_key].__class__.__name__))\n\nplt.show()"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-10.html#prioritized-experience-replay-per",
    "href": "publication/GDRL/GDRL-chapter-10.html#prioritized-experience-replay-per",
    "title": "Chapter 10: Sample-Efficient Value-Based Methods",
    "section": "Prioritized Experience Replay (PER)",
    "text": "Prioritized Experience Replay (PER)\n\nclass PrioritizedReplayBuffer():\n    def __init__(self, \n                 max_samples=10000, \n                 batch_size=64, \n                 rank_based=False,\n                 alpha=0.6, \n                 beta0=0.1, \n                 beta_rate=0.99992):\n        self.max_samples = max_samples\n        self.memory = np.empty(shape=(self.max_samples, 2), dtype=np.ndarray)\n        self.batch_size = batch_size\n        self.n_entries = 0\n        self.next_index = 0\n        self.td_error_index = 0\n        self.sample_index = 1\n        self.rank_based = rank_based # if not rank_based, then proportional\n        self.alpha = alpha # how much prioritization to use 0 is uniform (no priority), 1 is full priority\n        self.beta = beta0 # bias correction 0 is no correction 1 is full correction\n        self.beta0 = beta0 # beta0 is just beta's initial value\n        self.beta_rate = beta_rate\n\n    def update(self, idxs, td_errors):\n        self.memory[idxs, self.td_error_index] = np.abs(td_errors)\n        if self.rank_based:\n            sorted_arg = self.memory[:self.n_entries, self.td_error_index].argsort()[::-1]\n            self.memory[:self.n_entries] = self.memory[sorted_arg]\n\n    def store(self, sample):\n        priority = 1.0\n        if self.n_entries &gt; 0:\n            priority = self.memory[\n                :self.n_entries, \n                self.td_error_index].max()\n        self.memory[self.next_index, \n                    self.td_error_index] = priority\n        self.memory[self.next_index, \n                    self.sample_index] = np.array(sample)\n        self.n_entries = min(self.n_entries + 1, self.max_samples)\n        self.next_index += 1\n        self.next_index = self.next_index % self.max_samples\n\n    def _update_beta(self):\n        self.beta = min(1.0, self.beta * self.beta_rate**-1)\n        return self.beta\n\n    def sample(self, batch_size=None):\n        batch_size = self.batch_size if batch_size == None else batch_size\n        self._update_beta()\n        entries = self.memory[:self.n_entries]\n\n        if self.rank_based:\n            priorities = 1/(np.arange(self.n_entries) + 1)\n        else: # proportional\n            priorities = entries[:, self.td_error_index] + EPS\n        scaled_priorities = priorities**self.alpha        \n        probs = np.array(scaled_priorities/np.sum(scaled_priorities), dtype=np.float64)\n\n        weights = (self.n_entries * probs)**-self.beta\n        normalized_weights = weights/weights.max()\n        idxs = np.random.choice(self.n_entries, batch_size, replace=False, p=probs)\n        samples = np.array([entries[idx] for idx in idxs])\n        \n        samples_stacks = [np.vstack(batch_type) for batch_type in np.vstack(samples[:, self.sample_index]).T]\n        idxs_stack = np.vstack(idxs)\n        weights_stack = np.vstack(normalized_weights[idxs])\n        return idxs_stack, weights_stack, samples_stacks\n\n    def __len__(self):\n        return self.n_entries\n    \n    def __repr__(self):\n        return str(self.memory[:self.n_entries])\n    \n    def __str__(self):\n        return str(self.memory[:self.n_entries])\n\n\nb = PrioritizedReplayBuffer()\nplt.plot([b._update_beta() for _ in range(100000)])\nplt.title('PER Beta')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\nclass PER():\n    def __init__(self, \n                 replay_buffer_fn, \n                 value_model_fn, \n                 value_optimizer_fn, \n                 value_optimizer_lr,\n                 max_gradient_norm,\n                 training_strategy_fn,\n                 evaluation_strategy_fn,\n                 n_warmup_batches,\n                 update_target_every_steps,\n                 tau):\n        self.replay_buffer_fn = replay_buffer_fn\n        self.value_model_fn = value_model_fn\n        self.value_optimizer_fn = value_optimizer_fn\n        self.value_optimizer_lr = value_optimizer_lr\n        self.max_gradient_norm = max_gradient_norm\n        self.training_strategy_fn = training_strategy_fn\n        self.evaluation_strategy_fn = evaluation_strategy_fn\n        self.n_warmup_batches = n_warmup_batches\n        self.update_target_every_steps = update_target_every_steps\n        self.tau = tau\n\n    def optimize_model(self, experiences):\n        idxs, weights, \\\n        (states, actions, rewards, next_states, is_terminals) = experiences\n        weights = self.online_model.numpy_float_to_device(weights)\n        batch_size = len(is_terminals)\n        \n        argmax_a_q_sp = self.online_model(next_states).max(1)[1]\n        q_sp = self.target_model(next_states).detach()\n        max_a_q_sp = q_sp[\n            np.arange(batch_size), argmax_a_q_sp].unsqueeze(1)\n        target_q_sa = rewards + (self.gamma * max_a_q_sp * (1 - is_terminals))\n        q_sa = self.online_model(states).gather(1, actions)\n\n        td_error = q_sa - target_q_sa\n        value_loss = (weights * td_error).pow(2).mul(0.5).mean()\n        self.value_optimizer.zero_grad()\n        value_loss.backward()        \n        torch.nn.utils.clip_grad_norm_(self.online_model.parameters(), \n                                       self.max_gradient_norm)\n        self.value_optimizer.step()\n\n        priorities = np.abs(td_error.detach().cpu().numpy())\n        self.replay_buffer.update(idxs, priorities)\n\n    def interaction_step(self, state, env):\n        action = self.training_strategy.select_action(self.online_model, state)\n        new_state, reward, is_terminal, info = env.step(action)\n        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n        is_failure = is_terminal and not is_truncated\n        experience = (state, action, reward, new_state, float(is_failure))\n\n        self.replay_buffer.store(experience)\n        self.episode_reward[-1] += reward\n        self.episode_timestep[-1] += 1\n        self.episode_exploration[-1] += int(self.training_strategy.exploratory_action_taken)\n        return new_state, is_terminal\n    \n    def update_network(self, tau=None):\n        tau = self.tau if tau is None else tau\n        for target, online in zip(self.target_model.parameters(), \n                                  self.online_model.parameters()):\n            target_ratio = (1.0 - self.tau) * target.data\n            online_ratio = self.tau * online.data\n            mixed_weights = target_ratio + online_ratio\n            target.data.copy_(mixed_weights)\n\n    def train(self, make_env_fn, make_env_kargs, seed, gamma, \n              max_minutes, max_episodes, goal_mean_100_reward):\n        training_start, last_debug_time = time.time(), float('-inf')\n\n        self.checkpoint_dir = tempfile.mkdtemp()\n        self.make_env_fn = make_env_fn\n        self.make_env_kargs = make_env_kargs\n        self.seed = seed\n        self.gamma = gamma\n        \n        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n    \n        nS, nA = env.observation_space.shape[0], env.action_space.n\n        self.episode_timestep = []\n        self.episode_reward = []\n        self.episode_seconds = []\n        self.evaluation_scores = []        \n        self.episode_exploration = []\n        \n        self.target_model = self.value_model_fn(nS, nA)\n        self.online_model = self.value_model_fn(nS, nA)\n        self.update_network(tau=1.0)\n\n        self.value_optimizer = self.value_optimizer_fn(self.online_model, \n                                                       self.value_optimizer_lr)\n\n        self.replay_buffer = self.replay_buffer_fn()\n        self.training_strategy = training_strategy_fn()\n        self.evaluation_strategy = evaluation_strategy_fn() \n                    \n        result = np.empty((max_episodes, 5))\n        result[:] = np.nan\n        training_time = 0\n        for episode in range(1, max_episodes + 1):\n            episode_start = time.time()\n            \n            state, is_terminal = env.reset(), False\n            self.episode_reward.append(0.0)\n            self.episode_timestep.append(0.0)\n            self.episode_exploration.append(0.0)\n\n            for step in count():\n                state, is_terminal = self.interaction_step(state, env)\n                \n                min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n                if len(self.replay_buffer) &gt; min_samples:\n                    experiences = self.replay_buffer.sample()\n                    idxs, weights, samples = experiences\n                    experiences = self.online_model.load(samples)\n                    experiences = (idxs, weights) + (experiences,)\n                    self.optimize_model(experiences)\n                \n                if np.sum(self.episode_timestep) % self.update_target_every_steps == 0:\n                    self.update_network()\n\n                if is_terminal:\n                    gc.collect()\n                    break\n            \n            # stats\n            episode_elapsed = time.time() - episode_start\n            self.episode_seconds.append(episode_elapsed)\n            training_time += episode_elapsed\n            evaluation_score, _ = self.evaluate(self.online_model, env)\n            self.save_checkpoint(episode-1, self.online_model)\n\n            total_step = int(np.sum(self.episode_timestep))\n            self.evaluation_scores.append(evaluation_score)\n            \n            mean_10_reward = np.mean(self.episode_reward[-10:])\n            std_10_reward = np.std(self.episode_reward[-10:])\n            mean_100_reward = np.mean(self.episode_reward[-100:])\n            std_100_reward = np.std(self.episode_reward[-100:])\n            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n            lst_100_exp_rat = np.array(\n                self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n            mean_100_exp_rat = np.mean(lst_100_exp_rat)\n            std_100_exp_rat = np.std(lst_100_exp_rat)\n            \n            wallclock_elapsed = time.time() - training_start\n            result[episode-1] = total_step, mean_100_reward, \\\n                mean_100_eval_score, training_time, wallclock_elapsed\n            \n            reached_debug_time = time.time() - last_debug_time &gt;= LEAVE_PRINT_EVERY_N_SECS\n            reached_max_minutes = wallclock_elapsed &gt;= max_minutes * 60\n            reached_max_episodes = episode &gt;= max_episodes\n            reached_goal_mean_reward = mean_100_eval_score &gt;= goal_mean_100_reward\n            training_is_over = reached_max_minutes or \\\n                               reached_max_episodes or \\\n                               reached_goal_mean_reward\n\n            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n            debug_message = 'el {}, ep {:04}, ts {:06}, '\n            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n            debug_message = debug_message.format(\n                elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n                mean_100_eval_score, std_100_eval_score)\n            print(debug_message, end='\\r', flush=True)\n            if reached_debug_time or training_is_over:\n                print(ERASE_LINE + debug_message, flush=True)\n                last_debug_time = time.time()\n            if training_is_over:\n                if reached_max_minutes: print(u'--&gt; reached_max_minutes \\u2715')\n                if reached_max_episodes: print(u'--&gt; reached_max_episodes \\u2715')\n                if reached_goal_mean_reward: print(u'--&gt; reached_goal_mean_reward \\u2713')\n                break\n                \n        final_eval_score, score_std = self.evaluate(self.online_model, env, n_episodes=100)\n        wallclock_time = time.time() - training_start\n        print('Training complete.')\n        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n              ' {:.2f}s wall-clock time.\\n'.format(\n                  final_eval_score, score_std, training_time, wallclock_time))\n        env.close() ; del env\n        self.get_cleaned_checkpoints()\n        return result, final_eval_score, training_time, wallclock_time\n    \n    def evaluate(self, eval_policy_model, eval_env, n_episodes=1):\n        rs = []\n        for _ in range(n_episodes):\n            s, d = eval_env.reset(), False\n            rs.append(0)\n            for _ in count():\n                a = self.evaluation_strategy.select_action(eval_policy_model, s)\n                s, r, d, _ = eval_env.step(a)\n                rs[-1] += r\n                if d: break\n        return np.mean(rs), np.std(rs)\n\n    def get_cleaned_checkpoints(self, n_checkpoints=5):\n        try: \n            return self.checkpoint_paths\n        except AttributeError:\n            self.checkpoint_paths = {}\n\n        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n        last_ep = max(paths_dic.keys())\n        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n\n        for idx, path in paths_dic.items():\n            if idx in checkpoint_idxs:\n                self.checkpoint_paths[idx] = path\n            else:\n                os.unlink(path)\n\n        return self.checkpoint_paths\n\n    def demo_last(self, title='Fully-trained {} Agent', n_episodes=3, max_n_videos=3):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        last_ep = max(checkpoint_paths.keys())\n        self.online_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n\n        self.evaluate(self.online_model, env, n_episodes=n_episodes)\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def demo_progression(self, title='{} Agent progression', max_n_videos=5):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        for i in sorted(checkpoint_paths.keys()):\n            self.online_model.load_state_dict(torch.load(checkpoint_paths[i]))\n            self.evaluate(self.online_model, env, n_episodes=1)\n\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            subtitle_eps=sorted(checkpoint_paths.keys()),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def save_checkpoint(self, episode_idx, model):\n        torch.save(model.state_dict(), \n                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))\n\n\nper_results = []\nbest_agent, best_eval_score = None, float('-inf')\nfor seed in SEEDS:\n    environment_settings = {\n        'env_name': 'CartPole-v1',\n        'gamma': 1.00,\n        'max_minutes': 30,\n        'max_episodes': 10000,\n        'goal_mean_100_reward': 475\n    }\n\n    value_model_fn = lambda nS, nA: FCDuelingQ(nS, nA, hidden_dims=(512,128))\n    value_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr=lr)\n    value_optimizer_lr = 0.0005\n    max_gradient_norm = float('inf')\n\n    training_strategy_fn = lambda: EGreedyExpStrategy(init_epsilon=1.0,  \n                                                      min_epsilon=0.3, \n                                                      decay_steps=20000)\n    evaluation_strategy_fn = lambda: GreedyStrategy()\n\n    # replay_buffer_fn = lambda: ReplayBuffer(max_size=10000, batch_size=64)\n    # replay_buffer_fn = lambda: PrioritizedReplayBuffer(\n    #     max_samples=10000, batch_size=64, rank_based=True, \n    #     alpha=0.6, beta0=0.1, beta_rate=0.99995)\n    replay_buffer_fn = lambda: PrioritizedReplayBuffer(\n        max_samples=20000, batch_size=64, rank_based=False,\n        alpha=0.6, beta0=0.1, beta_rate=0.99995)\n    n_warmup_batches = 5\n    update_target_every_steps = 1\n    tau = 0.1\n\n    env_name, gamma, max_minutes, \\\n    max_episodes, goal_mean_100_reward = environment_settings.values()\n    agent = PER(replay_buffer_fn, \n                value_model_fn, \n                value_optimizer_fn, \n                value_optimizer_lr,\n                max_gradient_norm,\n                training_strategy_fn,\n                evaluation_strategy_fn,\n                n_warmup_batches,\n                update_target_every_steps,\n                tau)\n\n    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n    result, final_eval_score, training_time, wallclock_time = agent.train(\n        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n    per_results.append(result)\n    if final_eval_score &gt; best_eval_score:\n        best_eval_score = final_eval_score\n        best_agent = agent\nper_results = np.array(per_results)\n\nel 00:00:00, ep 0000, ts 000016, ar 10 016.0±000.0, 100 016.0±000.0, ex 100 0.3±0.0, ev 009.0±000.0\nel 00:01:00, ep 0114, ts 008195, ar 10 201.3±077.3, 100 077.5±074.4, ex 100 0.4±0.1, ev 249.5±092.0\nel 00:02:01, ep 0147, ts 015589, ar 10 214.1±041.9, 100 139.9±098.4, ex 100 0.3±0.1, ev 274.4±088.5\nel 00:03:02, ep 0172, ts 021758, ar 10 331.9±141.8, 100 188.4±107.7, ex 100 0.2±0.1, ev 312.0±108.1\nel 00:04:02, ep 0185, ts 027603, ar 10 477.6±053.6, 100 237.3±133.0, ex 100 0.2±0.1, ev 345.2±119.8\nel 00:05:04, ep 0197, ts 033460, ar 10 500.0±000.0, 100 284.9±142.2, ex 100 0.2±0.0, ev 367.5±124.7\nel 00:06:09, ep 0210, ts 039626, ar 10 500.0±000.0, 100 321.6±149.6, ex 100 0.2±0.0, ev 389.6±126.9\nel 00:07:12, ep 0222, ts 045626, ar 10 500.0±000.0, 100 356.6±151.1, ex 100 0.2±0.0, ev 409.6±125.3\nel 00:08:12, ep 0234, ts 051599, ar 10 497.3±008.1, 100 386.5±148.6, ex 100 0.1±0.0, ev 435.8±112.1\nel 00:09:14, ep 0250, ts 057696, ar 10 434.3±133.0, 100 415.7±143.2, ex 100 0.2±0.0, ev 472.2±083.6\nel 00:09:29, ep 0253, ts 059187, ar 10 433.4±132.5, 100 422.4±141.5, ex 100 0.2±0.0, ev 476.0±080.4\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 535.72s training time, 589.28s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000034, ar 10 034.0±000.0, 100 034.0±000.0, ex 100 0.6±0.0, ev 008.0±000.0\nel 00:01:02, ep 0122, ts 008486, ar 10 198.0±084.3, 100 079.3±070.4, ex 100 0.4±0.1, ev 247.9±093.5\nel 00:02:02, ep 0158, ts 015616, ar 10 185.6±058.9, 100 135.0±088.8, ex 100 0.3±0.1, ev 255.4±091.9\nel 00:03:03, ep 0186, ts 021644, ar 10 228.0±051.7, 100 179.8±082.1, ex 100 0.2±0.1, ev 270.1±098.8\nel 00:04:06, ep 0199, ts 027497, ar 10 464.1±071.6, 100 228.1±114.9, ex 100 0.2±0.1, ev 303.3±115.3\nel 00:05:08, ep 0213, ts 033219, ar 10 418.5±146.4, 100 265.1±136.0, ex 100 0.2±0.0, ev 338.4±128.6\nel 00:06:08, ep 0249, ts 038526, ar 10 018.2±003.3, 100 246.4±181.8, ex 100 0.2±0.0, ev 309.2±191.7\nel 00:07:09, ep 0313, ts 043396, ar 10 053.2±034.6, 100 101.8±163.0, ex 100 0.2±0.1, ev 240.6±238.8\nel 00:08:09, ep 0524, ts 048111, ar 10 017.8±004.7, 100 018.3±004.4, ex 100 0.1±0.1, ev 019.1±003.1\nel 00:09:09, ep 0701, ts 053099, ar 10 024.6±006.0, 100 027.2±017.9, ex 100 0.1±0.1, ev 030.8±024.7\nel 00:10:09, ep 0931, ts 057875, ar 10 015.5±002.6, 100 015.6±003.3, ex 100 0.1±0.1, ev 015.0±002.0\nel 00:11:09, ep 1171, ts 062577, ar 10 040.2±017.0, 100 024.2±009.0, ex 100 0.1±0.1, ev 027.0±011.4\nel 00:12:09, ep 1303, ts 067680, ar 10 034.4±011.4, 100 041.0±020.5, ex 100 0.2±0.1, ev 054.0±016.7\nel 00:13:09, ep 1573, ts 072256, ar 10 011.5±002.2, 100 011.8±002.6, ex 100 0.1±0.1, ev 010.3±001.4\nel 00:14:09, ep 1940, ts 076402, ar 10 011.3±001.7, 100 011.4±002.7, ex 100 0.1±0.1, ev 009.4±000.7\nel 00:15:09, ep 2308, ts 080536, ar 10 011.7±002.5, 100 011.2±002.1, ex 100 0.1±0.1, ev 009.3±000.8\nel 00:16:09, ep 2675, ts 084680, ar 10 010.1±001.6, 100 011.4±002.6, ex 100 0.1±0.1, ev 009.3±000.7\nel 00:17:09, ep 3045, ts 088810, ar 10 010.9±002.3, 100 011.2±002.1, ex 100 0.1±0.1, ev 009.3±000.8\nel 00:18:10, ep 3382, ts 093083, ar 10 011.3±003.2, 100 015.8±011.8, ex 100 0.1±0.1, ev 013.6±008.3\nel 00:19:10, ep 3741, ts 097242, ar 10 012.6±003.7, 100 011.7±002.8, ex 100 0.1±0.1, ev 009.4±000.7\nel 00:20:10, ep 4106, ts 101354, ar 10 010.7±001.6, 100 011.4±002.3, ex 100 0.1±0.1, ev 009.5±000.7\nel 00:21:10, ep 4439, ts 105573, ar 10 016.1±002.6, 100 014.2±003.0, ex 100 0.1±0.1, ev 012.7±001.5\nel 00:22:10, ep 4573, ts 110637, ar 10 084.0±021.5, 100 045.4±027.2, ex 100 0.2±0.1, ev 052.0±049.0\nel 00:23:10, ep 4745, ts 115531, ar 10 027.4±005.5, 100 027.3±008.9, ex 100 0.2±0.1, ev 025.9±005.4\nel 00:24:11, ep 4856, ts 120737, ar 10 061.3±012.7, 100 048.9±016.5, ex 100 0.2±0.1, ev 051.9±019.3\nel 00:25:11, ep 4917, ts 126095, ar 10 088.6±028.7, 100 077.0±030.3, ex 100 0.1±0.0, ev 084.7±034.7\nel 00:26:11, ep 4981, ts 131018, ar 10 097.2±053.8, 100 083.2±037.1, ex 100 0.2±0.1, ev 103.6±039.7\nel 00:27:12, ep 5043, ts 136095, ar 10 067.0±023.8, 100 079.6±035.5, ex 100 0.2±0.0, ev 090.9±036.6\nel 00:28:12, ep 5113, ts 141154, ar 10 071.8±021.8, 100 073.7±019.7, ex 100 0.2±0.0, ev 078.1±014.0\nel 00:29:17, ep 5160, ts 146794, ar 10 277.7±133.1, 100 093.7±077.0, ex 100 0.2±0.0, ev 107.9±091.7\nel 00:30:02, ep 5170, ts 150923, ar 10 412.9±115.2, 100 127.0±127.6, ex 100 0.2±0.0, ev 142.8±138.0\n--&gt; reached_max_minutes ✕\nTraining complete.\nFinal evaluation score 467.15±82.72 in 1709.67s training time, 1820.96s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000012, ar 10 012.0±000.0, 100 012.0±000.0, ex 100 0.4±0.0, ev 009.0±000.0\nel 00:01:01, ep 0122, ts 008475, ar 10 172.6±071.1, 100 079.5±075.3, ex 100 0.4±0.1, ev 199.4±102.0\nel 00:02:01, ep 0154, ts 015513, ar 10 220.0±056.3, 100 138.7±103.9, ex 100 0.3±0.1, ev 258.6±094.6\nel 00:03:01, ep 0175, ts 021061, ar 10 297.5±062.6, 100 186.7±104.0, ex 100 0.2±0.1, ev 282.1±090.5\nel 00:04:02, ep 0197, ts 026345, ar 10 233.9±065.7, 100 219.9±091.3, ex 100 0.2±0.1, ev 301.6±084.4\nel 00:05:06, ep 0213, ts 031711, ar 10 401.5±080.4, 100 247.4±098.5, ex 100 0.2±0.0, ev 324.6±095.0\nel 00:06:07, ep 0225, ts 037142, ar 10 443.1±116.9, 100 283.5±114.6, ex 100 0.2±0.0, ev 345.8±107.3\nel 00:07:10, ep 0241, ts 042511, ar 10 414.3±144.6, 100 300.6±126.7, ex 100 0.2±0.0, ev 373.6±116.9\nel 00:08:10, ep 0254, ts 047785, ar 10 387.5±124.9, 100 322.7±132.8, ex 100 0.2±0.0, ev 398.0±118.3\nel 00:09:12, ep 0267, ts 053628, ar 10 435.5±108.0, 100 350.6±135.8, ex 100 0.2±0.0, ev 421.4±112.3\nel 00:10:15, ep 0280, ts 059545, ar 10 456.8±091.1, 100 372.7±138.6, ex 100 0.2±0.0, ev 449.5±096.9\nel 00:11:10, ep 0293, ts 064692, ar 10 406.2±150.4, 100 394.2±138.7, ex 100 0.2±0.0, ev 475.4±071.7\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 632.94s training time, 690.34s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000012, ar 10 012.0±000.0, 100 012.0±000.0, ex 100 0.2±0.0, ev 011.0±000.0\nel 00:01:00, ep 0131, ts 007898, ar 10 166.4±056.3, 100 071.7±058.6, ex 100 0.4±0.1, ev 212.9±086.7\nel 00:02:01, ep 0174, ts 015106, ar 10 214.9±084.9, 100 127.0±074.3, ex 100 0.3±0.1, ev 214.6±078.4\nel 00:03:03, ep 0195, ts 021079, ar 10 307.3±124.0, 100 174.3±102.1, ex 100 0.2±0.1, ev 259.2±117.2\nel 00:04:03, ep 0207, ts 026571, ar 10 494.5±016.5, 100 219.2±136.3, ex 100 0.2±0.1, ev 291.1±137.0\nel 00:05:06, ep 0219, ts 032133, ar 10 460.0±120.0, 100 261.9±152.7, ex 100 0.2±0.0, ev 326.6±146.2\nel 00:06:10, ep 0232, ts 037842, ar 10 436.4±138.2, 100 298.6±161.9, ex 100 0.2±0.0, ev 361.5±144.8\nel 00:07:13, ep 0244, ts 043290, ar 10 479.8±060.6, 100 337.3±159.9, ex 100 0.2±0.0, ev 396.6±137.7\nel 00:08:17, ep 0256, ts 049040, ar 10 500.0±000.0, 100 374.6±153.1, ex 100 0.2±0.0, ev 429.5±121.0\nel 00:09:21, ep 0268, ts 054925, ar 10 488.5±034.5, 100 412.8±135.5, ex 100 0.2±0.0, ev 463.3±091.1\nel 00:09:44, ep 0272, ts 056925, ar 10 500.0±000.0, 100 422.8±130.7, ex 100 0.2±0.0, ev 475.1±074.2\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 551.45s training time, 604.56s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000039, ar 10 039.0±000.0, 100 039.0±000.0, ex 100 0.4±0.0, ev 038.0±000.0\nel 00:01:00, ep 0121, ts 008058, ar 10 162.4±078.2, 100 075.3±065.9, ex 100 0.4±0.1, ev 216.7±089.3\nel 00:02:04, ep 0162, ts 015542, ar 10 243.1±131.8, 100 135.6±089.3, ex 100 0.3±0.1, ev 219.5±083.3\nel 00:03:08, ep 0182, ts 021733, ar 10 343.2±163.8, 100 188.8±117.0, ex 100 0.2±0.1, ev 274.5±127.8\nel 00:04:12, ep 0198, ts 027564, ar 10 407.8±161.2, 100 229.7±139.8, ex 100 0.2±0.0, ev 317.1±146.5\nel 00:05:15, ep 0211, ts 033310, ar 10 424.6±151.1, 100 268.8±157.8, ex 100 0.2±0.0, ev 345.7±155.8\nel 00:06:16, ep 0225, ts 038808, ar 10 414.1±137.0, 100 301.9±163.2, ex 100 0.2±0.0, ev 390.4±146.7\nel 00:07:19, ep 0237, ts 044502, ar 10 469.4±063.8, 100 340.3±161.9, ex 100 0.2±0.0, ev 430.2±123.4\nel 00:08:19, ep 0250, ts 050042, ar 10 404.0±146.7, 100 372.1±156.6, ex 100 0.2±0.0, ev 465.9±092.4\nel 00:08:35, ep 0253, ts 051542, ar 10 435.3±129.4, 100 382.8±152.7, ex 100 0.2±0.0, ev 475.2±078.2\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 483.03s training time, 535.03s wall-clock time.\n\n\n\n\nbest_agent.demo_progression()\n\nPER Agent progression\n        Episode 0\n        \n        Episode 63\n        \n        Episode 126\n        \n        Episode 189\n        \n        Episode 253\n        \n\n\n\nbest_agent.demo_last()\n\nFully-trained PER Agent\n        Trial 0\n        \n        Trial 1\n        \n        Trial 2\n        \n\n\n\nper_max_t, per_max_r, per_max_s, per_max_sec, per_max_rt = np.max(per_results, axis=0).T\nper_min_t, per_min_r, per_min_s, per_min_sec, per_min_rt = np.min(per_results, axis=0).T\nper_mean_t, per_mean_r, per_mean_s, per_mean_sec, per_mean_rt = np.mean(per_results, axis=0).T\nper_x = np.arange(np.max(\n    (len(per_mean_s), len(dueling_ddqn_mean_s))))\n\n\nfig, axs = plt.subplots(5, 1, figsize=(15,30), sharey=False, sharex=True)\n\n# Dueling DDQN\naxs[0].plot(dueling_ddqn_max_r, 'r', linewidth=1)\naxs[0].plot(dueling_ddqn_min_r, 'r', linewidth=1)\naxs[0].plot(dueling_ddqn_mean_r, 'r:', label='Dueling DDQN', linewidth=2)\naxs[0].fill_between(\n    dueling_ddqn_x, dueling_ddqn_min_r, dueling_ddqn_max_r, facecolor='r', alpha=0.3)\n\naxs[1].plot(dueling_ddqn_max_s, 'r', linewidth=1)\naxs[1].plot(dueling_ddqn_min_s, 'r', linewidth=1)\naxs[1].plot(dueling_ddqn_mean_s, 'r:', label='Dueling DDQN', linewidth=2)\naxs[1].fill_between(\n    dueling_ddqn_x, dueling_ddqn_min_s, dueling_ddqn_max_s, facecolor='r', alpha=0.3)\n\naxs[2].plot(dueling_ddqn_max_t, 'r', linewidth=1)\naxs[2].plot(dueling_ddqn_min_t, 'r', linewidth=1)\naxs[2].plot(dueling_ddqn_mean_t, 'r:', label='Dueling DDQN', linewidth=2)\naxs[2].fill_between(\n    dueling_ddqn_x, dueling_ddqn_min_t, dueling_ddqn_max_t, facecolor='r', alpha=0.3)\n\naxs[3].plot(dueling_ddqn_max_sec, 'r', linewidth=1)\naxs[3].plot(dueling_ddqn_min_sec, 'r', linewidth=1)\naxs[3].plot(dueling_ddqn_mean_sec, 'r:', label='Dueling DDQN', linewidth=2)\naxs[3].fill_between(\n    dueling_ddqn_x, dueling_ddqn_min_sec, dueling_ddqn_max_sec, facecolor='r', alpha=0.3)\n\naxs[4].plot(dueling_ddqn_max_rt, 'r', linewidth=1)\naxs[4].plot(dueling_ddqn_min_rt, 'r', linewidth=1)\naxs[4].plot(dueling_ddqn_mean_rt, 'r:', label='Dueling DDQN', linewidth=2)\naxs[4].fill_between(\n    dueling_ddqn_x, dueling_ddqn_min_rt, dueling_ddqn_max_rt, facecolor='r', alpha=0.3)\n\n# PER\naxs[0].plot(per_max_r, 'k', linewidth=1)\naxs[0].plot(per_min_r, 'k', linewidth=1)\naxs[0].plot(per_mean_r, 'k', label='PER', linewidth=2)\naxs[0].fill_between(per_x, per_min_r, per_max_r, facecolor='k', alpha=0.3)\n\naxs[1].plot(per_max_s, 'k', linewidth=1)\naxs[1].plot(per_min_s, 'k', linewidth=1)\naxs[1].plot(per_mean_s, 'k', label='PER', linewidth=2)\naxs[1].fill_between(per_x, per_min_s, per_max_s, facecolor='k', alpha=0.3)\n\naxs[2].plot(per_max_t, 'k', linewidth=1)\naxs[2].plot(per_min_t, 'k', linewidth=1)\naxs[2].plot(per_mean_t, 'k', label='PER', linewidth=2)\naxs[2].fill_between(per_x, per_min_t, per_max_t, facecolor='k', alpha=0.3)\n\naxs[3].plot(per_max_sec, 'k', linewidth=1)\naxs[3].plot(per_min_sec, 'k', linewidth=1)\naxs[3].plot(per_mean_sec, 'k', label='PER', linewidth=2)\naxs[3].fill_between(per_x, per_min_sec, per_max_sec, facecolor='k', alpha=0.3)\n\naxs[4].plot(per_max_rt, 'k', linewidth=1)\naxs[4].plot(per_min_rt, 'k', linewidth=1)\naxs[4].plot(per_mean_rt, 'k', label='PER', linewidth=2)\naxs[4].fill_between(per_x, per_min_rt, per_max_rt, facecolor='k', alpha=0.3)\n\n# ALL\naxs[0].set_title('Moving Avg Reward (Training)')\naxs[1].set_title('Moving Avg Reward (Evaluation)')\naxs[2].set_title('Total Steps')\naxs[3].set_title('Training Time')\naxs[4].set_title('Wall-clock Time')\nplt.xlabel('Episodes')\naxs[0].legend(loc='upper left')\nplt.show()\n\n\n\n\n\nper_root_dir = os.path.join(RESULTS_DIR, 'per')\nnot os.path.exists(per_root_dir) and os.makedirs(per_root_dir)\n\nnp.save(os.path.join(per_root_dir, 'x'), per_x)\n\nnp.save(os.path.join(per_root_dir, 'max_r'), per_max_r)\nnp.save(os.path.join(per_root_dir, 'min_r'), per_min_r)\nnp.save(os.path.join(per_root_dir, 'mean_r'), per_mean_r)\n\nnp.save(os.path.join(per_root_dir, 'max_s'), per_max_s)\nnp.save(os.path.join(per_root_dir, 'min_s'), per_min_s )\nnp.save(os.path.join(per_root_dir, 'mean_s'), per_mean_s)\n\nnp.save(os.path.join(per_root_dir, 'max_t'), per_max_t)\nnp.save(os.path.join(per_root_dir, 'min_t'), per_min_t)\nnp.save(os.path.join(per_root_dir, 'mean_t'), per_mean_t)\n\nnp.save(os.path.join(per_root_dir, 'max_sec'), per_max_sec)\nnp.save(os.path.join(per_root_dir, 'min_sec'), per_min_sec)\nnp.save(os.path.join(per_root_dir, 'mean_sec'), per_mean_sec)\n\nnp.save(os.path.join(per_root_dir, 'max_rt'), per_max_rt)\nnp.save(os.path.join(per_root_dir, 'min_rt'), per_min_rt)\nnp.save(os.path.join(per_root_dir, 'mean_rt'), per_mean_rt)"
  },
  {
    "objectID": "posts/Solving-Linear-systems-3-variable/en/Solving-Linear-Systems-3-variables.html",
    "href": "posts/Solving-Linear-systems-3-variable/en/Solving-Linear-Systems-3-variables.html",
    "title": "Solving Linear Systems: 3 Variables",
    "section": "",
    "text": "In this notebook you will use NumPy linear algebra package to find the solutions to the system of linear equations. And it will Perform row reduction to bring matrix into row echelon form. We also try to find the solution for the system of linear equations using row reduced matrix and evaluate the determinant of the matrix to see again the connection between matrix singularity and the number of solutions of the linear system. This posts summarized the lecture “Linear Algebra for Machine Learning and Data Science” from Coursera."
  },
  {
    "objectID": "posts/Solving-Linear-systems-3-variable/en/Solving-Linear-Systems-3-variables.html#tldr",
    "href": "posts/Solving-Linear-systems-3-variable/en/Solving-Linear-Systems-3-variables.html#tldr",
    "title": "Solving Linear Systems: 3 Variables",
    "section": "",
    "text": "In this notebook you will use NumPy linear algebra package to find the solutions to the system of linear equations. And it will Perform row reduction to bring matrix into row echelon form. We also try to find the solution for the system of linear equations using row reduced matrix and evaluate the determinant of the matrix to see again the connection between matrix singularity and the number of solutions of the linear system. This posts summarized the lecture “Linear Algebra for Machine Learning and Data Science” from Coursera."
  },
  {
    "objectID": "posts/Solving-Linear-systems-3-variable/en/Solving-Linear-Systems-3-variables.html#packages",
    "href": "posts/Solving-Linear-systems-3-variable/en/Solving-Linear-Systems-3-variables.html#packages",
    "title": "Solving Linear Systems: 3 Variables",
    "section": "Packages",
    "text": "Packages\nLoad the NumPy package to access its functions.\n\nimport numpy as np\n\nprint(\"NumPy version: {}\".format(np.__version__))\n\nNumPy version: 1.19.5"
  },
  {
    "objectID": "posts/Solving-Linear-systems-3-variable/en/Solving-Linear-Systems-3-variables.html#representing-and-solving-a-system-of-linear-equations-using-matrices",
    "href": "posts/Solving-Linear-systems-3-variable/en/Solving-Linear-Systems-3-variables.html#representing-and-solving-a-system-of-linear-equations-using-matrices",
    "title": "Solving Linear Systems: 3 Variables",
    "section": "Representing and Solving a System of Linear Equations using Matrices",
    "text": "Representing and Solving a System of Linear Equations using Matrices\n\nSystem of Linear Equations\nHere is a system of linear equations (or linear system) with three equations and three unknown variables:\n\\[\\begin{cases}\n4x_1-3x_2+x_3=-10, \\\\ 2x_1+x_2+3x_3=0, \\\\ -x_1+2x_2-5x_3=17, \\end{cases}\\tag{1}\\]\nTo solve this system of linear equations means to find such values of the variables \\(x_1\\), \\(x_2\\), \\(x_3\\), that all of its equations are simultaneously satisfied.\n\n\nSolving Systems of Linear Equations using Matrices\nLet’s prepare to solve the linear system \\((1)\\) using NumPy. \\(A\\) will be a matrix, each row will represent one equation in the system and each column will correspond to the variable \\(x_1\\), \\(x_2\\), \\(x_3\\). And \\(b\\) is a 1-D array of the free (right side) coefficients:\n\nA = np.array([\n        [4, -3, 1],\n        [2, 1, 3],\n        [-1, 2, -5]\n    ], dtype=np.dtype(float))\n\nb = np.array([-10, 0, 17], dtype=np.dtype(float))\n\nprint(\"Matrix A:\")\nprint(A)\nprint(\"\\nArray b:\")\nprint(b)\n\nMatrix A:\n[[ 4. -3.  1.]\n [ 2.  1.  3.]\n [-1.  2. -5.]]\n\nArray b:\n[-10.   0.  17.]\n\n\nCheck the dimensions of \\(A\\) and \\(b\\) using shape() function:\n\nprint(f\"Shape of A: {np.shape(A)}\")\nprint(f\"Shape of b: {np.shape(b)}\")\n\nShape of A: (3, 3)\nShape of b: (3,)\n\n\nNow use np.linalg.solve(A, b) function to find the solution of the system \\((1)\\). The result will be saved in the 1-D array \\(x\\). The elements will correspond to the values of \\(x_1\\), \\(x_2\\) and \\(x_3\\):\n\nx = np.linalg.solve(A, b)\n\nprint(f\"Solution: {x}\")\n\nSolution: [ 1.  4. -2.]\n\n\nTry to substitute those values of \\(x_1\\), \\(x_2\\) and \\(x_3\\) into the original system of equations to check its consistency.\n\n\nEvaluating the Determinant of a Matrix\nMatrix \\(A\\) corresponding to the linear system \\((1)\\) is a square matrix - it has the same number of rows and columns. In the case of a square matrix it is possible to calculate its determinant - a real number that characterizes some properties of the matrix. A linear system containing three equations with three unknown variables will have one solution if and only if the matrix \\(A\\) has a non-zero determinant.\nLet’s calculate the determinant using np.linalg.det(A) function:\n\nd = np.linalg.det(A)\n\nprint(f\"Determinant of matrix A: {d:.2f}\")\n\nDeterminant of matrix A: -60.00\n\n\nPlease, note that its value is non-zero, as expected."
  },
  {
    "objectID": "posts/Solving-Linear-systems-3-variable/en/Solving-Linear-Systems-3-variables.html#solving-system-of-linear-equations-using-row-reduction",
    "href": "posts/Solving-Linear-systems-3-variable/en/Solving-Linear-Systems-3-variables.html#solving-system-of-linear-equations-using-row-reduction",
    "title": "Solving Linear Systems: 3 Variables",
    "section": "Solving System of Linear Equations using Row Reduction",
    "text": "Solving System of Linear Equations using Row Reduction\n\nPreparation for Row Reduction\nYou can see how easy it is to use contemporary packages to solve linear equations. However, for a deeper understanding of mathematical concepts, it is important to practice some solution techniques manually. The programming approach can still help here to reduce the number of arithmetical calculations, and focus on the method itself.\nHere you can practice the row reduction method for the linear system with three variables. To apply it, first, unify matrix \\(A\\) and array \\(b\\) into one matrix using np.hstack() function. Note that the shape of the originally defined array \\(b\\) was \\((3,)\\), to stack it with the \\((3, 3)\\) matrix you need to transform it so that it has the same number of dimensions. You can use .reshape((3, 1)) function:\n\nA_system = np.hstack((A, b.reshape((3, 1))))\n\nprint(A_system)\n\n[[  4.  -3.   1. -10.]\n [  2.   1.   3.   0.]\n [ -1.   2.  -5.  17.]]\n\n\n\n\nFunctions for Elementary Operations\nLet’s review elementary operations, which do not change the solution set of a linear system:\n\nMultiply any row by a non-zero number\nAdd two rows and exchange one of the original rows with the result of the addition\nSwap rows\n\nIn the case of larger systems you will need to apply elementary operations multiple times. Thus, it is convenient to define the corresponding Python functions. Investigate them in the following code cells with the examples:\n\n# exchange row_num of the matrix M with its multiple by row_num_multiple\n# Note: for simplicity, you can drop check if  row_num_multiple has non-zero value, which makes the operation valid\ndef MultiplyRow(M, row_num, row_num_multiple):\n    # .copy() function is required here to keep the original matrix without any changes\n    M_new = M.copy()\n    M_new[row_num] = M_new[row_num] * row_num_multiple\n    return M_new\n\nprint(\"Original matrix:\")\nprint(A_system)\nprint(\"\\nMatrix after its third row is multiplied by 2:\")\n# remember that indexing in Python starts from 0, thus index 2 will correspond to the third row\nprint(MultiplyRow(A_system, 2, 2))\n\nOriginal matrix:\n[[  4.  -3.   1. -10.]\n [  2.   1.   3.   0.]\n [ -1.   2.  -5.  17.]]\n\nMatrix after its third row is multiplied by 2:\n[[  4.  -3.   1. -10.]\n [  2.   1.   3.   0.]\n [ -2.   4. -10.  34.]]\n\n\n\n# multiply row_num_1 by row_num_1_multiple and add it to the row_num_2, \n# exchanging row_num_2 of the matrix M in the result\ndef AddRows(M, row_num_1, row_num_2, row_num_1_multiple):\n    M_new = M.copy()\n    M_new[row_num_2] = row_num_1_multiple * M_new[row_num_1] + M_new[row_num_2]\n    return M_new\n\nprint(\"Original matrix:\")\nprint(A_system)\nprint(\"\\nMatrix after exchange of the third row with the sum of itself and second row multiplied by 1/2:\")\nprint(AddRows(A_system, 1, 2, 1/2))\n\nOriginal matrix:\n[[  4.  -3.   1. -10.]\n [  2.   1.   3.   0.]\n [ -1.   2.  -5.  17.]]\n\nMatrix after exchange of the third row with the sum of itself and second row multiplied by 1/2:\n[[  4.   -3.    1.  -10. ]\n [  2.    1.    3.    0. ]\n [  0.    2.5  -3.5  17. ]]\n\n\n\n# exchange row_num_1 and row_num_2 of the matrix M\ndef SwapRows(M, row_num_1, row_num_2):\n    M_new = M.copy()\n    M_new[[row_num_1, row_num_2]] = M_new[[row_num_2, row_num_1]]\n    return M_new\n\nprint(\"Original matrix:\")\nprint(A_system)\nprint(\"\\nMatrix after exchange its first and third rows:\")\nprint(SwapRows(A_system, 0, 2))\n\nOriginal matrix:\n[[  4.  -3.   1. -10.]\n [  2.   1.   3.   0.]\n [ -1.   2.  -5.  17.]]\n\nMatrix after exchange its first and third rows:\n[[ -1.   2.  -5.  17.]\n [  2.   1.   3.   0.]\n [  4.  -3.   1. -10.]]\n\n\n\n\nRow Reduction and Solution of the Linear System\nNow you can use the defined operations to bring the matrix into row reduced form. To do this manually, it is convenient to have \\(1\\) or \\(-1\\) value in the first element of the first row (the arithmetics of operations is easier then). Performing calculations in Python, won’t provide much of a benefit, but it is better to do that for illustration purposes. So, let’s swap the first and third rows:\n\nA_ref = SwapRows(A_system, 0, 2)\n# Note: ref is an abbreviation of the row echelon form (row reduced form)\nprint(A_ref)\n\n[[ -1.   2.  -5.  17.]\n [  2.   1.   3.   0.]\n [  4.  -3.   1. -10.]]\n\n\nNow you would need to make such elementary operations, that the first elements in the second and third row become equal to zero:\n\n# multiply row 0 of the new matrix A_ref by 2 and add it to the row 1\nA_ref = AddRows(A_ref, 0, 1, 2)\nprint(A_ref)\n\n[[ -1.   2.  -5.  17.]\n [  0.   5.  -7.  34.]\n [  4.  -3.   1. -10.]]\n\n\n\n# multiply row 0 of the new matrix A_ref by 4 and add it to the row 2\nA_ref = AddRows(A_ref, 0, 2, 4)\nprint(A_ref)\n\n[[ -1.   2.  -5.  17.]\n [  0.   5.  -7.  34.]\n [  0.   5. -19.  58.]]\n\n\nThe next step will be to perform an operation by putting the second element in the third row equal to zero:\n\n# multiply row 1 of the new matrix A_ref by -1 and add it to the row 2\nA_ref = AddRows(A_ref, 1, 2, -1)\nprint(A_ref)\n\n[[ -1.   2.  -5.  17.]\n [  0.   5.  -7.  34.]\n [  0.   0. -12.  24.]]\n\n\nIt is easy now to find the value of \\(x_3\\) from the third row, as it corresponds to the equation \\(-12x_3=24\\). Let’s divide the row by -12:\n\n# multiply row 2 of the new matrix A_ref by -1/12\nA_ref = MultiplyRow(A_ref, 2, -1/12)\nprint(A_ref)\n\n[[-1.  2. -5. 17.]\n [ 0.  5. -7. 34.]\n [-0. -0.  1. -2.]]\n\n\nNow the second row of the matrix corresponds to the equation \\(5x_2-7x_3=34\\) and the first row to the equation \\(-x_1+2x_2-5x_3=17\\). Referring to the elements of the matrix, you can find the values of \\(x_2\\) and \\(x_1\\):\n\nx_3 = -2\nx_2 = (A_ref[1,3] - A_ref[1,2] * x_3) / A_ref[1,1]\nx_1 = (A_ref[0,3] - A_ref[0,2] * x_3 - A_ref[0,1] * x_2) / A_ref[0,0]\n\nprint(x_1, x_2, x_3)\n\n1.0 4.0 -2"
  },
  {
    "objectID": "posts/Solving-Linear-systems-3-variable/en/Solving-Linear-Systems-3-variables.html#system-of-linear-equations-with-no-solutions",
    "href": "posts/Solving-Linear-systems-3-variable/en/Solving-Linear-Systems-3-variables.html#system-of-linear-equations-with-no-solutions",
    "title": "Solving Linear Systems: 3 Variables",
    "section": "System of Linear Equations with No Solutions",
    "text": "System of Linear Equations with No Solutions\nGiven another system of linear equations:\n\\[\\begin{cases}\nx_1+x_2+x_3=2, \\\\ x_2-3x_3=1, \\\\ 2x_1+x_2+5x_3=0, \\end{cases}\\tag{2}\\]\nlet’s find the determinant of the corresponding matrix.\n\nA_2= np.array([\n        [1, 1, 1],\n        [0, 1, -3],\n        [2, 1, 5]\n    ], dtype=np.dtype(float))\n\nb_2 = np.array([2, 1, 0], dtype=np.dtype(float))\n\nd_2 = np.linalg.det(A_2)\n\nprint(f\"Determinant of matrix A_2: {d_2:.2f}\")\n\nDeterminant of matrix A_2: 0.00\n\n\nIt is equal to zero, thus the system cannot have one unique solution. It will have either infinitely many solutions or none. The consistency of it will depend on the free coefficients (right side coefficients). You can uncomment and run the code in the following cell to check that the np.linalg.solve() function will give an error due to singularity.\n\ntry:\n    x_2 = np.linalg.solve(A_2, b_2)\nexcept np.linalg.LinAlgError as err:\n    print(err)\n\nSingular matrix\n\n\nYou can check the system for consistency using ranks, but this is out of scope here (you can review this topic following the link). For now you can perform elementary operations to see that this particular system has no solutions:\n\nA_2_system = np.hstack((A_2, b_2.reshape((3, 1))))\nprint(A_2_system)\n\n[[ 1.  1.  1.  2.]\n [ 0.  1. -3.  1.]\n [ 2.  1.  5.  0.]]\n\n\n\n# multiply row 0 by -2 and add it to the row 1\nA_2_ref = AddRows(A_2_system, 0, 2, -2)\nprint(A_2_ref)\n\n[[ 1.  1.  1.  2.]\n [ 0.  1. -3.  1.]\n [ 0. -1.  3. -4.]]\n\n\n\n# add row 1 of the new matrix A_2_ref to the row 2\nA_2_ref = AddRows(A_2_ref, 1, 2, 1)\nprint(A_2_ref)\n\n[[ 1.  1.  1.  2.]\n [ 0.  1. -3.  1.]\n [ 0.  0.  0. -3.]]\n\n\nThe last row will correspond to the equation \\(0=-3\\) which has no solution. Thus the whole linear system \\((2)\\) has no solution."
  },
  {
    "objectID": "posts/Solving-Linear-systems-3-variable/en/Solving-Linear-Systems-3-variables.html#system-of-linear-equations-with-infinite-number-of-solutions",
    "href": "posts/Solving-Linear-systems-3-variable/en/Solving-Linear-Systems-3-variables.html#system-of-linear-equations-with-infinite-number-of-solutions",
    "title": "Solving Linear Systems: 3 Variables",
    "section": "System of Linear Equations with Infinite Number of Solutions",
    "text": "System of Linear Equations with Infinite Number of Solutions\nYou can bring system \\((2)\\) to consistency by changing only the free coefficients:\n\\[\\begin{cases}\nx_1+x_2+x_3=2, \\\\ x_2-3x_3=1, \\\\ 2x_1+x_2+5x_3=3. \\end{cases}\\tag{3}\\]\nDefine the new array of free coefficients:\n\nb_3 = np.array([2, 1, 3])\n\nPrepare the new matrix, corresponding to the system \\((3)\\):\n\nA_3_system = np.hstack((A_2, b_3.reshape((3, 1))))\nprint(A_3_system)\n\n[[ 1.  1.  1.  2.]\n [ 0.  1. -3.  1.]\n [ 2.  1.  5.  3.]]\n\n\n\n# multiply row 0 of the new matrix A_3_system by -2 and add it to the row 2\nA_3_ref = AddRows(A_3_system, 0, 2, -2)\nprint(A_3_ref)\n\n[[ 1.  1.  1.  2.]\n [ 0.  1. -3.  1.]\n [ 0. -1.  3. -1.]]\n\n\n\n# add row 1 of the new matrix A_3_ref to the row 2\nA_3_ref = AddRows(A_3_ref, 1, 2, 1)\nprint(A_3_ref)\n\n[[ 1.  1.  1.  2.]\n [ 0.  1. -3.  1.]\n [ 0.  0.  0.  0.]]\n\n\nThus from the corresponding linear system\n\\[\\begin{cases}\nx_1+x_2+x_3=2, \\\\ x_2-3x_3=1, \\\\ 0=0, \\end{cases}\\tag{4}\\]\nyou can find that \\(x_2=1+3x_3\\), substitute it into the first equation and find \\(x_1\\). Thus the solutions of the linear system \\((3)\\) are:\n\\[\\begin{cases}\nx_1=1-4x_3, \\\\ x_2=1+3x_3, \\end{cases}\\tag{5}\\]\nwhere \\(x_3\\) is any real number."
  },
  {
    "objectID": "posts/Principal-component-analysis/en/principal-component-analysis.html",
    "href": "posts/Principal-component-analysis/en/principal-component-analysis.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "Principal Component Analysis (PCA for short) is one of dimensionality reduction approaches. It can reduce dimensionality of data with low value while maintaining the important features. In this notebook, we will derive its process with simple example. Also, we will follow up another dimensionality reduction method called T-SNE (student T-distribution Stochastic Neighbor Embedding), and used it for MNIST dataset."
  },
  {
    "objectID": "posts/Principal-component-analysis/en/principal-component-analysis.html#tldr",
    "href": "posts/Principal-component-analysis/en/principal-component-analysis.html#tldr",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "Principal Component Analysis (PCA for short) is one of dimensionality reduction approaches. It can reduce dimensionality of data with low value while maintaining the important features. In this notebook, we will derive its process with simple example. Also, we will follow up another dimensionality reduction method called T-SNE (student T-distribution Stochastic Neighbor Embedding), and used it for MNIST dataset."
  },
  {
    "objectID": "posts/Principal-component-analysis/en/principal-component-analysis.html#packages",
    "href": "posts/Principal-component-analysis/en/principal-component-analysis.html#packages",
    "title": "Dimensionality Reduction",
    "section": "Packages",
    "text": "Packages\n\nimport numpy as np\nimport os\nimport sklearn\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Principal-component-analysis/en/principal-component-analysis.html#dataset",
    "href": "posts/Principal-component-analysis/en/principal-component-analysis.html#dataset",
    "title": "Dimensionality Reduction",
    "section": "Dataset",
    "text": "Dataset\nIn this example, we will build simple 2D data.\n\nnum_samples = 6\nnum_variables = 2\n\nX = np.empty((num_samples, num_variables))\nX.shape\n\n(6, 2)\n\n\n\nX[:, 0] = [10, 11, 8, 3, 2, 1]\nX[:, 1] = [6, 4, 5, 3, 4, 1]\n\nX\n\narray([[10.,  6.],\n       [11.,  4.],\n       [ 8.,  5.],\n       [ 3.,  3.],\n       [ 2.,  4.],\n       [ 1.,  1.]])\n\n\n\nplt.plot(X[:, 0], X[:, 1], 'ro')\n\nx_axis = np.linspace(-15, 15, 256)\ny_axis = np.linspace(-15, 15, 256)\n\nplt.plot(x_axis, [0 for i in range(256)], color='black')\nplt.plot([0 for i in range(256)], y_axis, color='black')\nplt.show()\n\n\n\n\nIn order to handle PCA, all data points need to be centered. To do this, we subtract data’s mean to all points.\n\nX_centered = X - X.mean(axis=0)\n\n\nplt.plot(X_centered[:, 0], X_centered[:, 1], 'ro')\n\nx_axis = np.linspace(-15, 15, 256)\ny_axis = np.linspace(-15, 15, 256)\n\nplt.plot(x_axis, [0 for i in range(256)], color='black')\nplt.plot([0 for i in range(256)], y_axis, color='black')\nplt.show()"
  },
  {
    "objectID": "posts/Principal-component-analysis/en/principal-component-analysis.html#pca-using-singular-value-decomposition",
    "href": "posts/Principal-component-analysis/en/principal-component-analysis.html#pca-using-singular-value-decomposition",
    "title": "Dimensionality Reduction",
    "section": "PCA using Singular Value Decomposition",
    "text": "PCA using Singular Value Decomposition\nAt first, we can apply Singular Value Decomposition (SVD for short) for PCA. As you can see, SVD requires \\(U\\), \\(\\Sigma\\), and \\(V^T\\). And we derive them from numpy’s svd method.\n\nU, s, Vt = np.linalg.svd(X_centered)\n\n\nX_centered\n\narray([[ 4.16666667,  2.16666667],\n       [ 5.16666667,  0.16666667],\n       [ 2.16666667,  1.16666667],\n       [-2.83333333, -0.83333333],\n       [-3.83333333,  0.16666667],\n       [-4.83333333, -2.83333333]])\n\n\n\nU\n\narray([[-0.45438817,  0.33204035, -0.30279191,  0.25107087,  0.06884814,\n         0.7237548 ],\n       [-0.48932622, -0.56161568,  0.03612313,  0.16710266,  0.62674286,\n        -0.15202922],\n       [-0.23745828,  0.18811092,  0.9395002 ,  0.03250777, -0.04314116,\n         0.15049689],\n       [ 0.29016107,  0.02141147,  0.05508422,  0.94555589, -0.04078069,\n        -0.12874322],\n       [ 0.35450988,  0.5288192 ,  0.02960519, -0.09596758,  0.76313937,\n        -0.04695799],\n       [ 0.53650172, -0.50876627,  0.1429714 , -0.06853934,  0.12863667,\n         0.64158938]])\n\n\n\ns\n\narray([10.17664827,  2.47032319])\n\n\n\nVt\n\narray([[-0.95415813, -0.29930295],\n       [-0.29930295,  0.95415813]])\n\n\nBut s from numpy.linalg.svd is the vectors with singular value, and we cannot use it for dot product. So we need to recover its dimension for dot product.\n\nS = np.zeros((6, 2))\nS[:2, :2] = np.diag(s)\n\nS\n\narray([[10.17664827,  0.        ],\n       [ 0.        ,  2.47032319],\n       [ 0.        ,  0.        ],\n       [ 0.        ,  0.        ],\n       [ 0.        ,  0.        ],\n       [ 0.        ,  0.        ]])\n\n\n\nnp.allclose(X_centered, U.dot(S).dot(Vt))\n\nTrue\n\n\n\nSingular Value Decomposition Compressing\nIn this notebook, SVD is used for PCA. But usually, it can use for data compression.\n\nm, n = X.shape\n\npruned_U = U[:m, :n]\n\npruned_U\n\narray([[-0.45438817,  0.33204035],\n       [-0.48932622, -0.56161568],\n       [-0.23745828,  0.18811092],\n       [ 0.29016107,  0.02141147],\n       [ 0.35450988,  0.5288192 ],\n       [ 0.53650172, -0.50876627]])\n\n\n\npruned_S = np.zeros((n, n))\npruned_S[:n, :n] = np.diag(s)\n\npruned_S\n\narray([[10.17664827,  0.        ],\n       [ 0.        ,  2.47032319]])\n\n\nAs you can see the dimension of \\(U\\) and \\(\\Sigma\\) is reduced from original one, but after SVD, its output is same.\n\nX2D = pruned_U.dot(pruned_S).dot(Vt)\n\nX2D\n\narray([[ 4.16666667,  2.16666667],\n       [ 5.16666667,  0.16666667],\n       [ 2.16666667,  1.16666667],\n       [-2.83333333, -0.83333333],\n       [-3.83333333,  0.16666667],\n       [-4.83333333, -2.83333333]])\n\n\n\nnp.allclose(X_centered, X2D)\n\nTrue"
  },
  {
    "objectID": "posts/Principal-component-analysis/en/principal-component-analysis.html#pricipal-components-analysis",
    "href": "posts/Principal-component-analysis/en/principal-component-analysis.html#pricipal-components-analysis",
    "title": "Dimensionality Reduction",
    "section": "Pricipal Components Analysis",
    "text": "Pricipal Components Analysis\nUsing this property, we can reduce the dimension from [6,2]*[2,1] = [6,1].\n\nW1 = Vt.T[:, 0]\n\nW1\n\narray([-0.95415813, -0.29930295])\n\n\n\nPC1 = X_centered.dot(W1)\n\nPC1\n\narray([-4.62414862, -4.97970085, -2.4165294 ,  2.95286717,  3.60772235,\n        5.45978934])\n\n\n\nplt.plot(PC1, [0 for i in range(0,6)], 'ro')\n\nplt.plot(x_axis, [0 for i in range(0,256)], color='black')\nplt.plot([0 for i in range(0,256)], y_axis, color='black')\nplt.show()\n\n\n\n\n\nW2 = Vt.T[:, 1]\nPC2 = X_centered.dot(W2)\n\nPC2\n\narray([ 0.82024699, -1.38737223,  0.46469476,  0.05289325,  1.30635434,\n       -1.25681711])\n\n\n\nplt.plot(PC2, [0 for i in range(0,6)], 'ro')\n\nplt.plot(x_axis, [0 for i in range(0,256)], color='black')\nplt.plot([0 for i in range(0,256)], y_axis, color='black')\nplt.show()\n\n\n\n\n\nPC1_variation = np.sum(PC1**2)/(6-1)\nPC2_variation = np.sum(PC2**2)/(6-1)\n\ntotal = PC1_variation + PC2_variation\n\nprint(\"PC1 : \", round((PC1_variation/total)*100, 2), \"%\")\nprint(\"PC2 : \", round((PC2_variation/total)*100, 2), \"%\")\n\nPC1 :  94.44 %\nPC2 :  5.56 %\n\n\nThrough this result, We can find out that PC1 contains more information for describing this data."
  },
  {
    "objectID": "posts/Principal-component-analysis/en/principal-component-analysis.html#mnist-with-pca-dimentionality-reduction",
    "href": "posts/Principal-component-analysis/en/principal-component-analysis.html#mnist-with-pca-dimentionality-reduction",
    "title": "Dimensionality Reduction",
    "section": "MNIST with PCA dimentionality reduction",
    "text": "MNIST with PCA dimentionality reduction\nNow, let’s apply PCA into MNIST dataset. As you already know, MNIST consists of grayscale 10 digit images that has 28 x 28 shape.\n\nPackages\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\n\nimport pandas as pd\nimport seaborn as sns\n\n\n\nDataset\n\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n\nprint(X_train.shape)\nprint(y_train.shape)\n\n(60000, 28, 28)\n(60000,)\n\n\n\nplt.imshow(X_train[0], cmap='binary')\nplt.title(y_train[0])\nplt.show()\n\n\n\n\n\n\nPCA from sklearn.decomposition\n\npca = PCA()\n\n\n\nReasonable choice of dimension\nEach image has 28 x 28 shape. That is 784 features to explain the image. In this case, how can we find the reasonable dimension for explain this 10 digit data? We can find it through explained_variance_ratio_. The lower its value, it can represent its information.\n\npca.n_components = 784\n\npca_data = pca.fit_transform(X_train.reshape(-1, 784))\n\n\nplt.grid()\nplt.plot(pca.explained_variance_ratio_)\nplt.xlabel(\"Principal Component\")\nplt.ylabel(\"Explained Variance\")\nplt.show()\n\n\n\n\nAs you can see, the value of nearly 200 can represent its information. So we can choose the target dimension by 200.\n\npca.n_components = 200\npca_data = pca.fit_transform(X_train.reshape(-1, 784))\n\n\nprint(\"Original : \", X_train.reshape(-1, 784).shape)\nprint(\"After PCA : \", pca_data.shape)\n\nOriginal :  (60000, 784)\nAfter PCA :  (60000, 200)\n\n\n\n\nVisualization\nWe extract first 2 features and display it.\n\n## Stack PC1, PC2, and labels\npca_data_label = np.vstack((pca_data[:1000, :2].T, y_train[:1000])).T\n\nprint(pca_data_label)\n\n[[ 123.93258866 -312.67426203    5.        ]\n [1011.71837587 -294.85703827    0.        ]\n [ -51.84960805  392.17315286    4.        ]\n ...\n [ 524.40668703   14.70697414    0.        ]\n [ 255.49511737 -744.52137464    3.        ]\n [-306.59035849 -149.58107313    6.        ]]\n\n\n\npca_df = pd.DataFrame(pca_data_label, columns=['PC1', 'PC2', 'label'])\n\n\npca_df.head()\n\n\n\n\n\n\n\n\nPC1\nPC2\nlabel\n\n\n\n\n0\n123.932589\n-312.674262\n5.0\n\n\n1\n1011.718376\n-294.857038\n0.0\n\n\n2\n-51.849608\n392.173153\n4.0\n\n\n3\n-799.127037\n-607.197217\n1.0\n\n\n4\n-382.754942\n730.542867\n9.0\n\n\n\n\n\n\n\n\nax = sns.FacetGrid(pca_df, hue='label', height=6).map(plt.scatter, 'PC1', 'PC2')\nax.add_legend(label_order=sorted(ax._legend_data.keys()))\nplt.show()\n\n\n\n\n\n\nPCA with normalization\nOr you can use normalized data with PCA.\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X_train.reshape(-1, 784))\n\n\nmu = np.mean(X_scaled)\nX_scaled -= mu\n\n\npca_data_scaled = pca.fit_transform(X_scaled)\n\n\npca_data_label = np.vstack((pca_data_scaled[:1000, :2].T, y_train[:1000])).T\npca_df_scaled = pd.DataFrame(pca_data_label, columns=['PC1', 'PC2', 'label'])\n\n\nax = sns.FacetGrid(pca_df_scaled, hue='label', height=6).map(plt.scatter, 'PC1', 'PC2')\nax.add_legend(label_order=sorted(ax._legend_data.keys()))\nplt.show()"
  },
  {
    "objectID": "posts/Principal-component-analysis/en/principal-component-analysis.html#t-sne",
    "href": "posts/Principal-component-analysis/en/principal-component-analysis.html#t-sne",
    "title": "Dimensionality Reduction",
    "section": "T-SNE",
    "text": "T-SNE\n\nt-Distributed Stochastic Neighbor Embedding\n\nDimensionality reduction method\nEspecially visualization\n\nDeveloper: Geoffrey Hinton & Laurens van der Maaten\nKey idea\n\nMove the data to new reduced space\nNearby data in the original space should be located nearby in the new space\n\nIdea (specifically)\n\nCompute the distribution of nearby dataset in the original space\nCompute the distribution of the dataset in the new space\nMinimize the distance of the distributions"
  },
  {
    "objectID": "posts/Principal-component-analysis/en/principal-component-analysis.html#t-sne-vs-pca",
    "href": "posts/Principal-component-analysis/en/principal-component-analysis.html#t-sne-vs-pca",
    "title": "Dimensionality Reduction",
    "section": "t-SNE vs PCA",
    "text": "t-SNE vs PCA\n\nPCA tries to find a global structure\n\nLow dimensional subspace\nCan lead to local inconsistencies\nFar away point can become nearest neighbors\n\nt-SNE tries to preserve local structure\n\nLow dimensional neighborhood should be the same as original neighborhood\nUnlike PCA almost only used for visualization\nNo easy way to embed new points\n\n\n\nfrom sklearn.manifold import TSNE\n\n\nX_transformed = TSNE(n_components=2).fit_transform(X_scaled)\n\n\npca_data_label = np.vstack((X_transformed[:1000, :2].T, y_train[:1000])).T\npca_df_transformed = pd.DataFrame(pca_data_label, columns=['TSNE1', 'TSNE2', 'label'])\n\n\nax = sns.FacetGrid(pca_df_transformed, hue='label', height=6).map(plt.scatter, 'TSNE1', 'TSNE2')\nax.add_legend(label_order=sorted(ax._legend_data.keys()))\nplt.show()"
  },
  {
    "objectID": "posts/Off-2-On-RL/kr/Off2OnRL.html",
    "href": "posts/Off-2-On-RL/kr/Off2OnRL.html",
    "title": "Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble",
    "section": "",
    "text": "저자: Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, Jinwoo Shin\n발표: CoRL 2021\n논문\nOpenReview\nCode (PyTorch)"
  },
  {
    "objectID": "posts/Off-2-On-RL/kr/Off2OnRL.html#tldr",
    "href": "posts/Off-2-On-RL/kr/Off2OnRL.html#tldr",
    "title": "Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble",
    "section": "TL;DR",
    "text": "TL;DR\nOffline RL에서 아무리 좋은 policy를 학습했다 하더라도, online RL을 통한 fine-tune을 수행하게 되면 state-action distribution shift로 인해서 bootstrap error가 커지는 현상이 발생하게 된다. 이를 방지하기 위해서 online에서 얻은 sample에 대해서 우선순위를 부여하면서, offline dataset에서 뽑은 near-on-policy sample을 사용하도록 하는 balanced replay 구조를 제안했다. 추가로 offline상에서 여러개의 Q-function을 pessimistic하게 학습하고 이에 대한 ensemble을 취해서, 초기 학습단계에서 state에 대한 unfamiliar action을 다루는데 overoptimism이 발생하는 것을 방지했다."
  },
  {
    "objectID": "posts/Off-2-On-RL/kr/Off2OnRL.html#내용-정리",
    "href": "posts/Off-2-On-RL/kr/Off2OnRL.html#내용-정리",
    "title": "Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble",
    "section": "내용 정리",
    "text": "내용 정리\n\nOffline RL Agent Fine-tuning\n이 챕터에서는 Offline RL에서 학습된 Agent를 Online에서 Fine-tuning했을 때 distribution shift에 의해서 성능 저하가 발생하고, 왜 해당 문제가 발생하는지에 대해서 설명했다. 또한 논문에서 제안하는 아이디어의 핵심적인 요소인 sample selection과 Offline Q-function을 선택하는 방법에 대해서 소개했다.\n\nDistribution Shift in Offline-to-Online RL\nOffline data과 Online data을 섞게 되면, offline buffer \\(\\cal{B}^{\\text{off}}\\) 에서 뽑은 Data인 \\(d^{\\text{off}}(s, a)\\) 와 online buffer \\(\\cal{B}^{\\text{on}}\\) 에서 뽑은 Data인 \\(d^{\\text{on}}(s, a)\\) 간의 distribution shift는 존재한다. 논문에서는 D4RL 환경 중 하나인 halfcheetah-random dataset의 state-action pair를 Variational AutoEncoder를 통해서 재생성하고, 여기에서 CQL agent로 돌렸을 때 나온 online sample과 offline sample을 비교했다.\n\n\n\nFigure 1: Distribution Shift\n\n\nFigure 1 에서 보여지는 것처럼 offline에서의 state-action distribution과 online에서의 state-action distribution 간의 shift가 발생하는 것을 확인할 수 있다. 이렇게 Distribution shift가 발생하게 되면, agent가 만약 unseen state-action을 마주치게 될 경우, 이 때 참고를 하게될 Q-value (이때 값은 bootstrapping에 사용될 값을 위해서 추정된 값이 된다.)가 매우 부정확하게 된다. 이렇게 부정확한 값을 바탕으로 Policy Iteration을 수행하게 되면 Offline RL에서 아무리 좋은 정책을 얻어도 망가지는 현상이 발생한다. 특히 Offline RL agent가 behavior policy, 즉 dataset을 쌓을 때의 policy보다 성능이 좋은 경우에는 Distribution shift에 의한 성능 저하가 심하게 나타난다. 그리고 offline dataset이 어느 한점이 치중해 있는, 소위 narrow distributed 한 case에서는 (보통 single policy로 학습한 경우) 실제 online RL을 수행할 때 기존의 distribution에서 이탈하는 경향이 더 심하게 나타난다.\n\n\nSample Selection\nOnline dataset은 fine-tune에 필요한 요소이긴하지만 distribution shift 문제로 인해서 잠재적으로 위험한 Out-of-Distribution (OoD) sample이기도 하다. 반면 Offline dataset은 in-distribution 이긴 하지만, fine-tune에 영향을 못 미치거나 느리게 영향을 미친다. 이를 실험적으로 보여주기 위해서 CQL로 학습한 offline agent를 SAC로 fine-tune을 수행했을 때, online dataset을 사용하지 않은 경우와 사용한 경우를 비교했다.\n\n\n\nFigure 2: Sample Selection\n\n\nAsymptotic하게 얻을 수 있는 Average return 자체는 Online data만을 사용했을 때가 Uniform, 즉 offline data와 online data를 같이 사용한 경우에 비해서보다 높았지만, Figure 2 에서 학습 초반부를 보면 Average return이 뚝 떨어지는 현상이 나타난다. 이와 같이 online sample만 가지고 fine-tuning하는 것은 unstable하며, 이는 online data 내에 존재하는 OoD sample이 bootstrap error를 야기하기 때문이다.\nOffline data와 online data를 같이 사용하는 경우에도 offline data와 online data간의 비율 문제로 인해서 value propagation이 느리게 나타나는 문제는 있지만, 그래도 online data만 썼을 때에 비하면 학습 초반부에는 어느 정도의 stability를 확보할 수 있다. 이때문에 논문의 아이디어도 offline data와 online data를 같이 사용하는 balanced replay에 대한 아이디어를 제안하게 되었고, 이는 유용하지만 잠재적으로 위험한 online sample과 안정적이긴 하지만 fine-tune이 느린 offline sample간의 trade-off를 잘 조절하는 목적을 가지고 있다.\n\n\nChoice of Offline Q-function\nOffline RL에서 발생하는 distribution shift 문제를 해결하는데, 대부분의 방법들은 CQL에서 제안하는 것처럼 OoD action에 대해서 최대한 conservative함을 유지하는 것인데, 이 논문에서도 역시 동일하게 pessimistically trained Q-function을 사용했다. 이를 보여주기 위한 실험으로 pessimistically trained Q-function을 사용했을 때와, Regularization이 적용되지 않은 Q-function \\((\\approx \\text{FQE})\\) 을 활용하여 online fine-tune한 결과를 비교했다.\n\n\n\nFigure 3: Choice of Offline Q-function\n\n\nFigure 3 의 첫번째 그래프는 halfcheetah-random data에 대한 fine-tune에 대한 결과인데, 이 그래프에서는 두 방법론 모두 비슷한 fine-tune 성능을 보여주고 있다. 문제는 두번째 그래프인데, halfcheetah-medium, 즉 expert data와 random data가 반반씩 섞인 데이터를 가지고 학습한 모델에서 fine-tune을 수행하면 Regularization 이 적용되지 않은 FQE-init의 초반부 성능이 확 저하가 되는 것을 확인할 수 있다. 이는 FQE-init이 novel state에 대한 OoD action에 대해서 overoptimistic한 값을 가지고 있기 때문에 발생하는 문제이다. 다르게 표현하면 policy가 안전하면서 이미 학습때 본 trajectory가 아닌 잠재적으로 나쁜 action 을 더 선호하는 경향이 나오는 것이다. 이런 이유로 인해서 논문에서 제안하고자 했던 두번째 아이디어는 처음에는 offline을 통해서 pessimistic Q function을 학습하고, fine-tune 단계에서 balanced replay를 통해 offline data와 online data를 섞어주면서 점진적으로 pessimism을 줄여나가는 것이다. 여기에 덧붙여서 한 agent에 의한 pessimism을 방지하기 위해서, 여러개의 agent를 동시에 학습한 후 각 agent의 Q-function에 대한 ensemble을 취하는 방식을 사용했다.\n\n\n\nMethod\n결과적으로 논문에서 제안하는 방식은 offline data와 online data를 같이 사용하는 balanced replay와 pessimistic Q-function을 사용하는 ensemble을 취하는 것이고, 구조는 Figure 4 과 같다.\n\n\n\nFigure 4: Off2OnRL\n\n\n\nBalanced Experience Replay\n물론 balanced replay 내에서 sampling을 하는데 있어 priority가 존재하고, 이를 토대로 sampling하는 형태 자체는 Prioritized Experience Replay (PER)에서 소개하는 방식과 비슷하다. 다만 TD-error를 기반으로 priority를 매기는 PER과는 다르게 Balanced Replay에서는 online-ness 라는 것을 계산해서 priority를 계산한다. 이 지수의 목적은 framework내 online RL에 나와있는 것처럼 online data를 우선적으로 사용하면서도, near-on-policy인 offline data를 fine-tune에 활용하고자 하는 것이다. 조금 더 자세하게 설명하자면, offline data와 online data가 섞인 buffer에서 transition \\((s, a, s')\\)을 sampling을 하되, 이 때 sampling 하는 확률을 일종의 density ratio \\(w(s, a) := d^{\\text{on}}(s, a) / d^{\\text{off}}(s, a)\\) 에 비례하게 정하자는 것이다. 물론 이 density ratio가 1에 가깝다는 것은 online data의 distribution과 offline data의 distribution이 동일하다는 것을 의미하고, 이 비율대로 뽑게 되면 offline buffer \\(\\cal{B}^{\\text{off}}\\) 에 속하는 데이터 중 최대한 near-on-policy에 해당하는 sample을 얻을 수 있게 되는 것이다. 하지만 위의 \\(w(s, a)\\) 를 계산하는데 필요한 정보인 \\(d^{\\text{on}}(s, a)\\) 와 \\(d^{\\text{off}}(s, a)\\) 는 실제로는 구하기 어려운 값이기 때문에, 논문에서는 \\(\\psi\\) 를 weight으로 가지는 network으로 학습된 \\(w_{\\psi}(s, a)\\)로 density ratio를 계산하는 방법을 취했다.\n\n\nPessimistic Q-Ensemble\nFigure 4 에 소개되어 있는 것처럼, 이 논문에서 제안하는 모델은 multiple agent가 pessimistic 하게 (\\(\\approx \\text{CQL}\\)) 학습한 Q-function 들을 ensemble하는 형태를 가지고 있고, 이는 offline rl에서 발생하는 distribution shift 문제를 대응하기 위함이라고 되어 있다. 구체적으로는 \\(N\\) 개의 CQL agent를 offline data로 학습시켜서 \\(\\{ Q_{\\theta_i}, \\pi_{\\phi_i}\\}^N_{i=1}\\) 를 얻은 후, 아래와 같은 Q-function과 policy을 가지는 agent들의 ensemble 모델을 만든다.\n\\[\nQ_{\\theta} := \\frac{1}{N} \\sum^N_{i=1} Q_{\\theta_i}, \\quad \\pi_{\\phi}(\\cdot | s) = \\cal{N}(\\frac{1}{N} \\sum^N_{i=1} \\mu_{\\phi_i}(s), \\frac{1}{N} \\sum^N_{i=1} (\\sigma^2_{\\phi_i}(s) + \\mu^2_{\\phi_i}(s)) - \\mu^2_{\\phi}(s))\n\\]\n참고로 각 network의 parameter인 \\(\\theta\\) 와 \\(\\phi\\) 는 fine-tuning 단계에서 업데이트되는데, 이렇게 되면 초기 fine-tuning 단계에서 나타는 unstable한 현상, 즉 해당 state에 취해지는 unseen action에 대한 pessimism을 유지할 수 있게 된다. (아무래도 한개일때보다는 여러개의 ensemble model이 조금더 안정된 형태가 아닐까.. 물론 그만큼 resource를 잡아먹겠지만….)"
  },
  {
    "objectID": "posts/Logistic-Regression-with-a-Neural-Network/kr/Logistic-Regression-with-a-Neural-Network.html",
    "href": "posts/Logistic-Regression-with-a-Neural-Network/kr/Logistic-Regression-with-a-Neural-Network.html",
    "title": "신경망으로 구현하는 Logistic Regression",
    "section": "",
    "text": "In this post, we will build a logistic regression classifier to recognize cats. This is the summary of lecture “Neural Networks and Deep Learning” from DeepLearning.AI. (slightly modified from original assignment)"
  },
  {
    "objectID": "posts/Logistic-Regression-with-a-Neural-Network/kr/Logistic-Regression-with-a-Neural-Network.html#tldr",
    "href": "posts/Logistic-Regression-with-a-Neural-Network/kr/Logistic-Regression-with-a-Neural-Network.html#tldr",
    "title": "신경망으로 구현하는 Logistic Regression",
    "section": "",
    "text": "In this post, we will build a logistic regression classifier to recognize cats. This is the summary of lecture “Neural Networks and Deep Learning” from DeepLearning.AI. (slightly modified from original assignment)"
  },
  {
    "objectID": "posts/Logistic-Regression-with-a-Neural-Network/kr/Logistic-Regression-with-a-Neural-Network.html#packages",
    "href": "posts/Logistic-Regression-with-a-Neural-Network/kr/Logistic-Regression-with-a-Neural-Network.html#packages",
    "title": "신경망으로 구현하는 Logistic Regression",
    "section": "Packages",
    "text": "Packages\nFirst, let’s run the cell below to import all the packages that you will need during this assignment. - numpy is the fundamental package for scientific computing with Python. - h5py is a common package to interact with a dataset that is stored on an H5 file. - matplotlib is a famous library to plot graphs in Python. - PIL and scipy are used here to test your model with your own picture at the end.\n\nimport numpy as np\nimport copy\nimport matplotlib.pyplot as plt\nimport h5py\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage"
  },
  {
    "objectID": "posts/Logistic-Regression-with-a-Neural-Network/kr/Logistic-Regression-with-a-Neural-Network.html#dataset",
    "href": "posts/Logistic-Regression-with-a-Neural-Network/kr/Logistic-Regression-with-a-Neural-Network.html#dataset",
    "title": "신경망으로 구현하는 Logistic Regression",
    "section": "Dataset",
    "text": "Dataset\nYou are given a dataset (“data.h5”) containing: - a training set of m_train images labeled as cat (y=1) or non-cat (y=0) - a test set of m_test images labeled as cat or non-cat - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).\nYou will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.\n\ndef load_dataset():\n    train_ds = h5py.File('./dataset/train_catvnoncat.h5', 'r')\n    train_set_x = np.array(train_ds['train_set_x'][:])\n    train_set_y = np.array(train_ds['train_set_y'][:])\n    \n    test_ds = h5py.File('./dataset/test_catvnoncat.h5', 'r')\n    test_set_x = np.array(test_ds['test_set_x'][:])\n    test_set_y = np.array(test_ds['test_set_y'][:])\n    \n    classes = np.array(test_ds['list_classes'][:])\n    \n    train_set_y = train_set_y.reshape((1, train_set_y.shape[0]))\n    test_set_y = test_set_y.reshape((1, test_set_y.shape[0]))\n    \n    return train_set_x, train_set_y, test_set_x, test_set_y, classes\n\n\ntrain_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\n\nWe added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing).\nEach line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the index value and re-run to see other images.\n\nindex = 30\nplt.imshow(train_set_x_orig[index])\nprint (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")\n\ny = [0], it's a 'non-cat' picture.\n\n\n\n\n\n\nindex = 25\nplt.imshow(train_set_x_orig[index])\nprint (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")\n\ny = [1], it's a 'cat' picture.\n\n\n\n\n\n\nInformation from dataset\nWe want to find out how many data do we have, and what shape each image have. Remember that train_set_x_orig is a numpy-array of shape (m_train, num_px, num_px, 3).\n\nm_train = train_set_x_orig.shape[0]\nm_test = test_set_x_orig.shape[0]\nnum_px = train_set_x_orig.shape[1]\n\nprint (\"Number of training examples: m_train = \" + str(m_train))\nprint (\"Number of testing examples: m_test = \" + str(m_test))\nprint (\"Height/Width of each image: num_px = \" + str(num_px))\nprint (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\nprint (\"train_set_x shape: \" + str(train_set_x_orig.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x shape: \" + str(test_set_x_orig.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))\n\nNumber of training examples: m_train = 209\nNumber of testing examples: m_test = 50\nHeight/Width of each image: num_px = 64\nEach image is of size: (64, 64, 3)\ntrain_set_x shape: (209, 64, 64, 3)\ntrain_set_y shape: (1, 209)\ntest_set_x shape: (50, 64, 64, 3)\ntest_set_y shape: (1, 50)\n\n\nFor convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px \\(*\\) num_px \\(*\\) 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.\n\n\nReshape dataset\nReshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px \\(*\\) num_px \\(*\\) 3, 1).\nA trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b\\(*\\)c\\(*\\)d, a) is to use:\nX_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X\n\ntrain_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\ntest_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n\nprint (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))\n\ntrain_set_x_flatten shape: (12288, 209)\ntrain_set_y shape: (1, 209)\ntest_set_x_flatten shape: (12288, 50)\ntest_set_y shape: (1, 50)\n\n\nTo represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\nOne common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean(\\(\\mu\\)) of the whole numpy array from each example, and then divide each example by the standard deviation(\\(\\sigma\\)) of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (which is the maximum value of a pixel channel).\nLet’s standardize our dataset.\n\ntrain_set_x = train_set_x_flatten / 255.\ntest_set_x = test_set_x_flatten / 255.\n\nWhat you need to remember:\nCommon steps for pre-processing a new dataset are: - Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …) - Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1) - “Standardize” the data"
  },
  {
    "objectID": "posts/Logistic-Regression-with-a-Neural-Network/kr/Logistic-Regression-with-a-Neural-Network.html#general-architecture-of-the-learning-algorithm",
    "href": "posts/Logistic-Regression-with-a-Neural-Network/kr/Logistic-Regression-with-a-Neural-Network.html#general-architecture-of-the-learning-algorithm",
    "title": "신경망으로 구현하는 Logistic Regression",
    "section": "General Architecture of the learning algorithm",
    "text": "General Architecture of the learning algorithm\nIt’s time to design a simple algorithm to distinguish cat images from non-cat images.\nYou will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why Logistic Regression is actually a very simple Neural Network!\n\nMathematical expression of the algorithm:\nFor one example \\(x^{(i)}\\): \\[z^{(i)} = w^T x^{(i)} + b \\tag{1}\\] \\[\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}\\] \\[ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}\\]\nThe cost is then computed by summing over all training examples: \\[ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}\\]"
  },
  {
    "objectID": "posts/Logistic-Regression-with-a-Neural-Network/kr/Logistic-Regression-with-a-Neural-Network.html#building-the-parts-of-our-algorithm",
    "href": "posts/Logistic-Regression-with-a-Neural-Network/kr/Logistic-Regression-with-a-Neural-Network.html#building-the-parts-of-our-algorithm",
    "title": "신경망으로 구현하는 Logistic Regression",
    "section": "Building the parts of our algorithm",
    "text": "Building the parts of our algorithm\nThe main steps for building a Neural Network are: 1. Define the model structure (such as number of input features) 2. Initialize the model’s parameters 3. Loop: - Calculate current loss (forward propagation) - Calculate current gradient (backward propagation) - Update parameters (gradient descent)\nYou often build 1-3 separately and integrate them into one function we call model().\n\nSigmoid\nwe need to implement implement sigmoid(). As you’ve seen in the figure above, you need to compute \\[sigmoid(z) = \\frac{1}{1 + e^{-z}}\\] for \\(z = w^T x + b\\) to make predictions.\n\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Arguments:\n    z -- A scalar or numpy array of any size.\n\n    Return:\n    s -- sigmoid(z)\n    \"\"\"\n    s = 1 / (1 + np.exp(-z))\n    return s\n\n\nprint (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))\n\nsigmoid([0, 2]) = [0.5        0.88079708]\n\n\n\nx = np.array([0.5, 0, 2.0])\noutput = sigmoid(x)\nprint(output)\n\n[0.62245933 0.5        0.88079708]\n\n\n\n\nInitializing parameters\nNow we need to implement parameter initialization in the cell below. You have to initialize w as a vector of zeros.\n\ndef initialize_with_zeros(dim):\n    \"\"\"\n    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n    \n    Argument:\n    dim -- size of the w vector we want (or number of parameters in this case)\n    \n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias) of type float\n    \"\"\"\n    w = np.zeros(shape=(dim, 1), dtype=np.float32)\n    b = 0.0\n    \n    return w, b\n\n\ndim = 2\nw, b = initialize_with_zeros(dim)\n\nassert type(b) == float\nprint (\"w = \" + str(w))\nprint (\"b = \" + str(b))\n\nw = [[0.]\n [0.]]\nb = 0.0\n\n\n\n\nForward and Backward propagation\nNow that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters. Note that, Forward Propagation: - You get X - You compute \\(A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})\\) - You calculate the cost function: \\(J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))\\)\nHere are the two formulas you will be using:\n\\[ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}\\] \\[ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}\\]\n\ndef propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n    \n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n    \n    m = X.shape[1]\n    \n    # forward propagation (from x to cost)\n    # compute activation\n    A = sigmoid(w.T @ X + b)\n    # compute cost by using np.dot to perform multiplication\n    cost = np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)) / -m\n    \n    # backward propagation (to find grad)\n    dw = X @ (A - Y).T / m\n    db = np.sum(A - Y) / m\n    \n    cost = np.squeeze(np.array(cost))\n    \n    grads = {'dw': dw, 'db': db}\n    return grads, cost\n\n\nw =  np.array([[1.], [2]])\nb = 1.5\nX = np.array([[1., -2., -1.], [3., 0.5, -3.2]])\nY = np.array([[1, 1, 0]])\ngrads, cost = propagate(w, b, X, Y)\n\nassert type(grads[\"dw\"]) == np.ndarray\nassert grads[\"dw\"].shape == (2, 1)\nassert type(grads[\"db\"]) == np.float64\n\n\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n\ndw = [[ 0.25071532]\n [-0.06604096]]\ndb = -0.12500404500439652\ncost = 0.15900537707692405\n\n\n\n\nOptimization\nYou have initialized your parameters. and also able to compute a cost function and its gradient. Now, you want to update the parameters using gradient descent.\n\ndef optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n    \n    w = copy.deepcopy(w)\n    b = copy.deepcopy(b)\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        # cost and gradient calculation\n        grads, cost = propagate(w, b, X, Y)\n        \n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        # update rule\n        w -= learning_rate * dw\n        b -= learning_rate * db\n        \n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n        \n            # Print the cost every 100 training iterations\n            if print_cost:\n                print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs\n\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n\nprint (\"w = \" + str(params[\"w\"]))\nprint (\"b = \" + str(params[\"b\"]))\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint(\"Costs = \" + str(costs))\n\nw = [[0.35627617]\n [0.60199214]]\nb = -0.14956979978997242\ndw = [[-0.21189539]\n [-0.33376766]]\ndb = -0.13290329100668044\nCosts = [array(0.5826722)]\n\n\n\n\nPredict\nThe previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function. There are two steps to computing predictions:\n\nCalculate \\(\\hat{Y} = A = \\sigma(w^T X + b)\\)\nConvert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this).\n\n\ndef predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1, m))\n    w = w.reshape(X.shape[0], 1)\n    \n    # compute vector 'A' predicting the probabilities of a cat being present in the picture\n    A = sigmoid(w.T @ X + b)\n    \n    for i in range(A.shape[1]):\n        # convert probabilities A[0, i] to actual predictions p[0, i]\n        if A[0, i] &gt; 0.5:\n            Y_prediction[0, i] = 1\n        else:\n            Y_prediction[0, i] = 0\n            \n    return Y_prediction\n\n\nw = np.array([[0.1124579], [0.23106775]])\nb = -0.3\nX = np.array([[1., -1.1, -3.2],[1.2, 2., 0.1]])\nprint (\"predictions = \" + str(predict(w, b, X)))\n\npredictions = [[1. 1. 0.]]\n\n\nWhat to remember:\nYou’ve implemented several functions that: - Initialize (w,b) - Optimize the loss iteratively to learn parameters (w,b): - Computing the cost and its gradient - Updating the parameters using gradient descent - Use the learned (w,b) to predict the labels for a given set of examples"
  },
  {
    "objectID": "posts/Logistic-Regression-with-a-Neural-Network/kr/Logistic-Regression-with-a-Neural-Network.html#merge-all-functions-into-a-model",
    "href": "posts/Logistic-Regression-with-a-Neural-Network/kr/Logistic-Regression-with-a-Neural-Network.html#merge-all-functions-into-a-model",
    "title": "신경망으로 구현하는 Logistic Regression",
    "section": "Merge all functions into a model",
    "text": "Merge all functions into a model\nYou will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n\ndef model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to True to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    w, b = initialize_with_zeros(dim=X_train.shape[0])\n    \n    # Gradient descent\n    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n    \n    # Retrieve parameters w and b from dictionary \"params\"\n    w = params['w']\n    b = params['b']\n    \n    # Predict test/train set examples\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    \n    # Print train/test Errors\n    if print_cost:\n        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    \n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d\n\n\nlogistic_regression_model = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)\n\nCost after iteration 0: 0.693147\nCost after iteration 100: 0.584508\nCost after iteration 200: 0.466949\nCost after iteration 300: 0.376007\nCost after iteration 400: 0.331463\nCost after iteration 500: 0.303273\nCost after iteration 600: 0.279880\nCost after iteration 700: 0.260042\nCost after iteration 800: 0.242941\nCost after iteration 900: 0.228004\nCost after iteration 1000: 0.214820\nCost after iteration 1100: 0.203078\nCost after iteration 1200: 0.192544\nCost after iteration 1300: 0.183033\nCost after iteration 1400: 0.174399\nCost after iteration 1500: 0.166521\nCost after iteration 1600: 0.159305\nCost after iteration 1700: 0.152667\nCost after iteration 1800: 0.146542\nCost after iteration 1900: 0.140872\ntrain accuracy: 99.04306220095694 %\ntest accuracy: 70.0 %\n\n\nComment: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test accuracy is 70%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier.\nAlso, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the index variable) you can look at predictions on pictures of the test set.\n\n# Example of a picture that was wrongly classified.\nindex = 1\nplt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\nprint (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[int(logistic_regression_model['Y_prediction_test'][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")\n\ny = 1, you predicted that it is a \"cat\" picture.\n\n\n\n\n\nLet’s also plot the cost function and the gradients.\n\n# Plot learning curve (with costs)\ncosts = np.squeeze(logistic_regression_model['costs'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per hundreds)')\nplt.title(\"Learning rate =\" + str(logistic_regression_model[\"learning_rate\"]))\nplt.show()\n\n\n\n\nInterpretation: You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting."
  },
  {
    "objectID": "posts/Logistic-Regression-with-a-Neural-Network/kr/Logistic-Regression-with-a-Neural-Network.html#further-analysis",
    "href": "posts/Logistic-Regression-with-a-Neural-Network/kr/Logistic-Regression-with-a-Neural-Network.html#further-analysis",
    "title": "신경망으로 구현하는 Logistic Regression",
    "section": "Further analysis",
    "text": "Further analysis\nLet’s analyze it further, and examine possible choices for the learning rate \\(\\alpha\\).\n\nChoice of learning rate\nReminder: In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate \\(\\alpha\\) determines how rapidly we update the parameters. If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned learning rate.\nLet’s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the learning_rates variable to contain, and see what happens.\n\nlearning_rates = [0.01, 0.001, 0.0001]\nmodels = {}\n\nfor lr in learning_rates:\n    print (\"Training a model with learning rate: \" + str(lr))\n    models[str(lr)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=1500, learning_rate=lr, print_cost=False)\n    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n\nfor lr in learning_rates:\n    plt.plot(np.squeeze(models[str(lr)][\"costs\"]), label=str(models[str(lr)][\"learning_rate\"]))\n\nplt.ylabel('cost')\nplt.xlabel('iterations (hundreds)')\n\nlegend = plt.legend(loc='upper center', shadow=True)\nframe = legend.get_frame()\nframe.set_facecolor('0.90')\nplt.show()\n\nTraining a model with learning rate: 0.01\n\n-------------------------------------------------------\n\nTraining a model with learning rate: 0.001\n\n-------------------------------------------------------\n\nTraining a model with learning rate: 0.0001\n\n-------------------------------------------------------\n\n\n\n\n\n\nWhat to remember from this assignment: 1. Preprocessing the dataset is important. 2. You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model(). 3. Tuning the learning rate (which is an example of a “hyperparameter”) can make a big difference to the algorithm.\nBibliography: - http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/ - https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
  },
  {
    "objectID": "posts/Linear-Transformation/en/Linear-Transformation.html",
    "href": "posts/Linear-Transformation/en/Linear-Transformation.html",
    "title": "Linear Transformation",
    "section": "",
    "text": "In this notebook, you will explore linear transformations, visualize their results and master matrix multiplication to apply various linear transformations. This post is summarized the lecture “Linear Algebra for Machine Learning and Data Science” from Coursera."
  },
  {
    "objectID": "posts/Linear-Transformation/en/Linear-Transformation.html#tldr",
    "href": "posts/Linear-Transformation/en/Linear-Transformation.html#tldr",
    "title": "Linear Transformation",
    "section": "",
    "text": "In this notebook, you will explore linear transformations, visualize their results and master matrix multiplication to apply various linear transformations. This post is summarized the lecture “Linear Algebra for Machine Learning and Data Science” from Coursera."
  },
  {
    "objectID": "posts/Linear-Transformation/en/Linear-Transformation.html#packages",
    "href": "posts/Linear-Transformation/en/Linear-Transformation.html#packages",
    "title": "Linear Transformation",
    "section": "Packages",
    "text": "Packages\nRun the following cell to load the package you’ll need.\n\nimport numpy as np\nimport cv2\n\nprint(\"NumPy version: \", np.__version__)\nprint(\"OpenCV version: \", cv2.__version__)\n\nNumPy version:  1.21.5\nOpenCV version:  4.5.4"
  },
  {
    "objectID": "posts/Linear-Transformation/en/Linear-Transformation.html#transformations",
    "href": "posts/Linear-Transformation/en/Linear-Transformation.html#transformations",
    "title": "Linear Transformation",
    "section": "Transformations",
    "text": "Transformations\nA transformation is a function from one vector space to another that respects the underlying (linear) structure of each vector space. Referring to a specific transformation, you can use a symbol, such as \\(T\\). Specifying the spaces containing the input and output vectors, e.g. \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\), you can write \\(T: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3\\). Transforming vector \\(v \\in \\mathbb{R}^2\\) into the vector \\(w\\in\\mathbb{R}^3\\) by the transformation \\(T\\), you can use the notation \\(T(v)=w\\) and read it as “T of v equals to w” or “vector w is an image of vector v with the transformation T”.\nThe following Python function corresponds to the transformation \\(T: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3\\) with the following symbolic formula:\n\\[T\\begin{pmatrix}\n          \\begin{bmatrix}\n           v_1 \\\\           \n           v_2\n          \\end{bmatrix}\\end{pmatrix}=\n          \\begin{bmatrix}\n           3v_1 \\\\\n           0 \\\\\n           -2v_2\n          \\end{bmatrix}\n          \\tag{1}\n          \\]\n\ndef T(v):\n    w = np.zeros((3,1))\n    w[0,0] = 3*v[0,0]\n    w[2,0] = -2*v[1,0]\n    \n    return w\n\nv = np.array([[3], [5]])\nw = T(v)\n\nprint(\"Original vector:\\n\", v, \"\\n\\n Result of the transformation:\\n\", w)\n\nOriginal vector:\n [[3]\n [5]] \n\n Result of the transformation:\n [[  9.]\n [  0.]\n [-10.]]"
  },
  {
    "objectID": "posts/Linear-Transformation/en/Linear-Transformation.html#linear-transformations",
    "href": "posts/Linear-Transformation/en/Linear-Transformation.html#linear-transformations",
    "title": "Linear Transformation",
    "section": "Linear Transformations",
    "text": "Linear Transformations\nA transformation \\(T\\) is said to be linear if the following two properties are true for any scalar \\(k\\), and any input vectors \\(u\\) and \\(v\\):\n\n\\(T(kv)=kT(v)\\),\n\\(T(u+v)=T(u)+T(v)\\).\n\nIn the example above \\(T\\) is a linear transformation:\n\\[T (kv) =\n          T \\begin{pmatrix}\\begin{bmatrix}\n          kv_1 \\\\\n          kv_2\n          \\end{bmatrix}\\end{pmatrix} =\n          \\begin{bmatrix}\n           3kv_1 \\\\\n           0 \\\\\n           -2kv_2\n          \\end{bmatrix} =\n          k\\begin{bmatrix}\n           3v_1 \\\\\n           0 \\\\\n           -2v_2\n          \\end{bmatrix} =\n          kT(v),\\tag{2}\\]\n\\[T (u+v) =\n          T \\begin{pmatrix}\\begin{bmatrix}\n          u_1 + v_1 \\\\\n          u_2 + v_2\n          \\end{bmatrix}\\end{pmatrix} =\n          \\begin{bmatrix}\n           3(u_1+v_1) \\\\\n           0 \\\\\n           -2(u_2+v_2)\n          \\end{bmatrix} =\n          \\begin{bmatrix}\n           3u_1 \\\\\n           0 \\\\\n           -2u_2\n          \\end{bmatrix} +\n          \\begin{bmatrix}\n           3v_1 \\\\\n           0 \\\\\n           -2v_2\n          \\end{bmatrix} =\n          T(u)+T(v).\\tag{3}\\]\nYou can change the values of \\(k\\) or vectors \\(u\\) and \\(v\\) in the cell below, to check that this is true for some specific values.\n\nu = np.array([[1], [-2]])\nv = np.array([[2], [4]])\n\nk = 7\n\nprint(\"T(k*v):\\n\", T(k*v), \"\\n k*T(v):\\n\", k*T(v), \"\\n\\n\")\nprint(\"T(u+v):\\n\", T(u+v), \"\\n T(u)+T(v):\\n\", T(u)+T(v))\n\nT(k*v):\n [[ 42.]\n [  0.]\n [-56.]] \n k*T(v):\n [[ 42.]\n [  0.]\n [-56.]] \n\n\nT(u+v):\n [[ 9.]\n [ 0.]\n [-4.]] \n T(u)+T(v):\n [[ 9.]\n [ 0.]\n [-4.]]\n\n\nSome examples of linear transformations are rotations, reflections, scaling (dilations), etc. In this lab you will explore a few of them."
  },
  {
    "objectID": "posts/Linear-Transformation/en/Linear-Transformation.html#transformations-defined-as-a-matrix-multiplication",
    "href": "posts/Linear-Transformation/en/Linear-Transformation.html#transformations-defined-as-a-matrix-multiplication",
    "title": "Linear Transformation",
    "section": "Transformations Defined as a Matrix Multiplication",
    "text": "Transformations Defined as a Matrix Multiplication\nLet \\(L: \\mathbb{R}^m \\rightarrow \\mathbb{R}^n\\) be defined by a matrix \\(A\\), where \\(L(v)=Av\\), multiplication of the matrix \\(A\\) (\\(n\\times m\\)) and vector \\(v\\) (\\(m\\times 1\\)) resulting in the vector \\(w\\) (\\(n\\times 1\\)).\nNow try to guess, what should be the elements of matrix \\(A\\), corresponding to the transformation \\(L: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3\\):\n\\[L\\begin{pmatrix}\n          \\begin{bmatrix}\n           v_1 \\\\           \n           v_2\n          \\end{bmatrix}\\end{pmatrix}=\n          \\begin{bmatrix}\n           3v_1 \\\\\n           0 \\\\\n           -2v_2\n          \\end{bmatrix}=\n          \\begin{bmatrix}\n           ? & ? \\\\\n           ? & ? \\\\\n           ? & ?\n          \\end{bmatrix}\n          \\begin{bmatrix}\n           v_1 \\\\\n           v_2\n          \\end{bmatrix}\n          \\tag{4}\n          \\]\nTo do that, write the transformation \\(L\\) as \\(Av\\) and then perform matrix multiplication: \\[L\\begin{pmatrix}\n          \\begin{bmatrix}\n           v_1 \\\\           \n           v_2\n          \\end{bmatrix}\\end{pmatrix}=\n          A\\begin{bmatrix}\n           v_1 \\\\           \n           v_2\n          \\end{bmatrix}=\n          \\begin{bmatrix}\n           a_{1,1} & a_{1,2} \\\\\n           a_{2,1} & a_{2,2} \\\\\n           a_{3,1} & a_{3,2}\n          \\end{bmatrix}\n          \\begin{bmatrix}\n           v_1 \\\\           \n           v_2\n          \\end{bmatrix}=\n          \\begin{bmatrix}\n           a_{1,1}v_1+a_{1,2}v_2 \\\\\n           a_{2,1}v_1+a_{2,2}v_2 \\\\\n           a_{3,1}v_1+a_{3,2}v_2 \\\\\n          \\end{bmatrix}=\n          \\begin{bmatrix}\n           3v_1 \\\\\n           0 \\\\\n           -2v_2\n          \\end{bmatrix}\\tag{5}\n          \\]\nCan you see now what should be the values of the elements \\(a_{i,j}\\) of matrix \\(A\\) to make the equalities \\((5)\\) correct? Find out the answer in the following code cell:\n\ndef L(v):\n    A = np.array([[3,0], [0,0], [0,-2]])\n    print(\"Transformation matrix:\\n\", A, \"\\n\")\n    w = A @ v\n    \n    return w\n\nv = np.array([[3], [5]])\nw = L(v)\n\nprint(\"Original vector:\\n\", v, \"\\n\\n Result of the transformation:\\n\", w)\n\nTransformation matrix:\n [[ 3  0]\n [ 0  0]\n [ 0 -2]] \n\nOriginal vector:\n [[3]\n [5]] \n\n Result of the transformation:\n [[  9]\n [  0]\n [-10]]\n\n\nEvery linear transformation can be carried out by matrix multiplication. And vice versa, carrying out matrix multiplication, it is natural to consider the linear transformation that it represents. It means you can associate the matrix with the linear transformation in some way. This is a key connection between linear transformations and matrix algebra."
  },
  {
    "objectID": "posts/Linear-Transformation/en/Linear-Transformation.html#standard-transformations-in-a-plane",
    "href": "posts/Linear-Transformation/en/Linear-Transformation.html#standard-transformations-in-a-plane",
    "title": "Linear Transformation",
    "section": "Standard Transformations in a Plane",
    "text": "Standard Transformations in a Plane\nAs discussed above in section 3, a linear transformation \\(L: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2\\) can be represented as a multiplication of a \\(2 \\times 2\\) matrix and a coordinate vector \\(v\\in\\mathbb{R}^2.\\) Note that so far you have been using some random vector \\(v\\in\\mathbb{R}^2.\\) (e.g. \\(v=\\begin{bmatrix}3 \\\\ 5\\end{bmatrix}\\)). To have a better intuition of what the transformation is really doing in the \\(\\mathbb{R}^2\\) space, it is wise to choose vector \\(v\\) in a less random way.\nA good choice would be vectors of a standard basis \\(e_1=\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}\\) and \\(e_2=\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}\\). Let’s apply linear transformation \\(L\\) to each of the vectors \\(e_1\\) and \\(e_2\\): \\(L(e_1)=Ae_1\\) and \\(L(e_2)=Ae_2\\). If you put vectors \\(\\{e_1, e_2\\}\\) into columns of a matrix and perform matrix multiplication\n\\[A\\begin{bmatrix}e_1 & e_2\\end{bmatrix}=\\begin{bmatrix}Ae_1 & Ae_2\\end{bmatrix}=\\begin{bmatrix}L(e_1) & L(e_2)\\end{bmatrix},\\tag{3}\\]\nyou can note that \\(\\begin{bmatrix}e_1 & e_2\\end{bmatrix}=\\begin{bmatrix}1 & 0 \\\\ 0 & 1\\end{bmatrix}\\) (identity matrix). Thus, \\(A\\begin{bmatrix}e_1 & e_2\\end{bmatrix} = AI=A\\), and\n\\[A=\\begin{bmatrix}L(e_1) & L(e_2)\\end{bmatrix}.\\tag{4}\\]\nThis is a matrix with the columns that are the images of the vectors of the standard basis.\nThis choice of vectors {\\(e_1, e_2\\)} provides opportinuty for the visual representation of the linear transformation \\(L\\) (you will see the examples below).\n\nExample 1: Horizontal Scaling (Dilation)\nHorizontal scaling (factor \\(2\\) in this example) can be defined considering transformation of a vector \\(e_1=\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}\\) into a vector \\(\\begin{bmatrix}2 \\\\ 0\\end{bmatrix}\\) and leaving vector \\(e_2=\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}\\) without any changes. The following function T_hscaling() corresponds to the horizontal scaling (factor \\(2\\)) of a vector. The second function transform_vectors() applies defined transformation to a set of vectors (here two vectors).\n\ndef T_hscaling(v):\n    A = np.array([[2,0], [0,1]])\n    w = A @ v\n    \n    return w\n    \n    \ndef transform_vectors(T, v1, v2):\n    V = np.hstack((v1.reshape(2,1), v2.reshape(2,1)))\n    W = T(V)\n    \n    return W\n    \ne1 = np.array([[1], [0]])\ne2 = np.array([[0], [1]])\n\ntransformation_result_hscaling = transform_vectors(T_hscaling, e1, e2)\n\nprint(\"Original vectors:\\n e1= \\n\", e1, \"\\n e2=\\n\", e2, \n      \"\\n\\n Result of the transformation (matrix form):\\n\", transformation_result_hscaling)\n\nOriginal vectors:\n e1= \n [[1]\n [0]] \n e2=\n [[0]\n [1]] \n\n Result of the transformation (matrix form):\n [[2 0]\n [0 1]]\n\n\nYou can get a visual understanding of the transformation, producing a plot which displays input vectors, and their transformations. Do not worry if the code in the following cell will not be clear - at this stage this is not important code to understand.\n\nimport matplotlib.pyplot as plt\n\ndef plot_transformation(T, e1, e2):\n    color_original = \"#129cab\"\n    color_transformed = \"#cc8933\"\n    \n    _, ax = plt.subplots(figsize=(7, 7))\n    ax.tick_params(axis='x', labelsize=14)\n    ax.tick_params(axis='y', labelsize=14)\n    ax.set_xticks(np.arange(-5, 5))\n    ax.set_yticks(np.arange(-5, 5))\n    \n    plt.axis([-5, 5, -5, 5])\n    plt.quiver([0, 0],[0, 0], [e1[0], e2[0]], [e1[1], e2[1]], color=color_original, angles='xy', scale_units='xy', scale=1)\n    plt.plot([0, e2[0], e1[0], e1[0]], \n             [0, e2[1], e2[1], e1[1]], \n             color=color_original)\n    e1_sgn = 0.4 * np.array([[1] if i==0 else [i] for i in np.sign(e1)])\n    ax.text(e1[0]-0.2+e1_sgn[0], e1[1]-0.2+e1_sgn[1], f'$e_1$', fontsize=14, color=color_original)\n    e2_sgn = 0.4 * np.array([[1] if i==0 else [i] for i in np.sign(e2)])\n    ax.text(e2[0]-0.2+e2_sgn[0], e2[1]-0.2+e2_sgn[1], f'$e_2$', fontsize=14, color=color_original)\n    \n    e1_transformed = T(e1)\n    e2_transformed = T(e2)\n    \n    plt.quiver([0, 0],[0, 0], [e1_transformed[0], e2_transformed[0]], [e1_transformed[1], e2_transformed[1]], \n               color=color_transformed, angles='xy', scale_units='xy', scale=1)\n    plt.plot([0,e2_transformed[0], e1_transformed[0]+e2_transformed[0], e1_transformed[0]], \n             [0,e2_transformed[1], e1_transformed[1]+e2_transformed[1], e1_transformed[1]], \n             color=color_transformed)\n    e1_transformed_sgn = 0.4 * np.array([[1] if i==0 else [i] for i in np.sign(e1_transformed)])\n    ax.text(e1_transformed[0]-0.2+e1_transformed_sgn[0], e1_transformed[1]-e1_transformed_sgn[1], \n            f'$T(e_1)$', fontsize=14, color=color_transformed)\n    e2_transformed_sgn = 0.4 * np.array([[1] if i==0 else [i] for i in np.sign(e2_transformed)])\n    ax.text(e2_transformed[0]-0.2+e2_transformed_sgn[0], e2_transformed[1]-e2_transformed_sgn[1], \n            f'$T(e_2)$', fontsize=14, color=color_transformed)\n    \n    plt.gca().set_aspect(\"equal\")\n    plt.grid()\n    plt.show()\n    \nplot_transformation(T_hscaling, e1, e2)\n\nC:\\Users\\kcsgo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\nC:\\Users\\kcsgo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\nC:\\Users\\kcsgo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\nC:\\Users\\kcsgo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n\n\n\n\n\nYou can observe that the polygon has been stretched in the horizontal direction as a result of the transformation.\n\n\nExample 2: Reflection about y-axis (the vertical axis)\nFunction T_reflection_yaxis() defined below corresponds to the reflection about y-axis:\n\ndef T_reflection_yaxis(v):\n    A = np.array([[-1,0], [0,1]])\n    w = A @ v\n    \n    return w\n    \ne1 = np.array([[1], [0]])\ne2 = np.array([[0], [1]])\n\ntransformation_result_reflection_yaxis = transform_vectors(T_reflection_yaxis, e1, e2)\n\nprint(\"Original vectors:\\n e1= \\n\", e1,\"\\n e2=\\n\", e2, \n      \"\\n\\n Result of the transformation (matrix form):\\n\", transformation_result_reflection_yaxis)\n\nOriginal vectors:\n e1= \n [[1]\n [0]] \n e2=\n [[0]\n [1]] \n\n Result of the transformation (matrix form):\n [[-1  0]\n [ 0  1]]\n\n\nYou can visualize this transformation:\n\nplot_transformation(T_reflection_yaxis, e1, e2)\n\nC:\\Users\\kcsgo\\anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  ary = asanyarray(ary)\nC:\\Users\\kcsgo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\nC:\\Users\\kcsgo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\nC:\\Users\\kcsgo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\nC:\\Users\\kcsgo\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n\n\n\n\n\nThere are many more standard linear transformations to explore. But now you have the required tools to apply them and visualize the results."
  },
  {
    "objectID": "posts/Linear-Transformation/en/Linear-Transformation.html#application-of-linear-transformations-computer-graphics",
    "href": "posts/Linear-Transformation/en/Linear-Transformation.html#application-of-linear-transformations-computer-graphics",
    "title": "Linear Transformation",
    "section": "Application of Linear Transformations: Computer Graphics",
    "text": "Application of Linear Transformations: Computer Graphics\nA large number of basic geometric shapes is used in computer graphics. Such shapes (e.g. triangles, quadrilaterals) are defined by their vertexes (corners). Linear transformations are often used to generate complex shapes from the basic ones, though scaling, reflection, rotation, shearing etc. It provides opportunity to manipulate those shapes efficiently.\nThe software responsible for rendering of a graphics, has to process the coordinates of millions of vertexes. The use of matrix multiplication to manipulate coordinates helps to merge multiple transformations together, just applying matrix multiplication one by one in a sequence. And another advantage is that the dedicated hardware, such as Graphics Processing Units (GPUs), is designed specifically to handle these calculations in large numbers with high speed.\nSo, matrix multiplication and linear transformations give you a super power, especially on scale!\nHere is an example where linear transformations could have helped to reduce the amount of work preparing the image:\n\nAll of the subleafs are similar and can be prepared as just linear transformations of one original leaf.\nLet’s see a simple example of two transformations applied to a leaf image. For the image transformations you can use an OpenCV library. First, upload and show the image:\n\nimg = cv2.imread('../images/leaf_original.png', 0)\nplt.imshow(img)\n\n&lt;matplotlib.image.AxesImage at 0x187ae0c7c08&gt;\n\n\n\n\n\nOf course, this is just a very simple leaf image (not a real example in preparation of the proper art work), but it will help you to get the idea how a few transformations can be applied in a row. Try to rotate the image 90 degrees clockwise and then apply a shear transformation, which can be visualized as:\n\nRotate the image:\n\nimg_rotated = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\nplt.imshow(img_rotated);\n\n\n\n\nApplying the shear you will get the following output:\n\nrows, cols = img_rotated.shape\n\nM = np.float32([[1, 0.5, 0], [0, 1, 0], [0, 0, 1]])\n\nimg_rotated_sheared = cv2.warpPerspective(img_rotated, M, (int(cols), int(rows)))\n\nplt.imshow(img_rotated_sheared);\n\n\n\n\nWhat if you will apply those two transformations in the opposite order? Do you think the result will be the same? Run the following code to check that:\n\nimg_sheared = cv2.warpPerspective(img, M, (int(cols), int(rows)))\nimg_sheared_rotated = cv2.rotate(img_sheared, cv2.ROTATE_90_CLOCKWISE)\n\nplt.imshow(img_sheared_rotated);\n\n\n\n\nComparing last two images, you can clearly see that the outputs are different. This is because linear transformation can be defined as a matrix multiplication. Then, applying two transformations in a row, e.g. with matrices \\(A\\) and \\(B\\), you perform multiplications \\(B(Av)=(BA)v\\), where \\(v\\) is a vector. And remember, that generally you cannot change the order in the matrix multiplication (most of the time \\(BA\\neq AB\\)). Let’s check that! Define two matrices, corresponding to the rotation and shear transformations:\n\nM_rotation_90_clockwise = np.array([[0, 1], [-1, 0]])\nM_shear_x = np.array([[1, 0.5], [0, 1]])\n\nprint(\"90 degrees clockwise rotation matrix:\\n\", M_rotation_90_clockwise)\nprint(\"Matrix for the shear along x-axis:\\n\", M_shear_x)\n\n90 degrees clockwise rotation matrix:\n [[ 0  1]\n [-1  0]]\nMatrix for the shear along x-axis:\n [[1.  0.5]\n [0.  1. ]]\n\n\nNow check that the results of their multiplications M_rotation_90_clockwise @ M_shear_x and M_shear_x @ M_rotation_90_clockwise are different:\n\nprint(\"M_rotation_90_clockwise by M_shear_x:\\n\", M_rotation_90_clockwise @ M_shear_x)\nprint(\"M_shear_x by M_rotation_90_clockwise:\\n\", M_shear_x @ M_rotation_90_clockwise)\n\nM_rotation_90_clockwise by M_shear_x:\n [[ 0.   1. ]\n [-1.  -0.5]]\nM_shear_x by M_rotation_90_clockwise:\n [[-0.5  1. ]\n [-1.   0. ]]\n\n\nThis simple example shows that you need to be aware of the mathematical objects and their properties in the applications."
  },
  {
    "objectID": "posts/kr.html",
    "href": "posts/kr.html",
    "title": "Chans Lecture Note",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n�������� ���� 2��\n\n\nFundamentals of Deep Learning (2nd)\n\n\n\n\nBook\n\n\nDeep Learning\n\n\n\n\n�Ѻ��̵��� ������ “�������� ���� (2��)”�� ���� ����\n\n\n\n\n\n\nMar 24, 2024\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\n러닝 레이\n\n\nLearning Ray\n\n\n\n\nBook\n\n\nRay\n\n\n\n\n한빛미디어에서 출판한 “러닝 레이”에 대한 리뷰\n\n\n\n\n\n\nFeb 26, 2024\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nActor-Critic Design Decisions\n\n\n\n\n\n\n\nLectureReview\n\n\nReinforcementLearning\n\n\n\n\nUC Berkeley CS285 (2023) Lecture 6-3 Review\n\n\n\n\n\n\nNov 15, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nBridging the Gap Between Offline and Online Reinforcement Learning Evaluation Methodologies\n\n\nOffline Sequential Evaluation\n\n\n\n\nPaperReview\n\n\nReinforcementLearning\n\n\n\n\nSequential Approach to evaluate Offline RL algorithms as a function of the training set size\n\n\n\n\n\n\nNov 9, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nKernel Density Estimation\n\n\n\n\n\n\n\nPython\n\n\nKAIST\n\n\nStatistics\n\n\n\n\nKAIST Data Science Programming 1 - 4-5-1. Kernel Density Estimation에 대한 내용입니다.\n\n\n\n\n\n\nNov 7, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nPearson Correlation\n\n\n\n\n\n\n\nPython\n\n\nKAIST\n\n\nStatistics\n\n\n\n\nKAIST Data Science Programming 1 - 4-4. Pearson Correlation에 대한 내용입니다.\n\n\n\n\n\n\nOct 31, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nFrom Evaluation to Actor Critic\n\n\n\n\n\n\n\nLectureReview\n\n\nReinforcementLearning\n\n\n\n\nUC Berkeley CS285 (2023) Lecture 6-2 Review\n\n\n\n\n\n\nOct 26, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nCovariance Matrix\n\n\n\n\n\n\n\nPython\n\n\nKAIST\n\n\nStatistics\n\n\n\n\nKAIST Data Science Programming 1 - 4-3. Covariance Matrix에 대한 내용입니다.\n\n\n\n\n\n\nOct 25, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nMultivariate Gaussian Distribution\n\n\n\n\n\n\n\nPython\n\n\nKAIST\n\n\nStatistics\n\n\n\n\nKAIST Data Science Programming 1 - 4.1 Multivariate Gaussian Distribution 에 대한 내용입니다.\n\n\n\n\n\n\nOct 24, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nActive Offline Policy Selection\n\n\nA-OPS\n\n\n\n\nPaperReview\n\n\nReinforcementLearning\n\n\n\n\nA sequential decision approach that combines logged data with online interaction to identify the best policy.\n\n\n\n\n\n\nOct 19, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nActor-Critic Algorithms\n\n\n\n\n\n\n\nLectureReview\n\n\nReinforcementLearning\n\n\n\n\nUC Berkeley CS285 (2023) Lecture 6-1 Review\n\n\n\n\n\n\nOct 19, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nOffline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble\n\n\nOff2OnRL\n\n\n\n\nPaperReview\n\n\nReinforcementLearning\n\n\n\n\nBalanced replay scheme that prioritizes samples encountered online while also encouraging the use of near-on-policy samples from offline dataset.\n\n\n\n\n\n\nOct 10, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nOffline Reinforcement Learning with Implicit Q-Learning\n\n\nImplicit Q-Learning (IQL)\n\n\n\n\nPaperReview\n\n\nReinforcementLearning\n\n\n\n\nOffline RL method with only dataset actions.\n\n\n\n\n\n\nAug 29, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\n신경망으로 구현하는 Logistic Regression\n\n\n\n\n\n\n\nPython\n\n\nCoursera\n\n\nDeepLearning.AI\n\n\n\n\n\n\n\n\n\n\n\nMay 11, 2022\n\n\nChanseok Kang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Chans Lecture Note",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nDimensionality Reduction\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nDifferentiation in python\n\n\n\n\n\n\n\nPython\n\n\nCoursera\n\n\nDeepLearning.AI\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nLinear Transformation\n\n\n\n\n\n\n\nPython\n\n\nCoursera\n\n\nDeepLearning.AI\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nMatrix Multiplication\n\n\n\n\n\n\n\nPython\n\n\nCoursera\n\n\nDeepLearning.AI\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nVector Operations: Scalar Multiplication, Sum and Dot Product of Vectors\n\n\n\n\n\n\n\nPython\n\n\nCoursera\n\n\nDeepLearning.AI\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nSolving Linear Systems: 3 Variables\n\n\n\n\n\n\n\nPython\n\n\nCoursera\n\n\nDeepLearning.AI\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nSolving Linear Systems: 2 Variables\n\n\n\n\n\n\n\nPython\n\n\nCoursera\n\n\nDeepLearning.AI\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to NumPy Arrays\n\n\n\n\n\n\n\nPython\n\n\nCoursera\n\n\nDeepLearning.AI\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nLogistic Regression with a Neural Network mindset\n\n\n\n\n\n\n\nPython\n\n\nCoursera\n\n\nDeepLearning.AI\n\n\n\n\n\n\n\n\n\n\n\nMay 11, 2022\n\n\nChanseok Kang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Fundamentals-of-Deep-Learning-2nd/kr/fundamentals-of-deep-learning-2nd.html",
    "href": "posts/Fundamentals-of-Deep-Learning-2nd/kr/fundamentals-of-deep-learning-2nd.html",
    "title": "�������� ���� 2��",
    "section": "",
    "text": "(�ش� ����Ʈ���� �Ұ��ϰ� �ִ� “�������� ���� (2��)” å�� �Ѻ��̵��κ��� �����޾����� �˷��帳�ϴ�.)\n?���� �ð��� �������� �������� ������ ���� ������ ��� �ִ��� ã�ƺ��� �Ѵ�. �Ұ� ������� �ص� ��� ���� ������ ���� ���� å���� ������ ���� ��������, ����� LLM�̳� ����AI ���� �ֽ� Ʈ���带 �ݿ��� å�鵵 ���� ������ �ִ°� ����. ���������δ� �׷� �ֽ� Ʈ���带 �ٷ� å�麸�� ���������� �������� �����ϴ� ������ ���ؼ� �� ������ å�� ������ ���ڴٴ� ������ ������ �ְ�, �׷� ������ ���ַ� ������ ������ ã�ƺ��� ���̴�. �׷��� ������ å�� ���� ���ٺ��� �ٷ����� �����̳� �����ϴ� ���뵵 �پ��ϴ�. � å�� �عٴں��� ó������ �����ϴ� å�鵵 �ְ�, �⺻���� ������ �˰��ִٴ� �����Ͽ� �ٷ� �̷����� ������ �ٷ�� å�鵵 �ִ�.\n\n�̹��� �Ұ��ϴ� “�������� ���� (2��)” å�� 6������ ���Դ� 1�ǿ��� ���� ���� �� �ǽ� ȯ���� ����Ǿ� �Ⱓ�� å�̴�. ��ÿ��� �ټ��÷η� �����ߴ� �ǽ� ������ �̹����� ������ġ�� ����Ǿ���. �ű⿡ ���ؿ� �ʿ��� �������� ������ ���ݴ� �����Ǿ �Ⱓ�Ǿ���. �׷��� ���� �տ��� ����� å�� �������� ���� ���ڸ� �̷а� �ǽ��� �����ϰ� ��ġ�� å�� �ǰڴ�.\n?å�� �о�� ������ ���ؿ� �ʿ��� ��������а� Ȯ�� ���� ������ �ʹݿ� �ٷ����� �־�, ó�� �������� ���ϴ� ������� �������� �������� ������ ������̳� �ñ����� �ؼ��� �� �ְ� �س���. ���� ���� �����̱� ������ �з��� ���� �ʰ�, �ɵ��ְ� �ٷ����� ������, �׷��� �޺κп� �ٷ��� ������ ��濡 ���ؼ� �� ��ġ�� �� �� ���� ���Ҵ�. ���� ������ ������ �� �ִ� ����, Ư�� ���� ������ ���� ��ü���� �׸����� ���� ����� �־�, ���� ���� ���� �̷� ���¸� ��±����� ���⿡ ���Ҵ� �� ����. �׸��� 2���� ���� �ֱٿ� �������� ���ټ��̳� �޸� ������ �𵨿� ���� ������ ���ԵǾ� �־�, �ش� �𵨿� ���ؼ� ���� �����غ��� �����غ����� �ϴ� ������� ������ �� �� �ϴ�.\n?�׷��� ���� “����”�̶� Ÿ��Ʋ�� �ް� �ִ� å�̱⿡ å�� ��ü �з��̶���� �� ������ �з��� ���������� ���� �κ��� ���� �ƽ��� ���� �ִ�. ��� 1�Ƕ��� ��޵� �����̱⵵ �Ѵ�. �����̶���� NLP, ��ȭ�н� ���� ������ �ڼ��ϰ� �ٷ����ٱ� ���ٴ� ���� NLP�� ���ߵǾ� ������ ���� �ִ�. Ư�� ��ȭ�н� ���� ������ ���� �Ǹ������� ������ ������ ���� ª�� �ٷ�����. �����ʵ� �ֱ� ����� ��޵Ǿ��ٱ� ���ٴ� �Ϲ����� ������ ���� ������ �ٷ�� �־���. ���� ������ ���̳� ������, �޸� ������ �𵨿� ���� ������ ������ �κ��� ������ �ٷ�� �ְ�, ���� ���� �������ϰ� �� �����Ǿ� �ִ�. �׷��� ���� ������ �ϰų� �����ϴ� ������Դ� ���� ���� å�� �� �� �ִ�. (�Ƹ� ���� ������ �ٷ�� ������ �����̰�����…) �׷��� �� �������� “����”�̶�� �ϱ⿣ ���� ������ �κ��� �ƴұ� �����Ѵ�."
  },
  {
    "objectID": "posts/DSP-lec4-4/kr/Pearson-Correlation.html",
    "href": "posts/DSP-lec4-4/kr/Pearson-Correlation.html",
    "title": "Pearson Correlation",
    "section": "",
    "text": "이전 정리에서 살펴봤던 Pearson Correlation을 가져와보자.\n\\[\n\\begin{aligned}\n\\rho_{X_i, X_j} = \\frac{cov(X_i, X_j)}{\\sigma_{X_i} \\sigma_{X_j}} &= \\frac{\\mathbb{E}[(X_i - \\mathbb{E}[X_i])(X_j - \\mathbb{E}[X_j])]}{\\sqrt{\\mathbb{E}[(X_i - \\mathbb{E}[X_i])(X_i - \\mathbb{E}[X_i])]}\\sqrt{\\mathbb{E}[(X_j - \\mathbb{E}[X_j])(X_j - \\mathbb{E}[X_j])]}} \\\\\n&= \\frac{\\mathbb{E}[X_i X_j] - \\mathbb{E}[X_i]\\mathbb{E}[X_j]}{\\sqrt{\\mathbb{E}[X_i^2] - (\\mathbb{E}[X_i])^2} \\sqrt{\\mathbb{E}[X_j^2] - (\\mathbb{E}[X_j])^2}}\n\\end{aligned}\n\\]\n이와 비슷한 성격을 띄는 metric 중 하나가 cosine similarity라는 것이 있는데, 다음과 같이 정의된다.\n\\[\ncos(\\theta) = \\frac{x_i \\cdot x_j}{\\Vert x_i \\Vert \\Vert x_j \\Vert} = \\frac{\\sum_{d=1}^n x_{i, d} x_{j_d}}{\\sqrt{\\sum_{d=1}^n x_{i, d}^2}\\sqrt{\\sum_{d=1}^n x_{j, d}^2}}\n\\]\n사실 이 값은 cosine 값이기 때문에 잘 알고 있는 것처럼 -1과 1 사이의 값을 가진다. 여기에서 만약 각 비교하는 변수들의 평균이 0이라고 가정을 해보면 다음과 같이 정리가 된다. (물론 막연하게 평균이 0이라는 조건을 단 것이긴 하지만, 어떤 분포가 있을 때 분포의 평균을 0으로 만드는 것은 단순히 shift를 하면 가능한 것이기에 이와 같은 가정을 둬도 성립하는 것이다.)\n\\[\ncos(\\theta) = \\frac{\\mathbb{E}[X_i X_j]}{\\sqrt{\\mathbb{E}[X_i^2]}\\sqrt{\\mathbb{E}[X_j]^2}} = \\frac{cov(X_i, X_j)}{\\sigma_{X_i} \\sigma_{X_j}} = \\rho_{X_i, X_j}\n\\]\n이전 정리에서는 Variance sun law를 사용해서 correlation coefficient의 값의 범위를 증명했었는데, 위와 같은 방법으로도 값의 범주를 찾아볼 수 있다."
  },
  {
    "objectID": "posts/DSP-lec4-4/kr/Pearson-Correlation.html#pearson-correlation-and-its-geometric-interpretation",
    "href": "posts/DSP-lec4-4/kr/Pearson-Correlation.html#pearson-correlation-and-its-geometric-interpretation",
    "title": "Pearson Correlation",
    "section": "",
    "text": "이전 정리에서 살펴봤던 Pearson Correlation을 가져와보자.\n\\[\n\\begin{aligned}\n\\rho_{X_i, X_j} = \\frac{cov(X_i, X_j)}{\\sigma_{X_i} \\sigma_{X_j}} &= \\frac{\\mathbb{E}[(X_i - \\mathbb{E}[X_i])(X_j - \\mathbb{E}[X_j])]}{\\sqrt{\\mathbb{E}[(X_i - \\mathbb{E}[X_i])(X_i - \\mathbb{E}[X_i])]}\\sqrt{\\mathbb{E}[(X_j - \\mathbb{E}[X_j])(X_j - \\mathbb{E}[X_j])]}} \\\\\n&= \\frac{\\mathbb{E}[X_i X_j] - \\mathbb{E}[X_i]\\mathbb{E}[X_j]}{\\sqrt{\\mathbb{E}[X_i^2] - (\\mathbb{E}[X_i])^2} \\sqrt{\\mathbb{E}[X_j^2] - (\\mathbb{E}[X_j])^2}}\n\\end{aligned}\n\\]\n이와 비슷한 성격을 띄는 metric 중 하나가 cosine similarity라는 것이 있는데, 다음과 같이 정의된다.\n\\[\ncos(\\theta) = \\frac{x_i \\cdot x_j}{\\Vert x_i \\Vert \\Vert x_j \\Vert} = \\frac{\\sum_{d=1}^n x_{i, d} x_{j_d}}{\\sqrt{\\sum_{d=1}^n x_{i, d}^2}\\sqrt{\\sum_{d=1}^n x_{j, d}^2}}\n\\]\n사실 이 값은 cosine 값이기 때문에 잘 알고 있는 것처럼 -1과 1 사이의 값을 가진다. 여기에서 만약 각 비교하는 변수들의 평균이 0이라고 가정을 해보면 다음과 같이 정리가 된다. (물론 막연하게 평균이 0이라는 조건을 단 것이긴 하지만, 어떤 분포가 있을 때 분포의 평균을 0으로 만드는 것은 단순히 shift를 하면 가능한 것이기에 이와 같은 가정을 둬도 성립하는 것이다.)\n\\[\ncos(\\theta) = \\frac{\\mathbb{E}[X_i X_j]}{\\sqrt{\\mathbb{E}[X_i^2]}\\sqrt{\\mathbb{E}[X_j]^2}} = \\frac{cov(X_i, X_j)}{\\sigma_{X_i} \\sigma_{X_j}} = \\rho_{X_i, X_j}\n\\]\n이전 정리에서는 Variance sun law를 사용해서 correlation coefficient의 값의 범위를 증명했었는데, 위와 같은 방법으로도 값의 범주를 찾아볼 수 있다."
  },
  {
    "objectID": "posts/DSP-lec4-4/kr/Pearson-Correlation.html#some-properties-of-pearson-correlation",
    "href": "posts/DSP-lec4-4/kr/Pearson-Correlation.html#some-properties-of-pearson-correlation",
    "title": "Pearson Correlation",
    "section": "Some Properties of Pearson Correlation",
    "text": "Some Properties of Pearson Correlation\nCorrelation 에는 다음과 같은 몇가지 성질을 가진다.\n\n\\(\\rho_{X_i, X_i} = 1\\) : 동일한 값에 대한 correlation coefficient는 항상 1을 가진다.\n\\(\\rho_{X_i, X_j} = \\rho_{X_j, X_i}\\) : Correlation coefficient의 근간이 되는 Covariance matrix 자체가 symmetric 성격을 가지기에 역시 동일한 성격을 가진다.\n\\(Corr(a X_i + b, X_j) = sign(a) Corr(X_i, X_j) \\text{ for any } a, b \\in R\\) : shift가 되는 값은 상관없이 scale이 되는 값의 부호 만 영향을 미친다.\n\\(-1 \\le \\rho_{X_i, X_j} \\le 1\\) : 앞에서 설명\n$_{X_i, X_j} = 1 X_j = a X_i + b a, b R a $ : 두 변수간의 correlation의 절대값이 1이라는 의미는 결국 두 변수간에 어떤 선형적인 관계가 형성되어 있다는 것을 의미하며, 그런 경우 분포가 가지는 randomness는 한 변수만 가지게 된다.\n\\(X_i\\) 와 \\(X_j\\) 가 independent하면 \\(\\rho_{X_i, X_j} = 0\\) 이다.\n위 성질의 역치는 성립하지 않는다. (i.e. \\(\\rho_{X_i, X_j} = 0\\) 이더라도 두 변수 \\(X_i\\)와 \\(X_j\\)는 반드시 independent할 필요가 없다.)\n단, \\((X_i, X_j)\\) 가 bivariate normal distribution 이면서 \\(\\rho_{X_i, X_j}\\)가 0이라면 이때 두 변수는 indepedent하다."
  },
  {
    "objectID": "posts/DSP-lec4-1/kr/Multivariate-gaussian-distribution.html",
    "href": "posts/DSP-lec4-1/kr/Multivariate-gaussian-distribution.html",
    "title": "Multivariate Gaussian Distribution",
    "section": "",
    "text": "2개의 random variables로 구성된 분포에서 정보를 얻고자 할 때 가장 처음 시도해볼 수 있는 것이 바로 scatter plot이다.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nobjData = pd.read_excel('../../data/default of credit card clients.xls', skiprows=[1, 2])\nplt.scatter(x=objData['X12'], y=objData['X13'])\nplt.xlabel('X12')\nplt.ylabel('X13')\nplt.show()\n\n\n\n\n위의 그림에서도 보이듯 이 scatterplot을 통해서 random variable 간의 correlation이나 outlier의 유무, 어디에 밀집되어 있는지 등의 정보를 얻을 수 있다. 이렇게 variable이 2개이상인 multivariate distribution에서 variable간의 관계를 측정(measure)하기 위해서는 결국 해당 distribution을 표현할 수 있는 model을 만들어야 한다. 이번 장에서는 Multivariate Gaussian(Normal) Distribution을 대상으로 설명한다."
  },
  {
    "objectID": "posts/DSP-lec4-1/kr/Multivariate-gaussian-distribution.html#relation-between-two-random-variables",
    "href": "posts/DSP-lec4-1/kr/Multivariate-gaussian-distribution.html#relation-between-two-random-variables",
    "title": "Multivariate Gaussian Distribution",
    "section": "",
    "text": "2개의 random variables로 구성된 분포에서 정보를 얻고자 할 때 가장 처음 시도해볼 수 있는 것이 바로 scatter plot이다.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nobjData = pd.read_excel('../../data/default of credit card clients.xls', skiprows=[1, 2])\nplt.scatter(x=objData['X12'], y=objData['X13'])\nplt.xlabel('X12')\nplt.ylabel('X13')\nplt.show()\n\n\n\n\n위의 그림에서도 보이듯 이 scatterplot을 통해서 random variable 간의 correlation이나 outlier의 유무, 어디에 밀집되어 있는지 등의 정보를 얻을 수 있다. 이렇게 variable이 2개이상인 multivariate distribution에서 variable간의 관계를 측정(measure)하기 위해서는 결국 해당 distribution을 표현할 수 있는 model을 만들어야 한다. 이번 장에서는 Multivariate Gaussian(Normal) Distribution을 대상으로 설명한다."
  },
  {
    "objectID": "posts/DSP-lec4-1/kr/Multivariate-gaussian-distribution.html#multivariate-gaussian-distribution",
    "href": "posts/DSP-lec4-1/kr/Multivariate-gaussian-distribution.html#multivariate-gaussian-distribution",
    "title": "Multivariate Gaussian Distribution",
    "section": "Multivariate Gaussian Distribution",
    "text": "Multivariate Gaussian Distribution\nGaussian Distribution의 probability density function은 아래와 같이 정의할 수 있다.\n\\[\n\\mathcal{N}(x \\vert \\mu, \\sigma^2) =  \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp(- \\frac{1}{2 \\sigma^2}(x - \\mu)^2)\n\\]\n이를 Multivariate로 확장하게 되면, 기존에 sample의 standard deviation (\\(\\sigma^2\\))를 사용하던게 Covariance (\\(\\Sigma\\))로 변하게 되고, 표현식이 행렬 형태로 주어지게 된다.\n\\[\n\\mathcal{N}(x \\vert \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{D/2}} \\frac{1}{\\vert \\Sigma \\vert^{1/2}} \\exp (-\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu))\n\\]\n(참고로 \\(D\\) 는 distribution의 dimension을 나타냄)\n위 식에 대한 MLE Solution을 구해보려면 우선 주어진 분포에 대한 log likelihood를 계산해볼 수 있다. (pdf내에 exponential form이 있기 때문에 log likelihood를 사용하면 해당 form을 줄일 수 있다.)\n\\[\n\\begin{aligned}\n\\ln \\mathcal{N}(x \\vert \\mu, \\Sigma) &= -\\frac{1}{2} \\ln | \\Sigma | - \\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu) + C \\quad (\\text{Vector form}) \\\\\n&= -\\frac{1}{2} \\ln | \\Sigma | - \\frac{1}{2} \\sum^N_{n=1} (x_n - \\mu)^T \\Sigma^{-1} (x_n - \\mu) + C \\quad (\\text{Element form})\n\\end{aligned}\n\\]\n여기서 trace trick 이라는 것을 쓰면, 중간의 \\(\\Sigma^{-1}\\) 부분을 따로 뺄수 있게 된다.\n\nNOTE: Trace trick을 간단하게 요약하면 다음과 같다. 1) \\(\\frac{d}{dA} \\log |A| = A^{-T} \\quad\\), 2) \\(\\frac{d}{dA}Tr[AB] = \\frac{d}{dA}Tr[BA] = B^T\\)\n\n그러면 정확한 값은 아니지만 기존의 값과 비례하는 형태로 아래와 같이 정리할 수 있다.\n\\[\n\\begin{aligned}\n\\ln \\mathcal{N}(x \\vert \\mu, \\Sigma) &\\propto -\\frac{1}{2} \\ln | \\Sigma | - \\frac{1}{2} \\sum^N_{n=1} Tr [ \\Sigma^{-1}(x_n - \\mu)(x_n - \\mu)^T] \\\\\n&= -\\frac{1}{2} \\ln | \\Sigma | - \\frac{1}{2} Tr[\\Sigma^{-1} \\sum^N_{n=1} ((x_n - \\mu)(x_n - \\mu)^T)]\n\\end{aligned}\n\\]\n이중 뒤에 있는 \\(\\sum^N_{n=1} ((x_n - \\mu)(x_n - \\mu)^T)\\) 부분은 각 sample을 mean 값으로 빼준 것에 대한 총합이므로 결국 variance라는 것을 알 수 있고, log likelihood가 variance에 비례하다는 결론을 얻을 수 있다.\nLog-likelihood의 optimization을 위해서, 위 식을 각각의 변수 (\\(\\mu, \\Sigma\\))로 미분하고, 미분한 식에 대한 solution을 구해보면 다음과 같이 된다.\n\\[\n\\frac{d}{d\\mu} \\ln \\mathcal{N}(X \\vert \\mu, \\Sigma) = 0 \\rightarrow -\\frac{1}{2} \\times 2 \\times -1 \\times \\Sigma^{-1} \\sum^N_{n=1}(x_n - \\hat{\\mu}) = 0 \\\\\n\\hat{\\mu} = \\frac{\\sum^N_{n=1}x_n}{N}\n\\]\n\\[\n\\frac{d}{d\\Sigma^{-1}} \\ln \\mathcal{N}(X \\vert \\mu, \\Sigma) = 0 \\\\\n\\hat{\\Sigma} = \\frac{1}{N} \\sum^N_{n=1}(x_n - \\hat{\\mu})(x_n - \\hat{\\mu})^T\n\\]\n이렇게 Estimated MLE Solution을 구하면 결국 mean 값은 기존 sample의 mean과 동일하고, covariance는 기존 sample의 variance를 sample의 크기로 나눠준 형태와 동일하다는 것을 확인할 수 있다."
  },
  {
    "objectID": "posts/CS285-lec6-1/kr/actor-critic-algorithm.html",
    "href": "posts/CS285-lec6-1/kr/actor-critic-algorithm.html",
    "title": "Actor-Critic Algorithms",
    "section": "",
    "text": "Policy Gradient에서는 다음과 같은 cost function을 minimize 하는 방향으로 학습이 이뤄진다.\n\\[\n\\nabla_{\\theta} J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=1}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i, t} | s_{i, t}) \\Big( \\sum_{t'=1}^T r(s_{i, t'}, a_{i, t'}) \\Big)\n\\]\n이 식에서 마지막에 있는 \\(\\sum_{t'=1}^T r(s_{i, t'}, a_{i, t'})\\) 부분을 reward-to-go (이동 보상)이라고 하는데, 식이 의미하는 것은 \\(i\\) 번째 iteration에서 얻을 수 있는 reward의 총합이다. 이렇게 reward의 총합으로 사용할 수도 있고, 이를 대신하여 \\(\\hat{Q}_{i, t}\\) 를 사용할 수도 있다. 이는 특정 state \\(s_{i, t}\\) 에서 action \\(a_{i, t}\\) 를 취했을 때, 얻을 수 있는 expected reward의 추정치를 나타내는 것인데, 그러면 이 추정치를 얼마나 정확하게 구할 수 있을까가 문제가 된다. 당연히 현재의 환경이 randomness를 띄는 이상, 동일한 trajectory를 수행해도 expected reward가 다를 수 있다. 그래서 single trajectory를 통해서 reward-to-go를 구하면, 그만큼 high variance를 가지게 되고, 동일한 trajectory를 여러번 수행하면서 누적된 reward를 바탕으로 reward-to-go를 구하면, low variance를 가질 것이다. 이렇게 우리가 만약 진짜 expected reward-to-go를 아래와 같이\n\\[\nQ(s_t, a_t) = \\sum_{t'=t}^T \\mathbb{E}_{\\pi_{\\theta}} [r(s_{t'}, a_{t'}) | s_t, a_t]\n\\]\n이라고 정의할 수 있다면, 앞에서 소개한 cost function의 마지막 부분도 위 식으로 대체할 수 있을 것이다.\n\\[\n\\nabla_{\\theta} J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=1}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i, t} | s_{i, t}) Q(s_{i, t}, a_{i, t})\n\\]"
  },
  {
    "objectID": "posts/CS285-lec6-1/kr/actor-critic-algorithm.html#improving-the-policy-gradient",
    "href": "posts/CS285-lec6-1/kr/actor-critic-algorithm.html#improving-the-policy-gradient",
    "title": "Actor-Critic Algorithms",
    "section": "",
    "text": "Policy Gradient에서는 다음과 같은 cost function을 minimize 하는 방향으로 학습이 이뤄진다.\n\\[\n\\nabla_{\\theta} J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=1}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i, t} | s_{i, t}) \\Big( \\sum_{t'=1}^T r(s_{i, t'}, a_{i, t'}) \\Big)\n\\]\n이 식에서 마지막에 있는 \\(\\sum_{t'=1}^T r(s_{i, t'}, a_{i, t'})\\) 부분을 reward-to-go (이동 보상)이라고 하는데, 식이 의미하는 것은 \\(i\\) 번째 iteration에서 얻을 수 있는 reward의 총합이다. 이렇게 reward의 총합으로 사용할 수도 있고, 이를 대신하여 \\(\\hat{Q}_{i, t}\\) 를 사용할 수도 있다. 이는 특정 state \\(s_{i, t}\\) 에서 action \\(a_{i, t}\\) 를 취했을 때, 얻을 수 있는 expected reward의 추정치를 나타내는 것인데, 그러면 이 추정치를 얼마나 정확하게 구할 수 있을까가 문제가 된다. 당연히 현재의 환경이 randomness를 띄는 이상, 동일한 trajectory를 수행해도 expected reward가 다를 수 있다. 그래서 single trajectory를 통해서 reward-to-go를 구하면, 그만큼 high variance를 가지게 되고, 동일한 trajectory를 여러번 수행하면서 누적된 reward를 바탕으로 reward-to-go를 구하면, low variance를 가질 것이다. 이렇게 우리가 만약 진짜 expected reward-to-go를 아래와 같이\n\\[\nQ(s_t, a_t) = \\sum_{t'=t}^T \\mathbb{E}_{\\pi_{\\theta}} [r(s_{t'}, a_{t'}) | s_t, a_t]\n\\]\n이라고 정의할 수 있다면, 앞에서 소개한 cost function의 마지막 부분도 위 식으로 대체할 수 있을 것이다.\n\\[\n\\nabla_{\\theta} J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=1}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i, t} | s_{i, t}) Q(s_{i, t}, a_{i, t})\n\\]"
  },
  {
    "objectID": "posts/CS285-lec6-1/kr/actor-critic-algorithm.html#what-about-the-baseline",
    "href": "posts/CS285-lec6-1/kr/actor-critic-algorithm.html#what-about-the-baseline",
    "title": "Actor-Critic Algorithms",
    "section": "What about the baseline?",
    "text": "What about the baseline?\n여기에서 단순히 Q-value를 사용하면 좋고 나쁜 정도의 scale에 의한 차이가 발생하게 되고, 이로 이해서 bias가 발생하게 된다. 이 bias를 없애기 위한 방법으로 그냥 Q-value를 사용하는게 아니라, Q-value의 average를 빼는 방법을 사용할 수 있는데, 이를 baseline 이라고 한다.\n\\[\n\\begin{aligned}\n\\nabla_{\\theta} J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=1}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i, t} | s_{i, t}) ( Q(s_{i, t}, a_{i, t}) - b) \\\\ \\text{where } b_t = \\frac{1}{N} \\sum_{i=1}^N Q(s_{i, t}, a_{i, t})\n\\end{aligned}\n\\]\n이렇게 baseline을 빼주면 Q-value의 평균보다 높은 부분과 낮은 부분이 명확하게 구분되면서 unbiased 한 성격을 띄게 되고, 이는 variance를 줄여주는 효과를 가지게 된다. 그런데 단순히 Q-value의 평균을 빼주는 것이 아니라, 다른 값으로도 baseline을 빼주는 것이 가능하다. 보통 Q-function은 state와 action에 대한 함수인데, 이를 state에 대한 함수, 즉 Value function $ V(s_t) = {a_t {}(a_t | s_t)} [Q(s_t, a_t)] $ 로 사용할 수도 있다. 그러면 기존의 cost function은 아래와 같이 변형될 수 있다.\n\\[\n\\nabla_{\\theta} J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=1}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i, t} | s_{i, t}) ( Q(s_{i, t}, a_{i, t}) - V(s_{i, t}))\n\\]\n이렇게 Q-value에서 V-value를 빼주는 것의 의미는 state \\(s_{i, t}\\) 에서 action \\(a_{i, t}\\) 를 취했을 때, \\(s_{i, t}\\) 에서 얻을 수 있는 평균 value보다 얼마나 좋은지를 나타내는 지표가 되는 것이다. 그러면 이에 대한 gradient descent를 취하게 되면 평균보다 좋은 action에 대해서는 encourage하고, 평균보다 나쁜 action에 대해서는 discourage하는 효과를 가지게 되고, 결과적으로 variance를 줄여주면서 policy gradient의 학습을 더 빠르게 만들어주는 효과를 가지게 된다. 참고로 이 부분을 advantage function 이라고 한다."
  },
  {
    "objectID": "posts/CS285-lec6-1/kr/actor-critic-algorithm.html#state-state-action-value-functions",
    "href": "posts/CS285-lec6-1/kr/actor-critic-algorithm.html#state-state-action-value-functions",
    "title": "Actor-Critic Algorithms",
    "section": "State & State-action value functions",
    "text": "State & State-action value functions\n그러면 advantage function을 계산할 때 필요한 각각의 value function이 어떤 의미를 가지는지 다시 살펴볼 필요가 있다.\n\\[\nQ^{\\pi}(s_t, a_t) = \\sum_{t'=t}^T \\mathbb{E}_{\\pi_{\\theta}} [r(s_{t'}, a_{t'}) | s_t, a_t]\n\\]\nQ value, 혹은 state-action value function 이라는 것은 state \\(s_t\\) 에서 \\(a_t\\) 를 취했을 때 얻을 수 있는 total reward를 말한다. policy마다 이 Q value가 다 다를수 있기 때문에 이를 명시하기 위해서 \\(\\pi\\) 라는 것을 Q에 넣어주는 표기를 취한다.\n\\[\nV^{\\pi}(s_t) = \\mathbb{E}_{a_t \\sim \\pi_{\\theta}(a_t | s_t)} [Q^{\\pi}(s_t, a_t)]\n\\]\nV value (state value function)는 state \\(s_t\\) 에서 얻을 수 있는 total reward의 기댓값을 말한다. 이 때는 policy \\(\\pi\\) 에 의해서 action이 결정되기 때문에 위의 Q value의 총합의 형태로 나오게 된다. 그리고 이어서 나오는 값이 앞에서 나온 advantage function이다.\n\\[\nA^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})\n\\]\nAdvantage function은 state \\(s_t\\) 에서 취한 action \\(a_t\\)가 평균보다 얼마나 좋은지, 말그대로 얼마나 advantage를 가지는지를 나타내는 값이다. 이를 활용하면 앞에서 소개한 Policy Gradient 식은 다음과 같이 정리할 수 있다.\n\\[\n\\nabla_{\\theta} J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=1}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i, t} | s_{i, t}) A^{\\pi}(s_{i, t}, a_{i, t})\n\\]\n만약 advantage의 추정값이 정확해진다면, 그만큼 잘된 action과 잘못된 action에 대한 차이가 명확해지고, 이는 variance를 줄여주는 효과를 가지게 된다. 물론 추정값이 정확하지 않으면서 bias가 생기는 문제를 해결하기 위해서 baseline을 빼주는 방법을 사용할 수 있다.\n\\[\n\\nabla_{\\theta} J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=1}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i, t} | s_{i, t}) \\big( \\sum^T_{t=t'} r(s_{i, t'}, a_{i, t'}) - b \\big)\n\\]\n이와 같이 total reward를 사용하는 monte calro estimate를 사용하게 되면 bias가 발생하지 않지만, 이렇게 계산한 total reward는 policy가 어떤 trajectory를 따라 갔느냐에 따라서 다르게 나오게 되기에, single sample에 대한 variance가 매우 크게 나타난다.\n\n\n\nFigure 1: policy_gradient\n\n\nFigure 1 에 나와 있는 것처럼 Policy Gradient의 수행은\n\n주어진 policy \\(\\pi_{\\theta}\\) 에 대해서 trajectory를 생성한다.\ntrajectory의 return을 추정할 수 있는 model을 학습한다.\nmodel을 기반으로 policy를 update한다.\n\n의 과정으로 나눠질텐데, 이제 성능을 개선하기 위해서는 trejectory의 return을 얼마나 정확하게 학습할 것인지가 중요해진다."
  },
  {
    "objectID": "posts/CS285-lec6-1/kr/actor-critic-algorithm.html#value-function-fitting",
    "href": "posts/CS285-lec6-1/kr/actor-critic-algorithm.html#value-function-fitting",
    "title": "Actor-Critic Algorithms",
    "section": "Value function fitting",
    "text": "Value function fitting\n그러면 소개된 내용처럼 다양한 value function (\\(Q^{\\pi}, V^{\\pi}, A^{\\pi}\\))이 있고, 이 중 어떤 것을 사용할 것인지 궁금할 수 있다. 먼저 Q value를 다시 나열해보면\n\\[ \\begin{aligned}\nQ^{\\pi}(s_t, a_t) &= \\sum_{t'=t}^T \\mathbb{E}_{\\pi_{\\theta}} [r(s_{t'}, a_{t'}) | s_t, a_t] \\\\\n&= r(s_t, a_t) + \\sum_{t'=t+1}^T \\mathbb{E}_{\\pi_{\\theta}} [r(s_{t'}, a_{t'}) | s_t, a_t] \\\\\n\\end{aligned}\n\\]\n와 같이 현재의 reward와 state \\(s_t\\), action \\(a_t\\) 에서의 future reward의 합으로 표현할 수 있는데, 사실 뒤의 식은 \\(s_{t+1}\\) 에서의 V value와 같다는 것을 알 수 있다. 그러면\n\\[\n\\begin{aligned}\nQ^{\\pi}(s_t, a_t) &= r(s_t, a_t) + \\mathbb{E}_{s_{t+1} \\sim p(s_{t+1} | s_t, a_t)} [V^{\\pi}(s_{t+1})] \\\\\n&\\approx r(s_t, a_t) + V^{\\pi}(s_{t+1})\n\\end{aligned}\n\\]\n라고 정리할 수 있다. 여기서 두번째 식을 보면 expectation 값이 하나의 V value로 근사화되어 있는 것이 보이는데, 물론 이 값은 정확한 값이 아니다. 앞에서 언급된 것처럼 single sample로 reward sum을 계산한 값은 variance가 크기 때문에 정확하다고 볼수는 없지만, 그래도 single sample에 대해서 근사한 값을 결과적으로 전체 sample에 대해서 fitting하게 되므로 결과적으로 이런 근사값을 그대로 사용할 수 있다. 그럼 이 값을 굳이 왜 구했느냐? 바로 아래 식으로 정리하기 위해서다.\n\\[\n\\begin{aligned}\nA^{\\pi}(s_t, a_t) &\\approx Q^{\\pi}(s_t, a_t) - V^{\\pi}(s_t) \\\\\n&\\approx r(s_t, a_t) + V^{\\pi}(s_{t+1}) - V^{\\pi}(s_t)\n\\end{aligned}\n\\]\nAdvantage function이 Q value와 V value로 이뤄져 있지만, Q value가 결국은 V value로 구할 수 있는 값이기에, 다시 정리를 하면 V value, 즉 state에만 dependent한 function이 되는 것이다. 이렇게 되면 굳이 state와 action 두가지 변수가 아닌 state 하나만 신경쓰면 되기 때문에 조금 더 학습이 용이해진다. 그러면 이제 V function만 잘 추정할 수 있는 신경망 모델을 학습시키면 된다. (물론 추후에 다루겠지만, state와 action을 모두 학습하는 형태도 존재한다.)"
  },
  {
    "objectID": "posts/CS285-lec6-1/kr/actor-critic-algorithm.html#policy-evaluation",
    "href": "posts/CS285-lec6-1/kr/actor-critic-algorithm.html#policy-evaluation",
    "title": "Actor-Critic Algorithms",
    "section": "Policy evaluation",
    "text": "Policy evaluation\n\\[\nV^{\\pi}(s_t) = \\sum_{t'=t}^T \\mathbb{E}_{\\pi_{\\theta}} [r(s_{t'}, a_{t'}) | s_t]\n\\]\n결과적으로 지금까지 하려는 일은 \\(V^{\\pi}\\) 를 계산하는 것인데, 이는 현재 state \\(s_t\\) 에서의 value를 계산하는 것이므로, 어떻게 보면 현재의 policy의 value를 평가하는 policy evaluation이라고 할 수 있다. 만약 initial state \\(s_1\\) 에서의 policy evaluation을 수행했다면, 그때의 policy gradient는 다음과 같을 것이다.\n\\[\nJ(\\theta) = \\mathbb{E}_{s_1 \\sim p(s_1)} [V^{\\pi}(s_1)]\n\\]\npolicy gradient에서는 앞에서 소개된 것처럼 Monte Carlo policy evaluation, 즉 처음부터 종료될 때까지의 모든 reward의 총합에 대한 기대치를 바탕으로 계산하는데, 사실 한개의 sample에 대해서만 expectation을 구하는 것이라면 굳이 기대값을 구하지 않더라도 \\(V^{\\pi}(s_t) \\approx \\sum_{t'=t}^T r(s_{t'}, a_{t'})\\) 로 근사화할 수 있다. 물론 정상적인 근사를 하려면 동일한 policy로 동일한 state에 접근했을때의 trajectory가 각각 다르게 나오기 때문에, 여러번 sampling을 한 값으로 근사하는 것이 맞겠지만,\n\\[\nV^{\\pi}(s_t) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t'=t}^T r(s_{t'}, a_{t'})\n\\]\nModel-free과 같은 설정에서는 동일한 trajectory에 대해서 여러번 반복 수행한다는 것이 불가능하기 때문에, 이렇게 근사화하는 것이 일반적이다. 그래서 보통 이렇게 Monte Carlo Policy Evaluation을 한다면 아예 simulator를 reset하고 다시 \\(s_1\\) 에서 시작하는 방식으로 진행되는 것을 확인할 수 있다."
  },
  {
    "objectID": "posts/CS285-lec6-1/kr/actor-critic-algorithm.html#monte-carlo-evaluation-with-function-approximation",
    "href": "posts/CS285-lec6-1/kr/actor-critic-algorithm.html#monte-carlo-evaluation-with-function-approximation",
    "title": "Actor-Critic Algorithms",
    "section": "Monte Carlo evaluation with function approximation",
    "text": "Monte Carlo evaluation with function approximation\n사실 state에 대한 value를 구하는 것은 어렵기 때문에 신경망을 사용한 function approximation 기법을 사용하는 것이 일반적이다. 이렇게 되면 비슷한 state에 대한 value는 비슷하게 나오는 일종의 generalization 효과도 가져오게 된다. 그러면 이제 문제는 강화학습이 아닌 어떻게 하면 신경망을 잘 학습시킬지에 대한 supervised regression 문제가 되며, 해당 신경망을 학습시키기 위한 데이터로 주어진 state \\(s_t\\) 와 이에 대한 value function \\(V^{\\pi}(s_t)\\) 를 사용하게 된다. 그러면 \\(\\phi\\)를 weight으로 가지는 신경망에 대한 loss function은 우리가 알고 있는 것처럼 MSE 방식으로 정의해도 된다.\n\\[\n\\mathcal{L}(\\phi) = \\frac{1}{2} \\sum_i \\Vert \\hat{V}^{\\pi}_{\\phi}(s_i) - y_i \\Vert^2\n\\]"
  },
  {
    "objectID": "posts/CS285-lec6-1/kr/actor-critic-algorithm.html#can-we-do-better",
    "href": "posts/CS285-lec6-1/kr/actor-critic-algorithm.html#can-we-do-better",
    "title": "Actor-Critic Algorithms",
    "section": "Can we do better?",
    "text": "Can we do better?\n우리가 label로 사용하고 있는 \\(V{\\pi}\\) 는 사실 expectation을 구할 수 없기 때문에 대체로 사용한 근사값이고 실제로는 아래와 같은 label을 사용하는 것이 맞다.\n\\[\n\\begin{aligned}\ny_{i, t} &= \\sum_{t'=t}^T \\mathbb{E}_{\\pi_{\\theta}}[r(s_{i, t'}, a_{i, t'}) | s_{i, t}] \\\\\n&\\approx r(s_{i, t}, a_{i, t}) + \\sum_{t'=t+1}^T \\mathbb{E}_{\\pi_{\\theta}}[r(s_{i, t'}, a_{i, t'}) | s_{i, t+1}]\n\\end{aligned}\n\\]\n그런데 뒤에 있는 식도 앞에서 근사화했던 논리를 그대로 가져오면 동일하게 value function으로 근사한 값으로 볼 수 있다. \\[\ny_{i, t} \\approx r(s_{i, t}, a_{i, t}) + V^{\\pi}(s_{i, t+1}) \\approx r(s_{i, t}, a_{i, t}) + \\hat{V}^{\\pi}_{\\phi}(s_{i, t+1})   \n\\]\n그래서 앞 장에서 소개한 것처럼 신경망을 사용한 function approximation을 동일하게 사용할 수 있는데, 사실 엄밀하게 말하면 이때 계산되는 function value는 실제로 구하는 value가 아니라, 이전에 주어진 training data를 통해서 학습된 것을 사용하는, bootstrap estimator를 통해 나온 값이다. 그러면 기존의 training data인 (\\(s_{i, t}, \\sum_{t'=t}^T r(s_{i, t'}, a_{i, t'})\\)) 가 아닌, (\\(s_{i, t}, r(s_{i, t}, a_{i, t}) + \\hat{V}^{\\pi}_{\\phi}(s_{i, t+1})\\)) 를 사용하게 되는데, 이렇게 이전에 학습된 모델을 사용하는 방식을 bootstrapping 이라고 하고,이전에 학습된 모델을 사용하면서 variance가 줄어들게 되고, 학습이 더 빠르게 이뤄지는 효과를 가지게 된다. 물론 신경망에 의존한 bootstrapped estimate을 사용하기 때문에 한번 잘못된 값이 나오면 이에 대한 bias가 커지는 trade-off가 존재한다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chans Lecture Note",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nDimensionality Reduction\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nDifferentiation in python\n\n\n\n\n\n\n\nPython\n\n\nCoursera\n\n\nDeepLearning.AI\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nLinear Transformation\n\n\n\n\n\n\n\nPython\n\n\nCoursera\n\n\nDeepLearning.AI\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nMatrix Multiplication\n\n\n\n\n\n\n\nPython\n\n\nCoursera\n\n\nDeepLearning.AI\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nVector Operations: Scalar Multiplication, Sum and Dot Product of Vectors\n\n\n\n\n\n\n\nPython\n\n\nCoursera\n\n\nDeepLearning.AI\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nSolving Linear Systems: 3 Variables\n\n\n\n\n\n\n\nPython\n\n\nCoursera\n\n\nDeepLearning.AI\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nSolving Linear Systems: 2 Variables\n\n\n\n\n\n\n\nPython\n\n\nCoursera\n\n\nDeepLearning.AI\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to NumPy Arrays\n\n\n\n\n\n\n\nPython\n\n\nCoursera\n\n\nDeepLearning.AI\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2023\n\n\nChanseok Kang\n\n\n\n\n\n\n  \n\n\n\n\nLogistic Regression with a Neural Network mindset\n\n\n\n\n\n\n\nPython\n\n\nCoursera\n\n\nDeepLearning.AI\n\n\n\n\n\n\n\n\n\n\n\nMay 11, 2022\n\n\nChanseok Kang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Chans Lecture Note",
    "section": "",
    "text": "In theory, there is no difference between theory and practice. In practice, there is."
  },
  {
    "objectID": "about.html#chanseok-kang",
    "href": "about.html#chanseok-kang",
    "title": "Chans Lecture Note",
    "section": "Chanseok Kang",
    "text": "Chanseok Kang\nI’m Research Engineer in LG Electronics. My role is to deploy energy-efficient algorithm in commercial/business product(refrigerator, washing machine, HVAC etc). Also, I have an experience on developing BSP drivers on 5G environment. I’m very interested in learning something new(AI, Embedded System, Mathematics). Especially, I enjoyed taking MOOC (Massive Open Online Course) when I have free time."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Chans Lecture Note",
    "section": "Education",
    "text": "Education\n Seoul National University | Seoul, South Korea  M.S in Electrical Engineering and Computer Science | Mar 2013 - Feb 2015 \n\nTopic: Manycore-based efficient Power Management Framework\nThesis: Hierachical Power Management Framework on Manycore systems using OS Migration Techniques\nAdvisor: Prof. Bernhard Egger\n\nChung-Ang University | Seoul, South Korea  B.S in Electrical and Electronics Engineering | Mar 2006 - Feb 2013"
  },
  {
    "objectID": "about.html#main-experience",
    "href": "about.html#main-experience",
    "title": "Chans Lecture Note",
    "section": "Main Experience",
    "text": "Main Experience\n LG Electronics | Specialist | Dec 2014 - present\nCarnegie Mellon University | Visiting Researcher | Apr 2019 - Aug 2019\nKorea Institute of Science and Technology (KIST) | Contract Researcher | Jun 2012 - Dec 2012"
  },
  {
    "objectID": "posts/Active-OPS/kr/active-offline-policy-selection.html",
    "href": "posts/Active-OPS/kr/active-offline-policy-selection.html",
    "title": "Active Offline Policy Selection",
    "section": "",
    "text": "저자: Ksenia Konyushkova, Yutian Chen, Tom Le Paine, Caglar Gulcehre, Cosmin Paduraru, Daniel J Mankowitz, Misha Denil, Nando de Freitas\n발표: NeurIPS 2021\n논문\nProject Site\nOpenReview\nCode (Tensorflow & Tensorflow Probability)"
  },
  {
    "objectID": "posts/Active-OPS/kr/active-offline-policy-selection.html#tldr",
    "href": "posts/Active-OPS/kr/active-offline-policy-selection.html#tldr",
    "title": "Active Offline Policy Selection",
    "section": "TL;DR",
    "text": "TL;DR\n이 논문에서는 offline rl policy를 로봇이나 추천 시스템과 같은 실제 도메인에 적용시킬 때 최적의 policy를 선택하는 방법에 대한 내용을 담고 있다. 기존에도 Off-policy Evaluation (OPE)에 대한 내용들이 다뤄지고 있었지만, 여전히 OPE에 의한 evaluation과 fully online evaluation간의 gap이 존재하던 것은 사실이다. 이 논문에서는 Active offline policy selection이란 방법을 통해서 offline data와 online interaction을 통해서 지속적으로 best policy를 찾는 방법에 대해서 다룬다. 먼저 OPE estimate을 사용해서 online evalution에 대한 warm start를 수행한 후, policy similarity를 나타내는 kernel을 활용한 Bayesian Optimization 을 통해서 다음으로 evaluate할 policy 를 선택하는 과정을 반복한다."
  },
  {
    "objectID": "posts/Active-OPS/kr/active-offline-policy-selection.html#introduction",
    "href": "posts/Active-OPS/kr/active-offline-policy-selection.html#introduction",
    "title": "Active Offline Policy Selection",
    "section": "Introduction",
    "text": "Introduction\n\n\n\nFigure 1: Off-policy Selection\n\n\n대부분의 offline rl 논문은 simulator에 의존해서 성능을 평가하고 이를 토대로 evaluation을 수행하지만 enviornment와의 interaction에 대한 cost가 큰 실제 환경에서는 이런 training과 evaluation 과정이 제한적일 수 밖에 없다. 이때문에 Offline RL 문제에서는 여러개의 알고리즘과 각각의 hyperparameter의 구성으로 모델을 학습시키고, Figure 1 에 소개되어 있는 것처럼 off-policy policy evalution (OPE)를 통해서 모델을 선택하는 과정을 거치게 된다. 일반적인 online RL처럼 policy evaluation을 하되, 주어진 데이터 내에서 policy evaluation을 수행하는 것인데, 문제는 이 OPE가 어떤 좋은 모델을 고르기에는 정확하지 않다는 것이다. 또한 OPE 자체가 주어진 데이터 내로 한정되어 evaluation을 수행하기 때문에 offline RL이 겪는 distribution shift 문제, 즉 data를 수집할 때 취했던 behavior policy와 실제 학습된 policy간의 차이가 존재하는 문제를 겪게 된다.\n그래서 논문에서 제안하는 Active Offline Policy Selection 는 Figure 1 의 두번째 그림에서 소개되고 있는 것처럼 offline data와 소량이긴 하지만 online evaluation 결과를 활용하여 best policy를 찾는 방법에 대한 것이다. 기본적으로는 Bayesian Optimization (BO)을 활용하고, 각 policy별 좋은 정도(논문에서는 expected return을 사용했다.)를 추정하는데 Gaussian Process (GP)를 사용했다. 핵심적인 요소는 딱 두가지가 있는데, 먼저 기존에 알려져있는 OPE 추정치를 환경상에 존재하는 추가적인 noise observation으로 활용해서, 실제로 online evaluation 할때 조금더 GP에 의한 hyperparameter optimization을 쉽게 할 수 있도록 일종의 warm start 역할을 수행하게 한다. 그 다음에 여러 개의 후보군 policy 들 중에서 동일 state에 대한 action을 어떻게 취했는지에 대한 similarity를 나타내는 kernel을 활용해서 모델링을 했고, 이를 통해서 다음ㅇ로 evaluate할 policy를 선택하게 된다. 이를 통해 굳이 실제 환경에서 수행하지 않더라도 best policy를 찾는 근거를 제공하기 때문에 그만큼 데이터 활용에 대한 효율성을 높일 수 있다. 자세한 내용은 다른 파트에서 부연 설명하고자 한다.\n\nOff-policy policy evaluation and selection\n보통 Policy Evalutation이라고 하는 것은 policy \\(\\pi\\) 의 가치를 구하는 것을 의미하며, discounted reward에 대한 expected sum을 통해서 계산할 수 있다.\n\\[\n\\mu_{\\pi} = \\mathbb{E}\\Big[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) \\Big], \\text{with } s_0 \\sim d_0(\\cdot), a_t \\sim \\pi(\\cdot|s_t), s_{t+1} \\sim T(\\cdot|s_t, a_t)\n\\]\nOnline RL에서는 해당 policy를 주어진 환경에서 직접 수행하면서 \\(\\mu_{\\pi}\\) 의 가치를 추정할 수 있지만, offline RL에서는 주어진 데이터만 활용한 OPE 기법을 사용한다. 일반적으로 OPE는 현재 policy가 아닌 별도의 behavior policy \\(\\pi_{\\beta}\\) 에 의해서 수집된 trajectory들의 집합 \\(\\mathcal{D}\\) 를 사용해서 value \\(\\hat{\\mu_{\\pi}}\\) 을 추정한다. 그리고 이어지는 Offline Policy Selection (OPS)을 통해 후보군 policy 들 중에서 가장 좋은 estimated value를 가지는 policy 를 찾는 과정이 이뤄진다. 이렇게 OPE 추정치를 직접적으로 활용하는 방법도 있고, 별도의 quality metric를 정해서 이를 통해서 policy를 선택하는 방법도 있다."
  },
  {
    "objectID": "posts/Active-OPS/kr/active-offline-policy-selection.html#active-offline-policy-selection",
    "href": "posts/Active-OPS/kr/active-offline-policy-selection.html#active-offline-policy-selection",
    "title": "Active Offline Policy Selection",
    "section": "Active Offline Policy Selection",
    "text": "Active Offline Policy Selection\n\nProblem Definition"
  },
  {
    "objectID": "posts/CS285-lec6-2/kr/from-evaluation-to-actor-critic.html",
    "href": "posts/CS285-lec6-2/kr/from-evaluation-to-actor-critic.html",
    "title": "From Evaluation to Actor Critic",
    "section": "",
    "text": "지난 글에서 다뤘던 기본적인 (batch) actor-critic 알고리즘은 아래와 같다.\n\n\n\\begin{algorithm} \\caption{(batch) actor-critic algorithm} \\begin{algorithmic} \\State sample \\{$s_i, a_i$\\} from $\\pi_{\\theta}(a|s)$ (interaction) \\State fit $\\hat{V}^{\\pi}_{\\phi}(s)$ to sampled reward sums \\State evaluate $\\hat{A}^{\\pi}(s_i, a_i) = r(s_i, a_i) + \\hat{V}^{\\pi}_{\\phi}(s_{i+1}) - \\hat{V}^{\\pi}_{\\phi}(s_i)$ \\State $\\nabla_{\\theta} \\approx \\sum_i \\nabla_{\\theta} \\log \\pi_{\\theta}(a_i|s_i) \\hat{A}^{\\pi}(s_i, a_i)$ \\State $\\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta}$ \\end{algorithmic} \\end{algorithm}\n\n\n결과적으로 actor-critic algorithm의 핵심은 샘플링된 reward sum을 estimate할 수 있는 value function \\(\\hat{V}^{\\pi}_{\\phi}(s)\\) 를 학습하는 것이고, 이를 바탕으로 advantage function \\(\\hat{A}^{\\pi}(s_i, a_i)\\) 까지 계산하는 것이다. 그리고 이전에 학습된 신경망에서 뽑은 값으로 next target을 구하는 bootstrapping 방식을 사용한다는 것까지 다뤘다."
  },
  {
    "objectID": "posts/CS285-lec6-2/kr/from-evaluation-to-actor-critic.html#an-actor-critic-algorithm",
    "href": "posts/CS285-lec6-2/kr/from-evaluation-to-actor-critic.html#an-actor-critic-algorithm",
    "title": "From Evaluation to Actor Critic",
    "section": "",
    "text": "지난 글에서 다뤘던 기본적인 (batch) actor-critic 알고리즘은 아래와 같다.\n\n\n\\begin{algorithm} \\caption{(batch) actor-critic algorithm} \\begin{algorithmic} \\State sample \\{$s_i, a_i$\\} from $\\pi_{\\theta}(a|s)$ (interaction) \\State fit $\\hat{V}^{\\pi}_{\\phi}(s)$ to sampled reward sums \\State evaluate $\\hat{A}^{\\pi}(s_i, a_i) = r(s_i, a_i) + \\hat{V}^{\\pi}_{\\phi}(s_{i+1}) - \\hat{V}^{\\pi}_{\\phi}(s_i)$ \\State $\\nabla_{\\theta} \\approx \\sum_i \\nabla_{\\theta} \\log \\pi_{\\theta}(a_i|s_i) \\hat{A}^{\\pi}(s_i, a_i)$ \\State $\\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta}$ \\end{algorithmic} \\end{algorithm}\n\n\n결과적으로 actor-critic algorithm의 핵심은 샘플링된 reward sum을 estimate할 수 있는 value function \\(\\hat{V}^{\\pi}_{\\phi}(s)\\) 를 학습하는 것이고, 이를 바탕으로 advantage function \\(\\hat{A}^{\\pi}(s_i, a_i)\\) 까지 계산하는 것이다. 그리고 이전에 학습된 신경망에서 뽑은 값으로 next target을 구하는 bootstrapping 방식을 사용한다는 것까지 다뤘다."
  },
  {
    "objectID": "posts/CS285-lec6-2/kr/from-evaluation-to-actor-critic.html#aside-discount-factors",
    "href": "posts/CS285-lec6-2/kr/from-evaluation-to-actor-critic.html#aside-discount-factors",
    "title": "From Evaluation to Actor Critic",
    "section": "Aside: discount factors",
    "text": "Aside: discount factors\n그래서 결국 value function을 fit 하는 문제는 아래와 같이 정의할 수 있게 된다.\n\\[\ny_{i, t} \\approx r(s_{i, t}, a_{i, t}) + \\hat{V}^{\\pi}_{\\phi}(s_{i, t+1})\n\\] \\[\n\\mathcal{L}(\\phi) = \\frac{1}{2} \\sum_{i} \\Vert \\hat{V}^{\\pi}_{\\phi}(s_{i, t}) - y_{i, t} \\Vert^2\n\\]\n그런데 만약, value function의 boundary, 즉 episode의 길이가 무한정이라면 어떻게 될까? 그 말은 episode가 끝나지 않으면서 계속 reward를 더하는 형태로 value function이 update될 것이고, 위 식에 따르면 \\(\\hat{V}_{\\phi}^{\\pi}\\) 도 역시 무한정으로 커지게 된다. 이를 다룰 수 있는 방법은 현재의 순간과 가까운 시점에 받은 reward에 대해서 더 큰 가중치를 주는 것이고, 이때 discount factor \\(\\gamma\\) 를 사용한다. 그러면 value function은 다시 정의할 수 있다.\n\\[\ny_{i, t} \\approx r(s_{i, t}, a_{i, t}) + \\gamma \\hat{V}^{\\pi}_{\\phi}(s_{i, t+1}) \\qquad ,\\gamma \\in [0, 1]\n\\]\n일반적으로 \\(\\gamma\\) 는 0.99라는 1에 가까운 값을 사용하는데, 현재 시점에 먼 시점에 reward를 받은 것에 대해서는 \\(\\gamma^n\\) 의 형태로 가중치가 감가된다. (n은 시점의 차이, 그래서 보통 번역본에는 감가 상수라고 표현되어 있다.) 보통 \\(\\gamma\\) 는 학습자가 정하는 hyperparameter로 알려져 있지만, 실제로 어떤 \\(\\gamma\\) 를 사용했느냐에 따라서 MDP가 변하는 경우도 존재한다. 강의에서는 \\(n\\) step후에 죽는 경우에 대한 예시를 들었는데, 어떤 값을 선택하냐에 따라서 episode가 끝나는 시점이 달라지면서, 결국에는 환경의 transition probability에 영향을 주기도 한다."
  },
  {
    "objectID": "posts/CS285-lec6-2/kr/from-evaluation-to-actor-critic.html#aside-discount-factors-for-policy-gradient",
    "href": "posts/CS285-lec6-2/kr/from-evaluation-to-actor-critic.html#aside-discount-factors-for-policy-gradient",
    "title": "From Evaluation to Actor Critic",
    "section": "Aside: discount factors for policy gradient",
    "text": "Aside: discount factors for policy gradient\n그러면 위에서 다룬 discount factor를 (monte carlo) policy gradient에 적용할 수 있느냐에 대한 의문이 생길 수 있다. 다르게 표현하면 policy gradient에서도 현재에 가까운 시점에 받은 reward의 비중을 더 크게 반영할 수 있느냐인데, 실제 식으로 넣게 된다면 해당 강의에서 다뤘던 reward-to-go 부분에 \\(\\gamma\\) 를 넣어주면 된다.\n\\[\n\\text{Option 1:} \\qquad \\nabla_{\\theta}J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i, t}|s_{i, t}) \\left( \\sum_{t'=t}^{T} \\gamma^{t'-t} r(s_{i, t'}, a_{i, t'}) \\right)\n\\]\n\\[\n\\text{Option 2:} \\qquad \\nabla_{\\theta}J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i, t}|s_{i, t}) \\right) \\left( \\sum_{t=1}^{T} \\gamma^{t-1} r(s_{i, t'}, a_{i, t'}) \\right)\n\\]\n여기에서 잠깐 Option 2에 대해서 살펴볼 필요가 있다. 이 식은 바로 위에 있는 Option 1과 유사해보이지만, 뒤에 reward가 더해지는 부분에서 reward-to-go가 아닌 처음 시점부터 보게끔 되어 있다는 부분에서 조금 차이가 있다. 그런데 사실 첫번째 bracket과 두번째 bracket의 시점이 \\(t=1\\) 부터 \\(T\\) 까지의 시점이므로, 두번째 bracket의 \\(\\gamma\\) 를 쪼개서 (\\(\\gamma \\rightarrow \\gamma^{1 \\sim t-1} \\times \\gamma^{t \\sim T}\\)) 첫번째 bracket에 넣어주면 Option 1과 유사한 식이 만들어진다.\n\\[\n\\nabla_{\\theta}J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N}  \\sum_{t=1}^{T} {\\color{red}\\gamma^{t-1}} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i, t}|s_{i, t}) \\left( \\sum_{t=1}^{T} \\gamma^{t-1} r(s_{i, t'}, a_{i, t'}) \\right)\n\\]\n사실 위 식의 \\(\\gamma^{t-1}\\) 의 역할은 reward에서의 discount factor의 역할과 비슷하다. reward에서의 discount factor는 정의 그대로 현재에 가까운 시점에 받은 reward에 대한 가중치를 의미하고, 위 식의 \\(\\gamma\\) 도 역시 현재에 가까운 시점에 취한 policy \\(\\pi_{\\theta}\\)에 가중치를 더 부여하겠다는 것을 의미한다. 예를 들어서 내가 현재 시점에 100 달러를 받으면서 한 행동과 1년 뒤에 100 달러를 받으면서 한 행동의 가치를 다르게 부여하겠다는 것이다. 어떻게 보면 Option 2에서 정의한 식이 실제 policy gradient를 계산하는데 더 적합하다고 볼 수도 있다. 강의에서는 이를 later steps matter less, 즉 뒤로 갈수록 더 적은 가중치를 부여한다고 표현하였다."
  },
  {
    "objectID": "posts/CS285-lec6-2/kr/from-evaluation-to-actor-critic.html#which-version-is-the-right-one",
    "href": "posts/CS285-lec6-2/kr/from-evaluation-to-actor-critic.html#which-version-is-the-right-one",
    "title": "From Evaluation to Actor Critic",
    "section": "Which version is the right one?",
    "text": "Which version is the right one?\n그럼에도 불구하고 실제로 사용하는 식은 Option 2가 아닌, Option 1이다. 왜일까? 사실 우리가 discount factor를 사용함으로써 원하는 것은 policy의 영향력을 변화시키는 것이 아니라, 현재와 가까운 reward에 가중치를 부여하는 것이다. 앞에서 언급한 것처럼 finite한 episode 상에서는 최대한 빠르게 goal에 도달하는 것을 원하기 때문에, 상식적으로는 Option 2처럼 policy에도 \\(\\gamma\\) 를 가하는게 맞는 것처럼 보일지는 몰라도 실제로는 Option 1처럼 reward-to-go term에만 \\(\\gamma\\) 를 사용하는 것이다. 그리고 이는 결국 policy gradient의 variance를 줄여주는 역할도 같이 수행한다."
  },
  {
    "objectID": "posts/CS285-lec6-2/kr/from-evaluation-to-actor-critic.html#actor-critic-algorithm-with-discount",
    "href": "posts/CS285-lec6-2/kr/from-evaluation-to-actor-critic.html#actor-critic-algorithm-with-discount",
    "title": "From Evaluation to Actor Critic",
    "section": "Actor-critic algorithm (with discount)",
    "text": "Actor-critic algorithm (with discount)\n그래서 결국 discount factor가 적용된 actor-critic algorithm은 아래와 같이 정의할 수 있다. 사실 Algorithm 1 에서 3번째 줄의 advantage function을 계산하는 부분에서 discount factor를 적용하는 차이가 생기는 것일뿐, 거의 동일하다.\n\n\n\\begin{algorithm} \\caption{(batch) actor-critic algorithm with discount} \\begin{algorithmic} \\State sample \\{$s_i, a_i$\\} from $\\pi_{\\theta}(a|s)$ (interaction) \\State fit $\\hat{V}^{\\pi}_{\\phi}(s)$ to sampled reward sums \\State evaluate $\\hat{A}^{\\pi}(s_i, a_i) = r(s_i, a_i) + {\\color{red} \\gamma}\\hat{V}^{\\pi}_{\\phi}(s_{i+1}) - \\hat{V}^{\\pi}_{\\phi}(s_i)$ \\State $\\nabla_{\\theta} \\approx \\sum_i \\nabla_{\\theta} \\log \\pi_{\\theta}(a_i|s_i) \\hat{A}^{\\pi}(s_i, a_i)$ \\State $\\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta}$ \\end{algorithmic} \\end{algorithm}\n\n\n혹은 online 상황에서 single time step에서 얻은 trajectory로 update하는 경우에는 아래와 같이 정의할 수 있다.\n\n\n\\begin{algorithm} \\caption{online actor-critic algorithm with discount} \\begin{algorithmic} \\State take action $a \\sim \\pi_{\\theta}(a|s)$, get $(s, a, s', r)$ \\State update $\\hat{V}^{\\pi}_{\\phi}$ using target $r + \\gamma \\hat{V}^{\\pi}_{\\phi}(s')$ \\State evaluate $\\hat{A}^{\\pi}(s, a) = r(s, a) + \\gamma \\hat{V}^{\\pi}_{\\phi}(s') - \\hat{V}^{\\pi}_{\\phi}(s)$ \\State $\\nabla_{\\theta} \\approx \\sum_i \\nabla_{\\theta} \\log \\pi_{\\theta}(a|s) \\hat{A}^{\\pi}(s, a)$ \\State $\\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta}$ \\end{algorithmic} \\end{algorithm}"
  },
  {
    "objectID": "posts/Differentiation-in-python/en/differentiation-in-python.html",
    "href": "posts/Differentiation-in-python/en/differentiation-in-python.html",
    "title": "Differentiation in python",
    "section": "",
    "text": "In this notebook, you explore which tools and libraries are available in Python to compute derivatives. You will perform symbolic differentiation with SymPy library, numerical with NumPy and automatic with JAX (based on Autograd). Comparing the speed of calculations, you will investigate the computational efficiency of those three methods. This posts summarized the lecture “Calculus for Machine Learning and Data Science” from Coursera."
  },
  {
    "objectID": "posts/Differentiation-in-python/en/differentiation-in-python.html#tldr",
    "href": "posts/Differentiation-in-python/en/differentiation-in-python.html#tldr",
    "title": "Differentiation in python",
    "section": "",
    "text": "In this notebook, you explore which tools and libraries are available in Python to compute derivatives. You will perform symbolic differentiation with SymPy library, numerical with NumPy and automatic with JAX (based on Autograd). Comparing the speed of calculations, you will investigate the computational efficiency of those three methods. This posts summarized the lecture “Calculus for Machine Learning and Data Science” from Coursera."
  },
  {
    "objectID": "posts/Differentiation-in-python/en/differentiation-in-python.html#functions-in-python",
    "href": "posts/Differentiation-in-python/en/differentiation-in-python.html#functions-in-python",
    "title": "Differentiation in python",
    "section": "Functions in Python",
    "text": "Functions in Python\nThis is just a reminder how to define functions in Python. A simple function \\(f\\left(x\\right) = x^2\\), it can be set up as:\n\ndef f(x):\n    return x**2\n\nprint(f(3))\n\n9\n\n\nYou can easily find the derivative of this function analytically. You can set it up as a separate function:\n\ndef dfdx(x):\n    return 2*x\n\nprint(dfdx(3))\n\n6\n\n\nSince you have been working with the NumPy arrays, you can apply the function to each element of an array:\n\nimport numpy as np\n\nx_array = np.array([1, 2, 3])\n\nprint(\"x: \\n\", x_array)\nprint(\"f(x) = x**2: \\n\", f(x_array))\nprint(\"f'(x) = 2x: \\n\", dfdx(x_array))\n\nx: \n [1 2 3]\nf(x) = x**2: \n [1 4 9]\nf'(x) = 2x: \n [2 4 6]\n\n\nNow you can apply those functions f and dfdx to an array of a larger size. The following code will plot function and its derivative (you don’t have to understand the details of the plot_f1_and_f2 function at this stage):\n\nimport matplotlib.pyplot as plt\n\n# Output of plotting commands is displayed inline within the Jupyter notebook.\n%matplotlib inline\n\ndef plot_f1_and_f2(f1, f2=None, x_min=-5, x_max=5, label1=\"f(x)\", label2=\"f'(x)\"):\n    x = np.linspace(x_min, x_max,100)\n\n    # Setting the axes at the centre.\n    fig = plt.figure()\n    ax = fig.add_subplot(1, 1, 1)\n    ax.spines['left'].set_position('center')\n    ax.spines['bottom'].set_position('zero')\n    ax.spines['right'].set_color('none')\n    ax.spines['top'].set_color('none')\n    ax.xaxis.set_ticks_position('bottom')\n    ax.yaxis.set_ticks_position('left')\n\n    plt.plot(x, f1(x), 'r', label=label1)\n    if not f2 is None:\n        # If f2 is an array, it is passed as it is to be plotted as unlinked points.\n        # If f2 is a function, f2(x) needs to be passed to plot it.        \n        if isinstance(f2, np.ndarray):\n            plt.plot(x, f2, 'bo', markersize=3, label=label2,)\n        else:\n            plt.plot(x, f2(x), 'b', label=label2)\n    plt.legend()\n\n    plt.show()\n    \nplot_f1_and_f2(f, dfdx)\n\n\n\n\nIn real life the functions are more complicated and it is not possible to calculate the derivatives analytically every time. Let’s explore which tools and libraries are available in Python for the computation of derivatives without manual derivation."
  },
  {
    "objectID": "posts/Differentiation-in-python/en/differentiation-in-python.html#symbolic-differentiation",
    "href": "posts/Differentiation-in-python/en/differentiation-in-python.html#symbolic-differentiation",
    "title": "Differentiation in python",
    "section": "Symbolic Differentiation",
    "text": "Symbolic Differentiation\nSymbolic computation deals with the computation of mathematical objects that are represented exactly, not approximately (e.g. \\(\\sqrt{2}\\) will be written as it is, not as \\(1.41421356237\\)). For differentiation it would mean that the output will be somehow similar to if you were computing derivatives by hand using rules (analytically). Thus, symbolic differentiation can produce exact derivatives.\n\nIntroduction to Symbolic Computation with SymPy\nLet’s explore symbolic differentiation in Python with commonly used SymPy library.\nIf you want to compute the approximate decimal value of \\(\\sqrt{18}\\), you could normally do it in the following way:\n\nimport math\n\nmath.sqrt(18)\n\n4.242640687119285\n\n\nThe output \\(4.242640687119285\\) is an approximate result. You may recall that \\(\\sqrt{18} = \\sqrt{9 \\cdot 2} = 3\\sqrt{2}\\) and see that it is pretty much impossible to deduct it from the approximate result. But with the symbolic computation systems the roots are not approximated with a decimal number but rather only simplified, so the output is exact:\n\nfrom sympy import *\n\n\nsqrt(18)\n\n\\(\\displaystyle 3 \\sqrt{2}\\)\n\n\nNumerical evaluation of the result is available, and you can set number of the digits to show in the approximated output:\n\nN(sqrt(18), 8)\n\n\\(\\displaystyle 4.2426407\\)\n\n\nIn SymPy variables are defined using symbols. In this particular library they need to be predefined (a list of them should be provided). Have a look in the cell below, how the symbolic expression, correspoinding to the mathematical expression \\(2x^2 - xy\\), is defined:\n\n# List of symbols.\nx, y = symbols('x y')\n# Definition of the expression.\nexpr = 2 * x**2 - x * y\nexpr\n\n\\(\\displaystyle 2 x^{2} - x y\\)\n\n\nNow you can perform various manipulations with this expression: add or subtract some terms, multiply by other expressions etc., just like if you were doing it by hands:\n\nexpr_manip = x * (expr + x * y + x**3)\nexpr_manip\n\n\\(\\displaystyle x \\left(x^{3} + 2 x^{2}\\right)\\)\n\n\nYou can also expand the expression:\n\nexpand(expr_manip)\n\n\\(\\displaystyle x^{4} + 2 x^{3}\\)\n\n\nOr factorise it:\n\nfactor(expr_manip)\n\n\\(\\displaystyle x^{3} \\left(x + 2\\right)\\)\n\n\nTo substitute particular values for the variables in the expression, you can use the following code:\n\nexpr.evalf(subs={x:-1, y:2})\n\n\\(\\displaystyle 4.0\\)\n\n\nThis can be used to evaluate a function \\(f\\left(x\\right) = x^2\\):\n\nf_symb = x ** 2\nf_symb.evalf(subs={x: 3})\n\n\\(\\displaystyle 9.0\\)\n\n\nYou might be wondering now, is it possible to evaluate the symbolic functions for each element of the array? At the beginning of the lab you have defined a NumPy array x_array:\n\nprint(x_array)\n\n[1 2 3]\n\n\nNow try to evaluate function f_symb for each element of the array. You will get an error:\n\ntry:\n    f_symb(x_array)\nexcept TypeError as err:\n    print(err)\n\n'Pow' object is not callable\n\n\nIt is possible to evaluate the symbolic functions for each element of the array, but you need to make a function NumPy-friendly first:\n\nfrom sympy.utilities.lambdify import lambdify\nf_symb_numpy = lambdify(x, f_symb, 'numpy')\n\nThe following code should work now:\n\nprint(\"x: \\n\", x_array)\nprint(\"f(x) = x**2: \\n\", f_symb_numpy(x_array))\n\nx: \n [1 2 3]\nf(x) = x**2: \n [1 4 9]\n\n\nSymPy has lots of great functions to manipulate expressions and perform various operations from calculus. More information about them can be found in the official documentation here.\n\n\nSymbolic Differentiation with SymPy\nLet’s try to find a derivative of a simple power function using SymPy:\n\ndiff(x**3,x)\n\n\\(\\displaystyle 3 x^{2}\\)\n\n\nSome standard functions can be used in the expression, and SymPy will apply required rules (sum, product, chain) to calculate the derivative:\n\ndfdx_composed = diff(exp(-2*x) + 3*sin(3*x), x)\ndfdx_composed\n\n\\(\\displaystyle 9 \\cos{\\left(3 x \\right)} - 2 e^{- 2 x}\\)\n\n\nNow calculate the derivative of the function f_symb defined in 2.1 and make it NumPy-friendly:\n\ndfdx_symb = diff(f_symb, x)\ndfdx_symb_numpy = lambdify(x, dfdx_symb, 'numpy')\n\nEvaluate function dfdx_symb_numpy for each element of the x_array:\n\nprint(\"x: \\n\", x_array)\nprint(\"f'(x) = 2x: \\n\", dfdx_symb_numpy(x_array))\n\nx: \n [1 2 3]\nf'(x) = 2x: \n [2 4 6]\n\n\nYou can apply symbolically defined functions to the arrays of larger size. The following code will plot function and its derivative, you can see that it works:\n\nplot_f1_and_f2(f_symb_numpy, dfdx_symb_numpy)\n\n\n\n\n\n\nLimitations of Symbolic Differentiation\nSymbolic Differentiation seems to be a great tool. But it also has some limitations. Sometimes the output expressions are too complicated and even not possible to evaluate. For example, find the derivative of the function \\[\\left|x\\right| = \\begin{cases} x, \\ \\text{if}\\ x &gt; 0\\\\  -x, \\ \\text{if}\\ x &lt; 0 \\\\ 0, \\ \\text{if}\\ x = 0\\end{cases}\\] Analytically, its derivative is: \\[\\frac{d}{dx}\\left(\\left|x\\right|\\right) = \\begin{cases} 1, \\ \\text{if}\\ x &gt; 0\\\\  -1, \\ \\text{if}\\ x &lt; 0\\\\\\ \\text{does not exist}, \\ \\text{if}\\ x = 0\\end{cases}\\]\nHave a look the output from the symbolic differentiation:\n\ndfdx_abs = diff(abs(x),x)\ndfdx_abs\n\n\\(\\displaystyle \\frac{\\left(\\operatorname{re}{\\left(x\\right)} \\frac{d}{d x} \\operatorname{re}{\\left(x\\right)} + \\operatorname{im}{\\left(x\\right)} \\frac{d}{d x} \\operatorname{im}{\\left(x\\right)}\\right) \\operatorname{sign}{\\left(x \\right)}}{x}\\)\n\n\nLooks complicated, but it would not be a problem if it was possible to evaluate. But check, that for \\(x=-2\\) instead of the derivative value \\(-1\\) it outputs some unevaluated expression:\n\ndfdx_abs.evalf(subs={x:-2})\n\n\\(\\displaystyle - \\left. \\frac{d}{d x} \\operatorname{re}{\\left(x\\right)} \\right|_{\\substack{ x=-2 }}\\)\n\n\nAnd in the NumPy friendly version it also will give an error:\n\ndfdx_abs_numpy = lambdify(x, dfdx_abs,'numpy')\n\ntry:\n    dfdx_abs_numpy(np.array([1, -2, 0]))\nexcept NameError as err:\n    print(err)\n\nname 'Derivative' is not defined\n\n\nIn fact, there are problems with the evaluation of the symbolic expressions wherever there is a “jump” in the derivative (e.g. function expressions are different for different intervals of \\(x\\)), like it happens with \\(\\frac{d}{dx}\\left(\\left|x\\right|\\right)\\).\nAlso, you can see in this example, that you can get a very complicated function as an output of symbolic computation. This is called expression swell, which results in unefficiently slow computations. You will see the example of that below after learning other differentiation libraries in Python."
  },
  {
    "objectID": "posts/Differentiation-in-python/en/differentiation-in-python.html#numerical-differentiation",
    "href": "posts/Differentiation-in-python/en/differentiation-in-python.html#numerical-differentiation",
    "title": "Differentiation in python",
    "section": "Numerical Differentiation",
    "text": "Numerical Differentiation\nThis method does not take into account the function expression. The only important thing is that the function can be evaluated in the nearby points \\(x\\) and \\(x+\\Delta x\\), where \\(\\Delta x\\) is sufficiently small. Then \\(\\frac{df}{dx}\\approx\\frac{f\\left(x + \\Delta x\\right) - f\\left(x\\right)}{\\Delta x}\\), which can be called a numerical approximation of the derivative.\nBased on that idea there are different approaches for the numerical approximations, which somehow vary in the computation speed and accuracy. However, for all of the methods the results are not accurate - there is a round off error. At this stage there is no need to go into details of various methods, it is enough to investigate one of the numerial differentiation functions, available in NumPy package.\n\nNumerical Differentiation with NumPy\nYou can call function np.gradient to find the derivative of function \\(f\\left(x\\right) = x^2\\) defined above. The first argument is an array of function values, the second defines the spacing \\(\\Delta x\\) for the evaluation. Here pass it as an array of \\(x\\) values, the differences will be calculated automatically. You can find the documentation here.\n\nx_array_2 = np.linspace(-5, 5, 100)\ndfdx_numerical = np.gradient(f(x_array_2), x_array_2)\n\nplot_f1_and_f2(dfdx_symb_numpy, dfdx_numerical, label1=\"f'(x) exact\", label2=\"f'(x) approximate\")\n\n\n\n\nTry to do numerical differentiation for more complicated function:\n\ndef f_composed(x):\n    return np.exp(-2*x) + 3*np.sin(3*x)\n\nplot_f1_and_f2(lambdify(x, dfdx_composed, 'numpy'), np.gradient(f_composed(x_array_2), x_array_2),\n              label1=\"f'(x) exact\", label2=\"f'(x) approximate\")\n\n\n\n\nThe results are pretty impressive, keeping in mind that it does not matter at all how the function was calculated - only the final values of it!\n\n\nLimitations of Numerical Differentiation\nObviously, the first downside of the numerical differentiation is that it is not exact. However, the accuracy of it is normally enough for machine learning applications. At this stage there is no need to evaluate errors of the numerical differentiation.\nAnother problem is similar to the one which appeared in the symbolic differentiation: it is inaccurate at the points where there are “jumps” of the derivative. Let’s compare the exact derivative of the absolute value function and with numerical approximation:\n\ndef dfdx_abs(x):\n    if x &gt; 0:\n        return 1\n    else:\n        if x &lt; 0:\n            return -1\n        else:\n            return None\n\nplot_f1_and_f2(np.vectorize(dfdx_abs), np.gradient(abs(x_array_2), x_array_2))\n\n\n\n\nYou can see that the results near the “jump” are \\(0.5\\) and \\(-0.5\\), while they should be \\(1\\) and \\(-1\\). These cases can give significant errors in the computations.\nBut the biggest problem with the numerical differentiation is slow speed. It requires function evalutation every time. In machine learning models there are hundreds of parameters and there are hundreds of derivatives to be calculated, performing full function evaluation every time slows down the computation process. You will see the example of it below."
  },
  {
    "objectID": "posts/Differentiation-in-python/en/differentiation-in-python.html#automatic-differentiation",
    "href": "posts/Differentiation-in-python/en/differentiation-in-python.html#automatic-differentiation",
    "title": "Differentiation in python",
    "section": "Automatic Differentiation",
    "text": "Automatic Differentiation\nAutomatic differentiation (autodiff) method breaks down the function into common functions (\\(sin\\), \\(cos\\), \\(log\\), power functions, etc.), and constructs the computational graph consisting of the basic functions. Then the chain rule is used to compute the derivative at any node of the graph. It is the most commonly used approach in machine learning applications and neural networks, as the computational graph for the function and its derivatives can be built during the construction of the neural network, saving in future computations.\nThe main disadvantage of it is implementational difficulty. However, nowadays there are libraries that are convenient to use, such as MyGrad, Autograd and JAX. Autograd and JAX are the most commonly used in the frameworks to build neural networks. JAX brings together Autograd functionality for optimization problems, and XLA (Accelerated Linear Algebra) compiler for parallel computing.\nThe syntax of Autograd and JAX are slightly different. It would be overwhelming to cover both at this stage. In this notebook you will be performing automatic differentiation using one of them: JAX.\n\nIntroduction to JAX\nTo begin with, load the required libraries. From jax package you need to load just a couple of functions for now (grad and vmap). Package jax.numpy is a wrapped NumPy, which pretty much replaces NumPy when JAX is used. It can be loaded as np as if it was an original NumPy in most of the cases. However, in this notebook you’ll upload it as jnp to distinguish them for now.\n\nfrom jax import grad, vmap\nimport jax.numpy as jnp\n\nCreate a new jnp array and check its type.\n\nx_array_jnp = jnp.array([1., 2., 3.])\n\nprint(\"Type of NumPy array:\", type(x_array))\nprint(\"Type of JAX NumPy array:\", type(x_array_jnp))\n# Please ignore the warning message if it appears.\n\nType of NumPy array: &lt;class 'numpy.ndarray'&gt;\nType of JAX NumPy array: &lt;class 'jaxlib.xla_extension.Array'&gt;\n\n\nThe same array can be created just converting previously defined x_array = np.array([1, 2, 3]), although in some cases JAX does not operate with integers, thus the values need to be converted to floats. You will see an example of it below.\n\nx_array_jnp = jnp.array(x_array.astype('float32'))\nprint(\"JAX NumPy array:\", x_array_jnp)\nprint(\"Type of JAX NumPy array:\", type(x_array_jnp))\n\nJAX NumPy array: [1. 2. 3.]\nType of JAX NumPy array: &lt;class 'jaxlib.xla_extension.Array'&gt;\n\n\nNote, that jnp array has a specific type jaxlib.xla_extension.Array. In most of the cases the same operators and functions are applicable to them as in the original NumPy, for example:\n\nprint(x_array_jnp * 2)\nprint(x_array_jnp[2])\n\n[2. 4. 6.]\n3.0\n\n\nBut sometimes working with jnp arrays the approach needs to be changed. In the following code, trying to assign a new value to one of the elements, you will get an error:\n\ntry:\n    x_array_jnp[2] = 4.0\nexcept TypeError as err:\n    print(err)\n\n'&lt;class 'jaxlib.xla_extension.Array'&gt;' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\n\n\nTo assign a new value to an element in the jnp array you need to apply functions .at[i], stating which element to update, and .set(value) to set a new value. These functions also operate out-of-place, the updated array is returned as a new array and the original array is not modified by the update.\n\ny_array_jnp = x_array_jnp.at[2].set(4.0)\nprint(y_array_jnp)\n\n[1. 2. 4.]\n\n\nAlthough, some of the JAX functions will work with arrays defined with np and jnp. In the following code you will get the same result in both lines:\n\nprint(jnp.log(x_array))\nprint(jnp.log(x_array_jnp))\n\n[0.        0.6931472 1.0986123]\n[0.        0.6931472 1.0986123]\n\n\nThis is probably confusing - which NumPy to use then? Usually when JAX is used, only jax.numpy gets imported as np, and used instead of the original one.\n\n\nAutomatic Differentiation with JAX\nTime to do automatic differentiation with JAX. The following code will calculate the derivative of the previously defined function \\(f\\left(x\\right) = x^2\\) at the point \\(x = 3\\):\n\nprint(\"Function value at x = 3:\", f(3.0))\nprint(\"Derivative value at x = 3:\",grad(f)(3.0))\n\nFunction value at x = 3: 9.0\nDerivative value at x = 3: 6.0\n\n\nVery easy, right? Keep in mind, please, that this cannot be done using integers. The following code will output an error:\n\ntry:\n    grad(f)(3)\nexcept TypeError as err:\n    print(err)\n\ngrad requires real- or complex-valued inputs (input dtype that is a sub-dtype of np.inexact), but got int32. If you want to use Boolean- or integer-valued inputs, use vjp or set allow_int to True.\n\n\nTry to apply the grad function to an array, calculating the derivative for each of its elements:\n\ntry:\n    grad(f)(x_array_jnp)\nexcept TypeError as err:\n    print(err)\n\nGradient only defined for scalar-output functions. Output had shape: (3,).\n\n\nThere is some broadcasting issue there. You don’t need to get into more details of this at this stage, function vmap can be used here to solve the problem.\nNote: Broadcasting is covered in the Course 1 of this Specialization “Linear Algebra”. You can also review it in the documentation here.\n\ndfdx_jax_vmap = vmap(grad(f))(x_array_jnp)\nprint(dfdx_jax_vmap)\n\n[2. 4. 6.]\n\n\nGreat, now vmap(grad(f)) can be used to calculate the derivative of function f for arrays of larger size and you can plot the output:\n\nplot_f1_and_f2(f, vmap(grad(f)))\n\n\n\n\nIn the following code you can comment/uncomment lines to visualize the common derivatives. All of them are found using JAX automatic differentiation. The results look pretty good!\n\ndef g(x):\n#     return x**3\n#     return 2*x**3 - 3*x**2 + 5\n#     return 1/x\n#     return jnp.exp(x)\n#     return jnp.log(x)\n#     return jnp.sin(x)\n#     return jnp.cos(x)\n    return jnp.abs(x)\n#     return jnp.abs(x)+jnp.sin(x)*jnp.cos(x)\n\nplot_f1_and_f2(g, vmap(grad(g)))"
  },
  {
    "objectID": "posts/Differentiation-in-python/en/differentiation-in-python.html#computational-efficiency-of-symbolic-numerical-and-automatic-differentiation",
    "href": "posts/Differentiation-in-python/en/differentiation-in-python.html#computational-efficiency-of-symbolic-numerical-and-automatic-differentiation",
    "title": "Differentiation in python",
    "section": "Computational Efficiency of Symbolic, Numerical and Automatic Differentiation",
    "text": "Computational Efficiency of Symbolic, Numerical and Automatic Differentiation\nIn previous sections, low computational efficiency of symbolic and numerical differentiation was discussed. Now it is time to compare speed of calculations for each of three approaches. Try to find the derivative of the same simple function \\(f\\left(x\\right) = x^2\\) multiple times, evaluating it for an array of a larger size, compare the results and time used:\n\nimport timeit, time\n\nx_array_large = np.linspace(-5, 5, 1000000)\n\ntic_symb = time.time()\nres_symb = lambdify(x, diff(f(x),x),'numpy')(x_array_large)\ntoc_symb = time.time()\ntime_symb = 1000 * (toc_symb - tic_symb)  # Time in ms.\n\ntic_numerical = time.time()\nres_numerical = np.gradient(f(x_array_large),x_array_large)\ntoc_numerical = time.time()\ntime_numerical = 1000 * (toc_numerical - tic_numerical)\n\ntic_jax = time.time()\nres_jax = vmap(grad(f))(jnp.array(x_array_large.astype('float32')))\ntoc_jax = time.time()\ntime_jax = 1000 * (toc_jax - tic_jax)\n\nprint(f\"Results\\nSymbolic Differentiation:\\n{res_symb}\\n\" + \n      f\"Numerical Differentiation:\\n{res_numerical}\\n\" + \n      f\"Automatic Differentiation:\\n{res_jax}\")\n\nprint(f\"\\n\\nTime\\nSymbolic Differentiation:\\n{time_symb} ms\\n\" + \n      f\"Numerical Differentiation:\\n{time_numerical} ms\\n\" + \n      f\"Automatic Differentiation:\\n{time_jax} ms\")\n\nResults\nSymbolic Differentiation:\n[-10.       -9.99998  -9.99996 ...   9.99996   9.99998  10.     ]\nNumerical Differentiation:\n[-9.99999 -9.99998 -9.99996 ...  9.99996  9.99998  9.99999]\nAutomatic Differentiation:\n[-10.       -9.99998  -9.99996 ...   9.99996   9.99998  10.     ]\n\n\nTime\nSymbolic Differentiation:\n3.3597946166992188 ms\nNumerical Differentiation:\n34.9583625793457 ms\nAutomatic Differentiation:\n153.17511558532715 ms\n\n\nThe results are pretty much the same, but the time used is different. Numerical approach is obviously inefficient when differentiation needs to be performed many times, which happens a lot training machine learning models. Symbolic and automatic approach seem to be performing similarly for this simple example. But if the function becomes a little bit more complicated, symbolic computation will experiance significant expression swell and the calculations will slow down.\nNote: Sometimes the execution time results may vary slightly, especially for automatic differentiation. You can run the code above a few time to see different outputs. That does not influence the conclusion that numerical differentiation is slower. timeit module can be used more efficiently to evaluate execution time of the codes, but that would unnecessary overcomplicate the codes here.\nTry to define some polynomial function, which should not be that hard to differentiate, and compare the computation time for its differentiation symbolically and automatically:\n\ndef f_polynomial_simple(x):\n    return 2*x**3 - 3*x**2 + 5\n\ndef f_polynomial(x):\n    for i in range(3):\n        x = f_polynomial_simple(x)\n    return x\n\ntic_polynomial_symb = time.time()\nres_polynomial_symb = lambdify(x, diff(f_polynomial(x),x),'numpy')(x_array_large)\ntoc_polynomial_symb = time.time()\ntime_polynomial_symb = 1000 * (toc_polynomial_symb - tic_polynomial_symb)\n\ntic_polynomial_jax = time.time()\nres_polynomial_jax = vmap(grad(f_polynomial))(jnp.array(x_array_large.astype('float32')))\ntoc_polynomial_jax = time.time()\ntime_polynomial_jax = 1000 * (toc_polynomial_jax - tic_polynomial_jax)\n\nprint(f\"Results\\nSymbolic Differentiation:\\n{res_polynomial_symb}\\n\" + \n      f\"Automatic Differentiation:\\n{res_polynomial_jax}\")\n\nprint(f\"\\n\\nTime\\nSymbolic Differentiation:\\n{time_polynomial_symb} ms\\n\" +  \n      f\"Automatic Differentiation:\\n{time_polynomial_jax} ms\")\n\nResults\nSymbolic Differentiation:\n[2.88570423e+24 2.88556400e+24 2.88542377e+24 ... 1.86202587e+22\n 1.86213384e+22 1.86224181e+22]\nAutomatic Differentiation:\n[2.8857043e+24 2.8855642e+24 2.8854241e+24 ... 1.8620253e+22 1.8621349e+22\n 1.8622416e+22]\n\n\nTime\nSymbolic Differentiation:\n911.6637706756592 ms\nAutomatic Differentiation:\n700.4127502441406 ms\n\n\nAgain, the results are similar, but automatic differentiation is times faster.\nWith the increase of function computation graph, the efficiency of automatic differentiation compared to other methods raises, because autodiff method uses chain rule!"
  },
  {
    "objectID": "posts/DSP-lec4-3/kr/Covariance-Matrix.html",
    "href": "posts/DSP-lec4-3/kr/Covariance-Matrix.html",
    "title": "Covariance Matrix",
    "section": "",
    "text": "두 개의 random variable 간의 Variance를 Matrix 형태로 표현한 것을 Covariance Matrix 라고 한다. 일반적으로 두 variable간의 관계를 표현한 것이기 때문에 square matrix 형태를 가지며, symmetric 하면서, positive semi-definite한 성격을 가진다. \\[\na^T \\Sigma a \\ge 0, \\text{for all }a \\in \\mathbb{R}^n\n\\]\n\nNOTE: Positive semi-definite 하다는 것은 Matrix가 symmetric하면서, 다음의 성격을 가지는 것을 말한다. \\(v^T A v \\ge 0 \\text{ for }v \\in \\mathbb{R}^n\\)\n\n그리고 covariance matrix의 구성요소는 다음과 같이 정의된다.\n\\[\n\\Sigma_{X_i, X_j} = cov[X_i, X_j] = \\mathbb{E}[(X_i - \\mathbb{E}[X_i])(X_j - \\mathbb{E}[X_j])]\n\\] (참고로 위 식은 이전 포스트에서 Covariance에 대한 MLE Solution을 유도하면서 구한 내용이다.)\n이렇게 대면 대각 성분 (diagnoal value)들은 같은 index에 대해서 평균값을 빼준 것을 제곱한 형태가 되므로 결국 variance라는 것을 알 수 있다.\n그런데 covariance 값을 위 식을 통해서 구할 수는 있어도, 이 값을 계산하는데 어떠한 normalization이 적용되지 않았기 때문에, 값만 보고 이게 어떤 의미다라는 것을 도출하기가 쉽지 않다. 예를 들어서 covariance가 1이다 라는 정보는 두 random variable 간의 상대적인 변화를 나타내기 때문에 값이 크다, 작다만 가지고 정보량을 얻을 수 없다. 그래서 이 값을 normalization한 후의 결과를 활용하는데, 이를 (Pearson) Correlation Coefficient라고 한다.\n\\[\n\\rho_{X_i, X_j} = \\frac{cov(X_i, X_j)}{\\sigma_{X_i} \\sigma_{X_j}} = (diag(\\Sigma))^{-\\frac{1}{2}} \\Sigma (diag(\\Sigma))^{-\\frac{1}{2}}\n\\]"
  },
  {
    "objectID": "posts/DSP-lec4-3/kr/Covariance-Matrix.html#covariance-matrix",
    "href": "posts/DSP-lec4-3/kr/Covariance-Matrix.html#covariance-matrix",
    "title": "Covariance Matrix",
    "section": "",
    "text": "두 개의 random variable 간의 Variance를 Matrix 형태로 표현한 것을 Covariance Matrix 라고 한다. 일반적으로 두 variable간의 관계를 표현한 것이기 때문에 square matrix 형태를 가지며, symmetric 하면서, positive semi-definite한 성격을 가진다. \\[\na^T \\Sigma a \\ge 0, \\text{for all }a \\in \\mathbb{R}^n\n\\]\n\nNOTE: Positive semi-definite 하다는 것은 Matrix가 symmetric하면서, 다음의 성격을 가지는 것을 말한다. \\(v^T A v \\ge 0 \\text{ for }v \\in \\mathbb{R}^n\\)\n\n그리고 covariance matrix의 구성요소는 다음과 같이 정의된다.\n\\[\n\\Sigma_{X_i, X_j} = cov[X_i, X_j] = \\mathbb{E}[(X_i - \\mathbb{E}[X_i])(X_j - \\mathbb{E}[X_j])]\n\\] (참고로 위 식은 이전 포스트에서 Covariance에 대한 MLE Solution을 유도하면서 구한 내용이다.)\n이렇게 대면 대각 성분 (diagnoal value)들은 같은 index에 대해서 평균값을 빼준 것을 제곱한 형태가 되므로 결국 variance라는 것을 알 수 있다.\n그런데 covariance 값을 위 식을 통해서 구할 수는 있어도, 이 값을 계산하는데 어떠한 normalization이 적용되지 않았기 때문에, 값만 보고 이게 어떤 의미다라는 것을 도출하기가 쉽지 않다. 예를 들어서 covariance가 1이다 라는 정보는 두 random variable 간의 상대적인 변화를 나타내기 때문에 값이 크다, 작다만 가지고 정보량을 얻을 수 없다. 그래서 이 값을 normalization한 후의 결과를 활용하는데, 이를 (Pearson) Correlation Coefficient라고 한다.\n\\[\n\\rho_{X_i, X_j} = \\frac{cov(X_i, X_j)}{\\sigma_{X_i} \\sigma_{X_j}} = (diag(\\Sigma))^{-\\frac{1}{2}} \\Sigma (diag(\\Sigma))^{-\\frac{1}{2}}\n\\]"
  },
  {
    "objectID": "posts/DSP-lec4-3/kr/Covariance-Matrix.html#pearson-correlation",
    "href": "posts/DSP-lec4-3/kr/Covariance-Matrix.html#pearson-correlation",
    "title": "Covariance Matrix",
    "section": "Pearson Correlation",
    "text": "Pearson Correlation\n그러면 위의 correlation coefficient를 풀어서 정의해보면 아래와 같다.\n\\[\n\\begin{aligned}\n\\rho_{X_i, X_j} = \\frac{cov(X_i, X_j)}{\\sigma_{X_i} \\sigma_{X_j}} &= \\frac{\\mathbb{E}[(X_i - \\mathbb{E}[X_i])(X_j - \\mathbb{E}[X_j])]}{\\sqrt{\\mathbb{E}[(X_i - \\mathbb{E}[X_i])(X_i - \\mathbb{E}[X_i])]}\\sqrt{\\mathbb{E}[(X_j - \\mathbb{E}[X_j])(X_j - \\mathbb{E}[X_j])]}} \\\\\n&= \\frac{\\mathbb{E}[X_i X_j] - \\mathbb{E}[X_i]\\mathbb{E}[X_j]}{\\sqrt{\\mathbb{E}[X_i^2] - (\\mathbb{E}[X_i])^2} \\sqrt{\\mathbb{E}[X_j^2] - (\\mathbb{E}[X_j])^2}}\n\\end{aligned}\n\\]\n그러면 이 값이 앞에서 설명한 것처럼 정말로 normalize된 값인지 확인을 해보기 위해서 inequality를 통한 값의 range를 구해본다. 먼저 range를 구하기 전에 variance sum law를 나열해본다.\n\\[\nVar(x + y) = Var(x) + Var(y) + 2 Cov(x, y)\n\\]\n여기서 \\(X / \\sigma_X\\) 라는 term으로 위의 variance sum law에 넣어보면 다음과 같은 식을 구할 수 있는데,\n\\[\nVar(\\frac{X_i}{\\sigma_{X_i}} \\pm \\frac{X_j}{\\sigma_{X_j}}) = Var(\\frac{X_i}{\\sigma_{X_i}}) + Var(\\frac{X_j}{\\sigma_{X_j}}) \\pm 2 Cov (\\frac{X_i}{\\sigma_{X_i}}, \\frac{X_j}{\\sigma_{X_j}})\n\\]\n위의 값은 Variance의 형태이므로 기본적으로 0보다 큰 값을 가진다. 거기에 각 variable의 \\(\\sigma\\) 는 상수와 같으므로 Variance 밖으로 뺄 수 있는데,\n\\[\n0 \\le \\frac{1}{\\sigma^2_{X_i}} Var(X_i) + \\frac{1}{\\sigma^2_{X_j}} Var(X_j) \\pm 2 Cov (\\frac{X_i}{\\sigma_{X_i}}, \\frac{X_j}{\\sigma_{X_j}})\n\\]\n결과적으로 같은 값이기에 1이 된다. 나머지 term을 마저 정리하면,\n\\[\n0 \\le 2 + 2 Cov (\\frac{X_i}{\\sigma_{X_i}}, \\frac{X_j}{\\sigma_{X_j}}) = 2 + 2 Corr (X_i, X_j) \\\\\n-1 \\le \\rho_{X_i, X_j} \\le 1\n\\]\n라는 결과를 얻을 수 있다. 즉 correlation coefficient는 -1과 1 사이의 값을 가지게 되며, 이 말은 어떤 variable간에 비교하여도 correlation coefficient의 크기에 따라서 값의 상관관계를 대략적으로 알수 있다는 것을 의미하게 된다."
  },
  {
    "objectID": "posts/DSP-lec4-5/kr/kernel-density-estimation.html",
    "href": "posts/DSP-lec4-5/kr/kernel-density-estimation.html",
    "title": "Kernel Density Estimation",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nobjData = pd.read_excel('../../data/default of credit card clients.xls', skiprows=[1, 2])\nobjFigure = sns.kdeplot(data=objData['X1'])\nplt.show()\n\n\n\n\nKernel Density Estimation이라는 것은 분포의 밀집한 정도, 즉 Density를 kernel을 사용해서 추정하는 방법이다. 위의 그림은 seaborn 패키지를 사용해서 KDE Plot을 그린 것이고, 이 그래프를 그린 함수를 \\(\\hat{f}_h(x)\\) 라고 하면 아래와 같이 정의할 수 있다.\n\\[\n\\hat{f}_h(x) = \\frac{1}{n} \\sum_{i=1}^n K_h(x - x_i) = \\frac{1}{nh}\\sum_{i=1}^n K(\\frac{x - x_i}{h})\n\\]\n여기에서 \\(K\\)를 kernel function이라고 표현한다. 사실 Density Estimation을 하는 것이 무슨 의미인지 궁금해할 수 있지만, 실제 값과 추정한 값 간의 차이 (\\(x - x_i\\)) 를 어떤 Kernel function을 적용해서 표현하겠다는 것을 의미하는 것이다. 이 차이를 \\(Z\\)라고 표현하면 Kernel function이 적용된 term은 다음과 같이 적용할 수 있다.\n\\[\nK_h(x - x_i) = \\frac{1}{h} K(\\frac{Z}{h})\n\\]\n이 때, Kernel은 다양한 kernel을 시도해볼 수 있다. 만약 Kernel을 uniform kernel을 써보겠다 하면, 이 말은 실제값과 추정치간의 차이가 -1과 1 사이의 값은 하나로 구분하겠다는 것을 의미하고 결국 다음과 같이 어떤 비례의 형태로 표현할 수 있다. 참고로 \\(\\mathcal{1}\\) 함수는 -1과 1 사이에서만 counting 하는 indicator function이다.\n\\[\nK(\\frac{Z}{h}) \\propto \\frac{1}{2} \\mathcal{1}_{-1 &lt; \\frac{Z}{h} &lt; 1}\n\\]\n반면 gaussian kernel을 사용하면 우리가 일반적으로 표현하는 gaussian distribution의 형태로 정의할 수 있다.\n\\[\nK(\\frac{Z}{h}) \\propto \\exp(-\\frac{1}{2} (\\frac{Z}{h})^2)\n\\]\n사실 어떤 분포를 정확히 추정하는 것은 거의 어렵다. 그렇기 때문에 위와 같이 실제값과 추정치간의 차이값으로 분포를 추정하는 방식이 쓰이는 것이고, 어떤 Kernel을 사용했느냐에 따라서 기존의 분포를 정확히 추정할 수 있고, 혹은 조금 smoothing된 형태로 추정할 수도 있는 것이다. 여기에 소개되어 있는 내용이지만, seaborn에서는 기본적으로 Gaussian Kernel을 사용하여 observation에 대한 결과를 smoothing해준다고 되어 있다.\n일단 kernel density estimation은 kernel을 어떤 것을 선택하냐만 다른 것이지 기존의 분포를 추정한다는 결과는 변함이 없기 때문에 일종의 non-parametric density estimation이라고 할 수 있다. 다만 output의 형태는 앞에서 소개한 것처럼 \\(h\\) 에 의해서 scale이 조절이 되기 때문에 다른 한편으로는 parameter가 존재한다고 할 수 있다. 참고로 kdeplot의 argument에는 bw (bin width) 값이 있는데, 이 bin width에 따라서 분포의 형태가 조금 다르게 표현되기도 한다. 아래의 그림은 그에 대한 결과이다.\n\nplt.figure(figsize=(10, 5))\nplt.subplot(121)\nobjFigure = sns.kdeplot(data=objData['X1'], fill=True, color='r', bw_adjust=1)\nplt.subplot(122)\nobjFigure = sns.kdeplot(data=objData['X1'], fill=True, color='b', bw_adjust=10000)\nplt.show()"
  },
  {
    "objectID": "posts/Implicit-Q-Learning/kr/IQL.html",
    "href": "posts/Implicit-Q-Learning/kr/IQL.html",
    "title": "Offline Reinforcement Learning with Implicit Q-Learning",
    "section": "",
    "text": "저자: Ilya kostrikov, Ashvin Nair, Sergey Levine\n발표: ICLR 2022\n논문\nOpenReview\nCode (Jax)"
  },
  {
    "objectID": "posts/Implicit-Q-Learning/kr/IQL.html#tldr",
    "href": "posts/Implicit-Q-Learning/kr/IQL.html#tldr",
    "title": "Offline Reinforcement Learning with Implicit Q-Learning",
    "section": "TL;DR",
    "text": "TL;DR\n이 논문은 Offline RL 알고리즘 중 하나인 Implicit Q-Learning에 대한 내용을 담고 있다. 이 알고리즘의 핵심은 가장 마지막에 학습된 policy를 가지고 unseen action에 대해서 평가를 하지 않고, state value function을 일종의 random variable로 간주하여, policy improvement step을 implicit하게 근사하는 것이다. 그리고 나서 해당 state에 대한 best action의 value를 추정하기 위해 해당 random variable에 대한 state conditional upper expectile을 구했다."
  },
  {
    "objectID": "posts/Implicit-Q-Learning/kr/IQL.html#내용-정리",
    "href": "posts/Implicit-Q-Learning/kr/IQL.html#내용-정리",
    "title": "Offline Reinforcement Learning with Implicit Q-Learning",
    "section": "내용 정리",
    "text": "내용 정리\n\nOffline RL의 문제점\nOnline (On-policy 나 Off-policy) RL 과는 다르게, Offline RL은 환경과의 interaction없이 기존에 수집한 데이터를 바탕으로 모델을 학습하는 구조로 되어 있다. 일반적으로 이 방식을 따르는 알고리즘들은 Approximate Dynamic Programming (ADP)의 형태를 띄게 된다.\n\n\n\n\n\n\nNote\n\n\n\nApproximate Dynamic Programming이란 Function Approximation 과정을 통해 state space를 discretize 하고, 이를 바탕으로 Dynamic Programming을 수행하는 방식이다. 이는 일반적인 Dynamic Programming의 경우, state space가 커지면 연산량이 기하급수적으로 증가하는 문제를 해결하기 위한 방법이다.\n\n\n이때 temporal difference error를 최소화하는 방향으로 학습이 이뤄지는데, 이에 대한 Loss는 Equation 1 와 같다.\n\\[\nL_{\\text{TD}}(\\theta) = \\mathbb{E}_{\\textcolor{red}{(s, a, s')} \\sim \\mathcal{D}} [(r(s, a) + \\gamma \\textcolor{red}{\\max_{a'} Q_{\\hat{\\theta}}(s', a')} - Q_{\\theta}(s, a))^2]\n\\tag{1}\\]\n여기서 \\(\\mathcal{D}\\) 는 수집된 dataset이고, \\(Q_{\\theta}(s, a)\\) 는 \\(\\theta\\) 로 파라미터화된 Q-function이다. 그리고 \\(Q_{\\hat{\\theta}}(s', a')\\) 는 Polyak averaging 같은 방식을 통해서 parameter를 soft update하는 target network을 의미하고, 우리가 찾는 policy \\(\\pi(s) = \\arg\\max_{a}Q_{\\theta}(s, a)\\) 가 된다. 대부분의 offline RL 알고리즘은 이 loss에 constraint를 추가하는 방식 등으로 loss를 바꿔서 적어도 학습된 policy가 수집된 데이터의 distribution을 따르게 하거나 아니면 policy에 constrain을 주는 방식으로 학습을 수행한다. Offline RL의 가장 큰 문제 중 하나는 수집된 데이터 내에 없는 action \\(a'\\), 즉 out-of-distribution action이 나올 경우 target network에 의해서 계산된 \\(Q_{\\hat{\\theta}}(s', a')\\) 가 너무 큰 값이 나오게 되는데, 이는 학습이 불안정해지는 overestimation의 원인이 된다.\n\n\nImplicit Q-Learning\n이 알고리즘의 목표는 Equation 1 에서 사용될 action 중 out-of-sample action, 다시 말해 \\(\\mathcal{D}\\) 에서 나오지 않은 action이 나올 경우를 완전히 배제하는 것이다. 이를 위해서 먼저 Equation 1 을 SARSA 형태로 바꾸고 dataset을 쌓을 때 사용했던 policy인 \\(\\pi_{\\beta}\\) (Behavior Policy)의 value를 학습하도록 fitted Q evaluation을 수행하는 것부터 시작해본다.\n\n\n\n\n\n\nNote\n\n\n\nFitted Q-evaluation (FQE) 이란 Fitted Q Iteration (FQI)이라고도 하며, 데이터를 사용해서 Q-function을 근사화하는 기법이다. 이를 위해서 먼저 Behavior policy를 통해서 data를 수집한 후 해당 데이터를 사용하여 Q-function을 근사화하는 모델을 학습하는 형태로 되어 있다. 이때 이 모델은 상태 \\(s\\) 와 action \\(a\\) 를 입력으로 받아서 \\(Q(s, a)\\) 를 출력하는 형태로 되어 있다. 이후에는 이 모델을 사용해서 target network를 업데이트하고, 이를 바탕으로 다시 Q-function을 근사화하는 과정을 반복한다.\n\n\n\\[\nL(\\theta) = \\mathbb{E}_{\\textcolor{red}{(s, a, s', a')} \\sim \\mathcal{D}} [(r(s, a) + \\gamma\\textcolor{red}{Q_{\\hat{\\theta}}(s', a')} - Q_{\\theta}(s, a))^2]\n\\tag{2}\\]\nEquation 1 와는 다르게, Equation 2 에서는 빨간색 부분에서 확인할 수 있는 것처럼 Q value를 계산할 때 사용하는 next action \\(a'\\) 도 dataset \\(\\mathcal{D}\\) 에서 나온 action이기 때문에 out-of-sample action이 나올 경우에 대한 문제가 발생하지 않는다. 또한 loss function 내에서 TD target (\\(r(s, a) + \\gamma Q_{\\hat{\\theta}}(s', a')\\) 을 예측할 수 있도록 학습할 때 mean sequared error를 사용하기 때문에 만약 dataset이 충분해서 sampling error가 없다고 가정할 수 있다면 이상적인 Q function을 찾을 수도 있게 된다.\n\\[\nQ_{\\theta^{*}}(s, a) \\approx r(s, a) + \\gamma \\mathbb{E}_{\\substack{s' \\sim p(\\cdot|s, a) \\\\ a' \\sim \\pi_{\\beta}(\\cdot|s)}}[Q_{\\hat{\\theta}}(s', a')]\n\\tag{3}\\]\n여기에 complex task에서 활용할 수 있는 multi-step dynamic programming의 이점을 활용하기 위해서 Equation 2 의 이점을 살리면서 약간 수정하는 형태를 취했다.\n\n\n\n\n\n\nNote\n\n\n\n일반적인 dynamic programming의 경우, \\(Q(s, a)\\) 를 계산할 때 \\(s\\) 에서 \\(a\\) 를 선택하고, 이후에 \\(s'\\) 에서 \\(a'\\) 를 선택하는 형태로 계산을 하게 된다. 이에 비해 multi-step dynamic programming은 \\(s\\) 에서 \\(a\\) 를 선택하고, 이후에 \\(s'\\) 에서 \\(a'\\) 를 선택하고, 이후에 \\(s''\\) 에서 \\(a''\\) 를 선택하는 형태로 확장된 식이다. 이렇게 하면 일반적인 dynamic programming에 비해 계산량이 많아지지만, 이를 통해서 복잡한 task에 대해서 좋은 결과를 얻을 수 있다는 장점이 있다.\n\n\n그리고 실제로 out-of-sample action이 나왔을 때, 학습된 Q function을 사용하지 않더라도 주어진 distribution 내에서의 maximum Q value 를 추정할 수 있도록 하기 위해서 Expectile Regression을 사용했다. 이에 대한 간단한 설명은 다음 section에서 하고, 최종적으로는 다음의 loss function을 기반으로 value function을 학습하게 된다.\n\\[\nL(\\theta) = \\mathbb{E}_{\\textcolor{red}{(s, a, s', a')} \\sim \\mathcal{D}} [(r(s, a) + \\gamma \\textcolor{red}{\\max_{\\substack{a' \\in \\mathcal{A} \\\\ \\text{s.t. } \\pi_{\\beta}(a'|s') \\gt 0}} Q_{\\hat{\\theta}}(s', a')} - Q_{\\theta}(s, a))^2]\n\\]\n\n\nExpectile Regression\nExpectile Regression은 quantile regression의 일종으로, 주어진 distribution 내에서의 quantile을 추정하는 방법이다. 어떤 random variable \\(X\\) 에 대한 expectile \\(\\tau \\in (0, 1)\\) 은 asymmetric least sqaures problem에 대한 solution으로 정의된다. 여기서 asymmetric least sqaures problem은 다음과 같다.\n\\[\n\\arg\\min_{m_\\tau} \\mathbb{E}_{x \\sim X}[L_2^{\\tau}(x - m_\\tau)]\n\\tag{4}\\]\n여기서 \\(L_2^{\\tau}(u) = | \\tau - \\mathbb{1}(u \\lt 0)| u^2\\) 이다.\n\n\n\nFigure 1: Expectile Regression\n\n\nFigure 1 의 첫번째 그림이 바로 \\(\\tau\\) 에 따른 assymentric squared loss를 보여주는 것이다. 0.5의 값을 가질 경우 일반적인 MSE loss가 되지만, \\(\\tau = 0.9\\) 일 경우, 양의 구간에 조금 더 가중치가 가해진 것을 그림을 통해서 확인할 수 있다. 결과적으로 \\(m_{\\tau}\\) 보다 작은 \\(x\\)가 loss에 미치는 영향을 줄이고, \\(m_{\\tau}\\) 보다 큰 \\(x\\)가 loss에 미치는 영향을 늘리는 효과를 가지게 된다. 이를 통해서 \\(\\tau\\) 에 따른 quantile을 추정할 수 있게 된다. 또한 Equation 4 의 \\(x\\) 를 \\(y\\) 로 대체함으로써 어떤 conditional distribution에 대한 expectile도 예측하게끔 설계할 수 있다.\n\\[\n\\arg\\min_{m_\\tau} \\mathbb{E}_{x \\sim X}[L_2^{\\tau}(x - m_\\tau)]\n\\tag{5}\\]\n위 식을 Stochastic Gradient Descent를 통해서 최적화를 하게 되면, 결과적으로 unbiased gradient를 구할수도 있고, 쉽게 구현할 수도 있다."
  },
  {
    "objectID": "posts/Introduction-to-NumPy-Arrays/en/introduction-to-numpy-arrays.html",
    "href": "posts/Introduction-to-NumPy-Arrays/en/introduction-to-numpy-arrays.html",
    "title": "Introduction to NumPy Arrays",
    "section": "",
    "text": "In this notebook, you will use NumPy to create 2-D arrays and easily compute mathematical operations. NumPy (Numerical Python) is an open-source package that is widely used in science and engineering. This posts summarized the lecture “Linear Algebra for Machine Learning and Data Science” from Coursera."
  },
  {
    "objectID": "posts/Introduction-to-NumPy-Arrays/en/introduction-to-numpy-arrays.html#tldr",
    "href": "posts/Introduction-to-NumPy-Arrays/en/introduction-to-numpy-arrays.html#tldr",
    "title": "Introduction to NumPy Arrays",
    "section": "",
    "text": "In this notebook, you will use NumPy to create 2-D arrays and easily compute mathematical operations. NumPy (Numerical Python) is an open-source package that is widely used in science and engineering. This posts summarized the lecture “Linear Algebra for Machine Learning and Data Science” from Coursera."
  },
  {
    "objectID": "posts/Introduction-to-NumPy-Arrays/en/introduction-to-numpy-arrays.html#about-jupyter-notebooks",
    "href": "posts/Introduction-to-NumPy-Arrays/en/introduction-to-numpy-arrays.html#about-jupyter-notebooks",
    "title": "Introduction to NumPy Arrays",
    "section": "About Jupyter Notebooks",
    "text": "About Jupyter Notebooks\nJupyter Notebooks are interactive coding journals that integrate live code, explanatory text, equations, visualizations and other multimedia resources, all in a single document. As a first exercise, run the test snippet below and the print statement cell for “Hello World”.\n\n#Run the \"Hello World\" in the cell below to print \"Hello World\". \ntest = \"Hello World\"\n\n\nprint (test)\n\nHello World"
  },
  {
    "objectID": "posts/Introduction-to-NumPy-Arrays/en/introduction-to-numpy-arrays.html#packages",
    "href": "posts/Introduction-to-NumPy-Arrays/en/introduction-to-numpy-arrays.html#packages",
    "title": "Introduction to NumPy Arrays",
    "section": "Packages",
    "text": "Packages\nBefore you get started, you have to import NumPy to load its functions. As you may notice, even though there is no expected output, when you run this cell, the Jupyter Notebook will have imported the package (often referred to as the library) and its functions. Try it for yourself and run the following cell.\n\nimport numpy as np\n\nprint(\"NumPy version: {}\".format(np.__version__))\n\nNumPy version: 1.21.5"
  },
  {
    "objectID": "posts/Introduction-to-NumPy-Arrays/en/introduction-to-numpy-arrays.html#advantages-of-using-numpy-arrays",
    "href": "posts/Introduction-to-NumPy-Arrays/en/introduction-to-numpy-arrays.html#advantages-of-using-numpy-arrays",
    "title": "Introduction to NumPy Arrays",
    "section": "Advantages of using NumPy arrays",
    "text": "Advantages of using NumPy arrays\nArrays are one of the core data structures of the NumPy library, essential for organizing your data. You can think of them as a grid of values, all of the same type. If you have used Python lists before, you may remember that they are convenient, as you can store different data types. However, Python lists are limited in functions and take up more space and time than NumPy arrays.\nNumPy provides an array object that is much faster and more compact than Python lists. Through its extensive API integration, the library offers many built-in functions that make computing much easier in only a few lines of code. This can be a huge advantage when performing math operations on large data.\nThe array object in NumPy is called ndarray meaning ‘n-dimensional array’. To begin with, you will use one of the most common array types: the one-dimensional array (‘1-D’). A 1-D array represents a standard list of values entirely in one dimension. Remember that in NumPy, all of the elements within the array are of the same type.\n\none_dimensional_arr = np.array([10, 12])\nprint(one_dimensional_arr)\n\n[10 12]"
  },
  {
    "objectID": "posts/Introduction-to-NumPy-Arrays/en/introduction-to-numpy-arrays.html#how-to-create-numpy-arrays",
    "href": "posts/Introduction-to-NumPy-Arrays/en/introduction-to-numpy-arrays.html#how-to-create-numpy-arrays",
    "title": "Introduction to NumPy Arrays",
    "section": "How to create NumPy arrays",
    "text": "How to create NumPy arrays\nThere are several ways to create an array in NumPy. You can create a 1-D array by simply using the function array() which takes in a list of values as an argument, and returns a 1-D array.\n\n# Create and print a numpy array 'a' containing the elements 1, 2, 3.\na = np.array([1, 2, 3])\nprint(a)\n\n[1 2 3]\n\n\nAnother way to implement an array is using numpy.arange(). This function will return an array of evenly spaced values within a given interval. To learn more about the arguments that this function takes, there is a powerful Jupyter Notebooks feature that allows you to access any function’s documentation by simply pressing shift+tab on your keyboard when clicking on the function. Give it a try for the built-in documentation of arange().\n\n# Create an array with 3 elements, starting from default number 0.\nb = np.arange(3)\nprint(b)\n\n[0 1 2]\n\n\n\n# Create an array that starts from integer 1, ends at 20, incremented by 3.\nc = np.arange(1, 20, 3)\nprint(c)\n\n[ 1  4  7 10 13 16 19]\n\n\nWhat if you wanted to create an array with five evenly spaced values in the interval from 0 to 100? As you may notice, here you have 3 parameters that the array should take. The start is the number 0, the end is number 100, and 5 is the number of the elements. NumPy has a function that allows you to do specifically this by using numpy.linspace().\n\nlin_spaced_arr = np.linspace(0, 100, 5)\nprint(lin_spaced_arr)\n\n[  0.  25.  50.  75. 100.]\n\n\nDid you notice that the output of the function is presented in the float value form (e.g. “0. 25.”)? The reason is that the default type for values in the numpy function is a floating point (np.float64). You can easily specify your data type using dtype. If you accessed the built-in documentation of the functions, you may have noticed that most functions take in an optional parameter dtype. In addition to float, Numpy has several other data types such as ‘int’, and ‘char’.\nTo change the type to integers, you need to set the dtype to int. You can do so, even with the previous functions. Feel free to try it out and modify the cells to output your prefered data type.\n\nlin_spaced_arr_int = np.linspace(0, 100, 5, dtype=int)\nprint(lin_spaced_arr_int)\n\n[  0  25  50  75 100]\n\n\n\nc_int = np.arange(1, 20, 3, dtype=int)\nprint(c_int)\n\n[ 1  4  7 10 13 16 19]\n\n\n\nb_float = np.arange(3, dtype=float)\nprint(b_float)\n\n[0. 1. 2.]\n\n\n\nchar_arr = np.array(['Welcome to Math for ML!'])\nprint(char_arr)\nprint(char_arr.dtype) # Prints the data type of the array\n\n['Welcome to Math for ML!']\n&lt;U23\n\n\nDid you notice that the output of the data type of the char_arr array is &lt;U23? This means that the string ('Welcome to Math for ML!') is less than a 23-character (23) unicode string (U) on a little-endian architecture. You can learn more about data types here."
  },
  {
    "objectID": "posts/Introduction-to-NumPy-Arrays/en/introduction-to-numpy-arrays.html#more-numpy-arrays",
    "href": "posts/Introduction-to-NumPy-Arrays/en/introduction-to-numpy-arrays.html#more-numpy-arrays",
    "title": "Introduction to NumPy Arrays",
    "section": "More NumPy Arrays",
    "text": "More NumPy Arrays\nOne of the advantages of using NumPy is that you can easily create arrays with built-in functions such as: - numpy.ones() - Returns a new array setting values to one. - numpy.zeros() - Returns a new array setting values to zero. - numpy.empty() - Returns a new uninitialized array. - numpy.random.rand() - Returns a new array with values chosen at random.\n\n# Return a new array with 3 elements of 1. \nones_arr = np.ones(3)\nprint(ones_arr)\n\n[1. 1. 1.]\n\n\n\n# Return a new array with 3 elements of 0.\nzeros_arr = np.zeros(3)\nprint(zeros_arr)\n\n[0. 0. 0.]\n\n\n\n# Return a new array with 3 elements without initializing entries.\nempt_arr = np.empty(3)\nprint(empt_arr)\n\n[0. 0. 0.]\n\n\n\n# Return a new array with 3 elements between 0 and 1 chosen at random.\nrand_arr = np.random.rand(3)\nprint(rand_arr)\n\n[0.00219529 0.50090917 0.95975525]\n\n\n\nNote: The difference between numpy.zeros and numpy.empty is that numpy.empty creates an array with uninitialized elements from available memory space, and numpy.zeros creates 0-initialized array. And maybe numpy.empty is faster to execute."
  },
  {
    "objectID": "posts/Introduction-to-NumPy-Arrays/en/introduction-to-numpy-arrays.html#finding-size-shape-dimension.",
    "href": "posts/Introduction-to-NumPy-Arrays/en/introduction-to-numpy-arrays.html#finding-size-shape-dimension.",
    "title": "Introduction to NumPy Arrays",
    "section": "Finding size, shape, dimension.",
    "text": "Finding size, shape, dimension.\nIn the future assignments, you will need to know how to find the size, dimension and shape of an array. Using the NumPy built-in functions you can easily identify the followings: - ndarray.ndim() - Returns the number of array dimensions. - ndarray.shape() - Returns the shape of the array. Each number in the tuple denotes the lengths of each corresponding dimension. - ndarray.size() - Counts the number of elements along a given axis. Returns the size of the array.\n\n# Dimension of the 2-D array multi_dim_arr\nmulti_dim_arr.ndim\n\n2\n\n\n\n# Shape of the 2-D array multi_dim_arr\n# Returns shape of 2 rows and 3 columns\nmulti_dim_arr.shape\n\n(2, 3)\n\n\n\n# Size of the array multi_dim_arr\n# Returns total number of elements\nmulti_dim_arr.size\n\n6"
  },
  {
    "objectID": "posts/Introduction-to-NumPy-Arrays/en/introduction-to-numpy-arrays.html#multiplying-vector-with-a-scalar-broadcasting",
    "href": "posts/Introduction-to-NumPy-Arrays/en/introduction-to-numpy-arrays.html#multiplying-vector-with-a-scalar-broadcasting",
    "title": "Introduction to NumPy Arrays",
    "section": "Multiplying vector with a scalar (broadcasting)",
    "text": "Multiplying vector with a scalar (broadcasting)\nSuppose you wanted to convert miles to kilometers using the NumPy array functions that you’ve learned so far. You can do this by carrying out an operation between an array (miles) and a single number (the conversion rate which is a scalar). Since, 1 mile = 1.6 km, NumPy computes each multiplication within each cell.\nThis concept is called broadcasting, which allows you to perform operations specifically on arrays of different shapes.\n\nvector = np.array([1, 2])\nvector * 1.6\n\narray([1.6, 3.2])\n\n\n\n\n\nbroadcasting"
  },
  {
    "objectID": "posts/Introduction-to-NumPy-Arrays/en/introduction-to-numpy-arrays.html#indexing",
    "href": "posts/Introduction-to-NumPy-Arrays/en/introduction-to-numpy-arrays.html#indexing",
    "title": "Introduction to NumPy Arrays",
    "section": "Indexing",
    "text": "Indexing\nLet us select specific elements from the arrays as given.\n\n# Select the third element of the array. Remember the counting starts from 0.\na = ([1, 2, 3, 4, 5])\nprint(a[2])\n\n# Select the first element of the array.\nprint(a[0])\n\n3\n1\n\n\nFor multi-dimensional arrays, to index a specific element, think of two indices - i selects the row, and j selects the column.\n\n# Indexing on a 2-D array\ntwo_dim = np.array(([1, 2, 3],\n          [4, 5, 6], \n          [7, 8, 9]))\n\n# Select element number 8 from the 2-D array using indices i, j.\nprint(two_dim[2][1])\n\n8"
  },
  {
    "objectID": "posts/Introduction-to-NumPy-Arrays/en/introduction-to-numpy-arrays.html#slicing",
    "href": "posts/Introduction-to-NumPy-Arrays/en/introduction-to-numpy-arrays.html#slicing",
    "title": "Introduction to NumPy Arrays",
    "section": "Slicing",
    "text": "Slicing\nSlicing gives you a sublist of elements that you specify from the array. The slice notation specifies a start and end value, and copies the list from start up to but not including the end (end-exlusive).\n\n# Slice the array a to give the output [2,3,4]\nsliced_arr = a[1:4]\nprint(sliced_arr)\n\n[2, 3, 4]\n\n\n\n# Slice the two_dim array to output the first two rows\nsliced_arr_1 = two_dim[0:2]\nsliced_arr_1\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\n\n# Similarily, slice the multi-dimensional array two_dim to output the last two rows\nsliced_two_dim_rows = two_dim[1:4]\nprint(sliced_two_dim_rows)\n\n[[4 5 6]\n [7 8 9]]\n\n\n\nsliced_two_dim_cols = two_dim[:,1]\nprint(sliced_two_dim_cols)\n\n[2 5 8]"
  },
  {
    "objectID": "posts/Learning-Ray/kr/learning-ray.html",
    "href": "posts/Learning-Ray/kr/learning-ray.html",
    "title": "러닝 레이",
    "section": "",
    "text": "(해당 포스트에서 소개하고 있는 “러닝 레이” 책은 한빛미디어로부터 제공받았음을 알려드립니다.)\n이제는 단순히 머신러닝 테스크를 로컬에서 돌리는 것 자체가 무의미한 시대에 들어서고 있다. 모델 자체의 크기도 점점 커짐과 더불어, 모델을 학습시킬 데이터의 사이즈도 점점 커져간다. 그래서 이제는 뭔가 인공지능 알고리즘의 파워라기 보다는 ML 인프라를 얼마나 효율적으로 사용하느냐가 모델 성능을 좌우하는 요인 중 하나가 된 것 같다. 이런 필요성 덕분에 MLOps의 개념도 예전보다 점점 중요하게 다뤄지는 것 같다.\n이런 ML 인프라를 쉽고, 효과적으로 구축해줄 수 있는 툴이 있겠지만, 많이 사용되는 툴 중 하나가 이 책에서 다룰 Ray가 아닐까 싶다. Ray는 Large Scale ML에 대응하기 위한 분산처리 프레임워크이고, 처음에는 버클리의 RISE lab에서 open source로 운영하던 프로젝트가 현재에는 anyscale이란 회사에서 계속 운영하고 있다. 그냥 어떻게 간단하게만 보면 Scalable ML platform으로만 볼 수 있겠지만, 다양한 모듈을 통해서 강화학습이나 Hyperparameter tuning, serving까지 다양한 기능을 하나의 플랫폼으로 제공하고 있다. 뭔가 현업에서 분산처리가 필요하고, 효과적인 인공지능 학습을 추구하는 일을 해야 한다면 아마 Ray는 한번쯤 들어봤을 것이다. 다만 아직까지 우리나라에서는 이를 활발하게 사용하는 사례가 아직까지는 많이 없는 것 같다.\n사실 개인적으로도 ray의 존재에 대해서는 일찍 알고 있었지만, 튜토리얼이나 공개되어 있는 예제들이 뭔가 내가 하고 있는 업무로 응용하기 어려운 부분이 있어서, 아 뭔가 한글로 된 자료나 책이 있으면 좋겠다 싶었는데, 이번에 번역본이 나왔다.\n\n이 책은 기본적으로 분산 학습에 대한 이해가 어느정도 되어 있다고 가정한 상태에서 Ray의 핵습 모듈인 Ray core와 Ray tune, RLlib 등의 설명과 이와 관련된 예시를 제공하고 있다. 특히 MLOps와 관련된 업무에 Ray를 적용할 경우, 뒷부분에서 다루는 분산 모델 훈련이나 스케일링, serving, 그리고 ray air를 다루는 후반부를 살펴보면 관련된 내용을 얻을 수 있을 것이다.\n책 자체가 그렇게 두껍지 않고, 그만큼 기본적인 내용을 아는 것을 전제로 설명하고 있어 전체적으로 간결하게 내용이 구성되어 있다. 어떻게 보면 기존의 오라일리의 “Learning” 시리즈처럼 뭔가 새로운 것에 대한 설명보다는 딱 실무에 최적화되어 있는 책이라고 할 수 있다. 개인적으로는 올해 새로하는 일에 Ray RLlib과 분산 학습 환경을 구성하고자 하는 목표를 가지고 있는데, 나름 이 책을 통해서 관련 내용을 얻었고, 조금 도움을 받을 수 있을 듯 하다. 특히 RLlib과 분산 어플리케이션에 소개되어 있는 내용은 강화학습을 어떻게 분산해서 처리할 수 있을까를 고민하는 사람에게는 도움이 될만한 내용을 다루고 있다. 또한 굳이 강화학습이 아니더라도, 지도학습에서 응용할 수 있는 예제도 포함되어 있기에 포괄적으로 프레임워크를 활용해보고자 하는 사람에게는 좋을 것이다.\n책의 아쉬운 점이라면 물론 대다수의 번역본이 가지는 한계일 수 있겠지만, 원서가 나온지 1년이 되면서 ray도 같이 outdate된 점이다. 현재 2.9 버전때까지 나온 ray 인데, 책에서 다루는 ray 2.2보다는 그래도 나름의 최신 버전으로라도 책이나 예제 구성이 되었으면 좋지 않았을까 하는 내용이 조금 들긴 했다. 물론 과거 버전으로도 대부분의 기능들이 계속 유지되고 있긴 하지만 말이다. (개인적으로도 최근 rllib을 테스트하다가 numpy 버전에 따른 문제가 조금 있어서, 아마 버전에 따른 문제가 있을 것 같긴 하다..)\n이 책의 큰 의미는 ray에 대해서 처음으로 출간된 한글 번역서라는 것이다. 그동안 영어로 인해서 조금 멀게 느껴졌던 ray를 활용하기 쉬워졌다는 점에서 잘 읽었다는 생각이 들었다. 특히 번역 내용도 깔끔하고 딱 실무에 적용하게 내용이 구성되어 있던 부분이 책 분량도 그렇고 개발자가 읽기에 딱 좋았다."
  },
  {
    "objectID": "posts/Logistic-Regression-with-a-Neural-Network/en/Logistic-Regression-with-a-Neural-Network.html",
    "href": "posts/Logistic-Regression-with-a-Neural-Network/en/Logistic-Regression-with-a-Neural-Network.html",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "",
    "text": "In this post, we will build a logistic regression classifier to recognize cats. This is the summary of lecture “Neural Networks and Deep Learning” from DeepLearning.AI. (slightly modified from original assignment)"
  },
  {
    "objectID": "posts/Logistic-Regression-with-a-Neural-Network/en/Logistic-Regression-with-a-Neural-Network.html#tldr",
    "href": "posts/Logistic-Regression-with-a-Neural-Network/en/Logistic-Regression-with-a-Neural-Network.html#tldr",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "",
    "text": "In this post, we will build a logistic regression classifier to recognize cats. This is the summary of lecture “Neural Networks and Deep Learning” from DeepLearning.AI. (slightly modified from original assignment)"
  },
  {
    "objectID": "posts/Logistic-Regression-with-a-Neural-Network/en/Logistic-Regression-with-a-Neural-Network.html#packages",
    "href": "posts/Logistic-Regression-with-a-Neural-Network/en/Logistic-Regression-with-a-Neural-Network.html#packages",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "Packages",
    "text": "Packages\nFirst, let’s run the cell below to import all the packages that you will need during this assignment. - numpy is the fundamental package for scientific computing with Python. - h5py is a common package to interact with a dataset that is stored on an H5 file. - matplotlib is a famous library to plot graphs in Python. - PIL and scipy are used here to test your model with your own picture at the end.\n\nimport numpy as np\nimport copy\nimport matplotlib.pyplot as plt\nimport h5py\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage"
  },
  {
    "objectID": "posts/Logistic-Regression-with-a-Neural-Network/en/Logistic-Regression-with-a-Neural-Network.html#dataset",
    "href": "posts/Logistic-Regression-with-a-Neural-Network/en/Logistic-Regression-with-a-Neural-Network.html#dataset",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "Dataset",
    "text": "Dataset\nYou are given a dataset (“data.h5”) containing: - a training set of m_train images labeled as cat (y=1) or non-cat (y=0) - a test set of m_test images labeled as cat or non-cat - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).\nYou will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.\n\ndef load_dataset():\n    train_ds = h5py.File('./dataset/train_catvnoncat.h5', 'r')\n    train_set_x = np.array(train_ds['train_set_x'][:])\n    train_set_y = np.array(train_ds['train_set_y'][:])\n    \n    test_ds = h5py.File('./dataset/test_catvnoncat.h5', 'r')\n    test_set_x = np.array(test_ds['test_set_x'][:])\n    test_set_y = np.array(test_ds['test_set_y'][:])\n    \n    classes = np.array(test_ds['list_classes'][:])\n    \n    train_set_y = train_set_y.reshape((1, train_set_y.shape[0]))\n    test_set_y = test_set_y.reshape((1, test_set_y.shape[0]))\n    \n    return train_set_x, train_set_y, test_set_x, test_set_y, classes\n\n\ntrain_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\n\nWe added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing).\nEach line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the index value and re-run to see other images.\n\nindex = 30\nplt.imshow(train_set_x_orig[index])\nprint (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")\n\ny = [0], it's a 'non-cat' picture.\n\n\n\n\n\n\nindex = 25\nplt.imshow(train_set_x_orig[index])\nprint (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")\n\ny = [1], it's a 'cat' picture.\n\n\n\n\n\n\nInformation from dataset\nWe want to find out how many data do we have, and what shape each image have. Remember that train_set_x_orig is a numpy-array of shape (m_train, num_px, num_px, 3).\n\nm_train = train_set_x_orig.shape[0]\nm_test = test_set_x_orig.shape[0]\nnum_px = train_set_x_orig.shape[1]\n\nprint (\"Number of training examples: m_train = \" + str(m_train))\nprint (\"Number of testing examples: m_test = \" + str(m_test))\nprint (\"Height/Width of each image: num_px = \" + str(num_px))\nprint (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\nprint (\"train_set_x shape: \" + str(train_set_x_orig.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x shape: \" + str(test_set_x_orig.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))\n\nNumber of training examples: m_train = 209\nNumber of testing examples: m_test = 50\nHeight/Width of each image: num_px = 64\nEach image is of size: (64, 64, 3)\ntrain_set_x shape: (209, 64, 64, 3)\ntrain_set_y shape: (1, 209)\ntest_set_x shape: (50, 64, 64, 3)\ntest_set_y shape: (1, 50)\n\n\nFor convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px \\(*\\) num_px \\(*\\) 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.\n\n\nReshape dataset\nReshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px \\(*\\) num_px \\(*\\) 3, 1).\nA trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b\\(*\\)c\\(*\\)d, a) is to use:\nX_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X\n\ntrain_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\ntest_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n\nprint (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))\n\ntrain_set_x_flatten shape: (12288, 209)\ntrain_set_y shape: (1, 209)\ntest_set_x_flatten shape: (12288, 50)\ntest_set_y shape: (1, 50)\n\n\nTo represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\nOne common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean(\\(\\mu\\)) of the whole numpy array from each example, and then divide each example by the standard deviation(\\(\\sigma\\)) of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (which is the maximum value of a pixel channel).\nLet’s standardize our dataset.\n\ntrain_set_x = train_set_x_flatten / 255.\ntest_set_x = test_set_x_flatten / 255.\n\nWhat you need to remember:\nCommon steps for pre-processing a new dataset are: - Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …) - Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1) - “Standardize” the data"
  },
  {
    "objectID": "posts/Logistic-Regression-with-a-Neural-Network/en/Logistic-Regression-with-a-Neural-Network.html#general-architecture-of-the-learning-algorithm",
    "href": "posts/Logistic-Regression-with-a-Neural-Network/en/Logistic-Regression-with-a-Neural-Network.html#general-architecture-of-the-learning-algorithm",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "General Architecture of the learning algorithm",
    "text": "General Architecture of the learning algorithm\nIt’s time to design a simple algorithm to distinguish cat images from non-cat images.\nYou will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why Logistic Regression is actually a very simple Neural Network!\n\nMathematical expression of the algorithm:\nFor one example \\(x^{(i)}\\): \\[z^{(i)} = w^T x^{(i)} + b \\tag{1}\\] \\[\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}\\] \\[ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}\\]\nThe cost is then computed by summing over all training examples: \\[ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}\\]"
  },
  {
    "objectID": "posts/Logistic-Regression-with-a-Neural-Network/en/Logistic-Regression-with-a-Neural-Network.html#building-the-parts-of-our-algorithm",
    "href": "posts/Logistic-Regression-with-a-Neural-Network/en/Logistic-Regression-with-a-Neural-Network.html#building-the-parts-of-our-algorithm",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "Building the parts of our algorithm",
    "text": "Building the parts of our algorithm\nThe main steps for building a Neural Network are: 1. Define the model structure (such as number of input features) 2. Initialize the model’s parameters 3. Loop: - Calculate current loss (forward propagation) - Calculate current gradient (backward propagation) - Update parameters (gradient descent)\nYou often build 1-3 separately and integrate them into one function we call model().\n\nSigmoid\nwe need to implement implement sigmoid(). As you’ve seen in the figure above, you need to compute \\[sigmoid(z) = \\frac{1}{1 + e^{-z}}\\] for \\(z = w^T x + b\\) to make predictions.\n\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Arguments:\n    z -- A scalar or numpy array of any size.\n\n    Return:\n    s -- sigmoid(z)\n    \"\"\"\n    s = 1 / (1 + np.exp(-z))\n    return s\n\n\nprint (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))\n\nsigmoid([0, 2]) = [0.5        0.88079708]\n\n\n\nx = np.array([0.5, 0, 2.0])\noutput = sigmoid(x)\nprint(output)\n\n[0.62245933 0.5        0.88079708]\n\n\n\n\nInitializing parameters\nNow we need to implement parameter initialization in the cell below. You have to initialize w as a vector of zeros.\n\ndef initialize_with_zeros(dim):\n    \"\"\"\n    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n    \n    Argument:\n    dim -- size of the w vector we want (or number of parameters in this case)\n    \n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias) of type float\n    \"\"\"\n    w = np.zeros(shape=(dim, 1), dtype=np.float32)\n    b = 0.0\n    \n    return w, b\n\n\ndim = 2\nw, b = initialize_with_zeros(dim)\n\nassert type(b) == float\nprint (\"w = \" + str(w))\nprint (\"b = \" + str(b))\n\nw = [[0.]\n [0.]]\nb = 0.0\n\n\n\n\nForward and Backward propagation\nNow that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters. Note that, Forward Propagation: - You get X - You compute \\(A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})\\) - You calculate the cost function: \\(J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))\\)\nHere are the two formulas you will be using:\n\\[ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}\\] \\[ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}\\]\n\ndef propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n    \n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n    \n    m = X.shape[1]\n    \n    # forward propagation (from x to cost)\n    # compute activation\n    A = sigmoid(w.T @ X + b)\n    # compute cost by using np.dot to perform multiplication\n    cost = np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)) / -m\n    \n    # backward propagation (to find grad)\n    dw = X @ (A - Y).T / m\n    db = np.sum(A - Y) / m\n    \n    cost = np.squeeze(np.array(cost))\n    \n    grads = {'dw': dw, 'db': db}\n    return grads, cost\n\n\nw =  np.array([[1.], [2]])\nb = 1.5\nX = np.array([[1., -2., -1.], [3., 0.5, -3.2]])\nY = np.array([[1, 1, 0]])\ngrads, cost = propagate(w, b, X, Y)\n\nassert type(grads[\"dw\"]) == np.ndarray\nassert grads[\"dw\"].shape == (2, 1)\nassert type(grads[\"db\"]) == np.float64\n\n\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))\n\ndw = [[ 0.25071532]\n [-0.06604096]]\ndb = -0.12500404500439652\ncost = 0.15900537707692405\n\n\n\n\nOptimization\nYou have initialized your parameters. and also able to compute a cost function and its gradient. Now, you want to update the parameters using gradient descent.\n\ndef optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n    \n    w = copy.deepcopy(w)\n    b = copy.deepcopy(b)\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        # cost and gradient calculation\n        grads, cost = propagate(w, b, X, Y)\n        \n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        # update rule\n        w -= learning_rate * dw\n        b -= learning_rate * db\n        \n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n        \n            # Print the cost every 100 training iterations\n            if print_cost:\n                print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs\n\n\nparams, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n\nprint (\"w = \" + str(params[\"w\"]))\nprint (\"b = \" + str(params[\"b\"]))\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint(\"Costs = \" + str(costs))\n\nw = [[0.35627617]\n [0.60199214]]\nb = -0.14956979978997242\ndw = [[-0.21189539]\n [-0.33376766]]\ndb = -0.13290329100668044\nCosts = [array(0.5826722)]\n\n\n\n\nPredict\nThe previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function. There are two steps to computing predictions:\n\nCalculate \\(\\hat{Y} = A = \\sigma(w^T X + b)\\)\nConvert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this).\n\n\ndef predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1, m))\n    w = w.reshape(X.shape[0], 1)\n    \n    # compute vector 'A' predicting the probabilities of a cat being present in the picture\n    A = sigmoid(w.T @ X + b)\n    \n    for i in range(A.shape[1]):\n        # convert probabilities A[0, i] to actual predictions p[0, i]\n        if A[0, i] &gt; 0.5:\n            Y_prediction[0, i] = 1\n        else:\n            Y_prediction[0, i] = 0\n            \n    return Y_prediction\n\n\nw = np.array([[0.1124579], [0.23106775]])\nb = -0.3\nX = np.array([[1., -1.1, -3.2],[1.2, 2., 0.1]])\nprint (\"predictions = \" + str(predict(w, b, X)))\n\npredictions = [[1. 1. 0.]]\n\n\nWhat to remember:\nYou’ve implemented several functions that: - Initialize (w,b) - Optimize the loss iteratively to learn parameters (w,b): - Computing the cost and its gradient - Updating the parameters using gradient descent - Use the learned (w,b) to predict the labels for a given set of examples"
  },
  {
    "objectID": "posts/Logistic-Regression-with-a-Neural-Network/en/Logistic-Regression-with-a-Neural-Network.html#merge-all-functions-into-a-model",
    "href": "posts/Logistic-Regression-with-a-Neural-Network/en/Logistic-Regression-with-a-Neural-Network.html#merge-all-functions-into-a-model",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "Merge all functions into a model",
    "text": "Merge all functions into a model\nYou will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n\ndef model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to True to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    w, b = initialize_with_zeros(dim=X_train.shape[0])\n    \n    # Gradient descent\n    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n    \n    # Retrieve parameters w and b from dictionary \"params\"\n    w = params['w']\n    b = params['b']\n    \n    # Predict test/train set examples\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n    \n    # Print train/test Errors\n    if print_cost:\n        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    \n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d\n\n\nlogistic_regression_model = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)\n\nCost after iteration 0: 0.693147\nCost after iteration 100: 0.584508\nCost after iteration 200: 0.466949\nCost after iteration 300: 0.376007\nCost after iteration 400: 0.331463\nCost after iteration 500: 0.303273\nCost after iteration 600: 0.279880\nCost after iteration 700: 0.260042\nCost after iteration 800: 0.242941\nCost after iteration 900: 0.228004\nCost after iteration 1000: 0.214820\nCost after iteration 1100: 0.203078\nCost after iteration 1200: 0.192544\nCost after iteration 1300: 0.183033\nCost after iteration 1400: 0.174399\nCost after iteration 1500: 0.166521\nCost after iteration 1600: 0.159305\nCost after iteration 1700: 0.152667\nCost after iteration 1800: 0.146542\nCost after iteration 1900: 0.140872\ntrain accuracy: 99.04306220095694 %\ntest accuracy: 70.0 %\n\n\nComment: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test accuracy is 70%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier.\nAlso, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the index variable) you can look at predictions on pictures of the test set.\n\n# Example of a picture that was wrongly classified.\nindex = 1\nplt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\nprint (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[int(logistic_regression_model['Y_prediction_test'][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")\n\ny = 1, you predicted that it is a \"cat\" picture.\n\n\n\n\n\nLet’s also plot the cost function and the gradients.\n\n# Plot learning curve (with costs)\ncosts = np.squeeze(logistic_regression_model['costs'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per hundreds)')\nplt.title(\"Learning rate =\" + str(logistic_regression_model[\"learning_rate\"]))\nplt.show()\n\n\n\n\nInterpretation: You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting."
  },
  {
    "objectID": "posts/Logistic-Regression-with-a-Neural-Network/en/Logistic-Regression-with-a-Neural-Network.html#further-analysis",
    "href": "posts/Logistic-Regression-with-a-Neural-Network/en/Logistic-Regression-with-a-Neural-Network.html#further-analysis",
    "title": "Logistic Regression with a Neural Network mindset",
    "section": "Further analysis",
    "text": "Further analysis\nLet’s analyze it further, and examine possible choices for the learning rate \\(\\alpha\\).\n\nChoice of learning rate\nReminder: In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate \\(\\alpha\\) determines how rapidly we update the parameters. If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned learning rate.\nLet’s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the learning_rates variable to contain, and see what happens.\n\nlearning_rates = [0.01, 0.001, 0.0001]\nmodels = {}\n\nfor lr in learning_rates:\n    print (\"Training a model with learning rate: \" + str(lr))\n    models[str(lr)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=1500, learning_rate=lr, print_cost=False)\n    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n\nfor lr in learning_rates:\n    plt.plot(np.squeeze(models[str(lr)][\"costs\"]), label=str(models[str(lr)][\"learning_rate\"]))\n\nplt.ylabel('cost')\nplt.xlabel('iterations (hundreds)')\n\nlegend = plt.legend(loc='upper center', shadow=True)\nframe = legend.get_frame()\nframe.set_facecolor('0.90')\nplt.show()\n\nTraining a model with learning rate: 0.01\n\n-------------------------------------------------------\n\nTraining a model with learning rate: 0.001\n\n-------------------------------------------------------\n\nTraining a model with learning rate: 0.0001\n\n-------------------------------------------------------\n\n\n\n\n\n\nWhat to remember from this assignment: 1. Preprocessing the dataset is important. 2. You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model(). 3. Tuning the learning rate (which is an example of a “hyperparameter”) can make a big difference to the algorithm.\nBibliography: - http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/ - https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
  },
  {
    "objectID": "posts/Matrix-Multiplication/en/Matrix-Multiplication.html",
    "href": "posts/Matrix-Multiplication/en/Matrix-Multiplication.html",
    "title": "Matrix Multiplication",
    "section": "",
    "text": "In this notebook, you will use NumPy functions to perform matrix multiplication and see how it can be used in the Machine Learning applications. This post is summarized the lecture “Linear Algebra for Machine Learning and Data Science” from Coursera."
  },
  {
    "objectID": "posts/Matrix-Multiplication/en/Matrix-Multiplication.html#tldr",
    "href": "posts/Matrix-Multiplication/en/Matrix-Multiplication.html#tldr",
    "title": "Matrix Multiplication",
    "section": "",
    "text": "In this notebook, you will use NumPy functions to perform matrix multiplication and see how it can be used in the Machine Learning applications. This post is summarized the lecture “Linear Algebra for Machine Learning and Data Science” from Coursera."
  },
  {
    "objectID": "posts/Matrix-Multiplication/en/Matrix-Multiplication.html#packages",
    "href": "posts/Matrix-Multiplication/en/Matrix-Multiplication.html#packages",
    "title": "Matrix Multiplication",
    "section": "Packages",
    "text": "Packages\nLoad the NumPy package to access its functions.\n\nimport numpy as np\n\nprint(\"NumPy version: \", np.__version__)\n\nNumPy version:  1.21.5"
  },
  {
    "objectID": "posts/Matrix-Multiplication/en/Matrix-Multiplication.html#definition-of-matrix-multiplication",
    "href": "posts/Matrix-Multiplication/en/Matrix-Multiplication.html#definition-of-matrix-multiplication",
    "title": "Matrix Multiplication",
    "section": "Definition of Matrix Multiplication",
    "text": "Definition of Matrix Multiplication\nIf \\(A\\) is an \\(m \\times n\\) matrix and \\(B\\) is an \\(n \\times p\\) matrix, the matrix product \\(C = AB\\) (denoted without multiplication signs or dots) is defined to be the \\(m \\times p\\) matrix such that \\(c_{ij}=a_{i1}b_{1j}+a_{i2}b_{2j}+\\ldots+a_{in}b_{nj}=\\sum_{k=1}^{n} a_{ik}b_{kj}, \\tag{4}\\)\nwhere \\(a_{ik}\\) are the elements of matrix \\(A\\), \\(b_{kj}\\) are the elements of matrix \\(B\\), and \\(i = 1, \\ldots, m\\), \\(k=1, \\ldots, n\\), \\(j = 1, \\ldots, p\\). In other words, \\(c_{ij}\\) is the dot product of the \\(i\\)-th row of \\(A\\) and the \\(j\\)-th column of \\(B\\)."
  },
  {
    "objectID": "posts/Matrix-Multiplication/en/Matrix-Multiplication.html#matrix-multiplication-using-python",
    "href": "posts/Matrix-Multiplication/en/Matrix-Multiplication.html#matrix-multiplication-using-python",
    "title": "Matrix Multiplication",
    "section": "Matrix Multiplication using Python",
    "text": "Matrix Multiplication using Python\nLike with the dot product, there are a few ways to perform matrix multiplication in Python. As discussed in the previous lab, the calculations are more efficient in the vectorized form. Let’s discuss the most commonly used functions in the vectorized form. First, define two matrices:\n\nA = np.array([[4, 9, 9], [9, 1, 6], [9, 2, 3]])\nprint(\"Matrix A (3 by 3):\\n\", A)\n\nB = np.array([[2, 2], [5, 7], [4, 4]])\nprint(\"Matrix B (3 by 2):\\n\", B)\n\nMatrix A (3 by 3):\n [[4 9 9]\n [9 1 6]\n [9 2 3]]\nMatrix B (3 by 2):\n [[2 2]\n [5 7]\n [4 4]]\n\n\nYou can multiply matrices \\(A\\) and \\(B\\) using NumPy package function np.matmul():\n\nnp.matmul(A, B)\n\narray([[ 89, 107],\n       [ 47,  49],\n       [ 40,  44]])\n\n\nWhich will output \\(3 \\times 2\\) matrix as a np.array. Python operator @ will also work here giving the same result:\n\nA @ B\n\narray([[ 89, 107],\n       [ 47,  49],\n       [ 40,  44]])"
  },
  {
    "objectID": "posts/Matrix-Multiplication/en/Matrix-Multiplication.html#matrix-convention-and-broadcasting",
    "href": "posts/Matrix-Multiplication/en/Matrix-Multiplication.html#matrix-convention-and-broadcasting",
    "title": "Matrix Multiplication",
    "section": "Matrix Convention and Broadcasting",
    "text": "Matrix Convention and Broadcasting\nMathematically, matrix multiplication is defined only if number of the columns of matrix \\(A\\) is equal to the number of the rows of matrix \\(B\\) (you can check again the definition in the section 1 and see that otherwise the dot products between rows and columns will not be defined).\nThus, in the example above (section 2), changing the order of matrices when performing the multiplication \\(BA\\) will not work as the above rule does not hold anymore. You can check it by running the cells below - both of them will give errors.\n\ntry:\n    np.matmul(B, A)\nexcept ValueError as err:\n    print(err)\n\nmatmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 3 is different from 2)\n\n\n\ntry:\n    B @ A\nexcept ValueError as err:\n    print(err)\n\nmatmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 3 is different from 2)\n\n\nSo when using matrix multiplication you will need to be very careful about the dimensions - the number of the columns in the first matrix should match the number of the rows in the second matrix. This is very important for your future understanding of Neural Networks and how they work.\nHowever, for multiplying of the vectors, NumPy has a shortcut. You can define two vectors \\(x\\) and \\(y\\) of the same size (which one can understand as two \\(3 \\times 1\\) matrices). If you check the shape of the vector \\(x\\), you can see that :\n\nx = np.array([1, -2, -5])\ny = np.array([4, 3, -1])\n\nprint(\"Shape of vector x:\", x.shape)\nprint(\"Number of dimensions of vector x:\", x.ndim)\nprint(\"Shape of vector x, reshaped to a matrix:\", x.reshape((3, 1)).shape)\nprint(\"Number of dimensions of vector x, reshaped to a matrix:\", x.reshape((3, 1)).ndim)\n\nShape of vector x: (3,)\nNumber of dimensions of vector x: 1\nShape of vector x, reshaped to a matrix: (3, 1)\nNumber of dimensions of vector x, reshaped to a matrix: 2\n\n\nFollowing the matrix convention, multiplication of matrices \\(3 \\times 1\\) and \\(3 \\times 1\\) is not defined. For matrix multiplication you would expect an error in the following cell, but let’s check the output:\n\nnp.matmul(x, y)\n\n3\n\n\nYou can see that there is no error and that the result is actually a dot product \\(x \\cdot y\\,\\)! So, vector \\(x\\) was automatically transposed into the vector \\(1 \\times 3\\) and matrix multiplication \\(x^Ty\\) was calculated. While this is very convenient, you need to keep in mind such functionality in Python and pay attention to not use it in a wrong way. The following cell will return an error:\n\ntry:\n    np.matmul(x.reshape((3, 1)), y.reshape((3, 1)))\nexcept ValueError as err:\n    print(err)\n\nmatmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 3 is different from 1)\n\n\nYou might have a question in you mind: does np.dot() function also work for matrix multiplication? Let’s try it:\n\nnp.dot(A, B)\n\narray([[ 89, 107],\n       [ 47,  49],\n       [ 40,  44]])\n\n\nYes, it works! What actually happens is what is called broadcasting in Python: NumPy broadcasts this dot product operation to all rows and all columns, you get the resultant product matrix. Broadcasting also works in other cases, for example:\n\nA - 2\n\narray([[ 2,  7,  7],\n       [ 7, -1,  4],\n       [ 7,  0,  1]])\n\n\nMathematically, subtraction of the \\(3 \\times 3\\) matrix \\(A\\) and a scalar is not defined, but Python broadcasts the scalar, creating a \\(3 \\times 3\\) np.array and performing subtraction element by element. A practical example of matrix multiplication can be seen in a linear regression model."
  },
  {
    "objectID": "posts/Offline-Sequential-Evaluation/kr/OSE.html",
    "href": "posts/Offline-Sequential-Evaluation/kr/OSE.html",
    "title": "Bridging the Gap Between Offline and Online Reinforcement Learning Evaluation Methodologies",
    "section": "",
    "text": "저자: Shivakanth Sujit, Pedro H.M Braga, Jorg Bornschein, Semira Ebrahimi Kahou\n발표: Offline Reinforcement Learning Workshop, NeurIPS 2022\n논문\nOpenReview"
  },
  {
    "objectID": "posts/Offline-Sequential-Evaluation/kr/OSE.html#tldr",
    "href": "posts/Offline-Sequential-Evaluation/kr/OSE.html#tldr",
    "title": "Bridging the Gap Between Offline and Online Reinforcement Learning Evaluation Methodologies",
    "section": "TL;DR",
    "text": "TL;DR\nDeep RL에서 가장 큰 어려움 중 하나는 학습을 위해서는 환경과의 interaction이 수없이 이뤄져야 한다는 것이다. 만약 로봇과 같이 interaction에서 소요되는 비용이 큰 경우에는 적용하기 어려운 부분이 존재한다. 이러한 문제를 해결하기 위해서 처음부터 환경과의 interaction 없이 존재하는 log만으로 학습하는 Offline RL 기법이 제안되고 있다. 하지만 Online RL과는 다르게 Offline RL을 평가하기 위한 어떤 정형화된 evaluation method가 많지 않고, 이 논문에서는 log를 활용하여 Offline RL 알고리즘을 Sequantial Evaluation 하는 방법에 대해서 소개하고 있다. Sequential Evaluation을 통해서 학습 과정에서 사용되는 데이터를 더 효율적으로 활용함과 동시에 Offline과 Online 사이에 발생하는 distribution shift에 대해서 적절하게 대응할 수 있다는 것이 주요 특징이다. 사실 워크샵 논문이라 내용이 길지 않을 뿐더러, 논문에서 소개하고 있는 방식이 어떤 수식에 의존한 알고리즘에 대한 내용이 아니라 evaluation을 하는 process에 대한 내용이어서 내용이 어렵지가 않다."
  },
  {
    "objectID": "posts/Offline-Sequential-Evaluation/kr/OSE.html#내용-소개",
    "href": "posts/Offline-Sequential-Evaluation/kr/OSE.html#내용-소개",
    "title": "Bridging the Gap Between Offline and Online Reinforcement Learning Evaluation Methodologies",
    "section": "내용 소개",
    "text": "내용 소개\n\nSquential Evaluation of Offline RL Algorithms\nOffline RL 알고리즘에 대한 evaluation을 하는 방법 중 하나는 dataset에 대해서 training을 여러 epoch 수행하는 것인데, 이러한 방식은 몇가지 문제를 가지고 있다. 첫째로, training시 모든 data를 사용하기 때문에 사용된 알고리즘이 sample efficiency 측면에서 좋은지를 평가하기 어렵다는 것이다. 이로 인해서 실제 환경에 적용했을 때도, 현재 사용하고 있는 학습데이터의 양이 충분한지 여부를 판단하기 어려운 문제로 이어진다. 또한, log를 쌓을 때 사용된 policy의 quality에 따라서 distribution change가 존재할 수 있다. 그리고 마지막으로, Online RL에서 사용되는 Evaluation 방식을 Offline RL에서 그대로 활용하기 때문에, 실질적으로 Offline RL의 성능이 어느 정도인지 (적어도 Online RL과 비교하려해도) 어려운 문제가 있다.\n그래서 논문에서 제안하는 방식은 시간에 따라서 학습하는 agent가 사용할 수 있는 데이터의 비율을 점점 변화시키면서 해당 데이터를 바탕으로 evaluation을 수행하는 것이다. (전제 데이터를 한번에 다 쓰지 말고…) 이를 위해서 기존의 Online Deep RL에서 많이 사용되는 replay-buffer 기반의 training scheme을 활용하게 된다. 대신 Online RL에서처럼 현재 학습하고 있는 agent의 experience를 넣는 것이 아니라, offline log를 시간에 따라서 조금씩 추가하는 것이 핵심이다. 그래서 새롭게 추가되는 샘플들과 buffer에서 sampling된 mini batch data를 바탕으로 gradient update를 수행한다. 상세한 알고리즘의 구성은 Algorithm 1 과 같다.\n\n\n\\begin{algorithm} \\caption{Sequential Evaluation in the offline setting} \\begin{algorithmic} \\Input{ Algorithm $A$, Offline data $\\mathcal{D} = \\{ s_t, a_t, r_t, s_{t+1}\\}_{t=1}^{T_0}$, increment-size $\\gamma$, gradient steps per increment $K$} \\State{Replay-buffer $\\mathcal{B} \\gets \\{s_t, a_t, r_t, s_{t+1}\\}_{t=1}^{T_0}$} \\State{$t \\gets T_0$} \\While{$t &lt; T$} \\State{Update replay-buffer $ \\mathcal{B} \\gets \\mathcal{B} \\cup \\{s_t, a_t, r_t, s_{t+1}\\}_{t}^{t+\\gamma}$} \\State{Sample a training batch, ensure new data is included: batch $\\sim \\mathcal{B}$} \\State{Perform training step with $A$ on batch} \\State{$t \\gets t + \\gamma$} \\For{$j=1, \\cdots, K$} \\State{Sample a training batch $\\sim \\mathcal{B}$} \\State{Perform training step with $A$ on batch} \\EndFor \\EndWhile \\end{algorithmic} \\end{algorithm}\n\n\n위 알고리즘을 통해서 epoch-style로 학습할 때 발생하는 문제를 대응할 수 있는데, 먼저 데이터를 증가시키는 정도를 나타내는 \\(\\gamma\\)와 학습을 수행하는 횟수를 나타내는 \\(K\\) 를 다르게 함으로써, dataset의 size가 변화함에 따라서 성능 변화의 추이(예를 들어서 어떤 size에서 bottleneck이 발생하는지…)를 확인할 수 있다. 그리고 알고리즘을 보면 알겠지만, Online RL에서 환경과의 interaction을 여러 번 수행하면서 evaluation 하는 것과 비슷한 형태로 되어 있어서, 이와 같은 방식을 통해서 replay-buffer의 size에 따라서 Offline RL의 성능 변화가 발생하는 것을 확인할 수 있다. 또한 모든 offline log가 replay-buffer에 들어간 이후에 online finetune을 수행하면서도 새롭게 추가된 데이터를 활용할 수 있기 때문에, Offline-to-Online RL 환경에서 Seamless하게 evaluation 수행할 수 있다. 무엇보다도, 알고리즘 자체를 개선했다기 보다는 데이터를 어떻게 활용할 것인지에 대한 방법을 제시한 것이기 때문에, 기존의 학습 과정을 크게 수정하지 않고도 적용할 수 있다는 장점이 있다."
  },
  {
    "objectID": "posts/Solving-Linear-Systems-2-variable/en/Solving-Linear-Systems-2-variables.html",
    "href": "posts/Solving-Linear-Systems-2-variable/en/Solving-Linear-Systems-2-variables.html",
    "title": "Solving Linear Systems: 2 Variables",
    "section": "",
    "text": "In this notebook you will use NumPy linear algebra package to find the solutions of the system of linear equations, and find the solution for the system of linear equations using elimination method. Also, you will evaluate the determinant of the matrix and examine the relationship between matrix singularity and number of solutions of the linear system. This posts summarized the lecture “Linear Algebra for Machine Learning and Data Science” from Coursera."
  },
  {
    "objectID": "posts/Solving-Linear-Systems-2-variable/en/Solving-Linear-Systems-2-variables.html#tldr",
    "href": "posts/Solving-Linear-Systems-2-variable/en/Solving-Linear-Systems-2-variables.html#tldr",
    "title": "Solving Linear Systems: 2 Variables",
    "section": "",
    "text": "In this notebook you will use NumPy linear algebra package to find the solutions of the system of linear equations, and find the solution for the system of linear equations using elimination method. Also, you will evaluate the determinant of the matrix and examine the relationship between matrix singularity and number of solutions of the linear system. This posts summarized the lecture “Linear Algebra for Machine Learning and Data Science” from Coursera."
  },
  {
    "objectID": "posts/Solving-Linear-Systems-2-variable/en/Solving-Linear-Systems-2-variables.html#packages",
    "href": "posts/Solving-Linear-Systems-2-variable/en/Solving-Linear-Systems-2-variables.html#packages",
    "title": "Solving Linear Systems: 2 Variables",
    "section": "Packages",
    "text": "Packages\nLoad NumPy package to access its functions.\n\nimport numpy as np\n\nprint(\"NumPy version: {}\".format(np.__version__))\n\nNumPy version: 1.21.5"
  },
  {
    "objectID": "posts/Solving-Linear-Systems-2-variable/en/Solving-Linear-Systems-2-variables.html#representing-and-solving-system-of-linear-equations-using-matrices",
    "href": "posts/Solving-Linear-Systems-2-variable/en/Solving-Linear-Systems-2-variables.html#representing-and-solving-system-of-linear-equations-using-matrices",
    "title": "Solving Linear Systems: 2 Variables",
    "section": "Representing and Solving System of Linear Equations using Matrices",
    "text": "Representing and Solving System of Linear Equations using Matrices\n\nSystem of Linear Equations\nA system of linear equations (or linear system) is a collection of one or more linear equations involving the same variables. For example:\n\\[\\begin{cases}\n-x_1+3x_2=7, \\\\ 3x_1+2x_2=1, \\end{cases}\\tag{1}\\]\nis a system of two equations with two unknown variables \\(x_1\\), \\(x_2\\). To solve a system of linear equations means to find such values of the variables \\(x_1\\), \\(x_2\\), that all of its equations are simultaneously satisfied.\nA linear system is inconsistent if it has no solution, and otherwise it is said to be consistent. Consistent system can have one or infinite number of solutions.\n\n\nSolving Systems of Linear Equations using Matrices\nLinear systems with two equations are easy to solve manually, but preparing for more complicated cases, you will investigate some solution techniques.\nNumPy linear algebra package provides quick and reliable way to solve the system of linear equations using function np.linalg.solve(A, b). Here \\(A\\) is a matrix, each row of which represents one equation in the system and each column corresponds to the variable \\(x_1\\), \\(x_2\\). And \\(b\\) is a 1-D array of the free (right side) coefficients. More information about the np.linalg.solve() function can be found in documentation.\nGiven the system of linear equations \\((1)\\), you can set matrix \\(A\\) and 1-D array \\(b\\) as:\n\nA = np.array([\n    [-1, 3],\n    [3, 2],\n], dtype=np.dtype(float))\n\nb = np.array([7, 1], dtype=np.dtype(float))\n\n\nprint(\"Matrix A: \")\nprint(A)\nprint(\"Matrix b: \")\nprint(b)\n\nMatrix A: \n[[-1.  3.]\n [ 3.  2.]]\nMatrix b: \n[7. 1.]\n\n\nCheck the dimensions of \\(A\\) and \\(b\\) using the shape attribute (you can also use np.shape() as an alternative):\n\nprint(f\"Shape of A: {A.shape}\")\nprint(f\"Shape of b: {b.shape}\")\n\nShape of A: (2, 2)\nShape of b: (2,)\n\n\n\nprint(f\"Shape of A: {np.shape(A)}\")\nprint(f\"Shape of b: {np.shape(b)}\")\n\nShape of A: (2, 2)\nShape of b: (2,)\n\n\nNow simply use np.linalg.solve(A, b) function to find the solution of the system \\((1)\\). The result will be saved in the 1-D array \\(x\\). The elements will correspond to the values of \\(x_1\\) and \\(x_2\\):\n\nx = np.linalg.solve(A, b)\nprint(f\"Solution: {x}\")\n\nSolution: [-1.  2.]\n\n\nTry to substitute those values of \\(x_1\\) and \\(x_2\\) into the original system of equations to check its consistency.\n\n\nEvaluating Determinant of a Matrix\nMatrix \\(A\\) corresponding to the linear system \\((1)\\) is a square matrix - it has the same number of rows and columns. In case of a square matrix it is possible to calculate its determinant - a real number which characterizes some properties of the matrix. Linear system containing two (or more) equations with the same number of unknown variables will have one solution if and only if matrix \\(A\\) has non-zero determinant.\nLet’s calculate the determinant using NumPy linear algebra package. You can do it with the np.linalg.det(A) function. More information about it can be found in documentation.\n\nd = np.linalg.det(A)\n\nprint(f\"Determinant of matrix A: {d: .2f}\")\n\nDeterminant of matrix A: -11.00\n\n\nNote that its value is non-zero, as expected for a system with exactly one solution."
  },
  {
    "objectID": "posts/Solving-Linear-Systems-2-variable/en/Solving-Linear-Systems-2-variables.html#solving-system-of-linear-equations-using-elimination-method",
    "href": "posts/Solving-Linear-Systems-2-variable/en/Solving-Linear-Systems-2-variables.html#solving-system-of-linear-equations-using-elimination-method",
    "title": "Solving Linear Systems: 2 Variables",
    "section": "Solving System of Linear Equations using Elimination Method",
    "text": "Solving System of Linear Equations using Elimination Method\nYou can see how easy it is to use contemporary packages to solve linear equations. However, for deeper understanding of mathematical concepts, it is important to practice some solution techniques manually. Programming approach can still help here to reduce the amount of arithmetical calculations, and focus on the method itself.\n\nElimination Method\nIn the elimination method you either add or subtract the equations of the linear system to get an equation with smaller number of variables. If needed, you can also multiply whole equation by non-zero number.\nTake a look at the system \\((1)\\) again:\n\\[\\begin{cases}\n-x_1+3x_2=7, \\\\ 3x_1+2x_2=1, \\end{cases}\\]\nMultiply first equation by 3, add it to the second equation and exchange the second equation with the result of this addition:\n\\[\\begin{cases}\n-x_1+3x_2=7, \\\\ 11x_2=22. \\end{cases}\\tag{2}\\]\nYou eliminated variable \\(x_1\\) from the second equation and brough it to the form where, dividing by \\(11\\), you can see the solution value for \\(x_2\\): \\(x_2=2\\). Now take \\(x_2=2\\) in the first equation:\n\\[\\begin{cases}\n-x_1+3 \\times 2=7, \\\\ x_2=2, \\end{cases}\\tag{3}\\]\nAnd find the solution:\n\\[\\begin{cases}\nx_1=-1, \\\\ x_2=2. \\end{cases}\\tag{4}\\]\nCheck that it’s the same one that you found in the previous section.\n\n\nPreparation for the Implementation of Elimination Method in the Code\nRepresenting the system in a matrix form as $\n\\[\\begin{bmatrix}\n-1 & 3 & 7 \\\\\n3 & 2 & 1\n\\end{bmatrix}\\]\n, $ you can apply the same operations to the rows of the matrix with Python code.\nUnify matrix \\(A\\) and array \\(b\\) into one matrix using np.hstack() function. Note that the shape of the originally defined array \\(b\\) was \\((2,)\\), to stack it with the \\((2, 2)\\) matrix you need to use .reshape((2, 1)) function:\n\nA_system = np.hstack((A, b.reshape(2, 1)))\n\nprint(A_system)\n\n[[-1.  3.  7.]\n [ 3.  2.  1.]]\n\n\nLet’s review how to extract a row of a matrix, which will help later to perform required operations with the rows. Remember, that indexing of arrays in Python starts from zero, so to extract second row of a matrix, you need to use the following code:\n\nprint(A_system[1])\n\n[3. 2. 1.]\n\n\n\n\nImplementation of Elimination Method\nLet’s apply some operations to the matrix \\(A\\_system\\) to eliminate variable . First, copy the matrix to keep the original one without any changes. Then multiply first row by 3, add it to the second row and exchange the second row with the result of this addition:\n\nNote: Function .copy() is used to keep the original matrix without any changes.\n\n\nA_system_res = A_system.copy()\n\n\nA_system_res[1] = 3 * A_system_res[0] + A_system_res[1]\n\nprint(A_system_res)\n\n[[-1.  3.  7.]\n [ 0. 11. 22.]]\n\n\nMultipy second row by \\(1/11\\):\n\nA_system_res[1] = 1/11 * A_system_res[1]\n\nprint(A_system_res)\n\n[[-1.  3.  7.]\n [ 0.  1.  2.]]\n\n\n\n\nGraphical Representation of the Solution\nA linear equation in two variables (here, \\(x_1\\) and \\(x_2\\)) is represented geometrically by a line which points \\((x_1, x_2)\\) make up the collection of solutions of the equation. This is called the graph of the linear equation. In case of the system of two equations there will be two lines corresponding to each of the equations, and the solution will be the intersection point of those lines.\nIn the following code you will define a function plot_lines() to plot the lines and use it later to represent the solution which you found earlier. Do not worry if the code in the following cell will not be clear - at this stage this is not important code to understand.\n\nimport matplotlib.pyplot as plt\n\ndef plot_lines(M):\n    x_1 = np.linspace(-10,10,100)\n    x_2_line_1 = (M[0,2] - M[0,0] * x_1) / M[0,1]\n    x_2_line_2 = (M[1,2] - M[1,0] * x_1) / M[1,1]\n    \n    _, ax = plt.subplots(figsize=(10, 10))\n    ax.plot(x_1, x_2_line_1, '-', linewidth=2, color='#0075ff',\n        label=f'$x_2={-M[0,0]/M[0,1]:.2f}x_1 + {M[0,2]/M[0,1]:.2f}$')\n    ax.plot(x_1, x_2_line_2, '-', linewidth=2, color='#ff7300',\n        label=f'$x_2={-M[1,0]/M[1,1]:.2f}x_1 + {M[1,2]/M[1,1]:.2f}$')\n\n    A = M[:, 0:-1]\n    b = M[:, -1::].flatten()\n    d = np.linalg.det(A)\n\n    if d != 0:\n        solution = np.linalg.solve(A,b) \n        ax.plot(solution[0], solution[1], '-o', mfc='none', \n            markersize=10, markeredgecolor='#ff0000', markeredgewidth=2)\n        ax.text(solution[0]-0.25, solution[1]+0.75, f'$(${solution[0]:.0f}$,{solution[1]:.0f})$', fontsize=14)\n    ax.tick_params(axis='x', labelsize=14)\n    ax.tick_params(axis='y', labelsize=14)\n    ax.set_xticks(np.arange(-10, 10))\n    ax.set_yticks(np.arange(-10, 10))\n\n    plt.xlabel('$x_1$', size=14)\n    plt.ylabel('$x_2$', size=14)\n    plt.legend(loc='upper right', fontsize=14)\n    plt.axis([-10, 10, -10, 10])\n\n    plt.grid()\n    plt.gca().set_aspect(\"equal\")\n\n    plt.show()\n\n\nplot_lines(A_system)"
  },
  {
    "objectID": "posts/Solving-Linear-Systems-2-variable/en/Solving-Linear-Systems-2-variables.html#system-of-linear-equations-with-no-solutions",
    "href": "posts/Solving-Linear-Systems-2-variable/en/Solving-Linear-Systems-2-variables.html#system-of-linear-equations-with-no-solutions",
    "title": "Solving Linear Systems: 2 Variables",
    "section": "System of Linear Equations with No Solutions",
    "text": "System of Linear Equations with No Solutions\nGiven another system of linear equations:\n\\[\\begin{cases}\n-x_1+3x_2=7, \\\\ 3x_1-9x_2=1, \\end{cases}\\tag{5}\\]\nlet’s find the determinant of the corresponding matrix.\n\nA_2 = np.array([\n        [-1, 3],\n        [3, -9]\n    ], dtype=np.dtype(float))\n\nb_2 = np.array([7, 1], dtype=np.dtype(float))\n\nd_2 = np.linalg.det(A_2)\n\nprint(f\"Determinant of matrix A_2: {d_2:.2f}\")\n\nDeterminant of matrix A_2: 0.00\n\n\nIt is equal to zero, thus the system cannot have one unique solution. It will have either infinitely many solutions or none. The consistency of it will depend on the free coefficients (right side coefficients). You can run the code in the following cell to check that the np.linalg.solve() function will give an error due to singularity.\n\ntry:\n    x_2 = np.linalg.solve(A_2, b_2)\nexcept np.linalg.LinAlgError as err:\n    print(err)\n\nSingular matrix\n\n\nPrepare to apply the elimination method, constructing the matrix, corresponding to this linear system:\n\nA_2_system = np.hstack((A_2, b_2.reshape((2, 1))))\nprint(A_2_system)\n\n[[-1.  3.  7.]\n [ 3. -9.  1.]]\n\n\nPerform elimination:\n\n# copy() matrix.\nA_2_system_res = A_2_system.copy()\n\n# Multiply row 0 by 3 and add it to the row 1.\nA_2_system_res[1] = 3 * A_2_system_res[0] + A_2_system_res[1]\nprint(A_2_system_res)\n\n[[-1.  3.  7.]\n [ 0.  0. 22.]]\n\n\nThe last row will correspond to the equation \\(0=22\\) which has no solution. Thus the whole linear system \\((5)\\) has no solutions. Let’s see what will be on the graph. Do you expect the corresponding two lines to intersect?\n\nplot_lines(A_2_system)"
  },
  {
    "objectID": "posts/Solving-Linear-Systems-2-variable/en/Solving-Linear-Systems-2-variables.html#system-of-linear-equations-with-infinite-number-of-solutions",
    "href": "posts/Solving-Linear-Systems-2-variable/en/Solving-Linear-Systems-2-variables.html#system-of-linear-equations-with-infinite-number-of-solutions",
    "title": "Solving Linear Systems: 2 Variables",
    "section": "System of Linear Equations with Infinite Number of Solutions",
    "text": "System of Linear Equations with Infinite Number of Solutions\nChanging free coefficients of the system \\((5)\\) you can bring it to consistency:\n\\[\\begin{cases}\n-x_1+3x_2=7, \\\\ 3x_1-9x_2=-21, \\end{cases}\\tag{6}\\]\n\nb_3 = np.array([7, -21], dtype=np.dtype(float))\n\nPrepare the new matrix, corresponding to the system \\((6)\\):\n\nA_3_system = np.hstack((A_2, b_3.reshape((2, 1))))\nprint(A_3_system)\n\n[[ -1.   3.   7.]\n [  3.  -9. -21.]]\n\n\nPerform elimination using elementary operations:\n\n# copy() matrix.\nA_3_system_res = A_3_system.copy()\n\n# Multiply row 0 by 3 and add it to the row 1.\nA_3_system_res[1] = 3 * A_3_system_res[0] + A_3_system_res[1]\nprint(A_3_system_res)\n\n[[-1.  3.  7.]\n [ 0.  0.  0.]]\n\n\nThus from the corresponding linear system\n\\[\\begin{cases}\n-x_1+3x_2=7, \\\\ 0=0, \\end{cases}\\tag{7}\\]\nthe solutions of the linear system \\((6)\\) are:\n\\[x_1=3x_2-7, \\tag{8}\\]\nwhere \\(x_2\\) is any real number.\nIf you plot the equations of the system, how many lines do you expect to see in the graph now? Check it, using the code below:\n\nplot_lines(A_3_system)"
  },
  {
    "objectID": "posts/vector-operations/en/vector-operations.html",
    "href": "posts/vector-operations/en/vector-operations.html",
    "title": "Vector Operations: Scalar Multiplication, Sum and Dot Product of Vectors",
    "section": "",
    "text": "In this notebook, you will use Python and NumPy functions to perform main vector operations: scalar multiplication, sum of vectors and their dot product. You will also investigate the speed of calculations using loop and vectorized forms of these main linear algebra operations."
  },
  {
    "objectID": "posts/vector-operations/en/vector-operations.html#tldr",
    "href": "posts/vector-operations/en/vector-operations.html#tldr",
    "title": "Vector Operations: Scalar Multiplication, Sum and Dot Product of Vectors",
    "section": "",
    "text": "In this notebook, you will use Python and NumPy functions to perform main vector operations: scalar multiplication, sum of vectors and their dot product. You will also investigate the speed of calculations using loop and vectorized forms of these main linear algebra operations."
  },
  {
    "objectID": "posts/vector-operations/en/vector-operations.html#packages",
    "href": "posts/vector-operations/en/vector-operations.html#packages",
    "title": "Vector Operations: Scalar Multiplication, Sum and Dot Product of Vectors",
    "section": "Packages",
    "text": "Packages\nLoad the NumPy package to access its functions.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"NumPy version: {}\".format(np.__version__))\n\nNumPy version: 1.21.5"
  },
  {
    "objectID": "posts/vector-operations/en/vector-operations.html#scalar-multiplication-and-sum-of-vectors",
    "href": "posts/vector-operations/en/vector-operations.html#scalar-multiplication-and-sum-of-vectors",
    "title": "Vector Operations: Scalar Multiplication, Sum and Dot Product of Vectors",
    "section": "Scalar Multiplication and Sum of Vectors",
    "text": "Scalar Multiplication and Sum of Vectors\n\nVisualization of a Vector \\(v\\in\\mathbb{R}^2\\)\nYou already have seen in the videos and labs, that vectors can be visualized as arrows, and it is easy to do it for a \\(v\\in\\mathbb{R}^2\\), e.g. \\(v=\\begin{bmatrix}  1 & 3 \\end{bmatrix}^T\\)\nThe following code will show the visualization.\n\ndef plot_vectors(list_v, list_label, list_color):\n    _, ax = plt.subplots(figsize=(10, 10))\n    ax.tick_params(axis='x', labelsize=14)\n    ax.tick_params(axis='y', labelsize=14)\n    ax.set_xticks(np.arange(-10, 10))\n    ax.set_yticks(np.arange(-10, 10))\n    \n    \n    plt.axis([-10, 10, -10, 10])\n    for i, v in enumerate(list_v):\n        sgn = 0.4 * np.array([[1] if i==0 else [i] for i in np.sign(v)])\n        plt.quiver(v[0], v[1], color=list_color[i], angles='xy', scale_units='xy', scale=1)\n        ax.text(v[0]-0.2+sgn[0], v[1]-0.2+sgn[1], list_label[i], fontsize=14, color=list_color[i])\n\n    plt.grid()\n    plt.gca().set_aspect(\"equal\")\n    plt.show()\n\nv = np.array([[1],[3]])\n# Arguments: list of vectors as NumPy arrays, labels, colors.\nplot_vectors([v], [f\"$v$\"], [\"black\"])\n\n\n\n\nThe vector is defined by its norm (length, magnitude) and direction, not its actual position. But for clarity and convenience vectors are often plotted starting in the origin (in \\(\\mathbb{R}^2\\) it is a point \\((0,0)\\)) .\n\n\nScalar Multiplication\nScalar multiplication of a vector \\(v=\\begin{bmatrix}  v_1 & v_2 & \\ldots & v_n \\end{bmatrix}^T\\in\\mathbb{R}^n\\) by a scalar \\(k\\) is a vector \\(kv=\\begin{bmatrix}  kv_1 & kv_2 & \\ldots & kv_n \\end{bmatrix}^T\\) (element by element multiplication). If \\(k&gt;0\\), then \\(kv\\) is a vector pointing in the same direction as \\(v\\) and it is \\(k\\) times as long as \\(v\\). If \\(k=0\\), then \\(kv\\) is a zero vector. If \\(k&lt;0\\), vector \\(kv\\) will be pointing in the opposite direction. In Python you can perform this operation with a * operator. Check out the example below:\n\nplot_vectors([v, 2*v, -2*v], [f\"$v$\", f\"$2v$\", f\"$-2v$\"], [\"black\", \"green\", \"blue\"])\n\n\n\n\n\n\nSum of Vectors\nSum of vectors (vector addition) can be performed by adding the corresponding components of the vectors: if \\(v=\\begin{bmatrix}  v_1 & v_2 & \\ldots & v_n \\end{bmatrix}^T\\in\\mathbb{R}^n\\) and\n\\(w=\\begin{bmatrix}  w_1 & w_2 & \\ldots & w_n \\end{bmatrix}^T\\in\\mathbb{R}^n\\), then \\(v + w=\\begin{bmatrix}  v_1 + w_1 & v_2 + w_2 & \\ldots & v_n + w_n \\end{bmatrix}^T\\in\\mathbb{R}^n\\). The so-called parallelogram law gives the rule for vector addition. For two vectors \\(u\\) and \\(v\\) represented by the adjacent sides (both in magnitude and direction) of a parallelogram drawn from a point, the vector sum \\(u+v\\) is is represented by the diagonal of the parallelogram drawn from the same point:\n\nIn Python you can either use + operator or NumPy function np.add(). In the following code you can uncomment the line to check that the result will be the same:\n\nv = np.array([[1],[3]])\nw = np.array([[4],[-1]])\n\nplot_vectors([v, w, v + w], [f\"$v$\", f\"$w$\", f\"$v + w$\"], [\"black\", \"black\", \"red\"])\n\n\n\n\n\nplot_vectors([v, w, np.add(v, w)], [f\"$v$\", f\"$w$\", f\"$v + w$\"], [\"black\", \"black\", \"red\"])\n\n\n\n\n\n\nNorm of a Vector\nThe norm of a vector \\(v\\) is denoted as \\(\\lvert v\\rvert\\). It is a nonnegative number that describes the extent of the vector in space (its length). The norm of a vector can be found using NumPy function np.linalg.norm():\n\nprint(\"Norm of a vector v is\", np.linalg.norm(v))\n\nNorm of a vector v is 3.1622776601683795"
  },
  {
    "objectID": "posts/vector-operations/en/vector-operations.html#dot-product",
    "href": "posts/vector-operations/en/vector-operations.html#dot-product",
    "title": "Vector Operations: Scalar Multiplication, Sum and Dot Product of Vectors",
    "section": "Dot Product",
    "text": "Dot Product\n\nAlgebraic Definition of the Dot Product\nThe dot product (or scalar product) is an algebraic operation that takes two vectors \\(x=\\begin{bmatrix}  x_1 & x_2 & \\ldots & x_n \\end{bmatrix}^T\\in\\mathbb{R}^n\\) and\n\\(y=\\begin{bmatrix}  y_1 & y_2 & \\ldots & y_n \\end{bmatrix}^T\\in\\mathbb{R}^n\\) and returns a single scalar. The dot product can be represented with a dot operator \\(x\\cdot y\\) and defined as:\n\\[x\\cdot y = \\sum_{i=1}^{n} x_iy_i = x_1y_1+x_2y_2+\\ldots+x_ny_n \\tag{1}\\]\n\n\nDot Product using Python\nThe simplest way to calculate dot product in Python is to take the sum of element by element multiplications. You can define the vectors \\(x\\) and \\(y\\) by listing their coordinates:\n\nx = [1, -2, -5]\ny = [4, 3, -1]\n\nNext, let’s define a function dot(x,y) for the dot product calculation:\n\ndef dot(x, y):\n    s=0\n    for xi, yi in zip(x, y):\n        s += xi * yi\n    return s\n\nFor the sake of simplicity, let’s assume that the vectors passed to the above function are always of the same size, so that you don’t need to perform additional checks.\nNow everything is ready to perform the dot product calculation calling the function dot(x,y):\n\nprint(\"The dot product of x and y is\", dot(x, y))\n\nThe dot product of x and y is 3\n\n\nDot product is very a commonly used operator, so NumPy linear algebra package provides quick way to calculate it using function np.dot():\n\nprint(\"np.dot(x,y) function returns dot product of x and y:\", np.dot(x, y)) \n\nnp.dot(x,y) function returns dot product of x and y: 3\n\n\nNote that you did not have to define vectors \\(x\\) and \\(y\\) as NumPy arrays, the function worked even with the lists. But there are alternative functions in Python, such as explicit operator @ for the dot product, which can be applied only to the NumPy arrays. You can run the following cell to check that.\n\nprint(\"This line output is a dot product of x and y: \", np.array(x) @ np.array(y))\n\nprint(\"\\nThis line output is an error:\")\ntry:\n    print(x @ y)\nexcept TypeError as err:\n    print(err)\n\nThis line output is a dot product of x and y:  3\n\nThis line output is an error:\nunsupported operand type(s) for @: 'list' and 'list'\n\n\nAs both np.dot() and @ operators are commonly used, it is recommended to define vectors as NumPy arrays to avoid errors. Let’s redefine vectors \\(x\\) and \\(y\\) as NumPy arrays to be safe:\n\nx = np.array(x)\ny = np.array(y)\n\n\n\nSpeed of Calculations in Vectorized Form\nDot product operations in Machine Learning applications are applied to the large vectors with hundreds or thousands of coordinates (called high dimensional vectors). Training models based on large datasets often takes hours and days even on powerful machines. Speed of calculations is crucial for the training and deployment of your models.\nIt is important to understand the difference in the speed of calculations using vectorized and the loop forms of the vectors and functions. In the loop form operations are performed one by one, while in the vectorized form they can be performed in parallel. In the section above you defined loop version of the dot product calculation (function dot()), while np.dot() and @ are the functions representing vectorized form.\nLet’s perform a simple experiment to compare their speed. Define new vectors \\(a\\) and \\(b\\) of the same size \\(1,000,000\\):\n\na = np.random.rand(1000000)\nb = np.random.rand(1000000)\n\nUse time.time() function to evaluate amount of time (in seconds) required to calculate dot product using the function dot(x,y) which you defined above:\n\nimport time\n\ntic = time.time()\nc = dot(a,b)\ntoc = time.time()\nprint(\"Dot product: \", c)\nprint (\"Time for the loop version:\" + str(1000*(toc-tic)) + \" ms\")\n\nDot product:  249642.47515074877\nTime for the loop version:205.99746704101562 ms\n\n\nNow compare it with the speed of the vectorized versions:\n\ntic = time.time()\nc = np.dot(a,b)\ntoc = time.time()\nprint(\"Dot product: \", c)\nprint (\"Time for the vectorized version, np.dot() function: \" + str(1000*(toc-tic)) + \" ms\")\n\nDot product:  249642.47515074533\nTime for the vectorized version, np.dot() function: 12.999773025512695 ms\n\n\n\ntic = time.time()\nc = a @ b\ntoc = time.time()\nprint(\"Dot product: \", c)\nprint (\"Time for the vectorized version, @ function: \" + str(1000*(toc-tic)) + \" ms\")\n\nDot product:  249642.47515074533\nTime for the vectorized version, @ function: 1.033782958984375 ms\n\n\nYou can see that vectorization is extremely beneficial in terms of the speed of calculations!\n\n\nGeometric Definition of the Dot Product\nIn Euclidean space, a Euclidean vector has both magnitude and direction. The dot product of two vectors \\(x\\) and \\(y\\) is defined by:\n\\[x\\cdot y = \\lvert x\\rvert \\lvert y\\rvert \\cos(\\theta),\\tag{2}\\]\nwhere \\(\\theta\\) is the angle between the two vectors:\n\nThis provides an easy way to test the orthogonality between vectors. If \\(x\\) and \\(y\\) are orthogonal (the angle between vectors is \\(90^{\\circ}\\)), then since \\(\\cos(90^{\\circ})=0\\), it implies that the dot product of any two orthogonal vectors must be \\(0\\). Let’s test it, taking two vectors \\(i\\) and \\(j\\) we know are orthogonal:\n\ni = np.array([1, 0, 0])\nj = np.array([0, 1, 0])\nprint(\"The dot product of i and j is\", dot(i, j))\n\nThe dot product of i and j is 0\n\n\n\n\nApplication of the Dot Product: Vector Similarity\nGeometric definition of a dot product is used in one of the applications - to evaluate vector similarity. In Natural Language Processing (NLP) words or phrases from vocabulary are mapped to a corresponding vector of real numbers. Similarity between two vectors can be defined as a cosine of the angle between them. When they point in the same direction, their similarity is 1 and it decreases with the increase of the angle.\nThen equation \\((2)\\) can be rearranged to evaluate cosine of the angle between vectors:\n\\(\\cos(\\theta)=\\frac{x \\cdot y}{\\lvert x\\rvert \\lvert y\\rvert}.\\tag{3}\\)\nZero value corresponds to the zero similarity between vectors (and words corresponding to those vectors). Largest value is when vectors point in the same direction, lowest value is when vectors point in the opposite directions.\nThis example of vector similarity is given to link the material with the Machine Learning applications."
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-11.html#tldr",
    "href": "publication/GDRL/GDRL-chapter-11.html#tldr",
    "title": "Chapter 11: Policy-Gradient and Actor-Critic Methods",
    "section": "TL;DR",
    "text": "TL;DR\n\n그로킹 심층 강화학습 중 11장 내용인 “정책 경사법과 액터-크리틱 학습 방법들”에 대한 내용입니다.\n\n\n\nCode\n!pip install tqdm numpy scikit-learn pyglet setuptools && \\\n!pip install gym asciinema pandas tabulate tornado==5.* PyBullet && \\\n!pip install git+https://github.com/pybox2d/pybox2d#egg=Box2D && \\\n!pip install git+https://github.com/mimoralea/gym-bandits#egg=gym-bandits && \\\n!pip install git+https://github.com/mimoralea/gym-walk#egg=gym-walk && \\\n!pip install git+https://github.com/mimoralea/gym-aima#egg=gym-aima && \\\n!pip install gym[atari]\n!pip install torch torchvision\n\n\n\nimport warnings ; warnings.filterwarnings('ignore')\nimport os\nos.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\nos.environ['CUDA_VISIBLE_DEVICES']=''\nos.environ['OMP_NUM_THREADS'] = '1'\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.multiprocessing as mp\nimport threading\n\nimport numpy as np\nfrom IPython.display import display\nfrom collections import namedtuple, deque\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nfrom itertools import cycle, count\nfrom textwrap import wrap\n\nimport matplotlib\nimport subprocess\nimport os.path\nimport tempfile\nimport random\nimport base64\nimport pprint\nimport glob\nimport time\nimport json\nimport sys\nimport gym\nimport io\nimport os\nimport gc\nimport platform\n\nfrom gym import wrappers\nfrom subprocess import check_output\nfrom IPython.display import HTML\n\nLEAVE_PRINT_EVERY_N_SECS = 30\nERASE_LINE = '\\x1b[2K'\nEPS = 1e-6\nRESULTS_DIR = os.path.join('.', 'gym-results')\nSEEDS = (12, 34, 56, 78, 90)\n\n%matplotlib inline\n\n\nplt.style.use('fivethirtyeight')\nparams = {\n    'figure.figsize': (15, 8),\n    'font.size': 24,\n    'legend.fontsize': 20,\n    'axes.titlesize': 28,\n    'axes.labelsize': 24,\n    'xtick.labelsize': 20,\n    'ytick.labelsize': 20\n}\npylab.rcParams.update(params)\nnp.set_printoptions(suppress=True)\n\n\ntorch.cuda.is_available()\n\nFalse\n\n\n\ndef get_make_env_fn(**kargs):\n    def make_env_fn(env_name, seed=None, render=None, record=False,\n                    unwrapped=False, monitor_mode=None, \n                    inner_wrappers=None, outer_wrappers=None):\n        mdir = tempfile.mkdtemp()\n        env = None\n        if render:\n            try:\n                env = gym.make(env_name, render=render)\n            except:\n                pass\n        if env is None:\n            env = gym.make(env_name)\n        if seed is not None: env.seed(seed)\n        env = env.unwrapped if unwrapped else env\n        if inner_wrappers:\n            for wrapper in inner_wrappers:\n                env = wrapper(env)\n        env = wrappers.Monitor(\n            env, mdir, force=True, \n            mode=monitor_mode, \n            video_callable=lambda e_idx: record) if monitor_mode else env\n        if outer_wrappers:\n            for wrapper in outer_wrappers:\n                env = wrapper(env)\n        return env\n    return make_env_fn, kargs\n\n\ndef get_videos_html(env_videos, title, max_n_videos=5):\n    videos = np.array(env_videos)\n    if len(videos) == 0:\n        return\n    \n    n_videos = max(1, min(max_n_videos, len(videos)))\n    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int) if n_videos &gt; 1 else [-1,]\n    videos = videos[idxs,...]\n\n    strm = '&lt;h2&gt;{}&lt;/h2&gt;'.format(title)\n    for video_path, meta_path in videos:\n        video = io.open(video_path, 'r+b').read()\n        encoded = base64.b64encode(video)\n\n        with open(meta_path) as data_file:    \n            meta = json.load(data_file)\n\n        html_tag = \"\"\"\n        &lt;h3&gt;{0}&lt;/h3&gt;\n        &lt;video width=\"960\" height=\"540\" controls&gt;\n            &lt;source src=\"data:video/mp4;base64,{1}\" type=\"video/mp4\" /&gt;\n        &lt;/video&gt;\"\"\"\n        strm += html_tag.format('Episode ' + str(meta['episode_id']), encoded.decode('ascii'))\n    return strm\n\n\nplatform.system()\n\n'Linux'\n\n\n\ndef get_gif_html(env_videos, title, subtitle_eps=None, max_n_videos=4):\n    videos = np.array(env_videos)\n    if len(videos) == 0:\n        return\n    \n    n_videos = max(1, min(max_n_videos, len(videos)))\n    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int) if n_videos &gt; 1 else [-1,]\n    videos = videos[idxs,...]\n\n    strm = '&lt;h2&gt;{}&lt;/h2&gt;'.format(title)\n    for video_path, meta_path in videos:\n        basename = os.path.splitext(video_path)[0]\n        gif_path = basename + '.gif'\n        if not os.path.exists(gif_path):\n            if platform.system() == 'Linux':\n                ps = subprocess.Popen(\n                    ('ffmpeg', \n                     '-i', video_path, \n                     '-r', '7',\n                     '-f', 'image2pipe', \n                     '-vcodec', 'ppm',\n                     '-crf', '20',\n                     '-vf', 'scale=512:-1',\n                     '-'), \n                    stdout=subprocess.PIPE,\n                    universal_newlines=True)\n                output = subprocess.check_output(\n                    ('convert',\n                     '-coalesce',\n                     '-delay', '7',\n                     '-loop', '0',\n                     '-fuzz', '2%',\n                     '+dither',\n                     '-deconstruct',\n                     '-layers', 'Optimize',\n                     '-', gif_path), \n                    stdin=ps.stdout)\n                ps.wait()\n            else:\n                ps = subprocess.Popen('ffmpeg -i {} -r 7 -f image2pipe \\\n                                      -vcodec ppm -crf 20 -vf scale=512:-1 - | \\\n                                      convert -coalesce -delay 7 -loop 0 -fuzz 2% \\\n                                      +dither -deconstruct -layers Optimize \\\n                                      - {}'.format(video_path, gif_path), \n                                      stdin=subprocess.PIPE, \n                                      shell=True)\n                ps.wait()\n\n        gif = io.open(gif_path, 'r+b').read()\n        encoded = base64.b64encode(gif)\n            \n        with open(meta_path) as data_file:    \n            meta = json.load(data_file)\n\n        html_tag = \"\"\"\n        &lt;h3&gt;{0}&lt;/h3&gt;\n        &lt;img src=\"data:image/gif;base64,{1}\" /&gt;\"\"\"\n        prefix = 'Trial ' if subtitle_eps is None else 'Episode '\n        sufix = str(meta['episode_id'] if subtitle_eps is None \\\n                    else subtitle_eps[meta['episode_id']])\n        strm += html_tag.format(prefix + sufix, encoded.decode('ascii'))\n    return strm"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-11.html#different-types-of-cart-pole-environments",
    "href": "publication/GDRL/GDRL-chapter-11.html#different-types-of-cart-pole-environments",
    "title": "Chapter 11: Policy-Gradient and Actor-Critic Methods",
    "section": "Different types of Cart Pole environments",
    "text": "Different types of Cart Pole environments\n\nclass DiscountedCartPole(gym.Wrapper):\n    def __init__(self, env):\n        gym.Wrapper.__init__(self, env)\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n    def step(self, a):\n        o, r, d, _ = self.env.step(a)\n        (x, x_dot, theta, theta_dot) = o\n        pole_fell =  x &lt; -self.env.unwrapped.x_threshold \\\n                    or x &gt; self.env.unwrapped.x_threshold \\\n                    or theta &lt; -self.env.unwrapped.theta_threshold_radians \\\n                    or theta &gt; self.env.unwrapped.theta_threshold_radians\n        r = -1 if pole_fell else 0\n        return o, r, d, _\n\n\nclass MCCartPole(gym.Wrapper):\n    def __init__(self, env):\n        gym.Wrapper.__init__(self, env)\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n    def step(self, a):\n        o, r, d, _ = self.env.step(a)\n        (x, x_dot, theta, theta_dot) = o\n        pole_fell =  x &lt; -self.env.unwrapped.x_threshold \\\n                    or x &gt; self.env.unwrapped.x_threshold \\\n                    or theta &lt; -self.env.unwrapped.theta_threshold_radians \\\n                    or theta &gt; self.env.unwrapped.theta_threshold_radians\n        if d:\n            if pole_fell:\n                r = 0 # done, in failure\n            else:\n                r = self.env._max_episode_steps # done, but successfully\n        return o, r, d, _"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-11.html#monte-carlo-reinforce",
    "href": "publication/GDRL/GDRL-chapter-11.html#monte-carlo-reinforce",
    "title": "Chapter 11: Policy-Gradient and Actor-Critic Methods",
    "section": "Monte-Carlo REINFORCE",
    "text": "Monte-Carlo REINFORCE\n\nclass FCDAP(nn.Module):\n    def __init__(self, \n                 input_dim, \n                 output_dim,\n                 hidden_dims=(32,32), \n                 activation_fc=F.relu):\n        super(FCDAP, self).__init__()\n        self.activation_fc = activation_fc\n\n        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n        self.hidden_layers = nn.ModuleList()\n        for i in range(len(hidden_dims)-1):\n            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n            self.hidden_layers.append(hidden_layer)\n        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n\n    def _format(self, state):\n        x = state\n        if not isinstance(x, torch.Tensor):\n            x = torch.tensor(x, \n                             dtype=torch.float32)\n            x = x.unsqueeze(0)\n        return x\n        \n    def forward(self, state):\n        x = self._format(state)\n        x = self.activation_fc(self.input_layer(x))\n        for hidden_layer in self.hidden_layers:\n            x = self.activation_fc(hidden_layer(x))\n        return self.output_layer(x)\n\n    def full_pass(self, state):\n        logits = self.forward(state)\n        dist = torch.distributions.Categorical(logits=logits)\n        action = dist.sample()\n        logpa = dist.log_prob(action).unsqueeze(-1)\n        entropy = dist.entropy().unsqueeze(-1)\n        is_exploratory = action != np.argmax(logits.detach().numpy())\n        return action.item(), is_exploratory.item(), logpa, entropy\n\n    def select_action(self, state):\n        logits = self.forward(state)\n        dist = torch.distributions.Categorical(logits=logits)\n        action = dist.sample()\n        return action.item()\n    \n    def select_greedy_action(self, state):\n        logits = self.forward(state)\n        return np.argmax(logits.detach().numpy())\n\n\nclass REINFORCE():\n    def __init__(self, policy_model_fn, policy_optimizer_fn, policy_optimizer_lr):\n        self.policy_model_fn = policy_model_fn\n        self.policy_optimizer_fn = policy_optimizer_fn\n        self.policy_optimizer_lr = policy_optimizer_lr\n\n    def optimize_model(self):\n        T = len(self.rewards)\n        discounts = np.logspace(0, T, num=T, base=self.gamma, endpoint=False)\n        returns = np.array([np.sum(discounts[:T-t] * self.rewards[t:]) for t in range(T)])\n\n        discounts = torch.FloatTensor(discounts).unsqueeze(1)\n        returns = torch.FloatTensor(returns).unsqueeze(1)\n        self.logpas = torch.cat(self.logpas)\n\n        policy_loss = -(discounts * returns * self.logpas).mean()\n        self.policy_optimizer.zero_grad()\n        policy_loss.backward()\n        self.policy_optimizer.step()\n\n    def interaction_step(self, state, env):\n        action, is_exploratory, logpa, _ = self.policy_model.full_pass(state)\n        new_state, reward, is_terminal, _ = env.step(action)\n\n        self.logpas.append(logpa)\n        self.rewards.append(reward)\n        \n        self.episode_reward[-1] += reward\n        self.episode_timestep[-1] += 1\n        self.episode_exploration[-1] += int(is_exploratory)\n\n        return new_state, is_terminal\n\n    def train(self, make_env_fn, make_env_kargs, seed, gamma, \n              max_minutes, max_episodes, goal_mean_100_reward):\n        training_start, last_debug_time = time.time(), float('-inf')\n\n        self.checkpoint_dir = tempfile.mkdtemp()\n        self.make_env_fn = make_env_fn\n        self.make_env_kargs = make_env_kargs\n        self.seed = seed\n        self.gamma = gamma\n        \n        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n    \n        nS, nA = env.observation_space.shape[0], env.action_space.n\n        self.episode_timestep = []\n        self.episode_reward = []\n        self.episode_seconds = []\n        self.episode_exploration = []\n        self.evaluation_scores = []\n        \n        self.policy_model = self.policy_model_fn(nS, nA)\n        self.policy_optimizer = self.policy_optimizer_fn(self.policy_model, \n                                                         self.policy_optimizer_lr)\n                    \n        result = np.empty((max_episodes, 5))\n        result[:] = np.nan\n        training_time = 0\n        for episode in range(1, max_episodes + 1):\n            episode_start = time.time()\n            \n            state, is_terminal = env.reset(), False\n            self.episode_reward.append(0.0)\n            self.episode_timestep.append(0.0)\n            self.episode_exploration.append(0.0)\n\n            # collect rollout\n            self.logpas, self.rewards = [], []\n            for step in count():\n                state, is_terminal = self.interaction_step(state, env)\n                if is_terminal:\n                    gc.collect()\n                    break\n\n            self.optimize_model()\n            \n            # stats\n            episode_elapsed = time.time() - episode_start\n            self.episode_seconds.append(episode_elapsed)\n            training_time += episode_elapsed\n            evaluation_score, _ = self.evaluate(self.policy_model, env)\n            self.save_checkpoint(episode-1, self.policy_model)\n\n            total_step = int(np.sum(self.episode_timestep))\n            self.evaluation_scores.append(evaluation_score)\n            \n            mean_10_reward = np.mean(self.episode_reward[-10:])\n            std_10_reward = np.std(self.episode_reward[-10:])\n            mean_100_reward = np.mean(self.episode_reward[-100:])\n            std_100_reward = np.std(self.episode_reward[-100:])\n            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n            lst_100_exp_rat = np.array(\n                self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n            mean_100_exp_rat = np.mean(lst_100_exp_rat)\n            std_100_exp_rat = np.std(lst_100_exp_rat)\n\n            wallclock_elapsed = time.time() - training_start\n            result[episode-1] = total_step, mean_100_reward, \\\n                mean_100_eval_score, training_time, wallclock_elapsed\n            \n            reached_debug_time = time.time() - last_debug_time &gt;= LEAVE_PRINT_EVERY_N_SECS\n            reached_max_minutes = wallclock_elapsed &gt;= max_minutes * 60\n            reached_max_episodes = episode &gt;= max_episodes\n            reached_goal_mean_reward = mean_100_eval_score &gt;= goal_mean_100_reward\n            training_is_over = reached_max_minutes or \\\n                               reached_max_episodes or \\\n                               reached_goal_mean_reward\n\n            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n            debug_message = 'el {}, ep {:04}, ts {:06}, '\n            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n            debug_message = debug_message.format(\n                elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n                mean_100_eval_score, std_100_eval_score)\n            print(debug_message, end='\\r', flush=True)\n            if reached_debug_time or training_is_over:\n                print(ERASE_LINE + debug_message, flush=True)\n                last_debug_time = time.time()\n            if training_is_over:\n                if reached_max_minutes: print(u'--&gt; reached_max_minutes \\u2715')\n                if reached_max_episodes: print(u'--&gt; reached_max_episodes \\u2715')\n                if reached_goal_mean_reward: print(u'--&gt; reached_goal_mean_reward \\u2713')\n                break\n                \n        final_eval_score, score_std = self.evaluate(self.policy_model, env, n_episodes=100)\n        wallclock_time = time.time() - training_start\n        print('Training complete.')\n        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n              ' {:.2f}s wall-clock time.\\n'.format(\n                  final_eval_score, score_std, training_time, wallclock_time))\n        env.close() ; del env\n        self.get_cleaned_checkpoints()\n        return result, final_eval_score, training_time, wallclock_time\n    \n    def evaluate(self, eval_policy_model, eval_env, n_episodes=1, greedy=True):\n        rs = []\n        for _ in range(n_episodes):\n            s, d = eval_env.reset(), False\n            rs.append(0)\n            for _ in count():\n                if greedy:\n                    a = eval_policy_model.select_greedy_action(s)\n                else: \n                    a = eval_policy_model.select_action(s)\n                s, r, d, _ = eval_env.step(a)\n                rs[-1] += r\n                if d: break\n        return np.mean(rs), np.std(rs)\n\n    def get_cleaned_checkpoints(self, n_checkpoints=5):\n        try: \n            return self.checkpoint_paths\n        except AttributeError:\n            self.checkpoint_paths = {}\n\n        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n        last_ep = max(paths_dic.keys())\n        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n\n        for idx, path in paths_dic.items():\n            if idx in checkpoint_idxs:\n                self.checkpoint_paths[idx] = path\n            else:\n                os.unlink(path)\n\n        return self.checkpoint_paths\n\n    def demo_last(self, title='Fully-trained {} Agent', n_episodes=3, max_n_videos=3):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        last_ep = max(checkpoint_paths.keys())\n        self.policy_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n\n        self.evaluate(self.policy_model, env, n_episodes=n_episodes)\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def demo_progression(self, title='{} Agent progression', max_n_videos=5):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        for i in sorted(checkpoint_paths.keys()):\n            self.policy_model.load_state_dict(torch.load(checkpoint_paths[i]))\n            self.evaluate(self.policy_model, env, n_episodes=1)\n\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            subtitle_eps=sorted(checkpoint_paths.keys()),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def save_checkpoint(self, episode_idx, model):\n        torch.save(model.state_dict(), \n                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))\n\n\nreinforce_results = []\nbest_agent, best_eval_score = None, float('-inf')\nfor seed in SEEDS:\n    environment_settings = {\n        'env_name': 'CartPole-v1',\n        'gamma': 1.00,\n        'max_minutes': 10,\n        'max_episodes': 10000,\n        'goal_mean_100_reward': 475\n    }\n\n    policy_model_fn = lambda nS, nA: FCDAP(nS, nA, hidden_dims=(128,64))\n    policy_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n    policy_optimizer_lr = 0.0005\n\n    env_name, gamma, max_minutes, \\\n    max_episodes, goal_mean_100_reward = environment_settings.values()\n    agent = REINFORCE(policy_model_fn, policy_optimizer_fn, policy_optimizer_lr)\n\n    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n    # make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name, unwrapped=True)\n    # make_env_fn, make_env_kargs = get_make_env_fn(\n    #     env_name=env_name, addon_wrappers=[MCCartPole,])\n    result, final_eval_score, training_time, wallclock_time = agent.train(\n        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n    reinforce_results.append(result)\n    if final_eval_score &gt; best_eval_score:\n        best_eval_score = final_eval_score\n        best_agent = agent\nreinforce_results = np.array(reinforce_results)\n\nel 00:00:00, ep 0000, ts 000020, ar 10 020.0±000.0, 100 020.0±000.0, ex 100 0.5±0.0, ev 012.0±000.0\nel 00:00:30, ep 0403, ts 030072, ar 10 207.1±092.1, 100 164.4±086.7, ex 100 0.3±0.0, ev 296.0±132.1\nel 00:01:00, ep 0604, ts 075023, ar 10 374.2±095.5, 100 237.9±112.4, ex 100 0.3±0.0, ev 306.8±145.2\nel 00:01:21, ep 0703, ts 107979, ar 10 373.6±129.8, 100 334.2±127.6, ex 100 0.3±0.0, ev 475.5±056.7\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 64.53s training time, 86.24s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000019, ar 10 019.0±000.0, 100 019.0±000.0, ex 100 0.5±0.0, ev 010.0±000.0\nel 00:00:30, ep 0485, ts 023911, ar 10 173.6±068.5, 100 110.1±075.4, ex 100 0.3±0.0, ev 239.3±128.7\nel 00:01:00, ep 0694, ts 065145, ar 10 223.7±063.6, 100 197.9±093.0, ex 100 0.3±0.0, ev 301.2±134.1\nel 00:01:30, ep 0856, ts 110977, ar 10 318.1±094.9, 100 319.8±123.0, ex 100 0.3±0.0, ev 447.4±103.9\nel 00:02:00, ep 1058, ts 154511, ar 10 184.5±024.9, 100 301.9±124.2, ex 100 0.3±0.0, ev 385.2±138.5\nel 00:02:29, ep 1196, ts 201289, ar 10 316.4±145.6, 100 389.7±115.9, ex 100 0.3±0.0, ev 475.4±058.3\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 119.75s training time, 154.79s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000016, ar 10 016.0±000.0, 100 016.0±000.0, ex 100 0.7±0.0, ev 014.0±000.0\nel 00:00:30, ep 0460, ts 023972, ar 10 197.6±096.7, 100 113.7±081.2, ex 100 0.3±0.1, ev 264.3±146.3\nel 00:01:00, ep 0716, ts 064889, ar 10 384.3±131.7, 100 258.2±189.5, ex 100 0.3±0.0, ev 321.4±196.5\nel 00:01:10, ep 0754, ts 082591, ar 10 494.7±015.9, 100 412.8±123.1, ex 100 0.3±0.0, ev 476.9±076.8\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 56.44s training time, 75.48s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000041, ar 10 041.0±000.0, 100 041.0±000.0, ex 100 0.5±0.0, ev 011.0±000.0\nel 00:00:30, ep 0397, ts 029474, ar 10 190.9±089.6, 100 144.0±073.6, ex 100 0.3±0.0, ev 289.7±125.7\nel 00:01:00, ep 0601, ts 071386, ar 10 354.8±106.5, 100 264.6±129.0, ex 100 0.3±0.0, ev 426.7±091.1\nel 00:01:30, ep 0805, ts 113800, ar 10 446.0±096.3, 100 263.2±146.0, ex 100 0.3±0.0, ev 320.8±162.1\nel 00:01:43, ep 0853, ts 134368, ar 10 404.3±083.7, 100 398.4±108.8, ex 100 0.3±0.0, ev 475.0±063.1\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 435.82±62.11 in 82.74s training time, 107.79s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000029, ar 10 029.0±000.0, 100 029.0±000.0, ex 100 0.6±0.0, ev 019.0±000.0\nel 00:00:30, ep 0459, ts 026790, ar 10 179.2±058.6, 100 119.9±065.1, ex 100 0.3±0.0, ev 205.7±124.6\nel 00:01:00, ep 0639, ts 069009, ar 10 398.4±119.3, 100 263.9±153.6, ex 100 0.2±0.0, ev 319.6±169.6\nel 00:01:30, ep 0783, ts 117147, ar 10 431.5±096.4, 100 357.9±128.2, ex 100 0.2±0.0, ev 391.5±127.9\nel 00:01:37, ep 0812, ts 128183, ar 10 379.3±118.5, 100 409.9±095.6, ex 100 0.2±0.0, ev 475.1±053.5\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 78.07s training time, 102.37s wall-clock time.\n\n\n\n\nbest_agent.demo_progression()\n\nREINFORCE Agent progression\n        Episode 0\n        \n        Episode 175\n        \n        Episode 351\n        \n        Episode 527\n        \n        Episode 703\n        \n\n\n\nbest_agent.demo_last()\n\nFully-trained REINFORCE Agent\n        Trial 0\n        \n        Trial 1\n        \n        Trial 2\n        \n\n\n\nreinforce_max_t, reinforce_max_r, reinforce_max_s, \\\n    reinforce_max_sec, reinforce_max_rt = np.max(reinforce_results, axis=0).T\nreinforce_min_t, reinforce_min_r, reinforce_min_s, \\\n    reinforce_min_sec, reinforce_min_rt = np.min(reinforce_results, axis=0).T\nreinforce_mean_t, reinforce_mean_r, reinforce_mean_s, \\\n    reinforce_mean_sec, reinforce_mean_rt = np.mean(reinforce_results, axis=0).T\nreinforce_x = np.arange(len(reinforce_mean_s))\n\n# reinforce_max_t, reinforce_max_r, reinforce_max_s, \\\n#     reinforce_max_sec, reinforce_max_rt = np.nanmax(reinforce_results, axis=0).T\n# reinforce_min_t, reinforce_min_r, reinforce_min_s, \\\n#     reinforce_min_sec, reinforce_min_rt = np.nanmin(reinforce_results, axis=0).T\n# reinforce_mean_t, reinforce_mean_r, reinforce_mean_s, \\\n#     reinforce_mean_sec, reinforce_mean_rt = np.nanmean(reinforce_results, axis=0).T\n# reinforce_x = np.arange(len(reinforce_mean_s))\n\n# change convergence checks to episode only (not minutes, not mean reward 'float('inf')' can help)\n\n\nfig, axs = plt.subplots(5, 1, figsize=(20,30), sharey=False, sharex=True)\n\n# REINFORCE\naxs[0].plot(reinforce_max_r, 'y', linewidth=1)\naxs[0].plot(reinforce_min_r, 'y', linewidth=1)\naxs[0].plot(reinforce_mean_r, 'y', label='REINFORCE', linewidth=2)\naxs[0].fill_between(reinforce_x, reinforce_min_r, reinforce_max_r, facecolor='y', alpha=0.3)\n\naxs[1].plot(reinforce_max_s, 'y', linewidth=1)\naxs[1].plot(reinforce_min_s, 'y', linewidth=1)\naxs[1].plot(reinforce_mean_s, 'y', label='REINFORCE', linewidth=2)\naxs[1].fill_between(reinforce_x, reinforce_min_s, reinforce_max_s, facecolor='y', alpha=0.3)\n\naxs[2].plot(reinforce_max_t, 'y', linewidth=1)\naxs[2].plot(reinforce_min_t, 'y', linewidth=1)\naxs[2].plot(reinforce_mean_t, 'y', label='REINFORCE', linewidth=2)\naxs[2].fill_between(reinforce_x, reinforce_min_t, reinforce_max_t, facecolor='y', alpha=0.3)\n\naxs[3].plot(reinforce_max_sec, 'y', linewidth=1)\naxs[3].plot(reinforce_min_sec, 'y', linewidth=1)\naxs[3].plot(reinforce_mean_sec, 'y', label='REINFORCE', linewidth=2)\naxs[3].fill_between(reinforce_x, reinforce_min_sec, reinforce_max_sec, facecolor='y', alpha=0.3)\n\naxs[4].plot(reinforce_max_rt, 'y', linewidth=1)\naxs[4].plot(reinforce_min_rt, 'y', linewidth=1)\naxs[4].plot(reinforce_mean_rt, 'y', label='REINFORCE', linewidth=2)\naxs[4].fill_between(reinforce_x, reinforce_min_rt, reinforce_max_rt, facecolor='y', alpha=0.3)\n\n# ALL\naxs[0].set_title('Moving Avg Reward (Training)')\naxs[1].set_title('Moving Avg Reward (Evaluation)')\naxs[2].set_title('Total Steps')\naxs[3].set_title('Training Time')\naxs[4].set_title('Wall-clock Time')\nplt.xlabel('Episodes')\naxs[0].legend(loc='upper left')\nplt.show()\n\n\n\n\n\nreinforce_root_dir = os.path.join(RESULTS_DIR, 'reinforce')\nnot os.path.exists(reinforce_root_dir) and os.makedirs(reinforce_root_dir)\n\nnp.save(os.path.join(reinforce_root_dir, 'x'), reinforce_x)\n\nnp.save(os.path.join(reinforce_root_dir, 'max_r'), reinforce_max_r)\nnp.save(os.path.join(reinforce_root_dir, 'min_r'), reinforce_min_r)\nnp.save(os.path.join(reinforce_root_dir, 'mean_r'), reinforce_mean_r)\n\nnp.save(os.path.join(reinforce_root_dir, 'max_s'), reinforce_max_s)\nnp.save(os.path.join(reinforce_root_dir, 'min_s'), reinforce_min_s )\nnp.save(os.path.join(reinforce_root_dir, 'mean_s'), reinforce_mean_s)\n\nnp.save(os.path.join(reinforce_root_dir, 'max_t'), reinforce_max_t)\nnp.save(os.path.join(reinforce_root_dir, 'min_t'), reinforce_min_t)\nnp.save(os.path.join(reinforce_root_dir, 'mean_t'), reinforce_mean_t)\n\nnp.save(os.path.join(reinforce_root_dir, 'max_sec'), reinforce_max_sec)\nnp.save(os.path.join(reinforce_root_dir, 'min_sec'), reinforce_min_sec)\nnp.save(os.path.join(reinforce_root_dir, 'mean_sec'), reinforce_mean_sec)\n\nnp.save(os.path.join(reinforce_root_dir, 'max_rt'), reinforce_max_rt)\nnp.save(os.path.join(reinforce_root_dir, 'min_rt'), reinforce_min_rt)\nnp.save(os.path.join(reinforce_root_dir, 'mean_rt'), reinforce_mean_rt)"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-11.html#monte-carlo-vpg",
    "href": "publication/GDRL/GDRL-chapter-11.html#monte-carlo-vpg",
    "title": "Chapter 11: Policy-Gradient and Actor-Critic Methods",
    "section": "Monte-Carlo VPG",
    "text": "Monte-Carlo VPG\n\nweight, probs, entropies = -0.001, [], []\nfor p in np.arange(0, 1.01, 0.01):\n    probs.append(p)\n    p = torch.FloatTensor([p, 1-p])\n    d = torch.distributions.Categorical(probs=p)\n    entropies.append(weight * d.entropy().item())\nplt.plot(probs, entropies)\nplt.xlabel('Probability of action A\\np(B)=1-p(A)', labelpad=20)\nplt.ylabel('Negative\\nweighted\\nentropy', labelpad=80, rotation=0)\nplt.title('Entropy contribution to the loss function\\n{}*entropy(π)'.format(weight), pad=30)\nplt.show()\n\n\n\n\n\nclass FCV(nn.Module):\n    def __init__(self, \n                 input_dim,\n                 hidden_dims=(32,32), \n                 activation_fc=F.relu):\n        super(FCV, self).__init__()\n        self.activation_fc = activation_fc\n\n        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n        self.hidden_layers = nn.ModuleList()\n        for i in range(len(hidden_dims)-1):\n            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n            self.hidden_layers.append(hidden_layer)\n        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n\n    def _format(self, state):\n        x = state\n        if not isinstance(x, torch.Tensor):\n            x = torch.tensor(x,\n                             dtype=torch.float32)\n            x = x.unsqueeze(0)\n        return x\n\n    def forward(self, state):\n        x = self._format(state)\n        x = self.activation_fc(self.input_layer(x))\n        for hidden_layer in self.hidden_layers:\n            x = self.activation_fc(hidden_layer(x))\n        return self.output_layer(x)\n\n\nclass VPG():\n    def __init__(self, \n                 policy_model_fn, \n                 policy_model_max_grad_norm, \n                 policy_optimizer_fn, \n                 policy_optimizer_lr,\n                 value_model_fn, \n                 value_model_max_grad_norm, \n                 value_optimizer_fn, \n                 value_optimizer_lr, \n                 entropy_loss_weight):\n        self.policy_model_fn = policy_model_fn\n        self.policy_model_max_grad_norm = policy_model_max_grad_norm\n        self.policy_optimizer_fn = policy_optimizer_fn\n        self.policy_optimizer_lr = policy_optimizer_lr\n        \n        self.value_model_fn = value_model_fn\n        self.value_model_max_grad_norm = value_model_max_grad_norm\n        self.value_optimizer_fn = value_optimizer_fn\n        self.value_optimizer_lr = value_optimizer_lr\n        \n        self.entropy_loss_weight = entropy_loss_weight\n\n    def optimize_model(self):\n        T = len(self.rewards)\n        discounts = np.logspace(0, T, num=T, base=self.gamma, endpoint=False)\n        returns = np.array([np.sum(discounts[:T-t] * self.rewards[t:]) for t in range(T)])\n        discounts = torch.FloatTensor(discounts[:-1]).unsqueeze(1)\n        returns = torch.FloatTensor(returns[:-1]).unsqueeze(1)\n\n        self.logpas = torch.cat(self.logpas)\n        self.entropies = torch.cat(self.entropies) \n        self.values = torch.cat(self.values)\n\n        value_error = returns - self.values\n        policy_loss = -(discounts * value_error.detach() * self.logpas).mean()\n        entropy_loss = -self.entropies.mean()\n        loss = policy_loss + self.entropy_loss_weight * entropy_loss\n        self.policy_optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), \n                                       self.policy_model_max_grad_norm)\n        self.policy_optimizer.step()\n\n        value_loss = value_error.pow(2).mul(0.5).mean()\n        self.value_optimizer.zero_grad()\n        value_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.value_model.parameters(), \n                                       self.value_model_max_grad_norm)\n        self.value_optimizer.step()\n        \n    def interaction_step(self, state, env):\n        action, is_exploratory, logpa, entropy = self.policy_model.full_pass(state)\n        new_state, reward, is_terminal, info = env.step(action)\n        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n\n        self.logpas.append(logpa)\n        self.entropies.append(entropy)\n        self.rewards.append(reward)\n        self.values.append(self.value_model(state))\n\n        self.episode_reward[-1] += reward\n        self.episode_timestep[-1] += 1\n        self.episode_exploration[-1] += int(is_exploratory)\n        return new_state, is_terminal, is_truncated\n\n    def train(self, make_env_fn, make_env_kargs, seed, gamma, \n              max_minutes, max_episodes, goal_mean_100_reward):\n        training_start, last_debug_time = time.time(), float('-inf')\n\n        self.checkpoint_dir = tempfile.mkdtemp()\n        self.make_env_fn = make_env_fn\n        self.make_env_kargs = make_env_kargs\n        self.seed = seed\n        self.gamma = gamma\n        \n        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n    \n        nS, nA = env.observation_space.shape[0], env.action_space.n\n        self.episode_timestep = []\n        self.episode_reward = []\n        self.episode_seconds = []\n        self.episode_exploration = []\n        self.evaluation_scores = []\n\n        self.policy_model = self.policy_model_fn(nS, nA)\n        self.policy_optimizer = self.policy_optimizer_fn(self.policy_model, \n                                                         self.policy_optimizer_lr)\n        \n        self.value_model = self.value_model_fn(nS)\n        self.value_optimizer = self.value_optimizer_fn(self.value_model, \n                                                       self.value_optimizer_lr)\n        result = np.empty((max_episodes, 5))\n        result[:] = np.nan\n        training_time = 0\n        \n        for episode in range(1, max_episodes + 1):\n            episode_start = time.time()\n            \n            state, is_terminal = env.reset(), False\n            self.episode_reward.append(0.0)\n            self.episode_timestep.append(0.0)\n            self.episode_exploration.append(0.0)\n\n            # collect rollout\n            self.logpas, self.entropies, self.rewards, self.values = [], [], [], []\n            for step in count():\n                state, is_terminal, is_truncated = self.interaction_step(state, env)\n                if is_terminal:\n                    gc.collect()\n                    break\n\n            is_failure = is_terminal and not is_truncated\n            next_value = 0 if is_failure else self.value_model(state).detach().item()\n            self.rewards.append(next_value)\n            self.optimize_model()\n\n            # stats\n            episode_elapsed = time.time() - episode_start\n            self.episode_seconds.append(episode_elapsed)\n            training_time += episode_elapsed\n            evaluation_score, _ = self.evaluate(self.policy_model, env)\n            self.save_checkpoint(episode-1, self.policy_model)\n\n            total_step = int(np.sum(self.episode_timestep))\n            self.evaluation_scores.append(evaluation_score)\n            \n            mean_10_reward = np.mean(self.episode_reward[-10:])\n            std_10_reward = np.std(self.episode_reward[-10:])\n            mean_100_reward = np.mean(self.episode_reward[-100:])\n            std_100_reward = np.std(self.episode_reward[-100:])\n            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n            lst_100_exp_rat = np.array(\n                self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n            mean_100_exp_rat = np.mean(lst_100_exp_rat)\n            std_100_exp_rat = np.std(lst_100_exp_rat)\n\n            wallclock_elapsed = time.time() - training_start\n            result[episode-1] = total_step, mean_100_reward, \\\n                mean_100_eval_score, training_time, wallclock_elapsed\n            \n            reached_debug_time = time.time() - last_debug_time &gt;= LEAVE_PRINT_EVERY_N_SECS\n            reached_max_minutes = wallclock_elapsed &gt;= max_minutes * 60\n            reached_max_episodes = episode &gt;= max_episodes\n            reached_goal_mean_reward = mean_100_eval_score &gt;= goal_mean_100_reward\n            training_is_over = reached_max_minutes or \\\n                               reached_max_episodes or \\\n                               reached_goal_mean_reward\n\n            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n            debug_message = 'el {}, ep {:04}, ts {:06}, '\n            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n            debug_message = debug_message.format(\n                elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n                mean_100_eval_score, std_100_eval_score)\n            print(debug_message, end='\\r', flush=True)\n            if reached_debug_time or training_is_over:\n                print(ERASE_LINE + debug_message, flush=True)\n                last_debug_time = time.time()\n            if training_is_over:\n                if reached_max_minutes: print(u'--&gt; reached_max_minutes \\u2715')\n                if reached_max_episodes: print(u'--&gt; reached_max_episodes \\u2715')\n                if reached_goal_mean_reward: print(u'--&gt; reached_goal_mean_reward \\u2713')\n                break\n\n        final_eval_score, score_std = self.evaluate(self.policy_model, env, n_episodes=100)\n        wallclock_time = time.time() - training_start\n        print('Training complete.')\n        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n              ' {:.2f}s wall-clock time.\\n'.format(\n                  final_eval_score, score_std, training_time, wallclock_time))\n        env.close() ; del env\n        self.get_cleaned_checkpoints()\n        return result, final_eval_score, training_time, wallclock_time\n\n    def evaluate(self, eval_policy_model, eval_env, n_episodes=1, greedy=True):\n        rs = []\n        for _ in range(n_episodes):\n            s, d = eval_env.reset(), False\n            rs.append(0)\n            for _ in count():\n                if greedy:\n                    a = eval_policy_model.select_greedy_action(s)\n                else: \n                    a = eval_policy_model.select_action(s)\n                s, r, d, _ = eval_env.step(a)\n                rs[-1] += r\n                if d: break\n        return np.mean(rs), np.std(rs)\n\n    def get_cleaned_checkpoints(self, n_checkpoints=5):\n        try: \n            return self.checkpoint_paths\n        except AttributeError:\n            self.checkpoint_paths = {}\n\n        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n        last_ep = max(paths_dic.keys())\n        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n\n        for idx, path in paths_dic.items():\n            if idx in checkpoint_idxs:\n                self.checkpoint_paths[idx] = path\n            else:\n                os.unlink(path)\n\n        return self.checkpoint_paths\n\n    def demo_last(self, title='Fully-trained {} Agent', n_episodes=3, max_n_videos=3):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        last_ep = max(checkpoint_paths.keys())\n        self.policy_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n\n        self.evaluate(self.policy_model, env, n_episodes=n_episodes)\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def demo_progression(self, title='{} Agent progression', max_n_videos=5):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        for i in sorted(checkpoint_paths.keys()):\n            self.policy_model.load_state_dict(torch.load(checkpoint_paths[i]))\n            self.evaluate(self.policy_model, env, n_episodes=1)\n\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            subtitle_eps=sorted(checkpoint_paths.keys()),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def save_checkpoint(self, episode_idx, model):\n        torch.save(model.state_dict(), \n                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))\n\n\nvpg_results = []\nbest_agent, best_eval_score = None, float('-inf')\nfor seed in SEEDS:\n    environment_settings = {\n        'env_name': 'CartPole-v1',\n        'gamma': 1.00,\n        'max_minutes': 10,\n        'max_episodes': 10000,\n        'goal_mean_100_reward': 475\n    }\n\n    policy_model_fn = lambda nS, nA: FCDAP(nS, nA, hidden_dims=(128,64))\n    policy_model_max_grad_norm = 1\n    policy_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n    policy_optimizer_lr = 0.0005\n\n    value_model_fn = lambda nS: FCV(nS, hidden_dims=(256,128))\n    value_model_max_grad_norm = float('inf')\n    value_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr=lr)\n    value_optimizer_lr = 0.0007\n\n    entropy_loss_weight = 0.001\n\n    env_name, gamma, max_minutes, \\\n    max_episodes, goal_mean_100_reward = environment_settings.values()\n    agent = VPG(policy_model_fn, \n                policy_model_max_grad_norm, \n                policy_optimizer_fn, \n                policy_optimizer_lr,\n                value_model_fn, \n                value_model_max_grad_norm, \n                value_optimizer_fn, \n                value_optimizer_lr, \n                entropy_loss_weight)\n\n    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n    result, final_eval_score, training_time, wallclock_time = agent.train(\n        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n    vpg_results.append(result)\n    if final_eval_score &gt; best_eval_score:\n        best_eval_score = final_eval_score\n        best_agent = agent\nvpg_results = np.array(vpg_results)\n\nel 00:00:00, ep 0000, ts 000015, ar 10 015.0±000.0, 100 015.0±000.0, ex 100 0.7±0.0, ev 022.0±000.0\nel 00:00:30, ep 0272, ts 023994, ar 10 204.4±074.4, 100 152.8±083.0, ex 100 0.3±0.0, ev 384.5±108.0\nel 00:01:00, ep 0392, ts 056416, ar 10 337.2±072.2, 100 281.3±126.7, ex 100 0.3±0.0, ev 426.8±109.9\nel 00:01:30, ep 0475, ts 091685, ar 10 485.0±045.0, 100 412.3±115.2, ex 100 0.3±0.0, ev 469.8±070.5\nel 00:01:36, ep 0490, ts 099102, ar 10 491.7±024.9, 100 433.8±106.0, ex 100 0.3±0.0, ev 475.6±064.9\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 470.15±48.91 in 80.51s training time, 100.92s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000016, ar 10 016.0±000.0, 100 016.0±000.0, ex 100 0.5±0.0, ev 010.0±000.0\nel 00:00:30, ep 0271, ts 023015, ar 10 224.7±140.5, 100 157.4±102.9, ex 100 0.3±0.0, ev 348.5±127.1\nel 00:01:00, ep 0378, ts 053590, ar 10 375.6±109.5, 100 289.5±108.4, ex 100 0.3±0.0, ev 470.0±066.1\nel 00:01:00, ep 0381, ts 054412, ar 10 364.8±130.6, 100 291.2±109.8, ex 100 0.3±0.0, ev 475.7±059.7\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 491.57±29.58 in 49.83s training time, 65.84s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000012, ar 10 012.0±000.0, 100 012.0±000.0, ex 100 0.2±0.0, ev 009.0±000.0\nel 00:00:30, ep 0255, ts 021235, ar 10 178.6±065.7, 100 145.2±081.3, ex 100 0.3±0.0, ev 392.6±116.4\nel 00:01:00, ep 0367, ts 053476, ar 10 319.4±129.0, 100 298.6±132.4, ex 100 0.3±0.0, ev 456.4±076.7\nel 00:01:10, ep 0398, ts 065780, ar 10 434.8±127.8, 100 352.9±128.5, ex 100 0.3±0.0, ev 475.2±052.5\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 497.89±9.70 in 56.72s training time, 75.84s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000015, ar 10 015.0±000.0, 100 015.0±000.0, ex 100 0.3±0.0, ev 011.0±000.0\nel 00:00:30, ep 0261, ts 023823, ar 10 235.1±082.2, 100 169.4±095.7, ex 100 0.3±0.0, ev 381.2±123.0\nel 00:00:54, ep 0344, ts 049251, ar 10 359.2±111.2, 100 285.9±113.9, ex 100 0.3±0.0, ev 476.1±053.5\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 499.79±2.09 in 44.73s training time, 59.04s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000019, ar 10 019.0±000.0, 100 019.0±000.0, ex 100 0.3±0.0, ev 009.0±000.0\nel 00:00:30, ep 0252, ts 021122, ar 10 233.9±078.6, 100 151.6±082.6, ex 100 0.3±0.0, ev 393.2±115.6\nel 00:01:00, ep 0357, ts 053059, ar 10 403.8±121.6, 100 305.5±119.6, ex 100 0.3±0.0, ev 470.1±062.5\nel 00:01:03, ep 0366, ts 056699, ar 10 412.9±111.0, 100 321.4±123.6, ex 100 0.3±0.0, ev 476.0±056.4\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 50.90s training time, 68.78s wall-clock time.\n\n\n\n\nbest_agent.demo_progression()\n\nVPG Agent progression\n        Episode 0\n        \n        Episode 91\n        \n        Episode 183\n        \n        Episode 274\n        \n        Episode 366\n        \n\n\n\nbest_agent.demo_last()\n\nFully-trained VPG Agent\n        Trial 0\n        \n        Trial 1\n        \n        Trial 2\n        \n\n\n\nvpg_max_t, vpg_max_r, vpg_max_s, vpg_max_sec, vpg_max_rt = np.max(vpg_results, axis=0).T\nvpg_min_t, vpg_min_r, vpg_min_s, vpg_min_sec, vpg_min_rt = np.min(vpg_results, axis=0).T\nvpg_mean_t, vpg_mean_r, vpg_mean_s, vpg_mean_sec, vpg_mean_rt = np.mean(vpg_results, axis=0).T\nvpg_x = np.arange(np.max((len(vpg_mean_s), len(reinforce_mean_s))))\n\n\nfig, axs = plt.subplots(5, 1, figsize=(20,30), sharey=False, sharex=True)\n\n# REINFORCE\naxs[0].plot(reinforce_max_r, 'y', linewidth=1)\naxs[0].plot(reinforce_min_r, 'y', linewidth=1)\naxs[0].plot(reinforce_mean_r, 'y', label='REINFORCE', linewidth=2)\naxs[0].fill_between(reinforce_x, reinforce_min_r, reinforce_max_r, facecolor='y', alpha=0.3)\n\naxs[1].plot(reinforce_max_s, 'y', linewidth=1)\naxs[1].plot(reinforce_min_s, 'y', linewidth=1)\naxs[1].plot(reinforce_mean_s, 'y', label='REINFORCE', linewidth=2)\naxs[1].fill_between(reinforce_x, reinforce_min_s, reinforce_max_s, facecolor='y', alpha=0.3)\n\naxs[2].plot(reinforce_max_t, 'y', linewidth=1)\naxs[2].plot(reinforce_min_t, 'y', linewidth=1)\naxs[2].plot(reinforce_mean_t, 'y', label='REINFORCE', linewidth=2)\naxs[2].fill_between(reinforce_x, reinforce_min_t, reinforce_max_t, facecolor='y', alpha=0.3)\n\naxs[3].plot(reinforce_max_sec, 'y', linewidth=1)\naxs[3].plot(reinforce_min_sec, 'y', linewidth=1)\naxs[3].plot(reinforce_mean_sec, 'y', label='REINFORCE', linewidth=2)\naxs[3].fill_between(reinforce_x, reinforce_min_sec, reinforce_max_sec, facecolor='y', alpha=0.3)\n\naxs[4].plot(reinforce_max_rt, 'y', linewidth=1)\naxs[4].plot(reinforce_min_rt, 'y', linewidth=1)\naxs[4].plot(reinforce_mean_rt, 'y', label='REINFORCE', linewidth=2)\naxs[4].fill_between(reinforce_x, reinforce_min_rt, reinforce_max_rt, facecolor='y', alpha=0.3)\n\n# VPG\naxs[0].plot(vpg_max_r, 'b', linewidth=1)\naxs[0].plot(vpg_min_r, 'b', linewidth=1)\naxs[0].plot(vpg_mean_r, 'b--', label='VPG', linewidth=2)\naxs[0].fill_between(vpg_x, vpg_min_r, vpg_max_r, facecolor='b', alpha=0.3)\n\naxs[1].plot(vpg_max_s, 'b', linewidth=1)\naxs[1].plot(vpg_min_s, 'b', linewidth=1)\naxs[1].plot(vpg_mean_s, 'b--', label='VPG', linewidth=2)\naxs[1].fill_between(vpg_x, vpg_min_s, vpg_max_s, facecolor='b', alpha=0.3)\n\naxs[2].plot(vpg_max_t, 'b', linewidth=1)\naxs[2].plot(vpg_min_t, 'b', linewidth=1)\naxs[2].plot(vpg_mean_t, 'b--', label='VPG', linewidth=2)\naxs[2].fill_between(vpg_x, vpg_min_t, vpg_max_t, facecolor='b', alpha=0.3)\n\naxs[3].plot(vpg_max_sec, 'b', linewidth=1)\naxs[3].plot(vpg_min_sec, 'b', linewidth=1)\naxs[3].plot(vpg_mean_sec, 'b--', label='VPG', linewidth=2)\naxs[3].fill_between(vpg_x, vpg_min_sec, vpg_max_sec, facecolor='b', alpha=0.3)\n\naxs[4].plot(vpg_max_rt, 'b', linewidth=1)\naxs[4].plot(vpg_min_rt, 'b', linewidth=1)\naxs[4].plot(vpg_mean_rt, 'b--', label='VPG', linewidth=2)\naxs[4].fill_between(vpg_x, vpg_min_rt, vpg_max_rt, facecolor='b', alpha=0.3)\n\n# ALL\naxs[0].set_title('Moving Avg Reward (Training)')\naxs[1].set_title('Moving Avg Reward (Evaluation)')\naxs[2].set_title('Total Steps')\naxs[3].set_title('Training Time')\naxs[4].set_title('Wall-clock Time')\nplt.xlabel('Episodes')\naxs[0].legend(loc='upper left')\nplt.show()\n\n\n\n\n\nvpg_root_dir = os.path.join(RESULTS_DIR, 'vpg')\nnot os.path.exists(vpg_root_dir) and os.makedirs(vpg_root_dir)\n\nnp.save(os.path.join(vpg_root_dir, 'x'), vpg_x)\n\nnp.save(os.path.join(vpg_root_dir, 'max_r'), vpg_max_r)\nnp.save(os.path.join(vpg_root_dir, 'min_r'), vpg_min_r)\nnp.save(os.path.join(vpg_root_dir, 'mean_r'), vpg_mean_r)\n\nnp.save(os.path.join(vpg_root_dir, 'max_s'), vpg_max_s)\nnp.save(os.path.join(vpg_root_dir, 'min_s'), vpg_min_s )\nnp.save(os.path.join(vpg_root_dir, 'mean_s'), vpg_mean_s)\n\nnp.save(os.path.join(vpg_root_dir, 'max_t'), vpg_max_t)\nnp.save(os.path.join(vpg_root_dir, 'min_t'), vpg_min_t)\nnp.save(os.path.join(vpg_root_dir, 'mean_t'), vpg_mean_t)\n\nnp.save(os.path.join(vpg_root_dir, 'max_sec'), vpg_max_sec)\nnp.save(os.path.join(vpg_root_dir, 'min_sec'), vpg_min_sec)\nnp.save(os.path.join(vpg_root_dir, 'mean_sec'), vpg_mean_sec)\n\nnp.save(os.path.join(vpg_root_dir, 'max_rt'), vpg_max_rt)\nnp.save(os.path.join(vpg_root_dir, 'min_rt'), vpg_min_rt)\nnp.save(os.path.join(vpg_root_dir, 'mean_rt'), vpg_mean_rt)"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-11.html#asynchronous-advantage-actor-critic-a3c",
    "href": "publication/GDRL/GDRL-chapter-11.html#asynchronous-advantage-actor-critic-a3c",
    "title": "Chapter 11: Policy-Gradient and Actor-Critic Methods",
    "section": "Asynchronous Advantage Actor-Critic (A3C)",
    "text": "Asynchronous Advantage Actor-Critic (A3C)\n\nclass SharedAdam(torch.optim.Adam):\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=False):\n        super(SharedAdam, self).__init__(\n            params, lr=lr, betas=betas, eps=eps, \n            weight_decay=weight_decay, amsgrad=amsgrad)\n        for group in self.param_groups:\n            for p in group['params']:\n                state = self.state[p]\n                state['step'] = 0\n                state['shared_step'] = torch.zeros(1).share_memory_()\n                state['exp_avg'] = torch.zeros_like(p.data).share_memory_()\n                state['exp_avg_sq'] = torch.zeros_like(p.data).share_memory_()\n                if weight_decay:\n                    state['weight_decay'] = torch.zeros_like(p.data).share_memory_()\n                if amsgrad:\n                    state['max_exp_avg_sq'] = torch.zeros_like(p.data).share_memory_()\n\n    def step(self, closure=None):\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                self.state[p]['steps'] = self.state[p]['shared_step'].item()\n                self.state[p]['shared_step'] += 1\n        super().step(closure)\n\n\nclass SharedRMSprop(torch.optim.RMSprop):\n    def __init__(self, params, lr=1e-2, alpha=0.99, eps=1e-8, weight_decay=0, momentum=0, centered=False):\n        super(SharedRMSprop, self).__init__(\n            params, lr=lr, alpha=alpha, \n            eps=eps, weight_decay=weight_decay, \n            momentum=momentum, centered=centered)\n        for group in self.param_groups:\n            for p in group['params']:\n                state = self.state[p]\n                state['step'] = 0\n                state['shared_step'] = torch.zeros(1).share_memory_()\n                state['square_avg'] = torch.zeros_like(p.data).share_memory_()\n                if weight_decay:\n                    state['weight_decay'] = torch.zeros_like(p.data).share_memory_()\n                if momentum &gt; 0:\n                    state['momentum_buffer'] = torch.zeros_like(p.data).share_memory_()\n                if centered:\n                    state['grad_avg'] = torch.zeros_like(p.data).share_memory_()\n\n    def step(self, closure=None):\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                self.state[p]['steps'] = self.state[p]['shared_step'].item()\n                self.state[p]['shared_step'] += 1\n        super().step(closure)\n\n\nclass A3C():\n    def __init__(self, \n                 policy_model_fn, \n                 policy_model_max_grad_norm, \n                 policy_optimizer_fn, \n                 policy_optimizer_lr,\n                 value_model_fn, \n                 value_model_max_grad_norm, \n                 value_optimizer_fn, \n                 value_optimizer_lr, \n                 entropy_loss_weight, \n                 max_n_steps, \n                 n_workers):\n        self.policy_model_fn = policy_model_fn\n        self.policy_model_max_grad_norm = policy_model_max_grad_norm\n        self.policy_optimizer_fn = policy_optimizer_fn\n        self.policy_optimizer_lr = policy_optimizer_lr\n        \n        self.value_model_fn = value_model_fn\n        self.value_model_max_grad_norm = value_model_max_grad_norm\n        self.value_optimizer_fn = value_optimizer_fn\n        self.value_optimizer_lr = value_optimizer_lr\n        \n        self.entropy_loss_weight = entropy_loss_weight\n        self.max_n_steps = max_n_steps\n        self.n_workers = n_workers\n\n    def optimize_model(self, logpas, entropies, rewards, values, \n                       local_policy_model, local_value_model):\n        T = len(rewards)\n        discounts = np.logspace(0, T, num=T, base=self.gamma, endpoint=False)\n        returns = np.array([np.sum(discounts[:T-t] * rewards[t:]) for t in range(T)])\n        discounts = torch.FloatTensor(discounts[:-1]).unsqueeze(1)\n        returns = torch.FloatTensor(returns[:-1]).unsqueeze(1)\n\n        logpas = torch.cat(logpas)\n        entropies = torch.cat(entropies)\n        values = torch.cat(values)\n\n        value_error = returns - values\n        policy_loss = -(discounts * value_error.detach() * logpas).mean()\n        entropy_loss = -entropies.mean()\n        loss = policy_loss + self.entropy_loss_weight * entropy_loss\n        self.shared_policy_optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(local_policy_model.parameters(), \n                                       self.policy_model_max_grad_norm)\n        for param, shared_param in zip(local_policy_model.parameters(), \n                                       self.shared_policy_model.parameters()):\n            if shared_param.grad is None:\n                shared_param._grad = param.grad\n        self.shared_policy_optimizer.step()\n        local_policy_model.load_state_dict(self.shared_policy_model.state_dict())\n\n        value_loss = value_error.pow(2).mul(0.5).mean()\n        self.shared_value_optimizer.zero_grad()\n        value_loss.backward()\n        torch.nn.utils.clip_grad_norm_(local_value_model.parameters(), \n                                       self.value_model_max_grad_norm)\n        for param, shared_param in zip(local_value_model.parameters(), \n                                       self.shared_value_model.parameters()):\n            if shared_param.grad is None:\n                shared_param._grad = param.grad\n        self.shared_value_optimizer.step()\n        local_value_model.load_state_dict(self.shared_value_model.state_dict())\n\n    @staticmethod\n    def interaction_step(state, env, local_policy_model, local_value_model,\n                         logpas, entropies, rewards, values):\n        action, is_exploratory, logpa, entropy = local_policy_model.full_pass(state)\n        new_state, reward, is_terminal, info = env.step(action)\n        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n\n        logpas.append(logpa)\n        entropies.append(entropy)\n        rewards.append(reward)\n        values.append(local_value_model(state))\n\n        return new_state, reward, is_terminal, is_truncated, is_exploratory\n\n    def work(self, rank):\n        last_debug_time = float('-inf')\n        self.stats['n_active_workers'].add_(1)\n        \n        local_seed = self.seed + rank\n        env = self.make_env_fn(**self.make_env_kargs, seed=local_seed)\n        torch.manual_seed(local_seed) ; np.random.seed(local_seed) ; random.seed(local_seed)\n\n        nS, nA = env.observation_space.shape[0], env.action_space.n\n        local_policy_model = self.policy_model_fn(nS, nA)\n        local_policy_model.load_state_dict(self.shared_policy_model.state_dict())\n        local_value_model = self.value_model_fn(nS)\n        local_value_model.load_state_dict(self.shared_value_model.state_dict())\n\n        global_episode_idx = self.stats['episode'].add_(1).item() - 1\n        while not self.get_out_signal:            \n            episode_start = time.time()\n            state, is_terminal = env.reset(), False\n            \n            # collect n_steps rollout\n            n_steps_start, total_episode_rewards = 0, 0\n            total_episode_steps, total_episode_exploration = 0, 0\n            logpas, entropies, rewards, values = [], [], [], []\n\n            for step in count(start=1):\n                state, reward, is_terminal, is_truncated, is_exploratory = self.interaction_step(\n                    state, env, local_policy_model, local_value_model, \n                    logpas, entropies, rewards, values)\n\n                total_episode_steps += 1\n                total_episode_rewards += reward\n                total_episode_exploration += int(is_exploratory)\n                \n                if is_terminal or step - n_steps_start == self.max_n_steps:\n                    is_failure = is_terminal and not is_truncated\n                    next_value = 0 if is_failure else local_value_model(state).detach().item()\n                    rewards.append(next_value)\n\n                    self.optimize_model(logpas, entropies, rewards, values, \n                                        local_policy_model, local_value_model)\n                    logpas, entropies, rewards, values = [], [], [], []\n                    n_steps_start = step\n                \n                if is_terminal:\n                    gc.collect()\n                    break\n\n            # save global stats\n            episode_elapsed = time.time() - episode_start\n            evaluation_score, _ = self.evaluate(local_policy_model, env)\n            self.save_checkpoint(global_episode_idx, local_policy_model)\n            \n            self.stats['episode_elapsed'][global_episode_idx].add_(episode_elapsed)\n            self.stats['episode_timestep'][global_episode_idx].add_(total_episode_steps)\n            self.stats['episode_reward'][global_episode_idx].add_(total_episode_rewards)\n            self.stats['episode_exploration'][global_episode_idx].add_(total_episode_exploration/total_episode_steps)\n            self.stats['evaluation_scores'][global_episode_idx].add_(evaluation_score)\n\n            mean_10_reward = self.stats[\n                'episode_reward'][:global_episode_idx+1][-10:].mean().item()\n            mean_100_reward = self.stats[\n                'episode_reward'][:global_episode_idx+1][-100:].mean().item()\n            mean_100_eval_score = self.stats[\n                'evaluation_scores'][:global_episode_idx+1][-100:].mean().item()\n            mean_100_exp_rat = self.stats[\n                'episode_exploration'][:global_episode_idx+1][-100:].mean().item()\n            std_10_reward = self.stats[\n                'episode_reward'][:global_episode_idx+1][-10:].std().item()\n            std_100_reward = self.stats[\n                'episode_reward'][:global_episode_idx+1][-100:].std().item()\n            std_100_eval_score = self.stats[\n                'evaluation_scores'][:global_episode_idx+1][-100:].std().item()\n            std_100_exp_rat = self.stats[\n                'episode_exploration'][:global_episode_idx+1][-100:].std().item()\n            if std_10_reward != std_10_reward: std_10_reward = 0            \n            if std_100_reward != std_100_reward: std_100_reward = 0\n            if std_100_eval_score != std_100_eval_score: std_100_eval_score = 0\n            if std_100_exp_rat != std_100_exp_rat: std_100_exp_rat = 0\n            global_n_steps = self.stats[\n                'episode_timestep'][:global_episode_idx+1].sum().item()\n            global_training_elapsed = self.stats[\n                'episode_elapsed'][:global_episode_idx+1].sum().item()\n            wallclock_elapsed = time.time() - self.training_start\n            \n            self.stats['result'][global_episode_idx][0].add_(global_n_steps)\n            self.stats['result'][global_episode_idx][1].add_(mean_100_reward)\n            self.stats['result'][global_episode_idx][2].add_(mean_100_eval_score)\n            self.stats['result'][global_episode_idx][3].add_(global_training_elapsed)\n            self.stats['result'][global_episode_idx][4].add_(wallclock_elapsed)\n\n            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - self.training_start))\n            debug_message = 'el {}, ep {:04}, ts {:06}, '\n            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n            debug_message = debug_message.format(\n                elapsed_str, global_episode_idx, global_n_steps, mean_10_reward, std_10_reward, \n                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n                mean_100_eval_score, std_100_eval_score)\n            \n            if rank == 0:\n                print(debug_message, end='\\r', flush=True)\n                if time.time() - last_debug_time &gt;= LEAVE_PRINT_EVERY_N_SECS:\n                    print(ERASE_LINE + debug_message, flush=True)\n                    last_debug_time = time.time()\n\n            with self.get_out_lock:\n                potential_next_global_episode_idx = self.stats['episode'].item()\n                self.reached_goal_mean_reward.add_(\n                    mean_100_eval_score &gt;= self.goal_mean_100_reward)\n                self.reached_max_minutes.add_(\n                    time.time() - self.training_start &gt;= self.max_minutes * 60)\n                self.reached_max_episodes.add_(\n                    potential_next_global_episode_idx &gt;= self.max_episodes)\n                if self.reached_max_episodes or \\\n                   self.reached_max_minutes or \\\n                   self.reached_goal_mean_reward:\n                    self.get_out_signal.add_(1)\n                    break\n                # else go work on another episode\n                global_episode_idx = self.stats['episode'].add_(1).item() - 1\n\n        while rank == 0 and self.stats['n_active_workers'].item() &gt; 1:\n            pass\n\n        if rank == 0:\n            print(ERASE_LINE + debug_message)\n            if self.reached_max_minutes: print(u'--&gt; reached_max_minutes \\u2715')\n            if self.reached_max_episodes: print(u'--&gt; reached_max_episodes \\u2715')\n            if self.reached_goal_mean_reward: print(u'--&gt; reached_goal_mean_reward \\u2713')\n\n        env.close() ; del env\n        self.stats['n_active_workers'].sub_(1)\n\n    def train(self, make_env_fn, make_env_kargs, seed, gamma, \n              max_minutes, max_episodes, goal_mean_100_reward):\n\n        self.checkpoint_dir = tempfile.mkdtemp()\n        self.make_env_fn = make_env_fn\n        self.make_env_kargs = make_env_kargs\n        self.seed = seed\n        self.gamma = gamma\n        self.max_minutes = max_minutes\n        self.max_episodes = max_episodes\n        self.goal_mean_100_reward = goal_mean_100_reward\n\n        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n        nS, nA = env.observation_space.shape[0], env.action_space.n\n        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n\n        self.stats = {}\n        self.stats['episode'] = torch.zeros(1, dtype=torch.int).share_memory_()\n        self.stats['result'] = torch.zeros([max_episodes, 5]).share_memory_()\n        self.stats['evaluation_scores'] = torch.zeros([max_episodes]).share_memory_()\n        self.stats['episode_reward'] = torch.zeros([max_episodes]).share_memory_()\n        self.stats['episode_timestep'] = torch.zeros([max_episodes], dtype=torch.int).share_memory_()\n        self.stats['episode_exploration'] = torch.zeros([max_episodes]).share_memory_()\n        self.stats['episode_elapsed'] = torch.zeros([max_episodes]).share_memory_()\n        self.stats['n_active_workers'] = torch.zeros(1, dtype=torch.int).share_memory_()\n\n        self.shared_policy_model = self.policy_model_fn(nS, nA).share_memory()\n        self.shared_policy_optimizer = self.policy_optimizer_fn(self.shared_policy_model, \n                                                                self.policy_optimizer_lr)\n        self.shared_value_model = self.value_model_fn(nS).share_memory()\n        self.shared_value_optimizer = self.value_optimizer_fn(self.shared_value_model, \n                                                              self.value_optimizer_lr)\n        self.get_out_lock = mp.Lock()\n        self.get_out_signal = torch.zeros(1, dtype=torch.int).share_memory_()\n        self.reached_max_minutes = torch.zeros(1, dtype=torch.int).share_memory_() \n        self.reached_max_episodes = torch.zeros(1, dtype=torch.int).share_memory_() \n        self.reached_goal_mean_reward  = torch.zeros(1, dtype=torch.int).share_memory_() \n        self.training_start = time.time()\n        workers = [mp.Process(target=self.work, args=(rank,)) for rank in range(self.n_workers)]\n        [w.start() for w in workers] ; [w.join() for w in workers]\n        wallclock_time = time.time() - self.training_start\n\n        final_eval_score, score_std = self.evaluate(self.shared_policy_model, env, n_episodes=100)\n        env.close() ; del env\n\n        final_episode = self.stats['episode'].item()\n        training_time = self.stats['episode_elapsed'][:final_episode+1].sum().item()\n\n        print('Training complete.')\n        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n              ' {:.2f}s wall-clock time.\\n'.format(\n                  final_eval_score, score_std, training_time, wallclock_time))\n\n        self.stats['result'] = self.stats['result'].numpy()\n        self.stats['result'][final_episode:, ...] = np.nan\n        self.get_cleaned_checkpoints()\n        return self.stats['result'], final_eval_score, training_time, wallclock_time\n\n    def evaluate(self, eval_policy_model, eval_env, n_episodes=1, greedy=True):\n        rs = []\n        for _ in range(n_episodes):\n            s, d = eval_env.reset(), False\n            rs.append(0)\n            for _ in count():\n                if greedy:\n                    a = eval_policy_model.select_greedy_action(s)\n                else: \n                    a = eval_policy_model.select_action(s)\n                s, r, d, _ = eval_env.step(a)\n                rs[-1] += r\n                if d: break\n        return np.mean(rs), np.std(rs)\n\n    def get_cleaned_checkpoints(self, n_checkpoints=5):\n        try: \n            return self.checkpoint_paths\n        except AttributeError:\n            self.checkpoint_paths = {}\n\n        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n        last_ep = max(paths_dic.keys())\n        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n\n        for idx, path in paths_dic.items():\n            if idx in checkpoint_idxs:\n                self.checkpoint_paths[idx] = path\n            else:\n                os.unlink(path)\n\n        return self.checkpoint_paths\n\n    def demo_last(self, title='Fully-trained {} Agent', n_episodes=3, max_n_videos=3):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        last_ep = max(checkpoint_paths.keys())\n        self.shared_policy_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n\n        self.evaluate(self.shared_policy_model, env, n_episodes=n_episodes)\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def demo_progression(self, title='{} Agent progression', max_n_videos=5):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        for i in sorted(checkpoint_paths.keys()):\n            self.shared_policy_model.load_state_dict(torch.load(checkpoint_paths[i]))\n            self.evaluate(self.shared_policy_model, env, n_episodes=1)\n\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            subtitle_eps=sorted(checkpoint_paths.keys()),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def save_checkpoint(self, episode_idx, model):\n        torch.save(model.state_dict(), \n                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))\n\n\na3c_results = []\nbest_agent, best_eval_score = None, float('-inf')\nfor seed in SEEDS:\n    environment_settings = {\n        'env_name': 'CartPole-v1',\n        'gamma': 1.00,\n        'max_minutes': 10,\n        'max_episodes': 10000,\n        'goal_mean_100_reward': 475\n    }\n    \n    policy_model_fn = lambda nS, nA: FCDAP(nS, nA, hidden_dims=(128,64))\n    policy_model_max_grad_norm = 1\n    policy_optimizer_fn = lambda net, lr: SharedAdam(net.parameters(), lr=lr)\n    policy_optimizer_lr = 0.0005\n\n    value_model_fn = lambda nS: FCV(nS, hidden_dims=(256,128))\n    value_model_max_grad_norm = float('inf')\n    value_optimizer_fn = lambda net, lr: SharedRMSprop(net.parameters(), lr=lr)\n    value_optimizer_lr = 0.0007\n\n    entropy_loss_weight = 0.001\n\n    max_n_steps = 50\n    n_workers = 8\n\n    env_name, gamma, max_minutes, \\\n    max_episodes, goal_mean_100_reward = environment_settings.values()\n    agent = A3C(policy_model_fn,\n                policy_model_max_grad_norm, \n                policy_optimizer_fn, \n                policy_optimizer_lr,\n                value_model_fn,\n                value_model_max_grad_norm,\n                value_optimizer_fn, \n                value_optimizer_lr,\n                entropy_loss_weight, \n                max_n_steps,\n                n_workers)\n\n    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n    result, final_eval_score, training_time, wallclock_time = agent.train(\n        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n    a3c_results.append(result)\n    if final_eval_score &gt; best_eval_score:\n        best_eval_score = final_eval_score\n        best_agent = agent\na3c_results = np.array(a3c_results)\n\nel 00:00:00, ep 0000, ts 000015, ar 10 015.0±000.0, 100 015.0±000.0, ex 100 0.7±0.0, ev 022.0±000.0\nel 00:00:16, ep 0508, ts 103193, ar 10 477.5±071.2, 100 438.5±098.4, ex 100 0.2±0.0, ev 479.5±058.2\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 113.05s training time, 16.92s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000016, ar 10 016.0±000.0, 100 016.0±000.0, ex 100 0.5±0.0, ev 010.0±000.0\nel 00:00:17, ep 0627, ts 102319, ar 10 496.3±011.7, 100 448.3±086.1, ex 100 0.2±0.0, ev 482.0±043.2\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 118.00s training time, 18.00s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000012, ar 10 012.0±000.0, 100 012.0±000.0, ex 100 0.2±0.0, ev 009.0±000.0\nel 00:00:30, ep 0890, ts 190166, ar 10 500.0±000.0, 100 479.0±060.0, ex 100 0.2±0.0, ev 486.6±042.5\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 202.03s training time, 30.05s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000015, ar 10 015.0±000.0, 100 015.0±000.0, ex 100 0.3±0.0, ev 011.0±000.0\nel 00:00:30, ep 0825, ts 197972, ar 10 481.7±041.5, 100 447.7±089.5, ex 100 0.1±0.0, ev 459.5±075.7\nel 00:00:35, ep 0916, ts 241390, ar 10 500.0±000.0, 100 477.4±063.6, ex 100 0.1±0.0, ev 475.9±068.2\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 244.94s training time, 36.04s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000019, ar 10 019.0±000.0, 100 019.0±000.0, ex 100 0.3±0.0, ev 009.0±000.0\nel 00:00:14, ep 0527, ts 072236, ar 10 500.0±000.0, 100 342.2±134.6, ex 100 0.3±0.0, ev 478.4±056.6\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 88.76s training time, 14.38s wall-clock time.\n\n\n\n\nbest_agent.demo_progression()\n\nA3C Agent progression\n        Episode 0\n        \n        Episode 129\n        \n        Episode 258\n        \n        Episode 387\n        \n        Episode 516\n        \n\n\n\nbest_agent.demo_last()\n\nFully-trained A3C Agent\n        Trial 0\n        \n        Trial 1\n        \n        Trial 2\n        \n\n\n\na3c_max_t, a3c_max_r, a3c_max_s, a3c_max_sec, a3c_max_rt = np.max(a3c_results, axis=0).T\na3c_min_t, a3c_min_r, a3c_min_s, a3c_min_sec, a3c_min_rt = np.min(a3c_results, axis=0).T\na3c_mean_t, a3c_mean_r, a3c_mean_s, a3c_mean_sec, a3c_mean_rt = np.mean(a3c_results, axis=0).T\na3c_x = np.arange(np.max((len(a3c_mean_s), len(vpg_mean_s), len(reinforce_mean_s))))\n\n\nfig, axs = plt.subplots(5, 1, figsize=(20,30), sharey=False, sharex=True)\n\n# REINFORCE\naxs[0].plot(reinforce_max_r, 'y', linewidth=1)\naxs[0].plot(reinforce_min_r, 'y', linewidth=1)\naxs[0].plot(reinforce_mean_r, 'y', label='REINFORCE', linewidth=2)\naxs[0].fill_between(reinforce_x, reinforce_min_r, reinforce_max_r, facecolor='y', alpha=0.3)\n\naxs[1].plot(reinforce_max_s, 'y', linewidth=1)\naxs[1].plot(reinforce_min_s, 'y', linewidth=1)\naxs[1].plot(reinforce_mean_s, 'y', label='REINFORCE', linewidth=2)\naxs[1].fill_between(reinforce_x, reinforce_min_s, reinforce_max_s, facecolor='y', alpha=0.3)\n\naxs[2].plot(reinforce_max_t, 'y', linewidth=1)\naxs[2].plot(reinforce_min_t, 'y', linewidth=1)\naxs[2].plot(reinforce_mean_t, 'y', label='REINFORCE', linewidth=2)\naxs[2].fill_between(reinforce_x, reinforce_min_t, reinforce_max_t, facecolor='y', alpha=0.3)\n\naxs[3].plot(reinforce_max_sec, 'y', linewidth=1)\naxs[3].plot(reinforce_min_sec, 'y', linewidth=1)\naxs[3].plot(reinforce_mean_sec, 'y', label='REINFORCE', linewidth=2)\naxs[3].fill_between(reinforce_x, reinforce_min_sec, reinforce_max_sec, facecolor='y', alpha=0.3)\n\naxs[4].plot(reinforce_max_rt, 'y', linewidth=1)\naxs[4].plot(reinforce_min_rt, 'y', linewidth=1)\naxs[4].plot(reinforce_mean_rt, 'y', label='REINFORCE', linewidth=2)\naxs[4].fill_between(reinforce_x, reinforce_min_rt, reinforce_max_rt, facecolor='y', alpha=0.3)\n\n# VPG\naxs[0].plot(vpg_max_r, 'b', linewidth=1)\naxs[0].plot(vpg_min_r, 'b', linewidth=1)\naxs[0].plot(vpg_mean_r, 'b--', label='VPG', linewidth=2)\naxs[0].fill_between(vpg_x, vpg_min_r, vpg_max_r, facecolor='b', alpha=0.3)\n\naxs[1].plot(vpg_max_s, 'b', linewidth=1)\naxs[1].plot(vpg_min_s, 'b', linewidth=1)\naxs[1].plot(vpg_mean_s, 'b--', label='VPG', linewidth=2)\naxs[1].fill_between(vpg_x, vpg_min_s, vpg_max_s, facecolor='b', alpha=0.3)\n\naxs[2].plot(vpg_max_t, 'b', linewidth=1)\naxs[2].plot(vpg_min_t, 'b', linewidth=1)\naxs[2].plot(vpg_mean_t, 'b--', label='VPG', linewidth=2)\naxs[2].fill_between(vpg_x, vpg_min_t, vpg_max_t, facecolor='b', alpha=0.3)\n\naxs[3].plot(vpg_max_sec, 'b', linewidth=1)\naxs[3].plot(vpg_min_sec, 'b', linewidth=1)\naxs[3].plot(vpg_mean_sec, 'b--', label='VPG', linewidth=2)\naxs[3].fill_between(vpg_x, vpg_min_sec, vpg_max_sec, facecolor='b', alpha=0.3)\n\naxs[4].plot(vpg_max_rt, 'b', linewidth=1)\naxs[4].plot(vpg_min_rt, 'b', linewidth=1)\naxs[4].plot(vpg_mean_rt, 'b--', label='VPG', linewidth=2)\naxs[4].fill_between(vpg_x, vpg_min_rt, vpg_max_rt, facecolor='b', alpha=0.3)\n\n# A3C\naxs[0].plot(a3c_max_r, 'g', linewidth=1)\naxs[0].plot(a3c_min_r, 'g', linewidth=1)\naxs[0].plot(a3c_mean_r, 'g-.', label='A3C', linewidth=2)\naxs[0].fill_between(a3c_x, a3c_min_r, a3c_max_r, facecolor='g', alpha=0.3)\n\naxs[1].plot(a3c_max_s, 'g', linewidth=1)\naxs[1].plot(a3c_min_s, 'g', linewidth=1)\naxs[1].plot(a3c_mean_s, 'g-.', label='A3C', linewidth=2)\naxs[1].fill_between(a3c_x, a3c_min_s, a3c_max_s, facecolor='g', alpha=0.3)\n\naxs[2].plot(a3c_max_t, 'g', linewidth=1)\naxs[2].plot(a3c_min_t, 'g', linewidth=1)\naxs[2].plot(a3c_mean_t, 'g-.', label='A3C', linewidth=2)\naxs[2].fill_between(a3c_x, a3c_min_t, a3c_max_t, facecolor='g', alpha=0.3)\n\naxs[3].plot(a3c_max_sec, 'g', linewidth=1)\naxs[3].plot(a3c_min_sec, 'g', linewidth=1)\naxs[3].plot(a3c_mean_sec, 'g-.', label='A3C', linewidth=2)\naxs[3].fill_between(a3c_x, a3c_min_sec, a3c_max_sec, facecolor='g', alpha=0.3)\n\naxs[4].plot(a3c_max_rt, 'g', linewidth=1)\naxs[4].plot(a3c_min_rt, 'g', linewidth=1)\naxs[4].plot(a3c_mean_rt, 'g-.', label='A3C', linewidth=2)\naxs[4].fill_between(a3c_x, a3c_min_rt, a3c_max_rt, facecolor='g', alpha=0.3)\n\n# ALL\naxs[0].set_title('Moving Avg Reward (Training)')\naxs[1].set_title('Moving Avg Reward (Evaluation)')\naxs[2].set_title('Total Steps')\naxs[3].set_title('Training Time')\naxs[4].set_title('Wall-clock Time')\nplt.xlabel('Episodes')\naxs[0].legend(loc='upper left')\nplt.show()\n\n\n\n\n\na3c_root_dir = os.path.join(RESULTS_DIR, 'a3c')\nnot os.path.exists(a3c_root_dir) and os.makedirs(a3c_root_dir)\n\nnp.save(os.path.join(a3c_root_dir, 'x'), a3c_x)\n\nnp.save(os.path.join(a3c_root_dir, 'max_r'), a3c_max_r)\nnp.save(os.path.join(a3c_root_dir, 'min_r'), a3c_min_r)\nnp.save(os.path.join(a3c_root_dir, 'mean_r'), a3c_mean_r)\n\nnp.save(os.path.join(a3c_root_dir, 'max_s'), a3c_max_s)\nnp.save(os.path.join(a3c_root_dir, 'min_s'), a3c_min_s )\nnp.save(os.path.join(a3c_root_dir, 'mean_s'), a3c_mean_s)\n\nnp.save(os.path.join(a3c_root_dir, 'max_t'), a3c_max_t)\nnp.save(os.path.join(a3c_root_dir, 'min_t'), a3c_min_t)\nnp.save(os.path.join(a3c_root_dir, 'mean_t'), a3c_mean_t)\n\nnp.save(os.path.join(a3c_root_dir, 'max_sec'), a3c_max_sec)\nnp.save(os.path.join(a3c_root_dir, 'min_sec'), a3c_min_sec)\nnp.save(os.path.join(a3c_root_dir, 'mean_sec'), a3c_mean_sec)\n\nnp.save(os.path.join(a3c_root_dir, 'max_rt'), a3c_max_rt)\nnp.save(os.path.join(a3c_root_dir, 'min_rt'), a3c_min_rt)\nnp.save(os.path.join(a3c_root_dir, 'mean_rt'), a3c_mean_rt)"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-11.html#generalized-advantage-estimation-gae",
    "href": "publication/GDRL/GDRL-chapter-11.html#generalized-advantage-estimation-gae",
    "title": "Chapter 11: Policy-Gradient and Actor-Critic Methods",
    "section": "Generalized Advantage Estimation (GAE)",
    "text": "Generalized Advantage Estimation (GAE)\n\nclass GAE():\n    def __init__(self,\n                 policy_model_fn,\n                 policy_model_max_grad_norm, \n                 policy_optimizer_fn, \n                 policy_optimizer_lr,\n                 value_model_fn,\n                 value_model_max_grad_norm,\n                 value_optimizer_fn, \n                 value_optimizer_lr,\n                 entropy_loss_weight, \n                 max_n_steps,\n                 n_workers,\n                 tau):\n        self.policy_model_fn = policy_model_fn\n        self.policy_model_max_grad_norm = policy_model_max_grad_norm\n        self.policy_optimizer_fn = policy_optimizer_fn\n        self.policy_optimizer_lr = policy_optimizer_lr\n\n        self.value_model_fn = value_model_fn\n        self.value_model_max_grad_norm = value_model_max_grad_norm\n        self.value_optimizer_fn = value_optimizer_fn\n        self.value_optimizer_lr = value_optimizer_lr\n\n        self.entropy_loss_weight = entropy_loss_weight\n\n        self.max_n_steps = max_n_steps\n        self.n_workers = n_workers\n        self.tau = tau\n\n    def optimize_model(self, logpas, entropies, rewards, values, \n                       local_policy_model, local_value_model):\n        T = len(rewards)\n        discounts = np.logspace(0, T, num=T, base=self.gamma, endpoint=False)\n        returns = np.array([np.sum(discounts[:T-t] * rewards[t:]) for t in range(T)])\n\n        logpas = torch.cat(logpas)\n        entropies = torch.cat(entropies)\n        values = torch.cat(values)\n\n        np_values = values.view(-1).data.numpy()\n        tau_discounts = np.logspace(0, T-1, num=T-1, base=self.gamma*self.tau, endpoint=False)\n        advs = rewards[:-1] + self.gamma * np_values[1:] - np_values[:-1]  \n        gaes = np.array([np.sum(tau_discounts[:T-1-t] * advs[t:]) for t in range(T-1)])\n\n        values = values[:-1,...]\n        discounts = torch.FloatTensor(discounts[:-1]).unsqueeze(1)\n        returns = torch.FloatTensor(returns[:-1]).unsqueeze(1)\n        gaes = torch.FloatTensor(gaes).unsqueeze(1)\n\n        policy_loss = -(discounts * gaes.detach() * logpas).mean()\n        entropy_loss = -entropies.mean()\n        loss = policy_loss + self.entropy_loss_weight * entropy_loss\n        self.shared_policy_optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(local_policy_model.parameters(), \n                                       self.policy_model_max_grad_norm)\n        for param, shared_param in zip(local_policy_model.parameters(), \n                                       self.shared_policy_model.parameters()):\n            if shared_param.grad is None:\n                shared_param._grad = param.grad\n        self.shared_policy_optimizer.step()\n        local_policy_model.load_state_dict(self.shared_policy_model.state_dict())\n\n        value_error = returns - values\n        value_loss = value_error.pow(2).mul(0.5).mean()\n        self.shared_value_optimizer.zero_grad()\n        value_loss.backward()\n        torch.nn.utils.clip_grad_norm_(local_value_model.parameters(), \n                                       self.value_model_max_grad_norm)\n        for param, shared_param in zip(local_value_model.parameters(), \n                                       self.shared_value_model.parameters()):\n            if shared_param.grad is None:\n                shared_param._grad = param.grad\n        self.shared_value_optimizer.step()\n        local_value_model.load_state_dict(self.shared_value_model.state_dict())\n\n    @staticmethod\n    def interaction_step(state, env, local_policy_model, local_value_model,\n                         logpas, entropies, rewards, values):\n        action, is_exploratory, logpa, entropy = local_policy_model.full_pass(state)\n        new_state, reward, is_terminal, info = env.step(action)\n        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n\n        logpas.append(logpa)\n        entropies.append(entropy)\n        rewards.append(reward)\n        values.append(local_value_model(state))\n\n        return new_state, reward, is_terminal, is_truncated, is_exploratory\n\n    def work(self, rank):\n        last_debug_time = float('-inf')\n        self.stats['n_active_workers'].add_(1)\n        \n        local_seed = self.seed + rank\n        env = self.make_env_fn(**self.make_env_kargs, seed=local_seed)\n        torch.manual_seed(local_seed) ; np.random.seed(local_seed) ; random.seed(local_seed)\n\n        nS, nA = env.observation_space.shape[0], env.action_space.n\n        local_policy_model = self.policy_model_fn(nS, nA)\n        local_policy_model.load_state_dict(self.shared_policy_model.state_dict())\n        local_value_model = self.value_model_fn(nS)\n        local_value_model.load_state_dict(self.shared_value_model.state_dict())\n\n        global_episode_idx = self.stats['episode'].add_(1).item() - 1\n        while not self.get_out_signal:            \n            episode_start = time.time()\n            state, is_terminal = env.reset(), False\n            \n            # collect n_steps rollout\n            n_steps_start, total_episode_rewards = 0, 0\n            total_episode_steps, total_episode_exploration = 0, 0\n            logpas, entropies, rewards, values = [], [], [], []\n\n            for step in count(start=1):\n                state, reward, is_terminal, is_truncated, is_exploratory = self.interaction_step(\n                    state, env, local_policy_model, local_value_model, \n                    logpas, entropies, rewards, values)\n\n                total_episode_steps += 1\n                total_episode_rewards += reward\n                total_episode_exploration += int(is_exploratory)\n                \n                if is_terminal or step - n_steps_start == self.max_n_steps:\n                    is_failure = is_terminal and not is_truncated\n                    next_value = 0 if is_failure else local_value_model(state).detach().item()\n                    rewards.append(next_value)\n                    values.append(torch.FloatTensor([[next_value,],]))\n\n                    self.optimize_model(logpas, entropies, rewards, values, \n                                        local_policy_model, local_value_model)\n                    logpas, entropies, rewards, values = [], [], [], []\n                    n_steps_start = step\n                \n                if is_terminal:\n                    gc.collect()\n                    break\n\n            # save global stats\n            episode_elapsed = time.time() - episode_start\n            evaluation_score, _ = self.evaluate(local_policy_model, env)\n            self.save_checkpoint(global_episode_idx, local_policy_model)\n\n            self.stats['episode_elapsed'][global_episode_idx].add_(episode_elapsed)\n            self.stats['episode_timestep'][global_episode_idx].add_(total_episode_steps)\n            self.stats['episode_reward'][global_episode_idx].add_(total_episode_rewards)\n            self.stats['episode_exploration'][global_episode_idx].add_(total_episode_exploration/total_episode_steps)\n            self.stats['evaluation_scores'][global_episode_idx].add_(evaluation_score)\n\n            mean_10_reward = self.stats[\n                'episode_reward'][:global_episode_idx+1][-10:].mean().item()\n            mean_100_reward = self.stats[\n                'episode_reward'][:global_episode_idx+1][-100:].mean().item()\n            mean_100_eval_score = self.stats[\n                'evaluation_scores'][:global_episode_idx+1][-100:].mean().item()\n            mean_100_exp_rat = self.stats[\n                'episode_exploration'][:global_episode_idx+1][-100:].mean().item()\n            std_10_reward = self.stats[\n                'episode_reward'][:global_episode_idx+1][-10:].std().item()\n            std_100_reward = self.stats[\n                'episode_reward'][:global_episode_idx+1][-100:].std().item()\n            std_100_eval_score = self.stats[\n                'evaluation_scores'][:global_episode_idx+1][-100:].std().item()\n            std_100_exp_rat = self.stats[\n                'episode_exploration'][:global_episode_idx+1][-100:].std().item()\n            if std_10_reward != std_10_reward: std_10_reward = 0            \n            if std_100_reward != std_100_reward: std_100_reward = 0\n            if std_100_eval_score != std_100_eval_score: std_100_eval_score = 0\n            if std_100_exp_rat != std_100_exp_rat: std_100_exp_rat = 0\n            global_n_steps = self.stats[\n                'episode_timestep'][:global_episode_idx+1].sum().item()\n            global_training_elapsed = self.stats[\n                'episode_elapsed'][:global_episode_idx+1].sum().item()\n            wallclock_elapsed = time.time() - self.training_start\n            \n            self.stats['result'][global_episode_idx][0].add_(global_n_steps)\n            self.stats['result'][global_episode_idx][1].add_(mean_100_reward)\n            self.stats['result'][global_episode_idx][2].add_(mean_100_eval_score)\n            self.stats['result'][global_episode_idx][3].add_(global_training_elapsed)\n            self.stats['result'][global_episode_idx][4].add_(wallclock_elapsed)\n\n            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - self.training_start))\n            debug_message = 'el {}, ep {:04}, ts {:06}, '\n            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n            debug_message = debug_message.format(\n                elapsed_str, global_episode_idx, global_n_steps, mean_10_reward, std_10_reward, \n                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n                mean_100_eval_score, std_100_eval_score)\n\n            if rank == 0:\n                print(debug_message, end='\\r', flush=True)\n                if time.time() - last_debug_time &gt;= LEAVE_PRINT_EVERY_N_SECS:\n                    print(ERASE_LINE + debug_message, flush=True)\n                    last_debug_time = time.time()\n\n            with self.get_out_lock:\n                potential_next_global_episode_idx = self.stats['episode'].item()\n                self.reached_goal_mean_reward.add_(\n                    mean_100_eval_score &gt;= self.goal_mean_100_reward)\n                self.reached_max_minutes.add_(\n                    time.time() - self.training_start &gt;= self.max_minutes * 60)\n                self.reached_max_episodes.add_(\n                    potential_next_global_episode_idx &gt;= self.max_episodes)\n                if self.reached_max_episodes or \\\n                   self.reached_max_minutes or \\\n                   self.reached_goal_mean_reward:\n                    self.get_out_signal.add_(1)\n                    break\n                # else go work on another episode\n                global_episode_idx = self.stats['episode'].add_(1).item() - 1\n\n        while rank == 0 and self.stats['n_active_workers'].item() &gt; 1:\n            pass\n\n        if rank == 0:\n            print(ERASE_LINE + debug_message)\n            if self.reached_max_minutes: print(u'--&gt; reached_max_minutes \\u2715')\n            if self.reached_max_episodes: print(u'--&gt; reached_max_episodes \\u2715')\n            if self.reached_goal_mean_reward: print(u'--&gt; reached_goal_mean_reward \\u2713')\n\n        env.close() ; del env\n        self.stats['n_active_workers'].sub_(1)\n\n\n    def train(self, make_env_fn, make_env_kargs, seed, gamma, \n              max_minutes, max_episodes, goal_mean_100_reward):\n\n        self.checkpoint_dir = tempfile.mkdtemp()\n        self.make_env_fn = make_env_fn\n        self.make_env_kargs = make_env_kargs\n        self.seed = seed\n        self.gamma = gamma\n        self.max_minutes = max_minutes\n        self.max_episodes = max_episodes\n        self.goal_mean_100_reward = goal_mean_100_reward\n\n        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n        nS, nA = env.observation_space.shape[0], env.action_space.n\n        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n\n        self.stats = {}\n        self.stats['episode'] = torch.zeros(1, dtype=torch.int).share_memory_()\n        self.stats['result'] = torch.zeros([max_episodes, 5]).share_memory_()\n        self.stats['evaluation_scores'] = torch.zeros([max_episodes]).share_memory_()\n        self.stats['episode_reward'] = torch.zeros([max_episodes]).share_memory_()\n        self.stats['episode_timestep'] = torch.zeros([max_episodes], dtype=torch.int).share_memory_()\n        self.stats['episode_exploration'] = torch.zeros([max_episodes]).share_memory_()\n        self.stats['episode_elapsed'] = torch.zeros([max_episodes]).share_memory_()\n        self.stats['n_active_workers'] = torch.zeros(1, dtype=torch.int).share_memory_()\n\n        self.shared_policy_model = self.policy_model_fn(nS, nA).share_memory()\n        self.shared_policy_optimizer = self.policy_optimizer_fn(self.shared_policy_model, \n                                                                self.policy_optimizer_lr)\n        self.shared_value_model = self.value_model_fn(nS).share_memory()\n        self.shared_value_optimizer = self.value_optimizer_fn(self.shared_value_model, \n                                                              self.value_optimizer_lr)\n\n        self.get_out_lock = mp.Lock()\n        self.get_out_signal = torch.zeros(1, dtype=torch.int).share_memory_()\n        self.reached_max_minutes = torch.zeros(1, dtype=torch.int).share_memory_() \n        self.reached_max_episodes = torch.zeros(1, dtype=torch.int).share_memory_() \n        self.reached_goal_mean_reward  = torch.zeros(1, dtype=torch.int).share_memory_() \n        self.training_start = time.time()\n        workers = [mp.Process(target=self.work, args=(rank,)) for rank in range(self.n_workers)]\n        [w.start() for w in workers] ; [w.join() for w in workers]\n        wallclock_time = time.time() - self.training_start\n\n        final_eval_score, score_std = self.evaluate(self.shared_policy_model, env, n_episodes=100)\n        env.close() ; del env\n\n        final_episode = self.stats['episode'].item()\n        training_time = self.stats['episode_elapsed'][:final_episode+1].sum().item()\n\n        print('Training complete.')\n        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n              ' {:.2f}s wall-clock time.\\n'.format(\n                  final_eval_score, score_std, training_time, wallclock_time))\n\n        self.stats['result'] = self.stats['result'].numpy()\n        self.stats['result'][final_episode:, ...] = np.nan\n        self.get_cleaned_checkpoints()\n        return self.stats['result'], final_eval_score, training_time, wallclock_time\n\n    def evaluate(self, eval_policy_model, eval_env, n_episodes=1, greedy=True):\n        rs = []\n        for _ in range(n_episodes):\n            s, d = eval_env.reset(), False\n            rs.append(0)\n            for _ in count():\n                if greedy:\n                    a = eval_policy_model.select_greedy_action(s)\n                else: \n                    a = eval_policy_model.select_action(s)\n                s, r, d, _ = eval_env.step(a)\n                rs[-1] += r\n                if d: break\n        return np.mean(rs), np.std(rs)\n\n    def get_cleaned_checkpoints(self, n_checkpoints=5):\n        try: \n            return self.checkpoint_paths\n        except AttributeError:\n            self.checkpoint_paths = {}\n\n        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n        last_ep = max(paths_dic.keys())\n        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n\n        for idx, path in paths_dic.items():\n            if idx in checkpoint_idxs:\n                self.checkpoint_paths[idx] = path\n            else:\n                os.unlink(path)\n\n        return self.checkpoint_paths\n\n    def demo_last(self, title='Fully-trained {} Agent', n_episodes=3, max_n_videos=3):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        last_ep = max(checkpoint_paths.keys())\n        self.shared_policy_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n\n        self.evaluate(self.shared_policy_model, env, n_episodes=n_episodes)\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def demo_progression(self, title='{} Agent progression', max_n_videos=5):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        for i in sorted(checkpoint_paths.keys()):\n            self.shared_policy_model.load_state_dict(torch.load(checkpoint_paths[i]))\n            self.evaluate(self.shared_policy_model, env, n_episodes=1)\n\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            subtitle_eps=sorted(checkpoint_paths.keys()),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def save_checkpoint(self, episode_idx, model):\n        torch.save(model.state_dict(), \n                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))\n\n\ngae_results = []\nbest_agent, best_eval_score = None, float('-inf')\nfor seed in SEEDS:\n    environment_settings = {\n        'env_name': 'CartPole-v1',\n        'gamma': 0.99,\n        'max_minutes': 10,\n        'max_episodes': 10000,\n        'goal_mean_100_reward': 475\n    }\n\n    policy_model_fn = lambda nS, nA: FCDAP(nS, nA, hidden_dims=(128,64))\n    policy_model_max_grad_norm = 1\n    policy_optimizer_fn = lambda net, lr: SharedAdam(net.parameters(), lr=lr)\n    policy_optimizer_lr = 0.0005\n\n    value_model_fn = lambda nS: FCV(nS, hidden_dims=(256,128))\n    value_model_max_grad_norm = float('inf')\n    value_optimizer_fn = lambda net, lr: SharedRMSprop(net.parameters(), lr=lr)\n    value_optimizer_lr = 0.0007\n\n    entropy_loss_weight = 0.001\n\n    max_n_steps = 50\n    n_workers = 8\n    tau = 0.95\n\n    env_name, gamma, max_minutes, \\\n    max_episodes, goal_mean_100_reward = environment_settings.values()\n    agent = GAE(policy_model_fn,\n                policy_model_max_grad_norm, \n                policy_optimizer_fn, \n                policy_optimizer_lr,\n                value_model_fn,\n                value_model_max_grad_norm,\n                value_optimizer_fn, \n                value_optimizer_lr, \n                entropy_loss_weight,\n                max_n_steps,\n                n_workers,\n                tau)\n\n    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n    result, final_eval_score, training_time, wallclock_time = agent.train(\n        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n    gae_results.append(result)\n    if final_eval_score &gt; best_eval_score:\n        best_eval_score = final_eval_score\n        best_agent = agent\ngae_results = np.array(gae_results)\n\nel 00:00:00, ep 0000, ts 000015, ar 10 015.0±000.0, 100 015.0±000.0, ex 100 0.7±0.0, ev 022.0±000.0\nel 00:00:30, ep 2707, ts 043577, ar 10 012.0±005.5, 100 012.1±003.6, ex 100 0.2±0.1, ev 009.3±001.2\nel 00:01:00, ep 5386, ts 076157, ar 10 020.2±009.2, 100 020.6±011.2, ex 100 0.4±0.1, ev 022.5±019.8\nel 00:01:30, ep 8070, ts 116692, ar 10 009.3±000.9, 100 009.4±000.9, ex 100 0.0±0.0, ev 009.4±000.7\nel 00:01:49, ep 9997, ts 134864, ar 10 008.8±003.2, 100 009.4±001.3, ex 100 0.0±0.0, ev 009.3±001.2\n--&gt; reached_max_episodes ✕\nTraining complete.\nFinal evaluation score 9.32±0.73 in 834.32s training time, 109.81s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000016, ar 10 016.0±000.0, 100 016.0±000.0, ex 100 0.5±0.0, ev 010.0±000.0\nel 00:00:21, ep 0673, ts 111604, ar 10 500.0±000.0, 100 445.1±101.0, ex 100 0.2±0.0, ev 477.6±054.9\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 141.37s training time, 21.34s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000012, ar 10 012.0±000.0, 100 012.0±000.0, ex 100 0.2±0.0, ev 009.0±000.0\nel 00:00:17, ep 0646, ts 089063, ar 10 431.9±162.1, 100 421.8±126.1, ex 100 0.3±0.0, ev 480.1±062.6\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 118.79s training time, 17.82s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000015, ar 10 015.0±000.0, 100 015.0±000.0, ex 100 0.3±0.0, ev 011.0±000.0\nel 00:00:20, ep 0589, ts 110357, ar 10 500.0±000.0, 100 461.9±078.2, ex 100 0.2±0.0, ev 492.5±036.0\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 136.27s training time, 20.47s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000019, ar 10 019.0±000.0, 100 019.0±000.0, ex 100 0.3±0.0, ev 009.0±000.0\nel 00:00:16, ep 0494, ts 075672, ar 10 409.1±093.6, 100 385.4±119.7, ex 100 0.3±0.0, ev 486.7±038.5\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 447.38±64.04 in 104.10s training time, 16.17s wall-clock time.\n\n\n\n\nbest_agent.demo_progression()\n\nGAE Agent progression\n        Episode 0\n        \n        Episode 169\n        \n        Episode 339\n        \n        Episode 508\n        \n        Episode 678\n        \n\n\n\nbest_agent.demo_last()\n\nFully-trained GAE Agent\n        Trial 0\n        \n        Trial 1\n        \n        Trial 2\n        \n\n\n\ngae_max_t, gae_max_r, gae_max_s, gae_max_sec, gae_max_rt = np.max(gae_results, axis=0).T\ngae_min_t, gae_min_r, gae_min_s, gae_min_sec, gae_min_rt = np.min(gae_results, axis=0).T\ngae_mean_t, gae_mean_r, gae_mean_s, gae_mean_sec, gae_mean_rt = np.mean(gae_results, axis=0).T\ngae_x = np.arange(np.max(\n    (len(gae_mean_s), len(a3c_mean_s), len(vpg_mean_s), len(reinforce_mean_s))))\n\n\nfig, axs = plt.subplots(5, 1, figsize=(20,30), sharey=False, sharex=True)\n\n# REINFORCE\naxs[0].plot(reinforce_max_r, 'y', linewidth=1)\naxs[0].plot(reinforce_min_r, 'y', linewidth=1)\naxs[0].plot(reinforce_mean_r, 'y', label='REINFORCE', linewidth=2)\naxs[0].fill_between(reinforce_x, reinforce_min_r, reinforce_max_r, facecolor='y', alpha=0.3)\n\naxs[1].plot(reinforce_max_s, 'y', linewidth=1)\naxs[1].plot(reinforce_min_s, 'y', linewidth=1)\naxs[1].plot(reinforce_mean_s, 'y', label='REINFORCE', linewidth=2)\naxs[1].fill_between(reinforce_x, reinforce_min_s, reinforce_max_s, facecolor='y', alpha=0.3)\n\naxs[2].plot(reinforce_max_t, 'y', linewidth=1)\naxs[2].plot(reinforce_min_t, 'y', linewidth=1)\naxs[2].plot(reinforce_mean_t, 'y', label='REINFORCE', linewidth=2)\naxs[2].fill_between(reinforce_x, reinforce_min_t, reinforce_max_t, facecolor='y', alpha=0.3)\n\naxs[3].plot(reinforce_max_sec, 'y', linewidth=1)\naxs[3].plot(reinforce_min_sec, 'y', linewidth=1)\naxs[3].plot(reinforce_mean_sec, 'y', label='REINFORCE', linewidth=2)\naxs[3].fill_between(reinforce_x, reinforce_min_sec, reinforce_max_sec, facecolor='y', alpha=0.3)\n\naxs[4].plot(reinforce_max_rt, 'y', linewidth=1)\naxs[4].plot(reinforce_min_rt, 'y', linewidth=1)\naxs[4].plot(reinforce_mean_rt, 'y', label='REINFORCE', linewidth=2)\naxs[4].fill_between(reinforce_x, reinforce_min_rt, reinforce_max_rt, facecolor='y', alpha=0.3)\n\n# VPG\naxs[0].plot(vpg_max_r, 'b', linewidth=1)\naxs[0].plot(vpg_min_r, 'b', linewidth=1)\naxs[0].plot(vpg_mean_r, 'b--', label='VPG', linewidth=2)\naxs[0].fill_between(vpg_x, vpg_min_r, vpg_max_r, facecolor='b', alpha=0.3)\n\naxs[1].plot(vpg_max_s, 'b', linewidth=1)\naxs[1].plot(vpg_min_s, 'b', linewidth=1)\naxs[1].plot(vpg_mean_s, 'b--', label='VPG', linewidth=2)\naxs[1].fill_between(vpg_x, vpg_min_s, vpg_max_s, facecolor='b', alpha=0.3)\n\naxs[2].plot(vpg_max_t, 'b', linewidth=1)\naxs[2].plot(vpg_min_t, 'b', linewidth=1)\naxs[2].plot(vpg_mean_t, 'b--', label='VPG', linewidth=2)\naxs[2].fill_between(vpg_x, vpg_min_t, vpg_max_t, facecolor='b', alpha=0.3)\n\naxs[3].plot(vpg_max_sec, 'b', linewidth=1)\naxs[3].plot(vpg_min_sec, 'b', linewidth=1)\naxs[3].plot(vpg_mean_sec, 'b--', label='VPG', linewidth=2)\naxs[3].fill_between(vpg_x, vpg_min_sec, vpg_max_sec, facecolor='b', alpha=0.3)\n\naxs[4].plot(vpg_max_rt, 'b', linewidth=1)\naxs[4].plot(vpg_min_rt, 'b', linewidth=1)\naxs[4].plot(vpg_mean_rt, 'b--', label='VPG', linewidth=2)\naxs[4].fill_between(vpg_x, vpg_min_rt, vpg_max_rt, facecolor='b', alpha=0.3)\n\n# A3C\naxs[0].plot(a3c_max_r, 'g', linewidth=1)\naxs[0].plot(a3c_min_r, 'g', linewidth=1)\naxs[0].plot(a3c_mean_r, 'g-.', label='A3C', linewidth=2)\naxs[0].fill_between(a3c_x, a3c_min_r, a3c_max_r, facecolor='g', alpha=0.3)\n\naxs[1].plot(a3c_max_s, 'g', linewidth=1)\naxs[1].plot(a3c_min_s, 'g', linewidth=1)\naxs[1].plot(a3c_mean_s, 'g-.', label='A3C', linewidth=2)\naxs[1].fill_between(a3c_x, a3c_min_s, a3c_max_s, facecolor='g', alpha=0.3)\n\naxs[2].plot(a3c_max_t, 'g', linewidth=1)\naxs[2].plot(a3c_min_t, 'g', linewidth=1)\naxs[2].plot(a3c_mean_t, 'g-.', label='A3C', linewidth=2)\naxs[2].fill_between(a3c_x, a3c_min_t, a3c_max_t, facecolor='g', alpha=0.3)\n\naxs[3].plot(a3c_max_sec, 'g', linewidth=1)\naxs[3].plot(a3c_min_sec, 'g', linewidth=1)\naxs[3].plot(a3c_mean_sec, 'g-.', label='A3C', linewidth=2)\naxs[3].fill_between(a3c_x, a3c_min_sec, a3c_max_sec, facecolor='g', alpha=0.3)\n\naxs[4].plot(a3c_max_rt, 'g', linewidth=1)\naxs[4].plot(a3c_min_rt, 'g', linewidth=1)\naxs[4].plot(a3c_mean_rt, 'g-.', label='A3C', linewidth=2)\naxs[4].fill_between(a3c_x, a3c_min_rt, a3c_max_rt, facecolor='g', alpha=0.3)\n\n# GAE\naxs[0].plot(gae_max_r, 'r', linewidth=1)\naxs[0].plot(gae_min_r, 'r', linewidth=1)\naxs[0].plot(gae_mean_r, 'r:', label='GAE', linewidth=2)\naxs[0].fill_between(gae_x, gae_min_r, gae_max_r, facecolor='r', alpha=0.3)\n\naxs[1].plot(gae_max_s, 'r', linewidth=1)\naxs[1].plot(gae_min_s, 'r', linewidth=1)\naxs[1].plot(gae_mean_s, 'r:', label='GAE', linewidth=2)\naxs[1].fill_between(gae_x, gae_min_s, gae_max_s, facecolor='r', alpha=0.3)\n\naxs[2].plot(gae_max_t, 'r', linewidth=1)\naxs[2].plot(gae_min_t, 'r', linewidth=1)\naxs[2].plot(gae_mean_t, 'r:', label='GAE', linewidth=2)\naxs[2].fill_between(gae_x, gae_min_t, gae_max_t, facecolor='r', alpha=0.3)\n\naxs[3].plot(gae_max_sec, 'r', linewidth=1)\naxs[3].plot(gae_min_sec, 'r', linewidth=1)\naxs[3].plot(gae_mean_sec, 'r:', label='GAE', linewidth=2)\naxs[3].fill_between(gae_x, gae_min_sec, gae_max_sec, facecolor='r', alpha=0.3)\n\naxs[4].plot(gae_max_rt, 'r', linewidth=1)\naxs[4].plot(gae_min_rt, 'r', linewidth=1)\naxs[4].plot(gae_mean_rt, 'r:', label='GAE', linewidth=2)\naxs[4].fill_between(gae_x, gae_min_rt, gae_max_rt, facecolor='r', alpha=0.3)\n\n# ALL\naxs[0].set_title('Moving Avg Reward (Training)')\naxs[1].set_title('Moving Avg Reward (Evaluation)')\naxs[2].set_title('Total Steps')\naxs[3].set_title('Training Time')\naxs[4].set_title('Wall-clock Time')\nplt.xlabel('Episodes')\naxs[0].legend(loc='upper left')\nplt.show()\n\n\n\n\n\ngae_root_dir = os.path.join(RESULTS_DIR, 'gae')\nnot os.path.exists(gae_root_dir) and os.makedirs(gae_root_dir)\n\nnp.save(os.path.join(gae_root_dir, 'x'), gae_x)\n\nnp.save(os.path.join(gae_root_dir, 'max_r'), gae_max_r)\nnp.save(os.path.join(gae_root_dir, 'min_r'), gae_min_r)\nnp.save(os.path.join(gae_root_dir, 'mean_r'), gae_mean_r)\n\nnp.save(os.path.join(gae_root_dir, 'max_s'), gae_max_s)\nnp.save(os.path.join(gae_root_dir, 'min_s'), gae_min_s )\nnp.save(os.path.join(gae_root_dir, 'mean_s'), gae_mean_s)\n\nnp.save(os.path.join(gae_root_dir, 'max_t'), gae_max_t)\nnp.save(os.path.join(gae_root_dir, 'min_t'), gae_min_t)\nnp.save(os.path.join(gae_root_dir, 'mean_t'), gae_mean_t)\n\nnp.save(os.path.join(gae_root_dir, 'max_sec'), gae_max_sec)\nnp.save(os.path.join(gae_root_dir, 'min_sec'), gae_min_sec)\nnp.save(os.path.join(gae_root_dir, 'mean_sec'), gae_mean_sec)\n\nnp.save(os.path.join(gae_root_dir, 'max_rt'), gae_max_rt)\nnp.save(os.path.join(gae_root_dir, 'min_rt'), gae_min_rt)\nnp.save(os.path.join(gae_root_dir, 'mean_rt'), gae_mean_rt)"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-11.html#advantage-actor-critic-a2c",
    "href": "publication/GDRL/GDRL-chapter-11.html#advantage-actor-critic-a2c",
    "title": "Chapter 11: Policy-Gradient and Actor-Critic Methods",
    "section": "Advantage Actor-Critic (A2C)",
    "text": "Advantage Actor-Critic (A2C)\n\nclass FCAC(nn.Module):\n    def __init__(self, \n                 input_dim, \n                 output_dim,\n                 hidden_dims=(32,32), \n                 activation_fc=F.relu):\n        super(FCAC, self).__init__()\n        self.activation_fc = activation_fc\n\n        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n        self.hidden_layers = nn.ModuleList()\n        for i in range(len(hidden_dims)-1):\n            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n            self.hidden_layers.append(hidden_layer)\n        self.value_output_layer = nn.Linear(hidden_dims[-1], 1)\n        self.policy_output_layer = nn.Linear(hidden_dims[-1], output_dim)\n\n    def _format(self, state):\n        x = state\n        if not isinstance(x, torch.Tensor):\n            x = torch.tensor(x, dtype=torch.float32)\n            if len(x.size()) == 1:\n                x = x.unsqueeze(0)\n        return x\n\n    def forward(self, state):\n        x = self._format(state)\n        x = self.activation_fc(self.input_layer(x))\n        for hidden_layer in self.hidden_layers:\n            x = self.activation_fc(hidden_layer(x))\n        return self.policy_output_layer(x), self.value_output_layer(x)\n    \n    def full_pass(self, state):\n        logits, value = self.forward(state)\n        dist = torch.distributions.Categorical(logits=logits)\n        action = dist.sample()\n        logpa = dist.log_prob(action).unsqueeze(-1)\n        entropy = dist.entropy().unsqueeze(-1)\n        action = action.item() if len(action) == 1 else action.data.numpy()\n        is_exploratory = action != np.argmax(logits.detach().numpy(), axis=int(len(state)!=1))\n        return action, is_exploratory, logpa, entropy, value\n\n    def select_action(self, state):\n        logits, _ = self.forward(state)\n        dist = torch.distributions.Categorical(logits=logits)\n        action = dist.sample()\n        action = action.item() if len(action) == 1 else action.data.numpy()\n        return action\n    \n    def select_greedy_action(self, state):\n        logits, _ = self.forward(state)\n        return np.argmax(logits.detach().numpy())\n    \n    def evaluate_state(self, state):\n        _, value = self.forward(state)\n        return value\n\n\nclass MultiprocessEnv(object):\n    def __init__(self, make_env_fn, make_env_kargs, seed, n_workers):\n        self.make_env_fn = make_env_fn\n        self.make_env_kargs = make_env_kargs\n        self.seed = seed\n        self.n_workers = n_workers\n        self.pipes = [mp.Pipe() for rank in range(self.n_workers)]\n        self.workers = [\n            mp.Process(\n                target=self.work, \n                args=(rank, self.pipes[rank][1])) for rank in range(self.n_workers)]\n        [w.start() for w in self.workers]\n        self.dones = {rank:False for rank in range(self.n_workers)}\n\n    def reset(self, rank=None, **kwargs):\n        if rank is not None:\n            parent_end, _ = self.pipes[rank]\n            self.send_msg(('reset', {}), rank)\n            o = parent_end.recv()\n            return o\n\n        self.broadcast_msg(('reset', kwargs))\n        return np.vstack([parent_end.recv() for parent_end, _ in self.pipes])\n\n    def step(self, actions):\n        assert len(actions) == self.n_workers\n        [self.send_msg(\n            ('step', {'action':actions[rank]}), \n            rank) for rank in range(self.n_workers)]\n        results = []\n        for rank in range(self.n_workers):\n            parent_end, _ = self.pipes[rank]\n            o, r, d, i = parent_end.recv()\n            results.append((o, \n                            np.array(r, dtype=np.float), \n                            np.array(d, dtype=np.float), \n                            i))\n        return [np.vstack(block) for block in np.array(results).T]\n\n    def close(self, **kwargs):\n        self.broadcast_msg(('close', kwargs))\n        [w.join() for w in self.workers]\n\n    def _past_limit(self, **kwargs):\n        self.broadcast_msg(('_past_limit', kwargs))\n        return np.vstack([parent_end.recv() for parent_end, _ in self.pipes])\n    \n    def work(self, rank, worker_end):\n        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed+rank)\n        while True:\n            cmd, kwargs = worker_end.recv()\n            if cmd == 'reset':\n                worker_end.send(env.reset(**kwargs))\n            elif cmd == 'step':\n                worker_end.send(env.step(**kwargs))\n            elif cmd == '_past_limit':\n                worker_end.send(env._elapsed_steps &gt;= env._max_episode_steps)\n            else:\n                # including close command \n                env.close(**kwargs) ; del env ; worker_end.close()\n                break\n\n    def send_msg(self, msg, rank):\n        parent_end, _ = self.pipes[rank]\n        parent_end.send(msg)\n\n    def broadcast_msg(self, msg):    \n        [parent_end.send(msg) for parent_end, _ in self.pipes]\n\n\nclass A2C():\n    def __init__(self, \n                 ac_model_fn, \n                 ac_model_max_grad_norm, \n                 ac_optimizer_fn, \n                 ac_optimizer_lr,\n                 policy_loss_weight, \n                 value_loss_weight,\n                 entropy_loss_weight,\n                 max_n_steps,\n                 n_workers,\n                 tau):\n        assert n_workers &gt; 1\n        self.ac_model_fn = ac_model_fn\n        self.ac_model_max_grad_norm = ac_model_max_grad_norm\n        self.ac_optimizer_fn = ac_optimizer_fn\n        self.ac_optimizer_lr = ac_optimizer_lr\n\n        self.policy_loss_weight = policy_loss_weight\n        self.value_loss_weight = value_loss_weight\n        self.entropy_loss_weight = entropy_loss_weight\n\n        self.max_n_steps = max_n_steps\n        self.n_workers = n_workers\n        self.tau = tau\n\n    def optimize_model(self):\n        logpas = torch.stack(self.logpas).squeeze()\n        entropies = torch.stack(self.entropies).squeeze()\n        values = torch.stack(self.values).squeeze()\n\n        T = len(self.rewards)\n        discounts = np.logspace(0, T, num=T, base=self.gamma, endpoint=False)\n        rewards = np.array(self.rewards).squeeze()\n        returns = np.array([[np.sum(discounts[:T-t] * rewards[t:, w]) for t in range(T)] \n                             for w in range(self.n_workers)])\n\n        np_values = values.data.numpy()\n        tau_discounts = np.logspace(0, T-1, num=T-1, base=self.gamma*self.tau, endpoint=False)\n        advs = rewards[:-1] + self.gamma * np_values[1:] - np_values[:-1]\n        gaes = np.array([[np.sum(tau_discounts[:T-1-t] * advs[t:, w]) for t in range(T-1)] \n                             for w in range(self.n_workers)])\n        discounted_gaes = discounts[:-1] * gaes\n        \n        values = values[:-1,...].view(-1).unsqueeze(1)\n        logpas = logpas.view(-1).unsqueeze(1)\n        entropies = entropies.view(-1).unsqueeze(1)\n        returns = torch.FloatTensor(returns.T[:-1]).view(-1).unsqueeze(1)\n        # refer to this issue: https://github.com/cezannec/capsule_net_pytorch/issues/4, I changed from view to reshape\n        discounted_gaes = torch.FloatTensor(discounted_gaes.T).reshape(-1).unsqueeze(1)\n        \n        T -= 1\n        T *= self.n_workers\n        assert returns.size() == (T, 1)\n        assert values.size() == (T, 1)\n        assert logpas.size() == (T, 1)\n        assert entropies.size() == (T, 1)\n\n        value_error = returns.detach() - values\n        value_loss = value_error.pow(2).mul(0.5).mean()\n        policy_loss = -(discounted_gaes.detach() * logpas).mean()\n        entropy_loss = -entropies.mean()\n        loss = self.policy_loss_weight * policy_loss + \\\n                self.value_loss_weight * value_loss + \\\n                self.entropy_loss_weight * entropy_loss\n\n        self.ac_optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.ac_model.parameters(), \n                                       self.ac_model_max_grad_norm)\n        self.ac_optimizer.step()\n\n    def interaction_step(self, states, envs):\n        actions, is_exploratory, logpas, entropies, values = self.ac_model.full_pass(states)\n        new_states, rewards, is_terminals, _ = envs.step(actions)\n\n        self.logpas.append(logpas) ; self.entropies.append(entropies)\n        self.rewards.append(rewards) ; self.values.append(values)\n        \n        self.running_reward += rewards\n        self.running_timestep += 1\n        self.running_exploration += is_exploratory[:,np.newaxis].astype(np.int)\n\n        return new_states, is_terminals\n\n    def train(self, make_envs_fn, make_env_fn, make_env_kargs, seed, gamma, \n              max_minutes, max_episodes, goal_mean_100_reward):\n        training_start, last_debug_time = time.time(), float('-inf')\n\n        self.checkpoint_dir = tempfile.mkdtemp()\n        self.make_envs_fn = make_envs_fn\n        self.make_env_fn = make_env_fn\n        self.make_env_kargs = make_env_kargs\n        self.seed = seed\n        self.gamma = gamma\n        \n        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n        envs = self.make_envs_fn(make_env_fn, make_env_kargs, self.seed, self.n_workers)\n        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n    \n        nS, nA = env.observation_space.shape[0], env.action_space.n\n        self.running_timestep = np.array([[0.],] * self.n_workers)\n        self.running_reward = np.array([[0.],] * self.n_workers)\n        self.running_exploration = np.array([[0.],] * self.n_workers)\n        self.running_seconds = np.array([[time.time()],] * self.n_workers)\n        self.episode_timestep, self.episode_reward = [], []\n        self.episode_seconds, self.evaluation_scores = [], []\n        self.episode_exploration = []\n\n        self.ac_model = self.ac_model_fn(nS, nA)\n        self.ac_optimizer = self.ac_optimizer_fn(self.ac_model, \n                                                 self.ac_optimizer_lr)\n        \n        result = np.empty((max_episodes, 5))\n        result[:] = np.nan\n        training_time = 0\n        states = envs.reset()\n        \n        # collect n_steps rollout\n        episode, n_steps_start = 0, 0\n        self.logpas, self.entropies, self.rewards, self.values = [], [], [], []\n        for step in count(start=1):\n            states, is_terminals = self.interaction_step(states, envs)\n            \n            if is_terminals.sum() or step - n_steps_start == self.max_n_steps:\n                past_limits_enforced = envs._past_limit()\n                is_failure = np.logical_and(is_terminals, np.logical_not(past_limits_enforced))\n                next_values = self.ac_model.evaluate_state(\n                    states).detach().numpy() * (1 - is_failure)\n                self.rewards.append(next_values) ; self.values.append(torch.Tensor(next_values))\n                self.optimize_model()\n                self.logpas, self.entropies, self.rewards, self.values = [], [], [], []\n                n_steps_start = step\n                \n            # stats\n            if is_terminals.sum():\n                episode_done = time.time()\n                evaluation_score, _ = self.evaluate(self.ac_model, env)\n                self.save_checkpoint(episode, self.ac_model)\n            \n                for i in range(self.n_workers):\n                    if is_terminals[i]:\n                        states[i] = envs.reset(rank=i)\n                        self.episode_timestep.append(self.running_timestep[i][0])\n                        self.episode_reward.append(self.running_reward[i][0])\n                        self.episode_exploration.append(self.running_exploration[i][0]/self.running_timestep[i][0])\n                        self.episode_seconds.append(episode_done - self.running_seconds[i][0])\n                        training_time += self.episode_seconds[-1]\n                        self.evaluation_scores.append(evaluation_score)\n                        episode += 1\n\n                        mean_10_reward = np.mean(self.episode_reward[-10:])\n                        std_10_reward = np.std(self.episode_reward[-10:])\n                        mean_100_reward = np.mean(self.episode_reward[-100:])\n                        std_100_reward = np.std(self.episode_reward[-100:])\n                        mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n                        std_100_eval_score = np.std(self.evaluation_scores[-100:])\n                        mean_100_exp_rat = np.mean(self.episode_exploration[-100:])\n                        std_100_exp_rat = np.std(self.episode_exploration[-100:])\n                        \n                        total_step = int(np.sum(self.episode_timestep))\n                        wallclock_elapsed = time.time() - training_start\n                        result[episode-1] = total_step, mean_100_reward, \\\n                            mean_100_eval_score, training_time, wallclock_elapsed\n\n                # debug stuff\n                reached_debug_time = time.time() - last_debug_time &gt;= LEAVE_PRINT_EVERY_N_SECS\n                reached_max_minutes = wallclock_elapsed &gt;= max_minutes * 60            \n                reached_max_episodes = episode + self.n_workers &gt;= max_episodes\n                reached_goal_mean_reward = mean_100_eval_score &gt;= goal_mean_100_reward\n                training_is_over = reached_max_minutes or \\\n                                   reached_max_episodes or \\\n                                   reached_goal_mean_reward\n\n                elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n                debug_message = 'el {}, ep {:04}, ts {:06}, '\n                debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n                debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n                debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n                debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n                debug_message = debug_message.format(\n                    elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n                    mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n                    mean_100_eval_score, std_100_eval_score)\n                print(debug_message, end='\\r', flush=True)\n                if reached_debug_time or training_is_over:\n                    print(ERASE_LINE + debug_message, flush=True)\n                    last_debug_time = time.time()\n                if training_is_over:\n                    if reached_max_minutes: print(u'--&gt; reached_max_minutes \\u2715')\n                    if reached_max_episodes: print(u'--&gt; reached_max_episodes \\u2715')\n                    if reached_goal_mean_reward: print(u'--&gt; reached_goal_mean_reward \\u2713')\n                    break\n\n                # reset running variables for next time around\n                self.running_timestep *= 1 - is_terminals\n                self.running_reward *= 1 - is_terminals\n                self.running_exploration *= 1 - is_terminals\n                self.running_seconds[is_terminals.astype(np.bool)] = time.time()\n\n        final_eval_score, score_std = self.evaluate(self.ac_model, env, n_episodes=100)\n        wallclock_time = time.time() - training_start\n        print('Training complete.')\n        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n              ' {:.2f}s wall-clock time.\\n'.format(\n                  final_eval_score, score_std, training_time, wallclock_time))\n        env.close() ; del env\n        envs.close() ; del envs\n        self.get_cleaned_checkpoints()\n        return result, final_eval_score, training_time, wallclock_time\n\n    def evaluate(self, eval_policy_model, eval_env, n_episodes=1, greedy=True):\n        rs = []\n        for _ in range(n_episodes):\n            s, d = eval_env.reset(), False\n            rs.append(0)\n            for _ in count():\n                if greedy:\n                    a = eval_policy_model.select_greedy_action(s)\n                else: \n                    a = eval_policy_model.select_action(s)\n                s, r, d, _ = eval_env.step(a)\n                rs[-1] += r\n                if d: break\n        return np.mean(rs), np.std(rs)\n\n    def get_cleaned_checkpoints(self, n_checkpoints=5):\n        try: \n            return self.checkpoint_paths\n        except AttributeError:\n            self.checkpoint_paths = {}\n\n        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n        last_ep = max(paths_dic.keys())\n        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n\n        for idx, path in paths_dic.items():\n            if idx in checkpoint_idxs:\n                self.checkpoint_paths[idx] = path\n            else:\n                os.unlink(path)\n\n        return self.checkpoint_paths\n\n    def demo_last(self, title='Fully-trained {} Agent', n_episodes=3, max_n_videos=3):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        last_ep = max(checkpoint_paths.keys())\n        self.ac_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n\n        self.evaluate(self.ac_model, env, n_episodes=n_episodes)\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def demo_progression(self, title='{} Agent progression', max_n_videos=5):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        for i in sorted(checkpoint_paths.keys()):\n            self.ac_model.load_state_dict(torch.load(checkpoint_paths[i]))\n            self.evaluate(self.ac_model, env, n_episodes=1)\n\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            subtitle_eps=sorted(checkpoint_paths.keys()),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def save_checkpoint(self, episode_idx, model):\n        torch.save(model.state_dict(), \n                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))\n\n\na2c_results = []\nbest_agent, best_eval_score = None, float('-inf')\nfor seed in SEEDS:\n    environment_settings = {\n        'env_name': 'CartPole-v1',\n        'gamma': 0.99,\n        'max_minutes': 10,\n        'max_episodes': 10000,\n        'goal_mean_100_reward': 475\n    }\n    \n    ac_model_fn = lambda nS, nA: FCAC(nS, nA, hidden_dims=(256,128))\n    ac_model_max_grad_norm = 1\n    # ac_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n    ac_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr=lr)\n    ac_optimizer_lr = 0.001\n\n    policy_loss_weight = 1.0\n    value_loss_weight = 0.6\n\n    entropy_loss_weight = 0.001\n\n    max_n_steps = 10\n    n_workers = 8\n    tau = 0.95\n\n    env_name, gamma, max_minutes, \\\n    max_episodes, goal_mean_100_reward = environment_settings.values()\n    agent = A2C(ac_model_fn, \n                ac_model_max_grad_norm,\n                ac_optimizer_fn,\n                ac_optimizer_lr,\n                policy_loss_weight,\n                value_loss_weight,\n                entropy_loss_weight,\n                max_n_steps,\n                n_workers,\n                tau)\n\n    make_envs_fn = lambda mef, mea, s, n: MultiprocessEnv(mef, mea, s, n) \n    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n    result, final_eval_score, training_time, wallclock_time = agent.train(make_envs_fn,\n                                                                          make_env_fn,\n                                                                          make_env_kargs,\n                                                                          seed,\n                                                                          gamma,\n                                                                          max_minutes,\n                                                                          max_episodes,\n                                                                          goal_mean_100_reward)\n    a2c_results.append(result)\n    if final_eval_score &gt; best_eval_score:\n        best_eval_score = final_eval_score\n        best_agent = agent\na2c_results = np.array(a2c_results)\n\nel 00:00:00, ep 0000, ts 000014, ar 10 014.0±000.0, 100 014.0±000.0, ex 100 0.5±0.0, ev 010.0±000.0\nel 00:00:30, ep 0635, ts 086300, ar 10 340.0±150.1, 100 268.4±170.6, ex 100 0.2±0.0, ev 393.2±155.1\nel 00:01:00, ep 0957, ts 201778, ar 10 230.7±178.3, 100 442.2±132.3, ex 100 0.2±0.0, ev 391.9±157.5\nel 00:01:20, ep 1108, ts 270963, ar 10 500.0±000.0, 100 488.8±068.8, ex 100 0.2±0.0, ev 476.1±069.7\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 599.12s training time, 86.05s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000014, ar 10 014.0±000.0, 100 014.0±000.0, ex 100 0.4±0.0, ev 011.0±000.0\nel 00:00:30, ep 0700, ts 089210, ar 10 462.8±074.7, 100 313.2±154.9, ex 100 0.2±0.0, ev 367.1±120.6\nel 00:00:37, ep 0751, ts 114449, ar 10 500.0±000.0, 100 430.7±129.6, ex 100 0.2±0.0, ev 475.8±056.6\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 274.62s training time, 43.15s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000009, ar 10 009.0±000.0, 100 009.0±000.0, ex 100 0.9±0.0, ev 010.0±000.0\nel 00:00:30, ep 0645, ts 092189, ar 10 480.8±039.9, 100 358.5±169.7, ex 100 0.2±0.0, ev 463.5±087.1\nel 00:00:32, ep 0662, ts 100689, ar 10 500.0±000.0, 100 384.3±167.2, ex 100 0.2±0.0, ev 475.9±071.8\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 240.26s training time, 38.35s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000010, ar 10 010.0±000.0, 100 010.0±000.0, ex 100 0.1±0.0, ev 009.0±000.0\nel 00:00:30, ep 0674, ts 093052, ar 10 488.7±033.9, 100 293.0±158.8, ex 100 0.2±0.0, ev 380.0±116.2\nel 00:00:39, ep 0741, ts 125628, ar 10 500.0±000.0, 100 450.3±109.4, ex 100 0.2±0.0, ev 475.6±062.6\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 295.73s training time, 45.91s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000012, ar 10 012.0±000.0, 100 012.0±000.0, ex 100 0.7±0.0, ev 026.0±000.0\nel 00:00:30, ep 0614, ts 090673, ar 10 481.3±038.0, 100 365.9±173.1, ex 100 0.2±0.0, ev 454.3±081.1\nel 00:00:35, ep 0649, ts 108173, ar 10 500.0±000.0, 100 452.3±121.3, ex 100 0.2±0.0, ev 475.9±064.4\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 259.40s training time, 41.02s wall-clock time.\n\n\n\n\nbest_agent.demo_progression()\n\nA2C Agent progression\n        Episode 0\n        \n        Episode 277\n        \n        Episode 554\n        \n        Episode 831\n        \n        Episode 1108\n        \n\n\n\nbest_agent.demo_last()\n\nFully-trained A2C Agent\n        Trial 0\n        \n        Trial 1\n        \n        Trial 2\n        \n\n\n\na2c_max_t, a2c_max_r, a2c_max_s, a2c_max_sec, a2c_max_rt = np.max(a2c_results, axis=0).T\na2c_min_t, a2c_min_r, a2c_min_s, a2c_min_sec, a2c_min_rt = np.min(a2c_results, axis=0).T\na2c_mean_t, a2c_mean_r, a2c_mean_s, a2c_mean_sec, a2c_mean_rt = np.mean(a2c_results, axis=0).T\na2c_x = np.arange(np.max(\n    (len(a2c_mean_s), len(gae_mean_s), len(a3c_mean_s), len(vpg_mean_s), len(reinforce_mean_s))))\n\n\nfig, axs = plt.subplots(5, 1, figsize=(20,30), sharey=False, sharex=True)\n\n# REINFORCE\naxs[0].plot(reinforce_max_r, 'y', linewidth=1)\naxs[0].plot(reinforce_min_r, 'y', linewidth=1)\naxs[0].plot(reinforce_mean_r, 'y', label='REINFORCE', linewidth=2)\naxs[0].fill_between(reinforce_x, reinforce_min_r, reinforce_max_r, facecolor='y', alpha=0.3)\n\naxs[1].plot(reinforce_max_s, 'y', linewidth=1)\naxs[1].plot(reinforce_min_s, 'y', linewidth=1)\naxs[1].plot(reinforce_mean_s, 'y', label='REINFORCE', linewidth=2)\naxs[1].fill_between(reinforce_x, reinforce_min_s, reinforce_max_s, facecolor='y', alpha=0.3)\n\naxs[2].plot(reinforce_max_t, 'y', linewidth=1)\naxs[2].plot(reinforce_min_t, 'y', linewidth=1)\naxs[2].plot(reinforce_mean_t, 'y', label='REINFORCE', linewidth=2)\naxs[2].fill_between(reinforce_x, reinforce_min_t, reinforce_max_t, facecolor='y', alpha=0.3)\n\naxs[3].plot(reinforce_max_sec, 'y', linewidth=1)\naxs[3].plot(reinforce_min_sec, 'y', linewidth=1)\naxs[3].plot(reinforce_mean_sec, 'y', label='REINFORCE', linewidth=2)\naxs[3].fill_between(reinforce_x, reinforce_min_sec, reinforce_max_sec, facecolor='y', alpha=0.3)\n\naxs[4].plot(reinforce_max_rt, 'y', linewidth=1)\naxs[4].plot(reinforce_min_rt, 'y', linewidth=1)\naxs[4].plot(reinforce_mean_rt, 'y', label='REINFORCE', linewidth=2)\naxs[4].fill_between(reinforce_x, reinforce_min_rt, reinforce_max_rt, facecolor='y', alpha=0.3)\n\n# VPG\naxs[0].plot(vpg_max_r, 'b', linewidth=1)\naxs[0].plot(vpg_min_r, 'b', linewidth=1)\naxs[0].plot(vpg_mean_r, 'b--', label='VPG', linewidth=2)\naxs[0].fill_between(vpg_x, vpg_min_r, vpg_max_r, facecolor='b', alpha=0.3)\n\naxs[1].plot(vpg_max_s, 'b', linewidth=1)\naxs[1].plot(vpg_min_s, 'b', linewidth=1)\naxs[1].plot(vpg_mean_s, 'b--', label='VPG', linewidth=2)\naxs[1].fill_between(vpg_x, vpg_min_s, vpg_max_s, facecolor='b', alpha=0.3)\n\naxs[2].plot(vpg_max_t, 'b', linewidth=1)\naxs[2].plot(vpg_min_t, 'b', linewidth=1)\naxs[2].plot(vpg_mean_t, 'b--', label='VPG', linewidth=2)\naxs[2].fill_between(vpg_x, vpg_min_t, vpg_max_t, facecolor='b', alpha=0.3)\n\naxs[3].plot(vpg_max_sec, 'b', linewidth=1)\naxs[3].plot(vpg_min_sec, 'b', linewidth=1)\naxs[3].plot(vpg_mean_sec, 'b--', label='VPG', linewidth=2)\naxs[3].fill_between(vpg_x, vpg_min_sec, vpg_max_sec, facecolor='b', alpha=0.3)\n\naxs[4].plot(vpg_max_rt, 'b', linewidth=1)\naxs[4].plot(vpg_min_rt, 'b', linewidth=1)\naxs[4].plot(vpg_mean_rt, 'b--', label='VPG', linewidth=2)\naxs[4].fill_between(vpg_x, vpg_min_rt, vpg_max_rt, facecolor='b', alpha=0.3)\n\n# A3C\naxs[0].plot(a3c_max_r, 'g', linewidth=1)\naxs[0].plot(a3c_min_r, 'g', linewidth=1)\naxs[0].plot(a3c_mean_r, 'g-.', label='A3C', linewidth=2)\naxs[0].fill_between(a3c_x, a3c_min_r, a3c_max_r, facecolor='g', alpha=0.3)\n\naxs[1].plot(a3c_max_s, 'g', linewidth=1)\naxs[1].plot(a3c_min_s, 'g', linewidth=1)\naxs[1].plot(a3c_mean_s, 'g-.', label='A3C', linewidth=2)\naxs[1].fill_between(a3c_x, a3c_min_s, a3c_max_s, facecolor='g', alpha=0.3)\n\naxs[2].plot(a3c_max_t, 'g', linewidth=1)\naxs[2].plot(a3c_min_t, 'g', linewidth=1)\naxs[2].plot(a3c_mean_t, 'g-.', label='A3C', linewidth=2)\naxs[2].fill_between(a3c_x, a3c_min_t, a3c_max_t, facecolor='g', alpha=0.3)\n\naxs[3].plot(a3c_max_sec, 'g', linewidth=1)\naxs[3].plot(a3c_min_sec, 'g', linewidth=1)\naxs[3].plot(a3c_mean_sec, 'g-.', label='A3C', linewidth=2)\naxs[3].fill_between(a3c_x, a3c_min_sec, a3c_max_sec, facecolor='g', alpha=0.3)\n\naxs[4].plot(a3c_max_rt, 'g', linewidth=1)\naxs[4].plot(a3c_min_rt, 'g', linewidth=1)\naxs[4].plot(a3c_mean_rt, 'g-.', label='A3C', linewidth=2)\naxs[4].fill_between(a3c_x, a3c_min_rt, a3c_max_rt, facecolor='g', alpha=0.3)\n\n# GAE\naxs[0].plot(gae_max_r, 'r', linewidth=1)\naxs[0].plot(gae_min_r, 'r', linewidth=1)\naxs[0].plot(gae_mean_r, 'r:', label='GAE', linewidth=2)\naxs[0].fill_between(gae_x, gae_min_r, gae_max_r, facecolor='r', alpha=0.3)\n\naxs[1].plot(gae_max_s, 'r', linewidth=1)\naxs[1].plot(gae_min_s, 'r', linewidth=1)\naxs[1].plot(gae_mean_s, 'r:', label='GAE', linewidth=2)\naxs[1].fill_between(gae_x, gae_min_s, gae_max_s, facecolor='r', alpha=0.3)\n\naxs[2].plot(gae_max_t, 'r', linewidth=1)\naxs[2].plot(gae_min_t, 'r', linewidth=1)\naxs[2].plot(gae_mean_t, 'r:', label='GAE', linewidth=2)\naxs[2].fill_between(gae_x, gae_min_t, gae_max_t, facecolor='r', alpha=0.3)\n\naxs[3].plot(gae_max_sec, 'r', linewidth=1)\naxs[3].plot(gae_min_sec, 'r', linewidth=1)\naxs[3].plot(gae_mean_sec, 'r:', label='GAE', linewidth=2)\naxs[3].fill_between(gae_x, gae_min_sec, gae_max_sec, facecolor='r', alpha=0.3)\n\naxs[4].plot(gae_max_rt, 'r', linewidth=1)\naxs[4].plot(gae_min_rt, 'r', linewidth=1)\naxs[4].plot(gae_mean_rt, 'r:', label='GAE', linewidth=2)\naxs[4].fill_between(gae_x, gae_min_rt, gae_max_rt, facecolor='r', alpha=0.3)\n\n# A2C\naxs[0].plot(a2c_max_r, 'k', linewidth=1)\naxs[0].plot(a2c_min_r, 'k', linewidth=1)\naxs[0].plot(a2c_mean_r, 'k', label='A2C', linewidth=2)\naxs[0].fill_between(a2c_x, a2c_min_r, a2c_max_r, facecolor='k', alpha=0.3)\n\naxs[1].plot(a2c_max_s, 'k', linewidth=1)\naxs[1].plot(a2c_min_s, 'k', linewidth=1)\naxs[1].plot(a2c_mean_s, 'k', label='A2C', linewidth=2)\naxs[1].fill_between(a2c_x, a2c_min_s, a2c_max_s, facecolor='k', alpha=0.3)\n\naxs[2].plot(a2c_max_t, 'k', linewidth=1)\naxs[2].plot(a2c_min_t, 'k', linewidth=1)\naxs[2].plot(a2c_mean_t, 'k', label='A2C', linewidth=2)\naxs[2].fill_between(a2c_x, a2c_min_t, a2c_max_t, facecolor='k', alpha=0.3)\n\naxs[3].plot(a2c_max_sec, 'k', linewidth=1)\naxs[3].plot(a2c_min_sec, 'k', linewidth=1)\naxs[3].plot(a2c_mean_sec, 'k', label='A2C', linewidth=2)\naxs[3].fill_between(a2c_x, a2c_min_sec, a2c_max_sec, facecolor='k', alpha=0.3)\n\naxs[4].plot(a2c_max_rt, 'k', linewidth=1)\naxs[4].plot(a2c_min_rt, 'k', linewidth=1)\naxs[4].plot(a2c_mean_rt, 'k', label='A2C', linewidth=2)\naxs[4].fill_between(a2c_x, a2c_min_rt, a2c_max_rt, facecolor='k', alpha=0.3)\n\n# ALL\naxs[0].set_title('Moving Avg Reward (Training)')\naxs[1].set_title('Moving Avg Reward (Evaluation)')\naxs[2].set_title('Total Steps')\naxs[3].set_title('Training Time')\naxs[4].set_title('Wall-clock Time')\nplt.xlabel('Episodes')\naxs[0].legend(loc='upper left')\nplt.show()\n\n\n\n\n\na2c_root_dir = os.path.join(RESULTS_DIR, 'a2c')\nnot os.path.exists(a2c_root_dir) and os.makedirs(a2c_root_dir)\n\nnp.save(os.path.join(a2c_root_dir, 'x'), a2c_x)\n\nnp.save(os.path.join(a2c_root_dir, 'max_r'), a2c_max_r)\nnp.save(os.path.join(a2c_root_dir, 'min_r'), a2c_min_r)\nnp.save(os.path.join(a2c_root_dir, 'mean_r'), a2c_mean_r)\n\nnp.save(os.path.join(a2c_root_dir, 'max_s'), a2c_max_s)\nnp.save(os.path.join(a2c_root_dir, 'min_s'), a2c_min_s )\nnp.save(os.path.join(a2c_root_dir, 'mean_s'), a2c_mean_s)\n\nnp.save(os.path.join(a2c_root_dir, 'max_t'), a2c_max_t)\nnp.save(os.path.join(a2c_root_dir, 'min_t'), a2c_min_t)\nnp.save(os.path.join(a2c_root_dir, 'mean_t'), a2c_mean_t)\n\nnp.save(os.path.join(a2c_root_dir, 'max_sec'), a2c_max_sec)\nnp.save(os.path.join(a2c_root_dir, 'min_sec'), a2c_min_sec)\nnp.save(os.path.join(a2c_root_dir, 'mean_sec'), a2c_mean_sec)\n\nnp.save(os.path.join(a2c_root_dir, 'max_rt'), a2c_max_rt)\nnp.save(os.path.join(a2c_root_dir, 'min_rt'), a2c_min_rt)\nnp.save(os.path.join(a2c_root_dir, 'mean_rt'), a2c_mean_rt)"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-2.html#tldr",
    "href": "publication/GDRL/GDRL-chapter-2.html#tldr",
    "title": "Chapter 2: Mathematical foundations of reinforcement learning",
    "section": "TL;DR",
    "text": "TL;DR\n\n그로킹 심층 강화학습 중 2장 내용인 “강화학습의 수학적 기초”에 대한 내용입니다.\n\n\n\nCode\n!pip install tqdm numpy scikit-learn pyglet setuptools && \\\n!pip install gym asciinema pandas tabulate tornado==5.* PyBullet && \\\n!pip install git+https://github.com/pybox2d/pybox2d#egg=Box2D && \\\n!pip install git+https://github.com/mimoralea/gym-bandits#egg=gym-bandits && \\\n!pip install git+https://github.com/mimoralea/gym-walk#egg=gym-walk && \\\n!pip install git+https://github.com/mimoralea/gym-aima#egg=gym-aima && \\\n!pip install gym[atari]\n\n\n\nimport gym, gym_walk, gym_aima"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-2.html#mdps",
    "href": "publication/GDRL/GDRL-chapter-2.html#mdps",
    "title": "Chapter 2: Mathematical foundations of reinforcement learning",
    "section": "MDPs",
    "text": "MDPs\n\nBandit Walk\n\n결정적인 환경 (100%의 확률로 원하는 행동을 수행)\n1개의 비종료 상태와, 2개의 종료 상태로 되어 있음\n보상은 “walk”의 가장 오른쪽 상태에 갔을때만 받음\n에피소드로 되어 있는 환경이며, 에이전트는 가장 왼쪽이나 오른쪽 셀에 갔을 때 종료됨\n(한번 어떠한 행동을 취한 후에 종료됨)\n에이전트는 상태 1에서 시작함. (walk의 중간입니다) T-1-T\n행동: 왼쪽 (0) 또는 오른쪽 (1)\n\n\nP = {\n    0: {\n        0: [(1.0, 0, 0.0, True)],\n        1: [(1.0, 0, 0.0, True)]\n    },\n    1: {\n        0: [(1.0, 0, 0.0, True)],\n        1: [(1.0, 2, 1.0, True)]\n    },\n    2: {\n        0: [(1.0, 2, 0.0, True)],\n        1: [(1.0, 2, 0.0, True)]\n    }\n}\n\nP\n\n{0: {0: [(1.0, 0, 0.0, True)], 1: [(1.0, 0, 0.0, True)]},\n 1: {0: [(1.0, 0, 0.0, True)], 1: [(1.0, 2, 1.0, True)]},\n 2: {0: [(1.0, 2, 0.0, True)], 1: [(1.0, 2, 0.0, True)]}}\n\n\n\nNote: OpenAI Gym에 구현되어 있는 BanditWalk env를 사용할 수 있음\n\n\nP = gym.make('BanditWalk-v0').env.P\nP\n\n{0: {0: [(1.0, 0, 0.0, True), (0.0, 0, 0.0, True), (0.0, 0, 0.0, True)],\n  1: [(1.0, 0, 0.0, True), (0.0, 0, 0.0, True), (0.0, 0, 0.0, True)]},\n 1: {0: [(1.0, 0, 0.0, True), (0.0, 1, 0.0, False), (0.0, 2, 1.0, True)],\n  1: [(1.0, 2, 1.0, True), (0.0, 1, 0.0, False), (0.0, 0, 0.0, True)]},\n 2: {0: [(1.0, 2, 0.0, True), (0.0, 2, 0.0, True), (0.0, 2, 0.0, True)],\n  1: [(1.0, 2, 0.0, True), (0.0, 2, 0.0, True), (0.0, 2, 0.0, True)]}}\n\n\n\n\nBandit Slippery Walk (BSW)\n\n확률적인 환경 (80%의 확률로 행동이 성공하고, 20%의 확률로 되돌아감)\n1개의 비종료 상태와, 2개의 종료 상태로 되어 있음\n보상은 “walk”의 가장 오른쪽 상태에 갔을때만 받음\n에피소드로 되어 있는 환경이며, 에이전트는 가장 왼쪽이나 오른쪽 셀에 갔을 때 종료됨\n(한번 어떠한 행동을 취한 후에 종료됨)\n에이전트는 상태 1에서 시작함 (walk의 중간입니다) T-1-T\n행동: 왼쪽 (0) 또는 오른쪽 (1)\n\n\nP = {\n    0: {\n        0: [(1.0, 0, 0.0, True)],\n        1: [(1.0, 0, 0.0, True)]\n    },\n    1: {\n        0: [(0.8, 0, 0.0, True), (0.2, 2, 1.0, True)],\n        1: [(0.8, 2, 1.0, True), (0.2, 0, 0.0, True)]\n    },\n    2: {\n        0: [(1.0, 2, 0.0, True)],\n        1: [(1.0, 2, 0.0, True)]\n    }\n}\n\nP\n\n{0: {0: [(1.0, 0, 0.0, True)], 1: [(1.0, 0, 0.0, True)]},\n 1: {0: [(0.8, 0, 0.0, True), (0.2, 2, 1.0, True)],\n  1: [(0.8, 2, 1.0, True), (0.2, 0, 0.0, True)]},\n 2: {0: [(1.0, 2, 0.0, True)], 1: [(1.0, 2, 0.0, True)]}}\n\n\n\nNote: OpenAI Gym에 구현되어 있는 BanditSlipperyWalk env를 사용할 수 있음\n\n\nP = gym.make('BanditSlipperyWalk-v0').env.P\nP\n\n{0: {0: [(0.8, 0, 0.0, True), (0.0, 0, 0.0, True), (0.2, 0, 0.0, True)],\n  1: [(0.8, 0, 0.0, True), (0.0, 0, 0.0, True), (0.2, 0, 0.0, True)]},\n 1: {0: [(0.8, 0, 0.0, True), (0.0, 1, 0.0, False), (0.2, 2, 1.0, True)],\n  1: [(0.8, 2, 1.0, True), (0.0, 1, 0.0, False), (0.2, 0, 0.0, True)]},\n 2: {0: [(0.8, 2, 0.0, True), (0.0, 2, 0.0, True), (0.2, 2, 0.0, True)],\n  1: [(0.8, 2, 0.0, True), (0.0, 2, 0.0, True), (0.2, 2, 0.0, True)]}}\n\n\n\n\nWalk three\n\n결정적인 환경\n3개의 비종료 상태와 2개의 종료 상태를 가짐\n보상은 “walk”의 가장 오른쪽 상태에 갔을때만 받음\n에피소드로 되어 있는 환경이며, 에이전트는 가장 왼쪽이나 오른쪽 셀에 갔을 때 종료됨\n에이전트는 상태 2에서 시작함. (walk의 중간) T-1-2-3-T\n행동: 왼쪽 (0) 또는 오른쪽 (1)\n\n\nP = {\n    0: {\n        0: [(1.0, 0, 0.0, True)],\n        1: [(1.0, 0, 0.0, True)]\n    },\n    1: {\n        0: [(1.0, 0, 0.0, True)],\n        1: [(1.0, 2, 0.0, False)]\n    },\n    2: {\n        0: [(1.0, 1, 0.0, False)],\n        1: [(1.0, 3, 0.0, False)]\n    },\n    3: {\n        0: [(1.0, 2, 0.0, False)],\n        1: [(1.0, 4, 1.0, True)]\n    },\n    4: {\n        0: [(1.0, 4, 0.0, True)],\n        1: [(1.0, 4, 0.0, True)]\n    }\n}\n\nP\n\n{0: {0: [(1.0, 0, 0.0, True)], 1: [(1.0, 0, 0.0, True)]},\n 1: {0: [(1.0, 0, 0.0, True)], 1: [(1.0, 2, 0.0, False)]},\n 2: {0: [(1.0, 1, 0.0, False)], 1: [(1.0, 3, 0.0, False)]},\n 3: {0: [(1.0, 2, 0.0, False)], 1: [(1.0, 4, 1.0, True)]},\n 4: {0: [(1.0, 4, 0.0, True)], 1: [(1.0, 4, 0.0, True)]}}\n\n\n\nNote: OpenAI Gym에 구현되어 있는 WalkThree env를 사용할 수 있습니다.\n\n\nP = gym.make('WalkThree-v0').env.P\nP\n\n{0: {0: [(1.0, 0, 0.0, True), (0.0, 0, 0.0, True), (0.0, 0, 0.0, True)],\n  1: [(1.0, 0, 0.0, True), (0.0, 0, 0.0, True), (0.0, 0, 0.0, True)]},\n 1: {0: [(1.0, 0, 0.0, True), (0.0, 1, 0.0, False), (0.0, 2, 0.0, False)],\n  1: [(1.0, 2, 0.0, False), (0.0, 1, 0.0, False), (0.0, 0, 0.0, True)]},\n 2: {0: [(1.0, 1, 0.0, False), (0.0, 2, 0.0, False), (0.0, 3, 0.0, False)],\n  1: [(1.0, 3, 0.0, False), (0.0, 2, 0.0, False), (0.0, 1, 0.0, False)]},\n 3: {0: [(1.0, 2, 0.0, False), (0.0, 3, 0.0, False), (0.0, 4, 1.0, True)],\n  1: [(1.0, 4, 1.0, True), (0.0, 3, 0.0, False), (0.0, 2, 0.0, False)]},\n 4: {0: [(1.0, 4, 0.0, True), (0.0, 4, 0.0, True), (0.0, 4, 0.0, True)],\n  1: [(1.0, 4, 0.0, True), (0.0, 4, 0.0, True), (0.0, 4, 0.0, True)]}}\n\n\n\n\nSlippery Walk Three\n\n확률적인 환경 (50%의 확률로 행동이 성공하고, 33.33%의 확률로 머물러있고\n16.66%의 확률로 되돌아감)\n3개의 비종료 상태와 2개의 종료 상태를 가짐\n보상은 “walk”의 가장 오른쪽 상태에 갔을때만 받음\n에피소드로 되어 있는 환경이며, 에이전트는 가장 왼쪽이나 오른쪽 셀에 갔을 때 종료됨\n에이전트는 상태 2에서 시작함 (walk의 중간) T-1-2-3-T\n행동: 왼쪽 (0) 또는 오른쪽 (1)\n\n\nP = {\n    0: {\n        0: [(1.0, 0, 0.0, True)],\n        1: [(1.0, 0, 0.0, True)]\n    },\n    1: {\n        0: [(0.5000000000000001, 0, 0.0, True),\n            (0.3333333333333333, 1, 0.0, False),\n            (0.16666666666666666, 2, 0.0, False)\n        ],\n        1: [(0.5000000000000001, 2, 0.0, False),\n            (0.3333333333333333, 1, 0.0, False),\n            (0.16666666666666666, 0, 0.0, True)\n        ]\n    },\n    2: {\n        0: [(0.5000000000000001, 1, 0.0, False),\n            (0.3333333333333333, 2, 0.0, False),\n            (0.16666666666666666, 3, 0.0, False)\n        ],\n        1: [(0.5000000000000001, 3, 0.0, False),\n            (0.3333333333333333, 2, 0.0, False),\n            (0.16666666666666666, 1, 0.0, False)\n        ]\n    },\n    3: {\n        0: [(0.5000000000000001, 2, 0.0, False),\n            (0.3333333333333333, 3, 0.0, False),\n            (0.16666666666666666, 4, 1.0, True)\n        ],\n        1: [(0.5000000000000001, 4, 1.0, True),\n            (0.3333333333333333, 3, 0.0, False),\n            (0.16666666666666666, 2, 0.0, False)\n        ]\n    },\n    4: {\n        0: [(1.0, 4, 0.0, True)],\n        1: [(1.0, 4, 0.0, True)]\n    }\n}\n\nP\n\n{0: {0: [(1.0, 0, 0.0, True)], 1: [(1.0, 0, 0.0, True)]},\n 1: {0: [(0.5000000000000001, 0, 0.0, True),\n   (0.3333333333333333, 1, 0.0, False),\n   (0.16666666666666666, 2, 0.0, False)],\n  1: [(0.5000000000000001, 2, 0.0, False),\n   (0.3333333333333333, 1, 0.0, False),\n   (0.16666666666666666, 0, 0.0, True)]},\n 2: {0: [(0.5000000000000001, 1, 0.0, False),\n   (0.3333333333333333, 2, 0.0, False),\n   (0.16666666666666666, 3, 0.0, False)],\n  1: [(0.5000000000000001, 3, 0.0, False),\n   (0.3333333333333333, 2, 0.0, False),\n   (0.16666666666666666, 1, 0.0, False)]},\n 3: {0: [(0.5000000000000001, 2, 0.0, False),\n   (0.3333333333333333, 3, 0.0, False),\n   (0.16666666666666666, 4, 1.0, True)],\n  1: [(0.5000000000000001, 4, 1.0, True),\n   (0.3333333333333333, 3, 0.0, False),\n   (0.16666666666666666, 2, 0.0, False)]},\n 4: {0: [(1.0, 4, 0.0, True)], 1: [(1.0, 4, 0.0, True)]}}\n\n\n\nNote: OpenAI Gym에 구현되어 있는 SlipperyWalkThree env를 사용할 수 있습니다.\n\n\nP = gym.make('SlipperyWalkThree-v0').env.P\nP\n\n{0: {0: [(0.5000000000000001, 0, 0.0, True),\n   (0.3333333333333333, 0, 0.0, True),\n   (0.16666666666666666, 0, 0.0, True)],\n  1: [(0.5000000000000001, 0, 0.0, True),\n   (0.3333333333333333, 0, 0.0, True),\n   (0.16666666666666666, 0, 0.0, True)]},\n 1: {0: [(0.5000000000000001, 0, 0.0, True),\n   (0.3333333333333333, 1, 0.0, False),\n   (0.16666666666666666, 2, 0.0, False)],\n  1: [(0.5000000000000001, 2, 0.0, False),\n   (0.3333333333333333, 1, 0.0, False),\n   (0.16666666666666666, 0, 0.0, True)]},\n 2: {0: [(0.5000000000000001, 1, 0.0, False),\n   (0.3333333333333333, 2, 0.0, False),\n   (0.16666666666666666, 3, 0.0, False)],\n  1: [(0.5000000000000001, 3, 0.0, False),\n   (0.3333333333333333, 2, 0.0, False),\n   (0.16666666666666666, 1, 0.0, False)]},\n 3: {0: [(0.5000000000000001, 2, 0.0, False),\n   (0.3333333333333333, 3, 0.0, False),\n   (0.16666666666666666, 4, 1.0, True)],\n  1: [(0.5000000000000001, 4, 1.0, True),\n   (0.3333333333333333, 3, 0.0, False),\n   (0.16666666666666666, 2, 0.0, False)]},\n 4: {0: [(0.5000000000000001, 4, 0.0, True),\n   (0.3333333333333333, 4, 0.0, True),\n   (0.16666666666666666, 4, 0.0, True)],\n  1: [(0.5000000000000001, 4, 0.0, True),\n   (0.3333333333333333, 4, 0.0, True),\n   (0.16666666666666666, 4, 0.0, True)]}}\n\n\n\n\nRandom Walk\n\n매우 확률적인 환경 (50%의 확률로 행동이 성공하고, 50%의 확률로 되돌아감)\n5개의 비종료 상태와 2개의 종료 상태로 되어 있음\n보상은 “walk”의 가장 오른쪽 상태에 갔을때만 받음\n에피소드로 되어 있는 환경이며, 에이전트는 가장 왼쪽이나 오른쪽 셀에 갔을 때 종료됨\n에이전트는 상태 3에서 시작함 (walk의 중간) T-1-2-3-4-5-T\n행동: 왼쪽 (0) 또는 오른쪽 (1)이나, 무작위로 동작하기 때문에 별 차이가 없음\n\n\nP = {\n    0: {\n        0: [(1.0, 0, 0.0, True)],\n        1: [(1.0, 0, 0.0, True)]\n    },\n    1: {\n        0: [(0.5, 0, 0.0, True), (0.5, 2, 0.0, False)],\n        1: [(0.5, 2, 0.0, False), (0.5, 0, 0.0, True)]\n    },\n    2: {\n        0: [(0.5, 1, 0.0, False), (0.5, 3, 0.0, False)],\n        1: [(0.5, 3, 0.0, False), (0.5, 1, 0.0, False)]\n    },\n    3: {\n        0: [(0.5, 2, 0.0, False), (0.5, 4, 0.0, False)],\n        1: [(0.5, 4, 0.0, False), (0.5, 2, 0.0, False)]\n    },\n    4: {\n        0: [(0.5, 3, 0.0, False), (0.5, 5, 0.0, False)],\n        1: [(0.5, 5, 0.0, False), (0.5, 3, 0.0, False)]\n    },\n    5: {\n        0: [(0.5, 4, 0.0, False), (0.5, 6, 1.0, True)],\n        1: [(0.5, 6, 1.0, True), (0.5, 4, 0.0, False)]\n    },\n    6: {\n        0: [(1.0, 6, 0.0, True)],\n        1: [(1.0, 6, 0.0, True)]\n    }\n}\n\nP\n\n{0: {0: [(1.0, 0, 0.0, True)], 1: [(1.0, 0, 0.0, True)]},\n 1: {0: [(0.5, 0, 0.0, True), (0.5, 2, 0.0, False)],\n  1: [(0.5, 2, 0.0, False), (0.5, 0, 0.0, True)]},\n 2: {0: [(0.5, 1, 0.0, False), (0.5, 3, 0.0, False)],\n  1: [(0.5, 3, 0.0, False), (0.5, 1, 0.0, False)]},\n 3: {0: [(0.5, 2, 0.0, False), (0.5, 4, 0.0, False)],\n  1: [(0.5, 4, 0.0, False), (0.5, 2, 0.0, False)]},\n 4: {0: [(0.5, 3, 0.0, False), (0.5, 5, 0.0, False)],\n  1: [(0.5, 5, 0.0, False), (0.5, 3, 0.0, False)]},\n 5: {0: [(0.5, 4, 0.0, False), (0.5, 6, 1.0, True)],\n  1: [(0.5, 6, 1.0, True), (0.5, 4, 0.0, False)]},\n 6: {0: [(1.0, 6, 0.0, True)], 1: [(1.0, 6, 0.0, True)]}}\n\n\n\nNote: OpenAI Gym에 구현되어 있는 RandomWalk env를 사용할 수 있습니다.\n\n\nP = gym.make('RandomWalk-v0').env.P\nP\n\n{0: {0: [(0.5, 0, 0.0, True), (0.0, 0, 0.0, True), (0.5, 0, 0.0, True)],\n  1: [(0.5, 0, 0.0, True), (0.0, 0, 0.0, True), (0.5, 0, 0.0, True)]},\n 1: {0: [(0.5, 0, 0.0, True), (0.0, 1, 0.0, False), (0.5, 2, 0.0, False)],\n  1: [(0.5, 2, 0.0, False), (0.0, 1, 0.0, False), (0.5, 0, 0.0, True)]},\n 2: {0: [(0.5, 1, 0.0, False), (0.0, 2, 0.0, False), (0.5, 3, 0.0, False)],\n  1: [(0.5, 3, 0.0, False), (0.0, 2, 0.0, False), (0.5, 1, 0.0, False)]},\n 3: {0: [(0.5, 2, 0.0, False), (0.0, 3, 0.0, False), (0.5, 4, 0.0, False)],\n  1: [(0.5, 4, 0.0, False), (0.0, 3, 0.0, False), (0.5, 2, 0.0, False)]},\n 4: {0: [(0.5, 3, 0.0, False), (0.0, 4, 0.0, False), (0.5, 5, 0.0, False)],\n  1: [(0.5, 5, 0.0, False), (0.0, 4, 0.0, False), (0.5, 3, 0.0, False)]},\n 5: {0: [(0.5, 4, 0.0, False), (0.0, 5, 0.0, False), (0.5, 6, 1.0, True)],\n  1: [(0.5, 6, 1.0, True), (0.0, 5, 0.0, False), (0.5, 4, 0.0, False)]},\n 6: {0: [(0.5, 6, 0.0, True), (0.0, 6, 0.0, True), (0.5, 6, 0.0, True)],\n  1: [(0.5, 6, 0.0, True), (0.0, 6, 0.0, True), (0.5, 6, 0.0, True)]}}\n\n\n\n\nGridWorld (Russll and Norvig의 Artificial Intelligence - Modern Approach책 참고)\n\n확률적인 환경 (80%의 확률로 행동이 성공하고, 20% 확률씩 양 옆으로 이동함)\n3x4 격자로 되어 있고, 12개의 상태(0-11)를 가짐\n시간당 -0.04 패널티를 가짐\n상태 3 (가장 오른쪽 구석)에 도달하면 +1 보상을 받음\n상태 7 (상태 3 아래)에 도달하면 -1 보상을 받음\n상태 5는 벽이고, 에이전트가 해당 셀로 들어갈 경우 다시 튕겨져 나옴\n에피소드로 된 환경이며, 에이전트가 상태 3이나 7에 도달할 때 종료됨\n에이전트는 상태 0 (가장 왼쪽 구석)에서 시작함\n행동: 왼쪽(0), 아래 (1), 오른쪽 (2), 위 (3)\n\n\nP = {\n    0: {\n        0: [(0.9, 0, -0.04, False),\n            (0.1, 4, -0.04, False)\n        ],\n        1: [(0.1, 0, -0.04, False), (0.8, 4, -0.04, False), (0.1, 1, -0.04, False)],\n        2: [(0.1, 4, -0.04, False), (0.8, 1, -0.04, False), (0.1, 0, -0.04, False)],\n        3: [(0.1, 1, -0.04, False), (0.8, 0, -0.04, False), (0.1, 0, -0.04, False)]\n    },\n    1: {\n        0: [(0.2, 1, -0.04, False),\n            (0.8, 0, -0.04, False)\n        ],\n        1: [(0.1, 0, -0.04, False), (0.8, 1, -0.04, False), (0.1, 2, -0.04, False)],\n        2: [(0.1, 1, -0.04, False), (0.8, 2, -0.04, False), (0.1, 1, -0.04, False)],\n        3: [(0.1, 2, -0.04, False), (0.8, 1, -0.04, False), (0.1, 0, -0.04, False)]\n    },\n    2: {\n        0: [(0.1, 2, -0.04, False),\n            (0.8, 1, -0.04, False),\n            (0.1, 6, -0.04, False)\n        ],\n        1: [(0.1, 1, -0.04, False), (0.8, 6, -0.04, False), (0.1, 3, 0.96, True)],\n        2: [(0.1, 6, -0.04, False), (0.8, 3, 0.96, True), (0.1, 2, -0.04, False)],\n        3: [(0.1, 3, 0.96, True), (0.8, 2, -0.04, False), (0.1, 1, -0.04, False)]\n    },\n    3: {\n        0: [(1.0, 3, 0, True)],\n        1: [(1.0, 3, 0, True)],\n        2: [(1.0, 3, 0, True)],\n        3: [(1.0, 3, 0, True)]\n    },\n    4: {\n        0: [(0.1, 0, -0.04, False),\n            (0.8, 4, -0.04, False),\n            (0.1, 8, -0.04, False)\n        ],\n        1: [(0.2, 4, -0.04, False), (0.8, 8, -0.04, False)],\n        2: [(0.1, 8, -0.04, False), (0.8, 4, -0.04, False), (0.1, 0, -0.04, False)],\n        3: [(0.2, 4, -0.04, False), (0.8, 0, -0.04, False)]\n    },\n    5: {\n        0: [(1.0, 5, 0, True)],\n        1: [(1.0, 5, 0, True)],\n        2: [(1.0, 5, 0, True)],\n        3: [(1.0, 5, 0, True)]\n    },\n    6: {\n        0: [(0.1, 2, -0.04, False),\n            (0.8, 6, -0.04, False),\n            (0.1, 10, -0.04, False)\n        ],\n        1: [(0.1, 6, -0.04, False), (0.8, 10, -0.04, False), (0.1, 7, -1.04, True)],\n        2: [(0.1, 10, -0.04, False), (0.8, 7, -1.04, True), (0.1, 2, -0.04, False)],\n        3: [(0.1, 7, -1.04, True), (0.8, 2, -0.04, False), (0.1, 6, -0.04, False)]\n    },\n    7: {\n        0: [(1.0, 7, 0, True)],\n        1: [(1.0, 7, 0, True)],\n        2: [(1.0, 7, 0, True)],\n        3: [(1.0, 7, 0, True)]\n    },\n    8: {\n        0: [(0.1, 4, -0.04, False),\n            (0.9, 8, -0.04, False)\n        ],\n        1: [(0.9, 8, -0.04, False), (0.1, 9, -0.04, False)],\n        2: [(0.1, 8, -0.04, False), (0.8, 9, -0.04, False), (0.1, 4, -0.04, False)],\n        3: [(0.1, 9, -0.04, False), (0.8, 4, -0.04, False), (0.1, 8, -0.04, False)]\n    },\n    9: {\n        0: [(0.2, 9, -0.04, False),\n            (0.8, 8, -0.04, False)\n        ],\n        1: [(0.1, 8, -0.04, False), (0.8, 9, -0.04, False), (0.1, 10, -0.04, False)],\n        2: [(0.2, 9, -0.04, False), (0.8, 10, -0.04, False)],\n        3: [(0.1, 10, -0.04, False),\n            (0.8, 9, -0.04, False),\n            (0.1, 8, -0.04, False)\n        ]\n    },\n    10: {\n        0: [(0.1, 6, -0.04, False),\n            (0.8, 9, -0.04, False),\n            (0.1, 10, -0.04, False)\n        ],\n        1: [(0.1, 9, -0.04, False),\n            (0.8, 10, -0.04, False),\n            (0.1, 11, -0.04, False)\n        ],\n        2: [(0.1, 10, -0.04, False),\n            (0.8, 11, -0.04, False),\n            (0.1, 6, -0.04, False)\n        ],\n        3: [(0.1, 11, -0.04, False),\n            (0.8, 6, -0.04, False),\n            (0.1, 9, -0.04, False)\n        ]\n    },\n    11: {\n        0: [(0.1, 7, -1.04, True),\n            (0.8, 10, -0.04, False),\n            (0.1, 11, -0.04, False)\n        ],\n        1: [(0.1, 10, -0.04, False),\n            (0.9, 11, -0.04, False)\n        ],\n        2: [(0.9, 11, -0.04, False), (0.1, 7, -1.04, True)],\n        3: [(0.1, 11, -0.04, False),\n            (0.8, 7, -1.04, True),\n            (0.1, 10, -0.04, False)\n        ]\n    }\n}\n\nP\n\n{0: {0: [(0.9, 0, -0.04, False), (0.1, 4, -0.04, False)],\n  1: [(0.1, 0, -0.04, False), (0.8, 4, -0.04, False), (0.1, 1, -0.04, False)],\n  2: [(0.1, 4, -0.04, False), (0.8, 1, -0.04, False), (0.1, 0, -0.04, False)],\n  3: [(0.1, 1, -0.04, False), (0.8, 0, -0.04, False), (0.1, 0, -0.04, False)]},\n 1: {0: [(0.2, 1, -0.04, False), (0.8, 0, -0.04, False)],\n  1: [(0.1, 0, -0.04, False), (0.8, 1, -0.04, False), (0.1, 2, -0.04, False)],\n  2: [(0.1, 1, -0.04, False), (0.8, 2, -0.04, False), (0.1, 1, -0.04, False)],\n  3: [(0.1, 2, -0.04, False), (0.8, 1, -0.04, False), (0.1, 0, -0.04, False)]},\n 2: {0: [(0.1, 2, -0.04, False),\n   (0.8, 1, -0.04, False),\n   (0.1, 6, -0.04, False)],\n  1: [(0.1, 1, -0.04, False), (0.8, 6, -0.04, False), (0.1, 3, 0.96, True)],\n  2: [(0.1, 6, -0.04, False), (0.8, 3, 0.96, True), (0.1, 2, -0.04, False)],\n  3: [(0.1, 3, 0.96, True), (0.8, 2, -0.04, False), (0.1, 1, -0.04, False)]},\n 3: {0: [(1.0, 3, 0, True)],\n  1: [(1.0, 3, 0, True)],\n  2: [(1.0, 3, 0, True)],\n  3: [(1.0, 3, 0, True)]},\n 4: {0: [(0.1, 0, -0.04, False),\n   (0.8, 4, -0.04, False),\n   (0.1, 8, -0.04, False)],\n  1: [(0.2, 4, -0.04, False), (0.8, 8, -0.04, False)],\n  2: [(0.1, 8, -0.04, False), (0.8, 4, -0.04, False), (0.1, 0, -0.04, False)],\n  3: [(0.2, 4, -0.04, False), (0.8, 0, -0.04, False)]},\n 5: {0: [(1.0, 5, 0, True)],\n  1: [(1.0, 5, 0, True)],\n  2: [(1.0, 5, 0, True)],\n  3: [(1.0, 5, 0, True)]},\n 6: {0: [(0.1, 2, -0.04, False),\n   (0.8, 6, -0.04, False),\n   (0.1, 10, -0.04, False)],\n  1: [(0.1, 6, -0.04, False), (0.8, 10, -0.04, False), (0.1, 7, -1.04, True)],\n  2: [(0.1, 10, -0.04, False), (0.8, 7, -1.04, True), (0.1, 2, -0.04, False)],\n  3: [(0.1, 7, -1.04, True), (0.8, 2, -0.04, False), (0.1, 6, -0.04, False)]},\n 7: {0: [(1.0, 7, 0, True)],\n  1: [(1.0, 7, 0, True)],\n  2: [(1.0, 7, 0, True)],\n  3: [(1.0, 7, 0, True)]},\n 8: {0: [(0.1, 4, -0.04, False), (0.9, 8, -0.04, False)],\n  1: [(0.9, 8, -0.04, False), (0.1, 9, -0.04, False)],\n  2: [(0.1, 8, -0.04, False), (0.8, 9, -0.04, False), (0.1, 4, -0.04, False)],\n  3: [(0.1, 9, -0.04, False), (0.8, 4, -0.04, False), (0.1, 8, -0.04, False)]},\n 9: {0: [(0.2, 9, -0.04, False), (0.8, 8, -0.04, False)],\n  1: [(0.1, 8, -0.04, False), (0.8, 9, -0.04, False), (0.1, 10, -0.04, False)],\n  2: [(0.2, 9, -0.04, False), (0.8, 10, -0.04, False)],\n  3: [(0.1, 10, -0.04, False),\n   (0.8, 9, -0.04, False),\n   (0.1, 8, -0.04, False)]},\n 10: {0: [(0.1, 6, -0.04, False),\n   (0.8, 9, -0.04, False),\n   (0.1, 10, -0.04, False)],\n  1: [(0.1, 9, -0.04, False),\n   (0.8, 10, -0.04, False),\n   (0.1, 11, -0.04, False)],\n  2: [(0.1, 10, -0.04, False),\n   (0.8, 11, -0.04, False),\n   (0.1, 6, -0.04, False)],\n  3: [(0.1, 11, -0.04, False),\n   (0.8, 6, -0.04, False),\n   (0.1, 9, -0.04, False)]},\n 11: {0: [(0.1, 7, -1.04, True),\n   (0.8, 10, -0.04, False),\n   (0.1, 11, -0.04, False)],\n  1: [(0.1, 10, -0.04, False), (0.9, 11, -0.04, False)],\n  2: [(0.9, 11, -0.04, False), (0.1, 7, -1.04, True)],\n  3: [(0.1, 11, -0.04, False),\n   (0.8, 7, -1.04, True),\n   (0.1, 10, -0.04, False)]}}\n\n\n\nNote: OpenAI Gym에서 RussellNorvigGridworld env를 사용할 수 있습니다.\n\n\nP = gym.make('RussellNorvigGridworld-v0').env.P\nP\n\n{0: {0: [(0.1, 0, -0.04, False),\n   (0.8, 0, -0.04, False),\n   (0.1, 4, -0.04, False)],\n  1: [(0.1, 0, -0.04, False), (0.8, 4, -0.04, False), (0.1, 1, -0.04, False)],\n  2: [(0.1, 4, -0.04, False), (0.8, 1, -0.04, False), (0.1, 0, -0.04, False)],\n  3: [(0.1, 1, -0.04, False), (0.8, 0, -0.04, False), (0.1, 0, -0.04, False)]},\n 1: {0: [(0.1, 1, -0.04, False),\n   (0.8, 0, -0.04, False),\n   (0.1, 1, -0.04, False)],\n  1: [(0.1, 0, -0.04, False), (0.8, 1, -0.04, False), (0.1, 2, -0.04, False)],\n  2: [(0.1, 1, -0.04, False), (0.8, 2, -0.04, False), (0.1, 1, -0.04, False)],\n  3: [(0.1, 2, -0.04, False), (0.8, 1, -0.04, False), (0.1, 0, -0.04, False)]},\n 2: {0: [(0.1, 2, -0.04, False),\n   (0.8, 1, -0.04, False),\n   (0.1, 6, -0.04, False)],\n  1: [(0.1, 1, -0.04, False), (0.8, 6, -0.04, False), (0.1, 3, 0.96, True)],\n  2: [(0.1, 6, -0.04, False), (0.8, 3, 0.96, True), (0.1, 2, -0.04, False)],\n  3: [(0.1, 3, 0.96, True), (0.8, 2, -0.04, False), (0.1, 1, -0.04, False)]},\n 3: {0: [(1.0, 3, 0, True)],\n  1: [(1.0, 3, 0, True)],\n  2: [(1.0, 3, 0, True)],\n  3: [(1.0, 3, 0, True)]},\n 4: {0: [(0.1, 0, -0.04, False),\n   (0.8, 4, -0.04, False),\n   (0.1, 8, -0.04, False)],\n  1: [(0.1, 4, -0.04, False), (0.8, 8, -0.04, False), (0.1, 4, -0.04, False)],\n  2: [(0.1, 8, -0.04, False), (0.8, 4, -0.04, False), (0.1, 0, -0.04, False)],\n  3: [(0.1, 4, -0.04, False), (0.8, 0, -0.04, False), (0.1, 4, -0.04, False)]},\n 5: {0: [(1.0, 5, 0, True)],\n  1: [(1.0, 5, 0, True)],\n  2: [(1.0, 5, 0, True)],\n  3: [(1.0, 5, 0, True)]},\n 6: {0: [(0.1, 2, -0.04, False),\n   (0.8, 6, -0.04, False),\n   (0.1, 10, -0.04, False)],\n  1: [(0.1, 6, -0.04, False), (0.8, 10, -0.04, False), (0.1, 7, -1.04, True)],\n  2: [(0.1, 10, -0.04, False), (0.8, 7, -1.04, True), (0.1, 2, -0.04, False)],\n  3: [(0.1, 7, -1.04, True), (0.8, 2, -0.04, False), (0.1, 6, -0.04, False)]},\n 7: {0: [(1.0, 7, 0, True)],\n  1: [(1.0, 7, 0, True)],\n  2: [(1.0, 7, 0, True)],\n  3: [(1.0, 7, 0, True)]},\n 8: {0: [(0.1, 4, -0.04, False),\n   (0.8, 8, -0.04, False),\n   (0.1, 8, -0.04, False)],\n  1: [(0.1, 8, -0.04, False), (0.8, 8, -0.04, False), (0.1, 9, -0.04, False)],\n  2: [(0.1, 8, -0.04, False), (0.8, 9, -0.04, False), (0.1, 4, -0.04, False)],\n  3: [(0.1, 9, -0.04, False), (0.8, 4, -0.04, False), (0.1, 8, -0.04, False)]},\n 9: {0: [(0.1, 9, -0.04, False),\n   (0.8, 8, -0.04, False),\n   (0.1, 9, -0.04, False)],\n  1: [(0.1, 8, -0.04, False), (0.8, 9, -0.04, False), (0.1, 10, -0.04, False)],\n  2: [(0.1, 9, -0.04, False), (0.8, 10, -0.04, False), (0.1, 9, -0.04, False)],\n  3: [(0.1, 10, -0.04, False),\n   (0.8, 9, -0.04, False),\n   (0.1, 8, -0.04, False)]},\n 10: {0: [(0.1, 6, -0.04, False),\n   (0.8, 9, -0.04, False),\n   (0.1, 10, -0.04, False)],\n  1: [(0.1, 9, -0.04, False),\n   (0.8, 10, -0.04, False),\n   (0.1, 11, -0.04, False)],\n  2: [(0.1, 10, -0.04, False),\n   (0.8, 11, -0.04, False),\n   (0.1, 6, -0.04, False)],\n  3: [(0.1, 11, -0.04, False),\n   (0.8, 6, -0.04, False),\n   (0.1, 9, -0.04, False)]},\n 11: {0: [(0.1, 7, -1.04, True),\n   (0.8, 10, -0.04, False),\n   (0.1, 11, -0.04, False)],\n  1: [(0.1, 10, -0.04, False),\n   (0.8, 11, -0.04, False),\n   (0.1, 11, -0.04, False)],\n  2: [(0.1, 11, -0.04, False), (0.8, 11, -0.04, False), (0.1, 7, -1.04, True)],\n  3: [(0.1, 11, -0.04, False),\n   (0.8, 7, -1.04, True),\n   (0.1, 10, -0.04, False)]}}\n\n\n\n\nFrozen Lake Gridworld\n\n매우 확률적인 환경 (33.33%의 확률로 행동이 성공하고, 33.33% 확률씩 양 옆으로 이동함)\n4x4 격자로 되어 있고, 16개의 상태(0-15)를 가짐\n상태 15 (가장 아래 가장자리)에 도달했을 때 +1 보상을 받고 나머지는 보상을 받지 않음\n상태 5, 7, 11, 12는 구멍이고, 에이전트가 구멍에 들어가면 패널티 없이 종료됨\n상태 15는 목표점이며, 역시 에피소드가 종료됨\n에이전트는 상태 0 (가장 왼쪽 구석)에서 시작함\n행동: 왼쪽(0), 아래 (1), 오른쪽 (2), 위 (3)\n\n\nP = {\n    0: {\n        0: [(0.6666666666666666, 0, 0.0, False),\n            (0.3333333333333333, 4, 0.0, False)\n        ],\n        1: [(0.3333333333333333, 0, 0.0, False),\n            (0.3333333333333333, 4, 0.0, False),\n            (0.3333333333333333, 1, 0.0, False)\n        ],\n        2: [(0.3333333333333333, 4, 0.0, False),\n            (0.3333333333333333, 1, 0.0, False),\n            (0.3333333333333333, 0, 0.0, False)\n        ],\n        3: [(0.3333333333333333, 1, 0.0, False),\n            (0.6666666666666666, 0, 0.0, False)\n        ]\n    },\n    1: {\n        0: [(0.3333333333333333, 1, 0.0, False),\n            (0.3333333333333333, 0, 0.0, False),\n            (0.3333333333333333, 5, 0.0, True)\n        ],\n        1: [(0.3333333333333333, 0, 0.0, False),\n            (0.3333333333333333, 5, 0.0, True),\n            (0.3333333333333333, 2, 0.0, False)\n        ],\n        2: [(0.3333333333333333, 5, 0.0, True),\n            (0.3333333333333333, 2, 0.0, False),\n            (0.3333333333333333, 1, 0.0, False)\n        ],\n        3: [(0.3333333333333333, 2, 0.0, False),\n            (0.3333333333333333, 1, 0.0, False),\n            (0.3333333333333333, 0, 0.0, False)\n        ]\n    },\n    2: {\n        0: [(0.3333333333333333, 2, 0.0, False),\n            (0.3333333333333333, 1, 0.0, False),\n            (0.3333333333333333, 6, 0.0, False)\n        ],\n        1: [(0.3333333333333333, 1, 0.0, False),\n            (0.3333333333333333, 6, 0.0, False),\n            (0.3333333333333333, 3, 0.0, False)\n        ],\n        2: [(0.3333333333333333, 6, 0.0, False),\n            (0.3333333333333333, 3, 0.0, False),\n            (0.3333333333333333, 2, 0.0, False)\n        ],\n        3: [(0.3333333333333333, 3, 0.0, False),\n            (0.3333333333333333, 2, 0.0, False),\n            (0.3333333333333333, 1, 0.0, False)\n        ]\n    },\n    3: {\n        0: [(0.3333333333333333, 3, 0.0, False),\n            (0.3333333333333333, 2, 0.0, False),\n            (0.3333333333333333, 7, 0.0, True)\n        ],\n        1: [(0.3333333333333333, 2, 0.0, False),\n            (0.3333333333333333, 7, 0.0, True),\n            (0.3333333333333333, 3, 0.0, False)\n        ],\n        2: [(0.3333333333333333, 7, 0.0, True),\n            (0.6666666666666666, 3, 0.0, False)\n        ],\n        3: [(0.6666666666666666, 3, 0.0, False),\n            (0.3333333333333333, 2, 0.0, False)\n        ]\n    },\n    4: {\n        0: [(0.3333333333333333, 0, 0.0, False),\n            (0.3333333333333333, 4, 0.0, False),\n            (0.3333333333333333, 8, 0.0, False)\n        ],\n        1: [(0.3333333333333333, 4, 0.0, False),\n            (0.3333333333333333, 8, 0.0, False),\n            (0.3333333333333333, 5, 0.0, True)\n        ],\n        2: [(0.3333333333333333, 8, 0.0, False),\n            (0.3333333333333333, 5, 0.0, True),\n            (0.3333333333333333, 0, 0.0, False)\n        ],\n        3: [(0.3333333333333333, 5, 0.0, True),\n            (0.3333333333333333, 0, 0.0, False),\n            (0.3333333333333333, 4, 0.0, False)\n        ]\n    },\n    5: {\n        0: [(1.0, 5, 0, True)],\n        1: [(1.0, 5, 0, True)],\n        2: [(1.0, 5, 0, True)],\n        3: [(1.0, 5, 0, True)]\n    },\n    6: {\n        0: [(0.3333333333333333, 2, 0.0, False),\n            (0.3333333333333333, 5, 0.0, True),\n            (0.3333333333333333, 10, 0.0, False)\n        ],\n        1: [(0.3333333333333333, 5, 0.0, True),\n            (0.3333333333333333, 10, 0.0, False),\n            (0.3333333333333333, 7, 0.0, True)\n        ],\n        2: [(0.3333333333333333, 10, 0.0, False),\n            (0.3333333333333333, 7, 0.0, True),\n            (0.3333333333333333, 2, 0.0, False)\n        ],\n        3: [(0.3333333333333333, 7, 0.0, True),\n            (0.3333333333333333, 2, 0.0, False),\n            (0.3333333333333333, 5, 0.0, True)\n        ]\n    },\n    7: {\n        0: [(1.0, 7, 0, True)],\n        1: [(1.0, 7, 0, True)],\n        2: [(1.0, 7, 0, True)],\n        3: [(1.0, 7, 0, True)]\n    },\n    8: {\n        0: [(0.3333333333333333, 4, 0.0, False),\n            (0.3333333333333333, 8, 0.0, False),\n            (0.3333333333333333, 12, 0.0, True)\n        ],\n        1: [(0.3333333333333333, 8, 0.0, False),\n            (0.3333333333333333, 12, 0.0, True),\n            (0.3333333333333333, 9, 0.0, False)\n        ],\n        2: [(0.3333333333333333, 12, 0.0, True),\n            (0.3333333333333333, 9, 0.0, False),\n            (0.3333333333333333, 4, 0.0, False)\n        ],\n        3: [(0.3333333333333333, 9, 0.0, False),\n            (0.3333333333333333, 4, 0.0, False),\n            (0.3333333333333333, 8, 0.0, False)\n        ]\n    },\n    9: {\n        0: [(0.3333333333333333, 5, 0.0, True),\n            (0.3333333333333333, 8, 0.0, False),\n            (0.3333333333333333, 13, 0.0, False)\n        ],\n        1: [(0.3333333333333333, 8, 0.0, False),\n            (0.3333333333333333, 13, 0.0, False),\n            (0.3333333333333333, 10, 0.0, False)\n        ],\n        2: [(0.3333333333333333, 13, 0.0, False),\n            (0.3333333333333333, 10, 0.0, False),\n            (0.3333333333333333, 5, 0.0, True)\n        ],\n        3: [(0.3333333333333333, 10, 0.0, False),\n            (0.3333333333333333, 5, 0.0, True),\n            (0.3333333333333333, 8, 0.0, False)\n        ]\n    },\n    10: {\n        0: [(0.3333333333333333, 6, 0.0, False),\n            (0.3333333333333333, 9, 0.0, False),\n            (0.3333333333333333, 14, 0.0, False)\n        ],\n        1: [(0.3333333333333333, 9, 0.0, False),\n            (0.3333333333333333, 14, 0.0, False),\n            (0.3333333333333333, 11, 0.0, True)\n        ],\n        2: [(0.3333333333333333, 14, 0.0, False),\n            (0.3333333333333333, 11, 0.0, True),\n            (0.3333333333333333, 6, 0.0, False)\n        ],\n        3: [(0.3333333333333333, 11, 0.0, True),\n            (0.3333333333333333, 6, 0.0, False),\n            (0.3333333333333333, 9, 0.0, False)\n        ]\n    },\n    11: {\n        0: [(1.0, 11, 0, True)],\n        1: [(1.0, 11, 0, True)],\n        2: [(1.0, 11, 0, True)],\n        3: [(1.0, 11, 0, True)]\n    },\n    12: {\n        0: [(1.0, 12, 0, True)],\n        1: [(1.0, 12, 0, True)],\n        2: [(1.0, 12, 0, True)],\n        3: [(1.0, 12, 0, True)]\n    },\n    13: {\n        0: [(0.3333333333333333, 9, 0.0, False),\n            (0.3333333333333333, 12, 0.0, True),\n            (0.3333333333333333, 13, 0.0, False)\n        ],\n        1: [(0.3333333333333333, 12, 0.0, True),\n            (0.3333333333333333, 13, 0.0, False),\n            (0.3333333333333333, 14, 0.0, False)\n        ],\n        2: [(0.3333333333333333, 13, 0.0, False),\n            (0.3333333333333333, 14, 0.0, False),\n            (0.3333333333333333, 9, 0.0, False)\n        ],\n        3: [(0.3333333333333333, 14, 0.0, False),\n            (0.3333333333333333, 9, 0.0, False),\n            (0.3333333333333333, 12, 0.0, True)\n        ]\n    },\n    14: {\n        0: [(0.3333333333333333, 10, 0.0, False),\n            (0.3333333333333333, 13, 0.0, False),\n            (0.3333333333333333, 14, 0.0, False)\n        ],\n        1: [(0.3333333333333333, 13, 0.0, False),\n            (0.3333333333333333, 14, 0.0, False),\n            (0.3333333333333333, 15, 1.0, True)\n        ],\n        2: [(0.3333333333333333, 14, 0.0, False),\n            (0.3333333333333333, 15, 1.0, True),\n            (0.3333333333333333, 10, 0.0, False)\n        ],\n        3: [(0.3333333333333333, 15, 1.0, True),\n            (0.3333333333333333, 10, 0.0, False),\n            (0.3333333333333333, 13, 0.0, False)\n        ]\n    },\n    15: {\n        0: [(1.0, 15, 0, True)],\n        1: [(1.0, 15, 0, True)],\n        2: [(1.0, 15, 0, True)],\n        3: [(1.0, 15, 0, True)]\n    }\n}\n\nP\n\n{0: {0: [(0.6666666666666666, 0, 0.0, False),\n   (0.3333333333333333, 4, 0.0, False)],\n  1: [(0.3333333333333333, 0, 0.0, False),\n   (0.3333333333333333, 4, 0.0, False),\n   (0.3333333333333333, 1, 0.0, False)],\n  2: [(0.3333333333333333, 4, 0.0, False),\n   (0.3333333333333333, 1, 0.0, False),\n   (0.3333333333333333, 0, 0.0, False)],\n  3: [(0.3333333333333333, 1, 0.0, False),\n   (0.6666666666666666, 0, 0.0, False)]},\n 1: {0: [(0.3333333333333333, 1, 0.0, False),\n   (0.3333333333333333, 0, 0.0, False),\n   (0.3333333333333333, 5, 0.0, True)],\n  1: [(0.3333333333333333, 0, 0.0, False),\n   (0.3333333333333333, 5, 0.0, True),\n   (0.3333333333333333, 2, 0.0, False)],\n  2: [(0.3333333333333333, 5, 0.0, True),\n   (0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 1, 0.0, False)],\n  3: [(0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 1, 0.0, False),\n   (0.3333333333333333, 0, 0.0, False)]},\n 2: {0: [(0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 1, 0.0, False),\n   (0.3333333333333333, 6, 0.0, False)],\n  1: [(0.3333333333333333, 1, 0.0, False),\n   (0.3333333333333333, 6, 0.0, False),\n   (0.3333333333333333, 3, 0.0, False)],\n  2: [(0.3333333333333333, 6, 0.0, False),\n   (0.3333333333333333, 3, 0.0, False),\n   (0.3333333333333333, 2, 0.0, False)],\n  3: [(0.3333333333333333, 3, 0.0, False),\n   (0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 1, 0.0, False)]},\n 3: {0: [(0.3333333333333333, 3, 0.0, False),\n   (0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 7, 0.0, True)],\n  1: [(0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 7, 0.0, True),\n   (0.3333333333333333, 3, 0.0, False)],\n  2: [(0.3333333333333333, 7, 0.0, True), (0.6666666666666666, 3, 0.0, False)],\n  3: [(0.6666666666666666, 3, 0.0, False),\n   (0.3333333333333333, 2, 0.0, False)]},\n 4: {0: [(0.3333333333333333, 0, 0.0, False),\n   (0.3333333333333333, 4, 0.0, False),\n   (0.3333333333333333, 8, 0.0, False)],\n  1: [(0.3333333333333333, 4, 0.0, False),\n   (0.3333333333333333, 8, 0.0, False),\n   (0.3333333333333333, 5, 0.0, True)],\n  2: [(0.3333333333333333, 8, 0.0, False),\n   (0.3333333333333333, 5, 0.0, True),\n   (0.3333333333333333, 0, 0.0, False)],\n  3: [(0.3333333333333333, 5, 0.0, True),\n   (0.3333333333333333, 0, 0.0, False),\n   (0.3333333333333333, 4, 0.0, False)]},\n 5: {0: [(1.0, 5, 0, True)],\n  1: [(1.0, 5, 0, True)],\n  2: [(1.0, 5, 0, True)],\n  3: [(1.0, 5, 0, True)]},\n 6: {0: [(0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 5, 0.0, True),\n   (0.3333333333333333, 10, 0.0, False)],\n  1: [(0.3333333333333333, 5, 0.0, True),\n   (0.3333333333333333, 10, 0.0, False),\n   (0.3333333333333333, 7, 0.0, True)],\n  2: [(0.3333333333333333, 10, 0.0, False),\n   (0.3333333333333333, 7, 0.0, True),\n   (0.3333333333333333, 2, 0.0, False)],\n  3: [(0.3333333333333333, 7, 0.0, True),\n   (0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 5, 0.0, True)]},\n 7: {0: [(1.0, 7, 0, True)],\n  1: [(1.0, 7, 0, True)],\n  2: [(1.0, 7, 0, True)],\n  3: [(1.0, 7, 0, True)]},\n 8: {0: [(0.3333333333333333, 4, 0.0, False),\n   (0.3333333333333333, 8, 0.0, False),\n   (0.3333333333333333, 12, 0.0, True)],\n  1: [(0.3333333333333333, 8, 0.0, False),\n   (0.3333333333333333, 12, 0.0, True),\n   (0.3333333333333333, 9, 0.0, False)],\n  2: [(0.3333333333333333, 12, 0.0, True),\n   (0.3333333333333333, 9, 0.0, False),\n   (0.3333333333333333, 4, 0.0, False)],\n  3: [(0.3333333333333333, 9, 0.0, False),\n   (0.3333333333333333, 4, 0.0, False),\n   (0.3333333333333333, 8, 0.0, False)]},\n 9: {0: [(0.3333333333333333, 5, 0.0, True),\n   (0.3333333333333333, 8, 0.0, False),\n   (0.3333333333333333, 13, 0.0, False)],\n  1: [(0.3333333333333333, 8, 0.0, False),\n   (0.3333333333333333, 13, 0.0, False),\n   (0.3333333333333333, 10, 0.0, False)],\n  2: [(0.3333333333333333, 13, 0.0, False),\n   (0.3333333333333333, 10, 0.0, False),\n   (0.3333333333333333, 5, 0.0, True)],\n  3: [(0.3333333333333333, 10, 0.0, False),\n   (0.3333333333333333, 5, 0.0, True),\n   (0.3333333333333333, 8, 0.0, False)]},\n 10: {0: [(0.3333333333333333, 6, 0.0, False),\n   (0.3333333333333333, 9, 0.0, False),\n   (0.3333333333333333, 14, 0.0, False)],\n  1: [(0.3333333333333333, 9, 0.0, False),\n   (0.3333333333333333, 14, 0.0, False),\n   (0.3333333333333333, 11, 0.0, True)],\n  2: [(0.3333333333333333, 14, 0.0, False),\n   (0.3333333333333333, 11, 0.0, True),\n   (0.3333333333333333, 6, 0.0, False)],\n  3: [(0.3333333333333333, 11, 0.0, True),\n   (0.3333333333333333, 6, 0.0, False),\n   (0.3333333333333333, 9, 0.0, False)]},\n 11: {0: [(1.0, 11, 0, True)],\n  1: [(1.0, 11, 0, True)],\n  2: [(1.0, 11, 0, True)],\n  3: [(1.0, 11, 0, True)]},\n 12: {0: [(1.0, 12, 0, True)],\n  1: [(1.0, 12, 0, True)],\n  2: [(1.0, 12, 0, True)],\n  3: [(1.0, 12, 0, True)]},\n 13: {0: [(0.3333333333333333, 9, 0.0, False),\n   (0.3333333333333333, 12, 0.0, True),\n   (0.3333333333333333, 13, 0.0, False)],\n  1: [(0.3333333333333333, 12, 0.0, True),\n   (0.3333333333333333, 13, 0.0, False),\n   (0.3333333333333333, 14, 0.0, False)],\n  2: [(0.3333333333333333, 13, 0.0, False),\n   (0.3333333333333333, 14, 0.0, False),\n   (0.3333333333333333, 9, 0.0, False)],\n  3: [(0.3333333333333333, 14, 0.0, False),\n   (0.3333333333333333, 9, 0.0, False),\n   (0.3333333333333333, 12, 0.0, True)]},\n 14: {0: [(0.3333333333333333, 10, 0.0, False),\n   (0.3333333333333333, 13, 0.0, False),\n   (0.3333333333333333, 14, 0.0, False)],\n  1: [(0.3333333333333333, 13, 0.0, False),\n   (0.3333333333333333, 14, 0.0, False),\n   (0.3333333333333333, 15, 1.0, True)],\n  2: [(0.3333333333333333, 14, 0.0, False),\n   (0.3333333333333333, 15, 1.0, True),\n   (0.3333333333333333, 10, 0.0, False)],\n  3: [(0.3333333333333333, 15, 1.0, True),\n   (0.3333333333333333, 10, 0.0, False),\n   (0.3333333333333333, 13, 0.0, False)]},\n 15: {0: [(1.0, 15, 0, True)],\n  1: [(1.0, 15, 0, True)],\n  2: [(1.0, 15, 0, True)],\n  3: [(1.0, 15, 0, True)]}}\n\n\n\nNote: OpenAI gym의 FrozenLake env를 사용할 수 있습니다.\n\n\nP = gym.make('FrozenLake-v0').env.P\nP\n\n{0: {0: [(0.3333333333333333, 0, 0.0, False),\n   (0.3333333333333333, 0, 0.0, False),\n   (0.3333333333333333, 4, 0.0, False)],\n  1: [(0.3333333333333333, 0, 0.0, False),\n   (0.3333333333333333, 4, 0.0, False),\n   (0.3333333333333333, 1, 0.0, False)],\n  2: [(0.3333333333333333, 4, 0.0, False),\n   (0.3333333333333333, 1, 0.0, False),\n   (0.3333333333333333, 0, 0.0, False)],\n  3: [(0.3333333333333333, 1, 0.0, False),\n   (0.3333333333333333, 0, 0.0, False),\n   (0.3333333333333333, 0, 0.0, False)]},\n 1: {0: [(0.3333333333333333, 1, 0.0, False),\n   (0.3333333333333333, 0, 0.0, False),\n   (0.3333333333333333, 5, 0.0, True)],\n  1: [(0.3333333333333333, 0, 0.0, False),\n   (0.3333333333333333, 5, 0.0, True),\n   (0.3333333333333333, 2, 0.0, False)],\n  2: [(0.3333333333333333, 5, 0.0, True),\n   (0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 1, 0.0, False)],\n  3: [(0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 1, 0.0, False),\n   (0.3333333333333333, 0, 0.0, False)]},\n 2: {0: [(0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 1, 0.0, False),\n   (0.3333333333333333, 6, 0.0, False)],\n  1: [(0.3333333333333333, 1, 0.0, False),\n   (0.3333333333333333, 6, 0.0, False),\n   (0.3333333333333333, 3, 0.0, False)],\n  2: [(0.3333333333333333, 6, 0.0, False),\n   (0.3333333333333333, 3, 0.0, False),\n   (0.3333333333333333, 2, 0.0, False)],\n  3: [(0.3333333333333333, 3, 0.0, False),\n   (0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 1, 0.0, False)]},\n 3: {0: [(0.3333333333333333, 3, 0.0, False),\n   (0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 7, 0.0, True)],\n  1: [(0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 7, 0.0, True),\n   (0.3333333333333333, 3, 0.0, False)],\n  2: [(0.3333333333333333, 7, 0.0, True),\n   (0.3333333333333333, 3, 0.0, False),\n   (0.3333333333333333, 3, 0.0, False)],\n  3: [(0.3333333333333333, 3, 0.0, False),\n   (0.3333333333333333, 3, 0.0, False),\n   (0.3333333333333333, 2, 0.0, False)]},\n 4: {0: [(0.3333333333333333, 0, 0.0, False),\n   (0.3333333333333333, 4, 0.0, False),\n   (0.3333333333333333, 8, 0.0, False)],\n  1: [(0.3333333333333333, 4, 0.0, False),\n   (0.3333333333333333, 8, 0.0, False),\n   (0.3333333333333333, 5, 0.0, True)],\n  2: [(0.3333333333333333, 8, 0.0, False),\n   (0.3333333333333333, 5, 0.0, True),\n   (0.3333333333333333, 0, 0.0, False)],\n  3: [(0.3333333333333333, 5, 0.0, True),\n   (0.3333333333333333, 0, 0.0, False),\n   (0.3333333333333333, 4, 0.0, False)]},\n 5: {0: [(1.0, 5, 0, True)],\n  1: [(1.0, 5, 0, True)],\n  2: [(1.0, 5, 0, True)],\n  3: [(1.0, 5, 0, True)]},\n 6: {0: [(0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 5, 0.0, True),\n   (0.3333333333333333, 10, 0.0, False)],\n  1: [(0.3333333333333333, 5, 0.0, True),\n   (0.3333333333333333, 10, 0.0, False),\n   (0.3333333333333333, 7, 0.0, True)],\n  2: [(0.3333333333333333, 10, 0.0, False),\n   (0.3333333333333333, 7, 0.0, True),\n   (0.3333333333333333, 2, 0.0, False)],\n  3: [(0.3333333333333333, 7, 0.0, True),\n   (0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 5, 0.0, True)]},\n 7: {0: [(1.0, 7, 0, True)],\n  1: [(1.0, 7, 0, True)],\n  2: [(1.0, 7, 0, True)],\n  3: [(1.0, 7, 0, True)]},\n 8: {0: [(0.3333333333333333, 4, 0.0, False),\n   (0.3333333333333333, 8, 0.0, False),\n   (0.3333333333333333, 12, 0.0, True)],\n  1: [(0.3333333333333333, 8, 0.0, False),\n   (0.3333333333333333, 12, 0.0, True),\n   (0.3333333333333333, 9, 0.0, False)],\n  2: [(0.3333333333333333, 12, 0.0, True),\n   (0.3333333333333333, 9, 0.0, False),\n   (0.3333333333333333, 4, 0.0, False)],\n  3: [(0.3333333333333333, 9, 0.0, False),\n   (0.3333333333333333, 4, 0.0, False),\n   (0.3333333333333333, 8, 0.0, False)]},\n 9: {0: [(0.3333333333333333, 5, 0.0, True),\n   (0.3333333333333333, 8, 0.0, False),\n   (0.3333333333333333, 13, 0.0, False)],\n  1: [(0.3333333333333333, 8, 0.0, False),\n   (0.3333333333333333, 13, 0.0, False),\n   (0.3333333333333333, 10, 0.0, False)],\n  2: [(0.3333333333333333, 13, 0.0, False),\n   (0.3333333333333333, 10, 0.0, False),\n   (0.3333333333333333, 5, 0.0, True)],\n  3: [(0.3333333333333333, 10, 0.0, False),\n   (0.3333333333333333, 5, 0.0, True),\n   (0.3333333333333333, 8, 0.0, False)]},\n 10: {0: [(0.3333333333333333, 6, 0.0, False),\n   (0.3333333333333333, 9, 0.0, False),\n   (0.3333333333333333, 14, 0.0, False)],\n  1: [(0.3333333333333333, 9, 0.0, False),\n   (0.3333333333333333, 14, 0.0, False),\n   (0.3333333333333333, 11, 0.0, True)],\n  2: [(0.3333333333333333, 14, 0.0, False),\n   (0.3333333333333333, 11, 0.0, True),\n   (0.3333333333333333, 6, 0.0, False)],\n  3: [(0.3333333333333333, 11, 0.0, True),\n   (0.3333333333333333, 6, 0.0, False),\n   (0.3333333333333333, 9, 0.0, False)]},\n 11: {0: [(1.0, 11, 0, True)],\n  1: [(1.0, 11, 0, True)],\n  2: [(1.0, 11, 0, True)],\n  3: [(1.0, 11, 0, True)]},\n 12: {0: [(1.0, 12, 0, True)],\n  1: [(1.0, 12, 0, True)],\n  2: [(1.0, 12, 0, True)],\n  3: [(1.0, 12, 0, True)]},\n 13: {0: [(0.3333333333333333, 9, 0.0, False),\n   (0.3333333333333333, 12, 0.0, True),\n   (0.3333333333333333, 13, 0.0, False)],\n  1: [(0.3333333333333333, 12, 0.0, True),\n   (0.3333333333333333, 13, 0.0, False),\n   (0.3333333333333333, 14, 0.0, False)],\n  2: [(0.3333333333333333, 13, 0.0, False),\n   (0.3333333333333333, 14, 0.0, False),\n   (0.3333333333333333, 9, 0.0, False)],\n  3: [(0.3333333333333333, 14, 0.0, False),\n   (0.3333333333333333, 9, 0.0, False),\n   (0.3333333333333333, 12, 0.0, True)]},\n 14: {0: [(0.3333333333333333, 10, 0.0, False),\n   (0.3333333333333333, 13, 0.0, False),\n   (0.3333333333333333, 14, 0.0, False)],\n  1: [(0.3333333333333333, 13, 0.0, False),\n   (0.3333333333333333, 14, 0.0, False),\n   (0.3333333333333333, 15, 1.0, True)],\n  2: [(0.3333333333333333, 14, 0.0, False),\n   (0.3333333333333333, 15, 1.0, True),\n   (0.3333333333333333, 10, 0.0, False)],\n  3: [(0.3333333333333333, 15, 1.0, True),\n   (0.3333333333333333, 10, 0.0, False),\n   (0.3333333333333333, 13, 0.0, False)]},\n 15: {0: [(1.0, 15, 0, True)],\n  1: [(1.0, 15, 0, True)],\n  2: [(1.0, 15, 0, True)],\n  3: [(1.0, 15, 0, True)]}}\n\n\n\n\nFrozenLake8x8\n\n참고로 MDP는 더 복잡하고 클 수 있습니다.\n64개의 상태를 가지는 FrozenLake도 살펴보기 바랍니다.\n\n\nenv = gym.make('FrozenLake8x8-v0')\nP = env.env.P\nP\n\n{0: {0: [(0.3333333333333333, 0, 0.0, False),\n   (0.3333333333333333, 0, 0.0, False),\n   (0.3333333333333333, 8, 0.0, False)],\n  1: [(0.3333333333333333, 0, 0.0, False),\n   (0.3333333333333333, 8, 0.0, False),\n   (0.3333333333333333, 1, 0.0, False)],\n  2: [(0.3333333333333333, 8, 0.0, False),\n   (0.3333333333333333, 1, 0.0, False),\n   (0.3333333333333333, 0, 0.0, False)],\n  3: [(0.3333333333333333, 1, 0.0, False),\n   (0.3333333333333333, 0, 0.0, False),\n   (0.3333333333333333, 0, 0.0, False)]},\n 1: {0: [(0.3333333333333333, 1, 0.0, False),\n   (0.3333333333333333, 0, 0.0, False),\n   (0.3333333333333333, 9, 0.0, False)],\n  1: [(0.3333333333333333, 0, 0.0, False),\n   (0.3333333333333333, 9, 0.0, False),\n   (0.3333333333333333, 2, 0.0, False)],\n  2: [(0.3333333333333333, 9, 0.0, False),\n   (0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 1, 0.0, False)],\n  3: [(0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 1, 0.0, False),\n   (0.3333333333333333, 0, 0.0, False)]},\n 2: {0: [(0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 1, 0.0, False),\n   (0.3333333333333333, 10, 0.0, False)],\n  1: [(0.3333333333333333, 1, 0.0, False),\n   (0.3333333333333333, 10, 0.0, False),\n   (0.3333333333333333, 3, 0.0, False)],\n  2: [(0.3333333333333333, 10, 0.0, False),\n   (0.3333333333333333, 3, 0.0, False),\n   (0.3333333333333333, 2, 0.0, False)],\n  3: [(0.3333333333333333, 3, 0.0, False),\n   (0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 1, 0.0, False)]},\n 3: {0: [(0.3333333333333333, 3, 0.0, False),\n   (0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 11, 0.0, False)],\n  1: [(0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 11, 0.0, False),\n   (0.3333333333333333, 4, 0.0, False)],\n  2: [(0.3333333333333333, 11, 0.0, False),\n   (0.3333333333333333, 4, 0.0, False),\n   (0.3333333333333333, 3, 0.0, False)],\n  3: [(0.3333333333333333, 4, 0.0, False),\n   (0.3333333333333333, 3, 0.0, False),\n   (0.3333333333333333, 2, 0.0, False)]},\n 4: {0: [(0.3333333333333333, 4, 0.0, False),\n   (0.3333333333333333, 3, 0.0, False),\n   (0.3333333333333333, 12, 0.0, False)],\n  1: [(0.3333333333333333, 3, 0.0, False),\n   (0.3333333333333333, 12, 0.0, False),\n   (0.3333333333333333, 5, 0.0, False)],\n  2: [(0.3333333333333333, 12, 0.0, False),\n   (0.3333333333333333, 5, 0.0, False),\n   (0.3333333333333333, 4, 0.0, False)],\n  3: [(0.3333333333333333, 5, 0.0, False),\n   (0.3333333333333333, 4, 0.0, False),\n   (0.3333333333333333, 3, 0.0, False)]},\n 5: {0: [(0.3333333333333333, 5, 0.0, False),\n   (0.3333333333333333, 4, 0.0, False),\n   (0.3333333333333333, 13, 0.0, False)],\n  1: [(0.3333333333333333, 4, 0.0, False),\n   (0.3333333333333333, 13, 0.0, False),\n   (0.3333333333333333, 6, 0.0, False)],\n  2: [(0.3333333333333333, 13, 0.0, False),\n   (0.3333333333333333, 6, 0.0, False),\n   (0.3333333333333333, 5, 0.0, False)],\n  3: [(0.3333333333333333, 6, 0.0, False),\n   (0.3333333333333333, 5, 0.0, False),\n   (0.3333333333333333, 4, 0.0, False)]},\n 6: {0: [(0.3333333333333333, 6, 0.0, False),\n   (0.3333333333333333, 5, 0.0, False),\n   (0.3333333333333333, 14, 0.0, False)],\n  1: [(0.3333333333333333, 5, 0.0, False),\n   (0.3333333333333333, 14, 0.0, False),\n   (0.3333333333333333, 7, 0.0, False)],\n  2: [(0.3333333333333333, 14, 0.0, False),\n   (0.3333333333333333, 7, 0.0, False),\n   (0.3333333333333333, 6, 0.0, False)],\n  3: [(0.3333333333333333, 7, 0.0, False),\n   (0.3333333333333333, 6, 0.0, False),\n   (0.3333333333333333, 5, 0.0, False)]},\n 7: {0: [(0.3333333333333333, 7, 0.0, False),\n   (0.3333333333333333, 6, 0.0, False),\n   (0.3333333333333333, 15, 0.0, False)],\n  1: [(0.3333333333333333, 6, 0.0, False),\n   (0.3333333333333333, 15, 0.0, False),\n   (0.3333333333333333, 7, 0.0, False)],\n  2: [(0.3333333333333333, 15, 0.0, False),\n   (0.3333333333333333, 7, 0.0, False),\n   (0.3333333333333333, 7, 0.0, False)],\n  3: [(0.3333333333333333, 7, 0.0, False),\n   (0.3333333333333333, 7, 0.0, False),\n   (0.3333333333333333, 6, 0.0, False)]},\n 8: {0: [(0.3333333333333333, 0, 0.0, False),\n   (0.3333333333333333, 8, 0.0, False),\n   (0.3333333333333333, 16, 0.0, False)],\n  1: [(0.3333333333333333, 8, 0.0, False),\n   (0.3333333333333333, 16, 0.0, False),\n   (0.3333333333333333, 9, 0.0, False)],\n  2: [(0.3333333333333333, 16, 0.0, False),\n   (0.3333333333333333, 9, 0.0, False),\n   (0.3333333333333333, 0, 0.0, False)],\n  3: [(0.3333333333333333, 9, 0.0, False),\n   (0.3333333333333333, 0, 0.0, False),\n   (0.3333333333333333, 8, 0.0, False)]},\n 9: {0: [(0.3333333333333333, 1, 0.0, False),\n   (0.3333333333333333, 8, 0.0, False),\n   (0.3333333333333333, 17, 0.0, False)],\n  1: [(0.3333333333333333, 8, 0.0, False),\n   (0.3333333333333333, 17, 0.0, False),\n   (0.3333333333333333, 10, 0.0, False)],\n  2: [(0.3333333333333333, 17, 0.0, False),\n   (0.3333333333333333, 10, 0.0, False),\n   (0.3333333333333333, 1, 0.0, False)],\n  3: [(0.3333333333333333, 10, 0.0, False),\n   (0.3333333333333333, 1, 0.0, False),\n   (0.3333333333333333, 8, 0.0, False)]},\n 10: {0: [(0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 9, 0.0, False),\n   (0.3333333333333333, 18, 0.0, False)],\n  1: [(0.3333333333333333, 9, 0.0, False),\n   (0.3333333333333333, 18, 0.0, False),\n   (0.3333333333333333, 11, 0.0, False)],\n  2: [(0.3333333333333333, 18, 0.0, False),\n   (0.3333333333333333, 11, 0.0, False),\n   (0.3333333333333333, 2, 0.0, False)],\n  3: [(0.3333333333333333, 11, 0.0, False),\n   (0.3333333333333333, 2, 0.0, False),\n   (0.3333333333333333, 9, 0.0, False)]},\n 11: {0: [(0.3333333333333333, 3, 0.0, False),\n   (0.3333333333333333, 10, 0.0, False),\n   (0.3333333333333333, 19, 0.0, True)],\n  1: [(0.3333333333333333, 10, 0.0, False),\n   (0.3333333333333333, 19, 0.0, True),\n   (0.3333333333333333, 12, 0.0, False)],\n  2: [(0.3333333333333333, 19, 0.0, True),\n   (0.3333333333333333, 12, 0.0, False),\n   (0.3333333333333333, 3, 0.0, False)],\n  3: [(0.3333333333333333, 12, 0.0, False),\n   (0.3333333333333333, 3, 0.0, False),\n   (0.3333333333333333, 10, 0.0, False)]},\n 12: {0: [(0.3333333333333333, 4, 0.0, False),\n   (0.3333333333333333, 11, 0.0, False),\n   (0.3333333333333333, 20, 0.0, False)],\n  1: [(0.3333333333333333, 11, 0.0, False),\n   (0.3333333333333333, 20, 0.0, False),\n   (0.3333333333333333, 13, 0.0, False)],\n  2: [(0.3333333333333333, 20, 0.0, False),\n   (0.3333333333333333, 13, 0.0, False),\n   (0.3333333333333333, 4, 0.0, False)],\n  3: [(0.3333333333333333, 13, 0.0, False),\n   (0.3333333333333333, 4, 0.0, False),\n   (0.3333333333333333, 11, 0.0, False)]},\n 13: {0: [(0.3333333333333333, 5, 0.0, False),\n   (0.3333333333333333, 12, 0.0, False),\n   (0.3333333333333333, 21, 0.0, False)],\n  1: [(0.3333333333333333, 12, 0.0, False),\n   (0.3333333333333333, 21, 0.0, False),\n   (0.3333333333333333, 14, 0.0, False)],\n  2: [(0.3333333333333333, 21, 0.0, False),\n   (0.3333333333333333, 14, 0.0, False),\n   (0.3333333333333333, 5, 0.0, False)],\n  3: [(0.3333333333333333, 14, 0.0, False),\n   (0.3333333333333333, 5, 0.0, False),\n   (0.3333333333333333, 12, 0.0, False)]},\n 14: {0: [(0.3333333333333333, 6, 0.0, False),\n   (0.3333333333333333, 13, 0.0, False),\n   (0.3333333333333333, 22, 0.0, False)],\n  1: [(0.3333333333333333, 13, 0.0, False),\n   (0.3333333333333333, 22, 0.0, False),\n   (0.3333333333333333, 15, 0.0, False)],\n  2: [(0.3333333333333333, 22, 0.0, False),\n   (0.3333333333333333, 15, 0.0, False),\n   (0.3333333333333333, 6, 0.0, False)],\n  3: [(0.3333333333333333, 15, 0.0, False),\n   (0.3333333333333333, 6, 0.0, False),\n   (0.3333333333333333, 13, 0.0, False)]},\n 15: {0: [(0.3333333333333333, 7, 0.0, False),\n   (0.3333333333333333, 14, 0.0, False),\n   (0.3333333333333333, 23, 0.0, False)],\n  1: [(0.3333333333333333, 14, 0.0, False),\n   (0.3333333333333333, 23, 0.0, False),\n   (0.3333333333333333, 15, 0.0, False)],\n  2: [(0.3333333333333333, 23, 0.0, False),\n   (0.3333333333333333, 15, 0.0, False),\n   (0.3333333333333333, 7, 0.0, False)],\n  3: [(0.3333333333333333, 15, 0.0, False),\n   (0.3333333333333333, 7, 0.0, False),\n   (0.3333333333333333, 14, 0.0, False)]},\n 16: {0: [(0.3333333333333333, 8, 0.0, False),\n   (0.3333333333333333, 16, 0.0, False),\n   (0.3333333333333333, 24, 0.0, False)],\n  1: [(0.3333333333333333, 16, 0.0, False),\n   (0.3333333333333333, 24, 0.0, False),\n   (0.3333333333333333, 17, 0.0, False)],\n  2: [(0.3333333333333333, 24, 0.0, False),\n   (0.3333333333333333, 17, 0.0, False),\n   (0.3333333333333333, 8, 0.0, False)],\n  3: [(0.3333333333333333, 17, 0.0, False),\n   (0.3333333333333333, 8, 0.0, False),\n   (0.3333333333333333, 16, 0.0, False)]},\n 17: {0: [(0.3333333333333333, 9, 0.0, False),\n   (0.3333333333333333, 16, 0.0, False),\n   (0.3333333333333333, 25, 0.0, False)],\n  1: [(0.3333333333333333, 16, 0.0, False),\n   (0.3333333333333333, 25, 0.0, False),\n   (0.3333333333333333, 18, 0.0, False)],\n  2: [(0.3333333333333333, 25, 0.0, False),\n   (0.3333333333333333, 18, 0.0, False),\n   (0.3333333333333333, 9, 0.0, False)],\n  3: [(0.3333333333333333, 18, 0.0, False),\n   (0.3333333333333333, 9, 0.0, False),\n   (0.3333333333333333, 16, 0.0, False)]},\n 18: {0: [(0.3333333333333333, 10, 0.0, False),\n   (0.3333333333333333, 17, 0.0, False),\n   (0.3333333333333333, 26, 0.0, False)],\n  1: [(0.3333333333333333, 17, 0.0, False),\n   (0.3333333333333333, 26, 0.0, False),\n   (0.3333333333333333, 19, 0.0, True)],\n  2: [(0.3333333333333333, 26, 0.0, False),\n   (0.3333333333333333, 19, 0.0, True),\n   (0.3333333333333333, 10, 0.0, False)],\n  3: [(0.3333333333333333, 19, 0.0, True),\n   (0.3333333333333333, 10, 0.0, False),\n   (0.3333333333333333, 17, 0.0, False)]},\n 19: {0: [(1.0, 19, 0, True)],\n  1: [(1.0, 19, 0, True)],\n  2: [(1.0, 19, 0, True)],\n  3: [(1.0, 19, 0, True)]},\n 20: {0: [(0.3333333333333333, 12, 0.0, False),\n   (0.3333333333333333, 19, 0.0, True),\n   (0.3333333333333333, 28, 0.0, False)],\n  1: [(0.3333333333333333, 19, 0.0, True),\n   (0.3333333333333333, 28, 0.0, False),\n   (0.3333333333333333, 21, 0.0, False)],\n  2: [(0.3333333333333333, 28, 0.0, False),\n   (0.3333333333333333, 21, 0.0, False),\n   (0.3333333333333333, 12, 0.0, False)],\n  3: [(0.3333333333333333, 21, 0.0, False),\n   (0.3333333333333333, 12, 0.0, False),\n   (0.3333333333333333, 19, 0.0, True)]},\n 21: {0: [(0.3333333333333333, 13, 0.0, False),\n   (0.3333333333333333, 20, 0.0, False),\n   (0.3333333333333333, 29, 0.0, True)],\n  1: [(0.3333333333333333, 20, 0.0, False),\n   (0.3333333333333333, 29, 0.0, True),\n   (0.3333333333333333, 22, 0.0, False)],\n  2: [(0.3333333333333333, 29, 0.0, True),\n   (0.3333333333333333, 22, 0.0, False),\n   (0.3333333333333333, 13, 0.0, False)],\n  3: [(0.3333333333333333, 22, 0.0, False),\n   (0.3333333333333333, 13, 0.0, False),\n   (0.3333333333333333, 20, 0.0, False)]},\n 22: {0: [(0.3333333333333333, 14, 0.0, False),\n   (0.3333333333333333, 21, 0.0, False),\n   (0.3333333333333333, 30, 0.0, False)],\n  1: [(0.3333333333333333, 21, 0.0, False),\n   (0.3333333333333333, 30, 0.0, False),\n   (0.3333333333333333, 23, 0.0, False)],\n  2: [(0.3333333333333333, 30, 0.0, False),\n   (0.3333333333333333, 23, 0.0, False),\n   (0.3333333333333333, 14, 0.0, False)],\n  3: [(0.3333333333333333, 23, 0.0, False),\n   (0.3333333333333333, 14, 0.0, False),\n   (0.3333333333333333, 21, 0.0, False)]},\n 23: {0: [(0.3333333333333333, 15, 0.0, False),\n   (0.3333333333333333, 22, 0.0, False),\n   (0.3333333333333333, 31, 0.0, False)],\n  1: [(0.3333333333333333, 22, 0.0, False),\n   (0.3333333333333333, 31, 0.0, False),\n   (0.3333333333333333, 23, 0.0, False)],\n  2: [(0.3333333333333333, 31, 0.0, False),\n   (0.3333333333333333, 23, 0.0, False),\n   (0.3333333333333333, 15, 0.0, False)],\n  3: [(0.3333333333333333, 23, 0.0, False),\n   (0.3333333333333333, 15, 0.0, False),\n   (0.3333333333333333, 22, 0.0, False)]},\n 24: {0: [(0.3333333333333333, 16, 0.0, False),\n   (0.3333333333333333, 24, 0.0, False),\n   (0.3333333333333333, 32, 0.0, False)],\n  1: [(0.3333333333333333, 24, 0.0, False),\n   (0.3333333333333333, 32, 0.0, False),\n   (0.3333333333333333, 25, 0.0, False)],\n  2: [(0.3333333333333333, 32, 0.0, False),\n   (0.3333333333333333, 25, 0.0, False),\n   (0.3333333333333333, 16, 0.0, False)],\n  3: [(0.3333333333333333, 25, 0.0, False),\n   (0.3333333333333333, 16, 0.0, False),\n   (0.3333333333333333, 24, 0.0, False)]},\n 25: {0: [(0.3333333333333333, 17, 0.0, False),\n   (0.3333333333333333, 24, 0.0, False),\n   (0.3333333333333333, 33, 0.0, False)],\n  1: [(0.3333333333333333, 24, 0.0, False),\n   (0.3333333333333333, 33, 0.0, False),\n   (0.3333333333333333, 26, 0.0, False)],\n  2: [(0.3333333333333333, 33, 0.0, False),\n   (0.3333333333333333, 26, 0.0, False),\n   (0.3333333333333333, 17, 0.0, False)],\n  3: [(0.3333333333333333, 26, 0.0, False),\n   (0.3333333333333333, 17, 0.0, False),\n   (0.3333333333333333, 24, 0.0, False)]},\n 26: {0: [(0.3333333333333333, 18, 0.0, False),\n   (0.3333333333333333, 25, 0.0, False),\n   (0.3333333333333333, 34, 0.0, False)],\n  1: [(0.3333333333333333, 25, 0.0, False),\n   (0.3333333333333333, 34, 0.0, False),\n   (0.3333333333333333, 27, 0.0, False)],\n  2: [(0.3333333333333333, 34, 0.0, False),\n   (0.3333333333333333, 27, 0.0, False),\n   (0.3333333333333333, 18, 0.0, False)],\n  3: [(0.3333333333333333, 27, 0.0, False),\n   (0.3333333333333333, 18, 0.0, False),\n   (0.3333333333333333, 25, 0.0, False)]},\n 27: {0: [(0.3333333333333333, 19, 0.0, True),\n   (0.3333333333333333, 26, 0.0, False),\n   (0.3333333333333333, 35, 0.0, True)],\n  1: [(0.3333333333333333, 26, 0.0, False),\n   (0.3333333333333333, 35, 0.0, True),\n   (0.3333333333333333, 28, 0.0, False)],\n  2: [(0.3333333333333333, 35, 0.0, True),\n   (0.3333333333333333, 28, 0.0, False),\n   (0.3333333333333333, 19, 0.0, True)],\n  3: [(0.3333333333333333, 28, 0.0, False),\n   (0.3333333333333333, 19, 0.0, True),\n   (0.3333333333333333, 26, 0.0, False)]},\n 28: {0: [(0.3333333333333333, 20, 0.0, False),\n   (0.3333333333333333, 27, 0.0, False),\n   (0.3333333333333333, 36, 0.0, False)],\n  1: [(0.3333333333333333, 27, 0.0, False),\n   (0.3333333333333333, 36, 0.0, False),\n   (0.3333333333333333, 29, 0.0, True)],\n  2: [(0.3333333333333333, 36, 0.0, False),\n   (0.3333333333333333, 29, 0.0, True),\n   (0.3333333333333333, 20, 0.0, False)],\n  3: [(0.3333333333333333, 29, 0.0, True),\n   (0.3333333333333333, 20, 0.0, False),\n   (0.3333333333333333, 27, 0.0, False)]},\n 29: {0: [(1.0, 29, 0, True)],\n  1: [(1.0, 29, 0, True)],\n  2: [(1.0, 29, 0, True)],\n  3: [(1.0, 29, 0, True)]},\n 30: {0: [(0.3333333333333333, 22, 0.0, False),\n   (0.3333333333333333, 29, 0.0, True),\n   (0.3333333333333333, 38, 0.0, False)],\n  1: [(0.3333333333333333, 29, 0.0, True),\n   (0.3333333333333333, 38, 0.0, False),\n   (0.3333333333333333, 31, 0.0, False)],\n  2: [(0.3333333333333333, 38, 0.0, False),\n   (0.3333333333333333, 31, 0.0, False),\n   (0.3333333333333333, 22, 0.0, False)],\n  3: [(0.3333333333333333, 31, 0.0, False),\n   (0.3333333333333333, 22, 0.0, False),\n   (0.3333333333333333, 29, 0.0, True)]},\n 31: {0: [(0.3333333333333333, 23, 0.0, False),\n   (0.3333333333333333, 30, 0.0, False),\n   (0.3333333333333333, 39, 0.0, False)],\n  1: [(0.3333333333333333, 30, 0.0, False),\n   (0.3333333333333333, 39, 0.0, False),\n   (0.3333333333333333, 31, 0.0, False)],\n  2: [(0.3333333333333333, 39, 0.0, False),\n   (0.3333333333333333, 31, 0.0, False),\n   (0.3333333333333333, 23, 0.0, False)],\n  3: [(0.3333333333333333, 31, 0.0, False),\n   (0.3333333333333333, 23, 0.0, False),\n   (0.3333333333333333, 30, 0.0, False)]},\n 32: {0: [(0.3333333333333333, 24, 0.0, False),\n   (0.3333333333333333, 32, 0.0, False),\n   (0.3333333333333333, 40, 0.0, False)],\n  1: [(0.3333333333333333, 32, 0.0, False),\n   (0.3333333333333333, 40, 0.0, False),\n   (0.3333333333333333, 33, 0.0, False)],\n  2: [(0.3333333333333333, 40, 0.0, False),\n   (0.3333333333333333, 33, 0.0, False),\n   (0.3333333333333333, 24, 0.0, False)],\n  3: [(0.3333333333333333, 33, 0.0, False),\n   (0.3333333333333333, 24, 0.0, False),\n   (0.3333333333333333, 32, 0.0, False)]},\n 33: {0: [(0.3333333333333333, 25, 0.0, False),\n   (0.3333333333333333, 32, 0.0, False),\n   (0.3333333333333333, 41, 0.0, True)],\n  1: [(0.3333333333333333, 32, 0.0, False),\n   (0.3333333333333333, 41, 0.0, True),\n   (0.3333333333333333, 34, 0.0, False)],\n  2: [(0.3333333333333333, 41, 0.0, True),\n   (0.3333333333333333, 34, 0.0, False),\n   (0.3333333333333333, 25, 0.0, False)],\n  3: [(0.3333333333333333, 34, 0.0, False),\n   (0.3333333333333333, 25, 0.0, False),\n   (0.3333333333333333, 32, 0.0, False)]},\n 34: {0: [(0.3333333333333333, 26, 0.0, False),\n   (0.3333333333333333, 33, 0.0, False),\n   (0.3333333333333333, 42, 0.0, True)],\n  1: [(0.3333333333333333, 33, 0.0, False),\n   (0.3333333333333333, 42, 0.0, True),\n   (0.3333333333333333, 35, 0.0, True)],\n  2: [(0.3333333333333333, 42, 0.0, True),\n   (0.3333333333333333, 35, 0.0, True),\n   (0.3333333333333333, 26, 0.0, False)],\n  3: [(0.3333333333333333, 35, 0.0, True),\n   (0.3333333333333333, 26, 0.0, False),\n   (0.3333333333333333, 33, 0.0, False)]},\n 35: {0: [(1.0, 35, 0, True)],\n  1: [(1.0, 35, 0, True)],\n  2: [(1.0, 35, 0, True)],\n  3: [(1.0, 35, 0, True)]},\n 36: {0: [(0.3333333333333333, 28, 0.0, False),\n   (0.3333333333333333, 35, 0.0, True),\n   (0.3333333333333333, 44, 0.0, False)],\n  1: [(0.3333333333333333, 35, 0.0, True),\n   (0.3333333333333333, 44, 0.0, False),\n   (0.3333333333333333, 37, 0.0, False)],\n  2: [(0.3333333333333333, 44, 0.0, False),\n   (0.3333333333333333, 37, 0.0, False),\n   (0.3333333333333333, 28, 0.0, False)],\n  3: [(0.3333333333333333, 37, 0.0, False),\n   (0.3333333333333333, 28, 0.0, False),\n   (0.3333333333333333, 35, 0.0, True)]},\n 37: {0: [(0.3333333333333333, 29, 0.0, True),\n   (0.3333333333333333, 36, 0.0, False),\n   (0.3333333333333333, 45, 0.0, False)],\n  1: [(0.3333333333333333, 36, 0.0, False),\n   (0.3333333333333333, 45, 0.0, False),\n   (0.3333333333333333, 38, 0.0, False)],\n  2: [(0.3333333333333333, 45, 0.0, False),\n   (0.3333333333333333, 38, 0.0, False),\n   (0.3333333333333333, 29, 0.0, True)],\n  3: [(0.3333333333333333, 38, 0.0, False),\n   (0.3333333333333333, 29, 0.0, True),\n   (0.3333333333333333, 36, 0.0, False)]},\n 38: {0: [(0.3333333333333333, 30, 0.0, False),\n   (0.3333333333333333, 37, 0.0, False),\n   (0.3333333333333333, 46, 0.0, True)],\n  1: [(0.3333333333333333, 37, 0.0, False),\n   (0.3333333333333333, 46, 0.0, True),\n   (0.3333333333333333, 39, 0.0, False)],\n  2: [(0.3333333333333333, 46, 0.0, True),\n   (0.3333333333333333, 39, 0.0, False),\n   (0.3333333333333333, 30, 0.0, False)],\n  3: [(0.3333333333333333, 39, 0.0, False),\n   (0.3333333333333333, 30, 0.0, False),\n   (0.3333333333333333, 37, 0.0, False)]},\n 39: {0: [(0.3333333333333333, 31, 0.0, False),\n   (0.3333333333333333, 38, 0.0, False),\n   (0.3333333333333333, 47, 0.0, False)],\n  1: [(0.3333333333333333, 38, 0.0, False),\n   (0.3333333333333333, 47, 0.0, False),\n   (0.3333333333333333, 39, 0.0, False)],\n  2: [(0.3333333333333333, 47, 0.0, False),\n   (0.3333333333333333, 39, 0.0, False),\n   (0.3333333333333333, 31, 0.0, False)],\n  3: [(0.3333333333333333, 39, 0.0, False),\n   (0.3333333333333333, 31, 0.0, False),\n   (0.3333333333333333, 38, 0.0, False)]},\n 40: {0: [(0.3333333333333333, 32, 0.0, False),\n   (0.3333333333333333, 40, 0.0, False),\n   (0.3333333333333333, 48, 0.0, False)],\n  1: [(0.3333333333333333, 40, 0.0, False),\n   (0.3333333333333333, 48, 0.0, False),\n   (0.3333333333333333, 41, 0.0, True)],\n  2: [(0.3333333333333333, 48, 0.0, False),\n   (0.3333333333333333, 41, 0.0, True),\n   (0.3333333333333333, 32, 0.0, False)],\n  3: [(0.3333333333333333, 41, 0.0, True),\n   (0.3333333333333333, 32, 0.0, False),\n   (0.3333333333333333, 40, 0.0, False)]},\n 41: {0: [(1.0, 41, 0, True)],\n  1: [(1.0, 41, 0, True)],\n  2: [(1.0, 41, 0, True)],\n  3: [(1.0, 41, 0, True)]},\n 42: {0: [(1.0, 42, 0, True)],\n  1: [(1.0, 42, 0, True)],\n  2: [(1.0, 42, 0, True)],\n  3: [(1.0, 42, 0, True)]},\n 43: {0: [(0.3333333333333333, 35, 0.0, True),\n   (0.3333333333333333, 42, 0.0, True),\n   (0.3333333333333333, 51, 0.0, False)],\n  1: [(0.3333333333333333, 42, 0.0, True),\n   (0.3333333333333333, 51, 0.0, False),\n   (0.3333333333333333, 44, 0.0, False)],\n  2: [(0.3333333333333333, 51, 0.0, False),\n   (0.3333333333333333, 44, 0.0, False),\n   (0.3333333333333333, 35, 0.0, True)],\n  3: [(0.3333333333333333, 44, 0.0, False),\n   (0.3333333333333333, 35, 0.0, True),\n   (0.3333333333333333, 42, 0.0, True)]},\n 44: {0: [(0.3333333333333333, 36, 0.0, False),\n   (0.3333333333333333, 43, 0.0, False),\n   (0.3333333333333333, 52, 0.0, True)],\n  1: [(0.3333333333333333, 43, 0.0, False),\n   (0.3333333333333333, 52, 0.0, True),\n   (0.3333333333333333, 45, 0.0, False)],\n  2: [(0.3333333333333333, 52, 0.0, True),\n   (0.3333333333333333, 45, 0.0, False),\n   (0.3333333333333333, 36, 0.0, False)],\n  3: [(0.3333333333333333, 45, 0.0, False),\n   (0.3333333333333333, 36, 0.0, False),\n   (0.3333333333333333, 43, 0.0, False)]},\n 45: {0: [(0.3333333333333333, 37, 0.0, False),\n   (0.3333333333333333, 44, 0.0, False),\n   (0.3333333333333333, 53, 0.0, False)],\n  1: [(0.3333333333333333, 44, 0.0, False),\n   (0.3333333333333333, 53, 0.0, False),\n   (0.3333333333333333, 46, 0.0, True)],\n  2: [(0.3333333333333333, 53, 0.0, False),\n   (0.3333333333333333, 46, 0.0, True),\n   (0.3333333333333333, 37, 0.0, False)],\n  3: [(0.3333333333333333, 46, 0.0, True),\n   (0.3333333333333333, 37, 0.0, False),\n   (0.3333333333333333, 44, 0.0, False)]},\n 46: {0: [(1.0, 46, 0, True)],\n  1: [(1.0, 46, 0, True)],\n  2: [(1.0, 46, 0, True)],\n  3: [(1.0, 46, 0, True)]},\n 47: {0: [(0.3333333333333333, 39, 0.0, False),\n   (0.3333333333333333, 46, 0.0, True),\n   (0.3333333333333333, 55, 0.0, False)],\n  1: [(0.3333333333333333, 46, 0.0, True),\n   (0.3333333333333333, 55, 0.0, False),\n   (0.3333333333333333, 47, 0.0, False)],\n  2: [(0.3333333333333333, 55, 0.0, False),\n   (0.3333333333333333, 47, 0.0, False),\n   (0.3333333333333333, 39, 0.0, False)],\n  3: [(0.3333333333333333, 47, 0.0, False),\n   (0.3333333333333333, 39, 0.0, False),\n   (0.3333333333333333, 46, 0.0, True)]},\n 48: {0: [(0.3333333333333333, 40, 0.0, False),\n   (0.3333333333333333, 48, 0.0, False),\n   (0.3333333333333333, 56, 0.0, False)],\n  1: [(0.3333333333333333, 48, 0.0, False),\n   (0.3333333333333333, 56, 0.0, False),\n   (0.3333333333333333, 49, 0.0, True)],\n  2: [(0.3333333333333333, 56, 0.0, False),\n   (0.3333333333333333, 49, 0.0, True),\n   (0.3333333333333333, 40, 0.0, False)],\n  3: [(0.3333333333333333, 49, 0.0, True),\n   (0.3333333333333333, 40, 0.0, False),\n   (0.3333333333333333, 48, 0.0, False)]},\n 49: {0: [(1.0, 49, 0, True)],\n  1: [(1.0, 49, 0, True)],\n  2: [(1.0, 49, 0, True)],\n  3: [(1.0, 49, 0, True)]},\n 50: {0: [(0.3333333333333333, 42, 0.0, True),\n   (0.3333333333333333, 49, 0.0, True),\n   (0.3333333333333333, 58, 0.0, False)],\n  1: [(0.3333333333333333, 49, 0.0, True),\n   (0.3333333333333333, 58, 0.0, False),\n   (0.3333333333333333, 51, 0.0, False)],\n  2: [(0.3333333333333333, 58, 0.0, False),\n   (0.3333333333333333, 51, 0.0, False),\n   (0.3333333333333333, 42, 0.0, True)],\n  3: [(0.3333333333333333, 51, 0.0, False),\n   (0.3333333333333333, 42, 0.0, True),\n   (0.3333333333333333, 49, 0.0, True)]},\n 51: {0: [(0.3333333333333333, 43, 0.0, False),\n   (0.3333333333333333, 50, 0.0, False),\n   (0.3333333333333333, 59, 0.0, True)],\n  1: [(0.3333333333333333, 50, 0.0, False),\n   (0.3333333333333333, 59, 0.0, True),\n   (0.3333333333333333, 52, 0.0, True)],\n  2: [(0.3333333333333333, 59, 0.0, True),\n   (0.3333333333333333, 52, 0.0, True),\n   (0.3333333333333333, 43, 0.0, False)],\n  3: [(0.3333333333333333, 52, 0.0, True),\n   (0.3333333333333333, 43, 0.0, False),\n   (0.3333333333333333, 50, 0.0, False)]},\n 52: {0: [(1.0, 52, 0, True)],\n  1: [(1.0, 52, 0, True)],\n  2: [(1.0, 52, 0, True)],\n  3: [(1.0, 52, 0, True)]},\n 53: {0: [(0.3333333333333333, 45, 0.0, False),\n   (0.3333333333333333, 52, 0.0, True),\n   (0.3333333333333333, 61, 0.0, False)],\n  1: [(0.3333333333333333, 52, 0.0, True),\n   (0.3333333333333333, 61, 0.0, False),\n   (0.3333333333333333, 54, 0.0, True)],\n  2: [(0.3333333333333333, 61, 0.0, False),\n   (0.3333333333333333, 54, 0.0, True),\n   (0.3333333333333333, 45, 0.0, False)],\n  3: [(0.3333333333333333, 54, 0.0, True),\n   (0.3333333333333333, 45, 0.0, False),\n   (0.3333333333333333, 52, 0.0, True)]},\n 54: {0: [(1.0, 54, 0, True)],\n  1: [(1.0, 54, 0, True)],\n  2: [(1.0, 54, 0, True)],\n  3: [(1.0, 54, 0, True)]},\n 55: {0: [(0.3333333333333333, 47, 0.0, False),\n   (0.3333333333333333, 54, 0.0, True),\n   (0.3333333333333333, 63, 1.0, True)],\n  1: [(0.3333333333333333, 54, 0.0, True),\n   (0.3333333333333333, 63, 1.0, True),\n   (0.3333333333333333, 55, 0.0, False)],\n  2: [(0.3333333333333333, 63, 1.0, True),\n   (0.3333333333333333, 55, 0.0, False),\n   (0.3333333333333333, 47, 0.0, False)],\n  3: [(0.3333333333333333, 55, 0.0, False),\n   (0.3333333333333333, 47, 0.0, False),\n   (0.3333333333333333, 54, 0.0, True)]},\n 56: {0: [(0.3333333333333333, 48, 0.0, False),\n   (0.3333333333333333, 56, 0.0, False),\n   (0.3333333333333333, 56, 0.0, False)],\n  1: [(0.3333333333333333, 56, 0.0, False),\n   (0.3333333333333333, 56, 0.0, False),\n   (0.3333333333333333, 57, 0.0, False)],\n  2: [(0.3333333333333333, 56, 0.0, False),\n   (0.3333333333333333, 57, 0.0, False),\n   (0.3333333333333333, 48, 0.0, False)],\n  3: [(0.3333333333333333, 57, 0.0, False),\n   (0.3333333333333333, 48, 0.0, False),\n   (0.3333333333333333, 56, 0.0, False)]},\n 57: {0: [(0.3333333333333333, 49, 0.0, True),\n   (0.3333333333333333, 56, 0.0, False),\n   (0.3333333333333333, 57, 0.0, False)],\n  1: [(0.3333333333333333, 56, 0.0, False),\n   (0.3333333333333333, 57, 0.0, False),\n   (0.3333333333333333, 58, 0.0, False)],\n  2: [(0.3333333333333333, 57, 0.0, False),\n   (0.3333333333333333, 58, 0.0, False),\n   (0.3333333333333333, 49, 0.0, True)],\n  3: [(0.3333333333333333, 58, 0.0, False),\n   (0.3333333333333333, 49, 0.0, True),\n   (0.3333333333333333, 56, 0.0, False)]},\n 58: {0: [(0.3333333333333333, 50, 0.0, False),\n   (0.3333333333333333, 57, 0.0, False),\n   (0.3333333333333333, 58, 0.0, False)],\n  1: [(0.3333333333333333, 57, 0.0, False),\n   (0.3333333333333333, 58, 0.0, False),\n   (0.3333333333333333, 59, 0.0, True)],\n  2: [(0.3333333333333333, 58, 0.0, False),\n   (0.3333333333333333, 59, 0.0, True),\n   (0.3333333333333333, 50, 0.0, False)],\n  3: [(0.3333333333333333, 59, 0.0, True),\n   (0.3333333333333333, 50, 0.0, False),\n   (0.3333333333333333, 57, 0.0, False)]},\n 59: {0: [(1.0, 59, 0, True)],\n  1: [(1.0, 59, 0, True)],\n  2: [(1.0, 59, 0, True)],\n  3: [(1.0, 59, 0, True)]},\n 60: {0: [(0.3333333333333333, 52, 0.0, True),\n   (0.3333333333333333, 59, 0.0, True),\n   (0.3333333333333333, 60, 0.0, False)],\n  1: [(0.3333333333333333, 59, 0.0, True),\n   (0.3333333333333333, 60, 0.0, False),\n   (0.3333333333333333, 61, 0.0, False)],\n  2: [(0.3333333333333333, 60, 0.0, False),\n   (0.3333333333333333, 61, 0.0, False),\n   (0.3333333333333333, 52, 0.0, True)],\n  3: [(0.3333333333333333, 61, 0.0, False),\n   (0.3333333333333333, 52, 0.0, True),\n   (0.3333333333333333, 59, 0.0, True)]},\n 61: {0: [(0.3333333333333333, 53, 0.0, False),\n   (0.3333333333333333, 60, 0.0, False),\n   (0.3333333333333333, 61, 0.0, False)],\n  1: [(0.3333333333333333, 60, 0.0, False),\n   (0.3333333333333333, 61, 0.0, False),\n   (0.3333333333333333, 62, 0.0, False)],\n  2: [(0.3333333333333333, 61, 0.0, False),\n   (0.3333333333333333, 62, 0.0, False),\n   (0.3333333333333333, 53, 0.0, False)],\n  3: [(0.3333333333333333, 62, 0.0, False),\n   (0.3333333333333333, 53, 0.0, False),\n   (0.3333333333333333, 60, 0.0, False)]},\n 62: {0: [(0.3333333333333333, 54, 0.0, True),\n   (0.3333333333333333, 61, 0.0, False),\n   (0.3333333333333333, 62, 0.0, False)],\n  1: [(0.3333333333333333, 61, 0.0, False),\n   (0.3333333333333333, 62, 0.0, False),\n   (0.3333333333333333, 63, 1.0, True)],\n  2: [(0.3333333333333333, 62, 0.0, False),\n   (0.3333333333333333, 63, 1.0, True),\n   (0.3333333333333333, 54, 0.0, True)],\n  3: [(0.3333333333333333, 63, 1.0, True),\n   (0.3333333333333333, 54, 0.0, True),\n   (0.3333333333333333, 61, 0.0, False)]},\n 63: {0: [(1.0, 63, 0, True)],\n  1: [(1.0, 63, 0, True)],\n  2: [(1.0, 63, 0, True)],\n  3: [(1.0, 63, 0, True)]}}"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-4.html#tldr",
    "href": "publication/GDRL/GDRL-chapter-4.html#tldr",
    "title": "Chapter 4: Balancing the gathering and use of information",
    "section": "TL;DR",
    "text": "TL;DR\n\n그로킹 심층 강화학습 중 4장 내용인 “정보의 수집과 사용간의 균형”에 대한 내용입니다.\n\n\n\nCode\n!pip install tqdm numpy scikit-learn pyglet setuptools && \\\n!pip install gym asciinema pandas tabulate tornado==5.* PyBullet && \\\n!pip install git+https://github.com/pybox2d/pybox2d#egg=Box2D && \\\n!pip install git+https://github.com/mimoralea/gym-bandits#egg=gym-bandits && \\\n!pip install git+https://github.com/mimoralea/gym-walk#egg=gym-walk && \\\n!pip install git+https://github.com/mimoralea/gym-aima#egg=gym-aima && \\\n!pip install gym[atari]"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-4.html#밴딧-bandits",
    "href": "publication/GDRL/GDRL-chapter-4.html#밴딧-bandits",
    "title": "Chapter 4: Balancing the gathering and use of information",
    "section": "밴딧 (Bandits)",
    "text": "밴딧 (Bandits)\n\nimport warnings ; warnings.filterwarnings('ignore')\n\nimport gym\nimport gym_bandits\nimport numpy as np\nfrom scipy.special import softmax as softmax_fn\nfrom pprint import pprint\nfrom tqdm import tqdm_notebook as tqdm\n\nfrom itertools import cycle\n\nimport sys\nimport random\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nSEEDS = (12, 34, 56, 78, 90)\n\n%matplotlib inline\n\n\nplt.style.use('fivethirtyeight')\nparams = {\n    'figure.figsize': (15, 8),\n    'font.size': 24,\n    'legend.fontsize': 20,\n    'axes.titlesize': 28,\n    'axes.labelsize': 24,\n    'xtick.labelsize': 20,\n    'ytick.labelsize': 20\n}\npylab.rcParams.update(params)\nnp.set_printoptions(suppress=True)\n\n\n기본 전략\n\ndef pure_exploitation(env, n_episodes=1000):\n    Q = np.zeros((env.action_space.n), dtype=np.float64)\n    N = np.zeros((env.action_space.n), dtype=np.int)\n\n    Qe = np.empty((n_episodes, env.action_space.n), dtype=np.float64)\n    returns = np.empty(n_episodes, dtype=np.float64)\n    actions = np.empty(n_episodes, dtype=np.int)\n    name = 'Pure exploitation'\n    for e in tqdm(range(n_episodes), \n                  desc='Episodes for: ' + name, \n                  leave=False):\n        action = np.argmax(Q)\n\n        _, reward, _, _ = env.step(action)\n        N[action] += 1\n        Q[action] = Q[action] + (reward - Q[action])/N[action]\n        \n        Qe[e] = Q\n        returns[e] = reward\n        actions[e] = action\n    return name, returns, Qe, actions\n\n\ndef pure_exploration(env, n_episodes=1000):\n    Q = np.zeros((env.action_space.n), dtype=np.float64)\n    N = np.zeros((env.action_space.n), dtype=np.int)\n\n    Qe = np.empty((n_episodes, env.action_space.n), dtype=np.float64)\n    returns = np.empty(n_episodes, dtype=np.float64)\n    actions = np.empty(n_episodes, dtype=np.int)\n    name = 'Pure exploration'\n    for e in tqdm(range(n_episodes), \n                  desc='Episodes for: ' + name, \n                  leave=False):\n        action = np.random.randint(len(Q))\n        \n        _, reward, _, _ = env.step(action)\n        N[action] += 1\n        Q[action] = Q[action] + (reward - Q[action])/N[action]\n        \n        Qe[e] = Q\n        returns[e] = reward\n        actions[e] = action\n    return name, returns, Qe, actions\n\n\n\n간단한 전략들\n\ndef epsilon_greedy(env, epsilon=0.01, n_episodes=1000):\n    Q = np.zeros((env.action_space.n), dtype=np.float64)\n    N = np.zeros((env.action_space.n), dtype=np.int)\n\n    Qe = np.empty((n_episodes, env.action_space.n), dtype=np.float64)\n    returns = np.empty(n_episodes, dtype=np.float64)\n    actions = np.empty(n_episodes, dtype=np.int)\n    name = 'Epsilon-Greedy {}'.format(epsilon)\n    for e in tqdm(range(n_episodes), \n                  desc='Episodes for: ' + name, \n                  leave=False):\n        if np.random.uniform() &gt; epsilon:\n            action = np.argmax(Q)\n        else:\n            action = np.random.randint(len(Q))\n\n        _, reward, _, _ = env.step(action)\n        N[action] += 1\n        Q[action] = Q[action] + (reward - Q[action])/N[action]\n        \n        Qe[e] = Q\n        returns[e] = reward\n        actions[e] = action\n    return name, returns, Qe, actions\n\n\ndef lin_dec_epsilon_greedy(env,\n                           init_epsilon=1.0,\n                           min_epsilon=0.01, \n                           decay_ratio=0.05, \n                           n_episodes=1000):\n    Q = np.zeros((env.action_space.n), dtype=np.float64)\n    N = np.zeros((env.action_space.n), dtype=np.int)\n\n    Qe = np.empty((n_episodes, env.action_space.n), dtype=np.float64)\n    returns = np.empty(n_episodes, dtype=np.float64)\n    actions = np.empty(n_episodes, dtype=np.int)\n    name = 'Lin Epsilon-Greedy {}, {}, {}'.format(init_epsilon, \n                                          min_epsilon, \n                                          decay_ratio)\n    for e in tqdm(range(n_episodes), \n                  desc='Episodes for: ' + name, \n                  leave=False):\n        decay_episodes = n_episodes * decay_ratio\n        epsilon = 1 - e / decay_episodes\n        epsilon *= init_epsilon - min_epsilon\n        epsilon += min_epsilon\n        epsilon = np.clip(epsilon, min_epsilon, init_epsilon)\n        if np.random.uniform() &gt; epsilon:\n            action = np.argmax(Q)\n        else:\n            action = np.random.randint(len(Q))\n\n        _, reward, _, _ = env.step(action)\n        N[action] += 1\n        Q[action] = Q[action] + (reward - Q[action])/N[action]\n        \n        Qe[e] = Q\n        returns[e] = reward\n        actions[e] = action\n    return name, returns, Qe, actions\n\n\ndef exp_dec_epsilon_greedy(env, \n                           init_epsilon=1.0,\n                           min_epsilon=0.01,\n                           decay_ratio=0.1,\n                           n_episodes=1000):\n    Q = np.zeros((env.action_space.n), dtype=np.float64)\n    N = np.zeros((env.action_space.n), dtype=np.int)\n\n    Qe = np.empty((n_episodes, env.action_space.n), dtype=np.float64)\n    returns = np.empty(n_episodes, dtype=np.float64)\n    actions = np.empty(n_episodes, dtype=np.int)\n\n    decay_episodes = int(n_episodes * decay_ratio)\n    rem_episodes = n_episodes - decay_episodes\n    epsilons = 0.01\n    epsilons /= np.logspace(-2, 0, decay_episodes)\n    epsilons *= init_epsilon - min_epsilon\n    epsilons += min_epsilon\n    epsilons = np.pad(epsilons, (0, rem_episodes), 'edge')\n    name = 'Exp Epsilon-Greedy {}, {}, {}'.format(init_epsilon, \n                                          min_epsilon, \n                                          decay_ratio)\n    for e in tqdm(range(n_episodes), \n                  desc='Episodes for: ' + name, \n                  leave=False):\n        if np.random.uniform() &gt; epsilons[e]:\n            action = np.argmax(Q)\n        else:\n            action = np.random.randint(len(Q))\n\n        _, reward, _, _ = env.step(action)\n        N[action] += 1\n        Q[action] = Q[action] + (reward - Q[action])/N[action]\n        \n        Qe[e] = Q\n        returns[e] = reward\n        actions[e] = action\n    return name, returns, Qe, actions\n\n\ndef optimistic_initialization(env, \n                              optimistic_estimate=1.0,\n                              initial_count=100,\n                              n_episodes=1000):\n    Q = np.full((env.action_space.n), optimistic_estimate, dtype=np.float64)\n    N = np.full((env.action_space.n), initial_count, dtype=np.int)\n    \n    Qe = np.empty((n_episodes, env.action_space.n), dtype=np.float64)\n    returns = np.empty(n_episodes, dtype=np.float64)\n    actions = np.empty(n_episodes, dtype=np.int)\n    name = 'Optimistic {}, {}'.format(optimistic_estimate, \n                                     initial_count)\n    for e in tqdm(range(n_episodes), \n                  desc='Episodes for: ' + name, \n                  leave=False):\n        action = np.argmax(Q)\n\n        _, reward, _, _ = env.step(action)\n        N[action] += 1\n        Q[action] = Q[action] + (reward - Q[action])/N[action]\n\n        Qe[e] = Q\n        returns[e] = reward\n        actions[e] = action\n    return name, returns, Qe, actions\n\n\n\n두 개의 팔을 가진 밴딧 환경\n\nb2_Vs = []\nfor seed in SEEDS:\n    env_name = 'BanditTwoArmedUniform-v0'\n    env = gym.make(env_name, seed=seed) ; env.reset()\n    b2_Q = np.array(env.env.p_dist * env.env.r_dist)\n    print('Two-Armed Bandit environment with seed', seed)\n    print('Probability of reward:', env.env.p_dist)\n    print('Reward:', env.env.r_dist)\n    print('Q(.):', b2_Q)\n    b2_Vs.append(np.max(b2_Q))\n    print('V*:', b2_Vs[-1])\n    print()\nprint('Mean V* across all seeds:', np.mean(b2_Vs))\n\nTwo-Armed Bandit environment with seed 12\nProbability of reward: [0.41630234 0.5545003 ]\nReward: [1 1]\nQ(.): [0.41630234 0.5545003 ]\nV*: 0.5545003042316209\n\nTwo-Armed Bandit environment with seed 34\nProbability of reward: [0.88039337 0.56881791]\nReward: [1 1]\nQ(.): [0.88039337 0.56881791]\nV*: 0.8803933660102791\n\nTwo-Armed Bandit environment with seed 56\nProbability of reward: [0.44859284 0.9499771 ]\nReward: [1 1]\nQ(.): [0.44859284 0.9499771 ]\nV*: 0.9499771030206514\n\nTwo-Armed Bandit environment with seed 78\nProbability of reward: [0.53235706 0.84511988]\nReward: [1 1]\nQ(.): [0.53235706 0.84511988]\nV*: 0.8451198776828125\n\nTwo-Armed Bandit environment with seed 90\nProbability of reward: [0.56461729 0.91744039]\nReward: [1 1]\nQ(.): [0.56461729 0.91744039]\nV*: 0.9174403942290458\n\nMean V* across all seeds: 0.8294862090348818\n\n\n\n두 팔 밴딧 환경에서 간단한 전략 수행\n\ndef b2_run_simple_strategies_experiment(env_name='BanditTwoArmedUniform-v0'):\n    results = {}\n    experiments = [\n        # baseline strategies\n        lambda env: pure_exploitation(env),\n        lambda env: pure_exploration(env),\n\n        # epsilon greedy\n        lambda env: epsilon_greedy(env, epsilon=0.07),\n        lambda env: epsilon_greedy(env, epsilon=0.1),\n\n        # epsilon greedy linearly decaying\n        lambda env: lin_dec_epsilon_greedy(env, \n                                           init_epsilon=1.0,\n                                           min_epsilon=0.0,\n                                           decay_ratio=0.1),\n        lambda env: lin_dec_epsilon_greedy(env, \n                                           init_epsilon=0.3,\n                                           min_epsilon=0.001,\n                                           decay_ratio=0.1),\n\n        # epsilon greedy exponentially decaying\n        lambda env: exp_dec_epsilon_greedy(env, \n                                           init_epsilon=1.0, \n                                           min_epsilon=0.0,\n                                           decay_ratio=0.1),\n        lambda env: exp_dec_epsilon_greedy(env, \n                                           init_epsilon=0.3, \n                                           min_epsilon=0.0, \n                                           decay_ratio=0.3),\n\n        # optimistic\n        lambda env: optimistic_initialization(env, \n                                              optimistic_estimate=1.0, \n                                              initial_count=10),\n        lambda env: optimistic_initialization(env, \n                                              optimistic_estimate=1.0, \n                                              initial_count=50),\n    ]\n    for env_seed in tqdm(SEEDS, desc='All experiments'):\n        env = gym.make(env_name, seed=env_seed) ; env.reset()\n        true_Q = np.array(env.env.p_dist * env.env.r_dist)\n        opt_V = np.max(true_Q)\n        for seed in tqdm(SEEDS, desc='All environments', leave=False):\n            for experiment in tqdm(experiments, \n                                   desc='Experiments with seed {}'.format(seed), \n                                   leave=False):\n                env.seed(seed) ; np.random.seed(seed) ; random.seed(seed)\n                name, Re, Qe, Ae = experiment(env)\n                Ae = np.expand_dims(Ae, -1)\n                episode_mean_rew = np.cumsum(Re) / (np.arange(len(Re)) + 1)\n                Q_selected = np.take_along_axis(\n                    np.tile(true_Q, Ae.shape), Ae, axis=1).squeeze()\n                regret = opt_V - Q_selected\n                cum_regret = np.cumsum(regret)\n                if name not in results.keys(): results[name] = {}\n                if 'Re' not in results[name].keys(): results[name]['Re'] = []\n                if 'Qe' not in results[name].keys(): results[name]['Qe'] = []\n                if 'Ae' not in results[name].keys(): results[name]['Ae'] = []\n                if 'cum_regret' not in results[name].keys(): \n                    results[name]['cum_regret'] = []\n                if 'episode_mean_rew' not in results[name].keys(): \n                    results[name]['episode_mean_rew'] = []\n\n                results[name]['Re'].append(Re)\n                results[name]['Qe'].append(Qe)\n                results[name]['Ae'].append(Ae)\n                results[name]['cum_regret'].append(cum_regret)\n                results[name]['episode_mean_rew'].append(episode_mean_rew)\n    return results\n\nb2_results_s = b2_run_simple_strategies_experiment()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n실행에 대한 결과 그래프 출력\n\nfig, axs = plt.subplots(5, 1, figsize=(28, 28), sharey=False, sharex=False)\n\nlines = [\"-\",\"--\",\":\",\"-.\"]\nlinecycler = cycle(lines)\nmin_reg, max_ret = float('inf'), float('-inf')\nfor label, result in b2_results_s.items():\n    color = next(linecycler)\n\n    # reward\n    episode_mean_rew = np.array(result['episode_mean_rew'])\n    mean_episode_mean_rew = np.mean(episode_mean_rew, axis=0)\n\n    axs[0].plot(mean_episode_mean_rew, color, linewidth=2, label=label)\n\n    axs[1].plot(mean_episode_mean_rew, color, linewidth=2, label=label)\n    axs[1].set_xscale('log')\n    \n    axs[2].plot(mean_episode_mean_rew, color, linewidth=2, label=label)\n    if max_ret &lt; mean_episode_mean_rew[-1]: max_ret = mean_episode_mean_rew[-1]\n    axs[2].axis((mean_episode_mean_rew.shape[0]*0.989,\n                 mean_episode_mean_rew.shape[0],\n                 max_ret-0.005,\n                 max_ret+0.0001))\n\n    # regret\n    cum_regret = np.array(result['cum_regret'])\n    mean_cum_regret = np.mean(cum_regret, axis=0)\n\n    axs[3].plot(mean_cum_regret, color, linewidth=2, label=label)\n    \n    axs[4].plot(mean_cum_regret, color, linewidth=2, label=label)\n    if min_reg &gt; mean_cum_regret[-1]: min_reg = mean_cum_regret[-1]\n    plt.axis((mean_cum_regret.shape[0]*0.989,\n              mean_cum_regret.shape[0],\n              min_reg-0.5,\n              min_reg+5))\n\n    # config plot\n    axs[0].set_title('Mean Episode Reward')\n    axs[1].set_title('Mean Episode Reward (Log scale)')\n    axs[2].set_title('Mean Episode Reward (Zoom on best)')\n    axs[3].set_title('Total Regret')\n    axs[4].set_title('Total Regret (Zoom on best)')\n    plt.xlabel('Episodes')\n    axs[0].legend(loc='upper left')\n\nplt.show()\n\n\n\n\n\n\n\n조금 더 발전된 전략들\n\ndef softmax(env, \n            init_temp=float('inf'), \n            min_temp=0.0,\n            decay_ratio=0.04,\n            n_episodes=1000):\n    Q = np.zeros((env.action_space.n), dtype=np.float64)\n    N = np.zeros((env.action_space.n), dtype=np.int)\n\n    Qe = np.empty((n_episodes, env.action_space.n), dtype=np.float64)\n    returns = np.empty(n_episodes, dtype=np.float64)\n    actions = np.empty(n_episodes, dtype=np.int)\n    name = 'Lin SoftMax {}, {}, {}'.format(init_temp, \n                                           min_temp,\n                                           decay_ratio)\n    # can't really use infinity\n    init_temp = min(init_temp,\n                    sys.float_info.max)\n    # can't really use zero\n    min_temp = max(min_temp,\n                   np.nextafter(np.float32(0), \n                                np.float32(1)))\n    for e in tqdm(range(n_episodes),\n                  desc='Episodes for: ' + name, \n                  leave=False):\n        decay_episodes = n_episodes * decay_ratio\n        temp = 1 - e / decay_episodes\n        temp *= init_temp - min_temp\n        temp += min_temp\n        temp = np.clip(temp, min_temp, init_temp)\n\n        scaled_Q = Q / temp\n        norm_Q = scaled_Q - np.max(scaled_Q)\n        exp_Q = np.exp(norm_Q)\n        probs = exp_Q / np.sum(exp_Q)\n        assert np.isclose(probs.sum(), 1.0)\n\n        action = np.random.choice(np.arange(len(probs)), \n                                  size=1, \n                                  p=probs)[0]\n\n        _, reward, _, _ = env.step(action)\n        N[action] += 1\n        Q[action] = Q[action] + (reward - Q[action])/N[action]\n        \n        Qe[e] = Q\n        returns[e] = reward\n        actions[e] = action\n    return name, returns, Qe, actions\n\n\ndef upper_confidence_bound(env, \n                           c=2, \n                           n_episodes=1000):\n    Q = np.zeros((env.action_space.n), dtype=np.float64)\n    N = np.zeros((env.action_space.n), dtype=np.int)\n    \n    Qe = np.empty((n_episodes, env.action_space.n), dtype=np.float64)\n    returns = np.empty(n_episodes, dtype=np.float64)\n    actions = np.empty(n_episodes, dtype=np.int)\n    name = 'UCB {}'.format(c)\n    for e in tqdm(range(n_episodes), \n                  desc='Episodes for: ' + name, \n                  leave=False):\n        action = e\n        if e &gt;= len(Q):\n            U = np.sqrt(c * np.log(e)/N)\n            action = np.argmax(Q + U)\n\n        _, reward, _, _ = env.step(action)\n        N[action] += 1\n        Q[action] = Q[action] + (reward - Q[action])/N[action]\n        \n        Qe[e] = Q\n        returns[e] = reward\n        actions[e] = action\n    return name, returns, Qe, actions\n\n\ndef thompson_sampling(env, \n                      alpha=1,\n                      beta=0,\n                      n_episodes=1000):\n    Q = np.zeros((env.action_space.n), dtype=np.float64)\n    N = np.zeros((env.action_space.n), dtype=np.int)\n    \n    Qe = np.empty((n_episodes, env.action_space.n), dtype=np.float64)\n    returns = np.empty(n_episodes, dtype=np.float64)\n    actions = np.empty(n_episodes, dtype=np.int)\n    name = 'Thompson Sampling {}, {}'.format(alpha, beta)\n    for e in tqdm(range(n_episodes), \n                  desc='Episodes for: ' + name, \n                  leave=False):\n        samples = np.random.normal(\n            loc=Q, scale=alpha/(np.sqrt(N) + beta))\n        action = np.argmax(samples)\n\n        _, reward, _, _ = env.step(action)\n        N[action] += 1\n        Q[action] = Q[action] + (reward - Q[action])/N[action]\n\n        Qe[e] = Q\n        returns[e] = reward\n        actions[e] = action\n    return name, returns, Qe, actions\n\n\n두팔 밴딧 환경에서의 개선된 전략들의 수행\n\ndef b2_run_advanced_strategies_experiment(env_name='BanditTwoArmedUniform-v0'):\n    results = {}\n    experiments = [\n        # baseline strategies \n        lambda env: pure_exploitation(env),\n        lambda env: pure_exploration(env),\n\n        # best from simple strategies\n        lambda env: optimistic_initialization(env, \n                                              optimistic_estimate=1.0, \n                                              initial_count=10),\n        lambda env: exp_dec_epsilon_greedy(env, \n                                           init_epsilon=0.3, \n                                           min_epsilon=0.0, \n                                           decay_ratio=0.3),\n\n        # softmax\n        lambda env: softmax(env, \n                            init_temp=float('inf'), \n                            min_temp=0.0, \n                            decay_ratio=0.005),\n        lambda env: softmax(env, \n                            init_temp=100, \n                            min_temp=0.01, \n                            decay_ratio=0.01),\n\n        # ucb\n        lambda env: upper_confidence_bound(env, c=0.2),\n        lambda env: upper_confidence_bound(env, c=0.5),\n\n        # thompson sampling\n        lambda env: thompson_sampling(env, alpha=1, beta=1),\n        lambda env: thompson_sampling(env, alpha=0.5, beta=0.5),\n    ]\n    for env_seed in tqdm(SEEDS, desc='All experiments'):\n        env = gym.make(env_name, seed=env_seed) ; env.reset()\n        true_Q = np.array(env.env.p_dist * env.env.r_dist)\n        opt_V = np.max(true_Q)\n        for seed in tqdm(SEEDS, desc='All environments', leave=False):\n            for experiment in tqdm(experiments, \n                                   desc='Experiments with seed {}'.format(seed), \n                                   leave=False):\n                env.seed(seed) ; np.random.seed(seed) ; random.seed(seed)\n                name, Re, Qe, Ae = experiment(env)\n                Ae = np.expand_dims(Ae, -1)\n                episode_mean_rew = np.cumsum(Re) / (np.arange(len(Re)) + 1)\n                Q_selected = np.take_along_axis(\n                    np.tile(true_Q, Ae.shape), Ae, axis=1).squeeze()\n                regret = opt_V - Q_selected\n                cum_regret = np.cumsum(regret)\n                if name not in results.keys(): results[name] = {}\n                if 'Re' not in results[name].keys(): results[name]['Re'] = []\n                if 'Qe' not in results[name].keys(): results[name]['Qe'] = []\n                if 'Ae' not in results[name].keys(): results[name]['Ae'] = []\n                if 'cum_regret' not in results[name].keys(): \n                    results[name]['cum_regret'] = []\n                if 'episode_mean_rew' not in results[name].keys(): \n                    results[name]['episode_mean_rew'] = []\n\n                results[name]['Re'].append(Re)\n                results[name]['Qe'].append(Qe)\n                results[name]['Ae'].append(Ae)\n                results[name]['cum_regret'].append(cum_regret)\n                results[name]['episode_mean_rew'].append(episode_mean_rew)\n    return results\n\nb2_results_a = b2_run_advanced_strategies_experiment()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n실행에 대한 결과 그래프 출력\n\nfig, axs = plt.subplots(5, 1, figsize=(28, 28), sharey=False, sharex=False)\n\nlines = [\"-\",\"--\",\":\",\"-.\"]\nlinecycler = cycle(lines)\nmin_reg, max_ret = float('inf'), float('-inf')\nfor label, result in b2_results_a.items():\n    color = next(linecycler)\n\n    # reward\n    episode_mean_rew = np.array(result['episode_mean_rew'])\n    mean_episode_mean_rew = np.mean(episode_mean_rew, axis=0)\n\n    axs[0].plot(mean_episode_mean_rew, color, linewidth=2, label=label)\n\n    axs[1].plot(mean_episode_mean_rew, color, linewidth=2, label=label)\n    axs[1].set_xscale('log')\n    \n    axs[2].plot(mean_episode_mean_rew, color, linewidth=2, label=label)\n    if max_ret &lt; mean_episode_mean_rew[-1]: max_ret = mean_episode_mean_rew[-1]\n    axs[2].axis((mean_episode_mean_rew.shape[0]*0.989,\n                 mean_episode_mean_rew.shape[0],\n                 max_ret-0.004,\n                 max_ret+0.0001))\n\n    # regret\n    cum_regret = np.array(result['cum_regret'])\n    mean_cum_regret = np.mean(cum_regret, axis=0)\n\n    axs[3].plot(mean_cum_regret, color, linewidth=2, label=label)\n    \n    axs[4].plot(mean_cum_regret, color, linewidth=2, label=label)\n    if min_reg &gt; mean_cum_regret[-1]: min_reg = mean_cum_regret[-1]\n    plt.axis((mean_cum_regret.shape[0]*0.989,\n              mean_cum_regret.shape[0],\n              min_reg-1,\n              min_reg+4))\n\n    # config plot\n    axs[0].set_title('Mean Episode Reward')\n    axs[1].set_title('Mean Episode Reward (Log scale)')\n    axs[2].set_title('Mean Episode Reward (Zoom on best)')\n    axs[3].set_title('Total Regret')\n    axs[4].set_title('Total Regret (Zoom on best)')\n    plt.xlabel('Episodes')\n    axs[0].legend(loc='upper left')\n    \nplt.show()\n\n\n\n\n\n\n\n열 팔 가우시안 밴딧 환경\n\nb10_Vs = []\nfor seed in SEEDS:\n    env_name = 'BanditTenArmedGaussian-v0'\n    env = gym.make(env_name, seed=seed) ; env.reset()\n    r_dist = np.array(env.env.r_dist)[:,0]\n    b10_Q = np.array(env.env.p_dist * r_dist)\n    print('10-Armed Bandit environment with seed', seed)\n    print('Probability of reward:', env.env.p_dist)\n    print('Reward:', r_dist)\n    print('Q(.):', b10_Q)\n    b10_Vs.append(np.max(b10_Q))\n    print('V*:', b10_Vs[-1])\n    print()\nprint('Mean V* across all seeds:', np.mean(b10_Vs))\n\n10-Armed Bandit environment with seed 12\nProbability of reward: [1 1 1 1 1 1 1 1 1 1]\nReward: [ 1.38503828 -2.12704259 -2.04412697 -0.67407396  0.63734453  1.58553551\n  2.64476297  0.34536369 -1.3928017  -0.13044506]\nQ(.): [ 1.38503828 -2.12704259 -2.04412697 -0.67407396  0.63734453  1.58553551\n  2.64476297  0.34536369 -1.3928017  -0.13044506]\nV*: 2.6447629665055974\n\n10-Armed Bandit environment with seed 34\nProbability of reward: [1 1 1 1 1 1 1 1 1 1]\nReward: [ 0.18060298  0.9982891   1.56491649 -0.5319185   0.05065747 -0.38137431\n -0.37199852  0.78790366  1.00121956 -0.00984009]\nQ(.): [ 0.18060298  0.9982891   1.56491649 -0.5319185   0.05065747 -0.38137431\n -0.37199852  0.78790366  1.00121956 -0.00984009]\nV*: 1.5649164942952658\n\n10-Armed Bandit environment with seed 56\nProbability of reward: [1 1 1 1 1 1 1 1 1 1]\nReward: [ 0.62499037 -0.07140136  0.92729309  0.04536638  0.84506588 -0.18313555\n  0.00476172  1.55827457 -0.87910825 -0.00429599]\nQ(.): [ 0.62499037 -0.07140136  0.92729309  0.04536638  0.84506588 -0.18313555\n  0.00476172  1.55827457 -0.87910825 -0.00429599]\nV*: 1.5582745674137135\n\n10-Armed Bandit environment with seed 78\nProbability of reward: [1 1 1 1 1 1 1 1 1 1]\nReward: [ 1.20523565  0.11299807  0.66357907 -0.29196638  1.01421424 -0.72565023\n  1.16574679  1.70303914  0.77572013 -1.38797678]\nQ(.): [ 1.20523565  0.11299807  0.66357907 -0.29196638  1.01421424 -0.72565023\n  1.16574679  1.70303914  0.77572013 -1.38797678]\nV*: 1.7030391402728304\n\n10-Armed Bandit environment with seed 90\nProbability of reward: [1 1 1 1 1 1 1 1 1 1]\nReward: [ 0.81161829  0.12563368 -0.2520508  -0.55127142  0.53276387  0.19875864\n  0.04448967 -0.37178956 -0.25712615  0.04091966]\nQ(.): [ 0.81161829  0.12563368 -0.2520508  -0.55127142  0.53276387  0.19875864\n  0.04448967 -0.37178956 -0.25712615  0.04091966]\nV*: 0.8116182893597546\n\nMean V* across all seeds: 1.6565222915694324\n\n\n\n열팔 밴딧 환경에서의 간단한 전략 수행\n\ndef b10_run_simple_strategies_experiment(env_name='BanditTenArmedGaussian-v0'):\n    results = {}\n    experiments = [\n        # baseline strategies\n        lambda env: pure_exploitation(env),\n        lambda env: pure_exploration(env),\n\n        # epsilon greedy\n        lambda env: epsilon_greedy(env, epsilon=0.07),\n        lambda env: epsilon_greedy(env, epsilon=0.1),\n\n        # epsilon greedy linearly decaying\n        lambda env: lin_dec_epsilon_greedy(env, \n                                           init_epsilon=1.0,\n                                           min_epsilon=0.0,\n                                           decay_ratio=0.1),\n        lambda env: lin_dec_epsilon_greedy(env, \n                                           init_epsilon=0.3,\n                                           min_epsilon=0.001,\n                                           decay_ratio=0.1),\n\n        # epsilon greedy exponentially decaying\n        lambda env: exp_dec_epsilon_greedy(env, \n                                           init_epsilon=1.0, \n                                           min_epsilon=0.0,\n                                           decay_ratio=0.1),\n        lambda env: exp_dec_epsilon_greedy(env, \n                                           init_epsilon=0.3, \n                                           min_epsilon=0.0, \n                                           decay_ratio=0.3),\n\n        # optimistic\n        lambda env: optimistic_initialization(env, \n                                              optimistic_estimate=1.0, \n                                              initial_count=10),\n        lambda env: optimistic_initialization(env, \n                                              optimistic_estimate=1.0, \n                                              initial_count=50),\n    ]\n    for env_seed in tqdm(SEEDS, desc='All experiments'):\n        env = gym.make(env_name, seed=env_seed) ; env.reset()\n        r_dist = np.array(env.env.r_dist)[:,0]\n        true_Q = np.array(env.env.p_dist * r_dist)\n        opt_V = np.max(true_Q)\n        for seed in tqdm(SEEDS, desc='All environments', leave=False):\n            for experiment in tqdm(experiments, \n                                   desc='Experiments with seed {}'.format(seed), \n                                   leave=False):\n                env.seed(seed) ; np.random.seed(seed) ; random.seed(seed)\n                name, Re, Qe, Ae = experiment(env)\n                Ae = np.expand_dims(Ae, -1)\n                episode_mean_rew = np.cumsum(Re) / (np.arange(len(Re)) + 1)\n                Q_selected = np.take_along_axis(\n                    np.tile(true_Q, Ae.shape), Ae, axis=1).squeeze()\n                regret = opt_V - Q_selected\n                cum_regret = np.cumsum(regret)\n                if name not in results.keys(): results[name] = {}\n                if 'Re' not in results[name].keys(): results[name]['Re'] = []\n                if 'Qe' not in results[name].keys(): results[name]['Qe'] = []\n                if 'Ae' not in results[name].keys(): results[name]['Ae'] = []\n                if 'cum_regret' not in results[name].keys(): \n                    results[name]['cum_regret'] = []\n                if 'episode_mean_rew' not in results[name].keys(): \n                    results[name]['episode_mean_rew'] = []\n\n                results[name]['Re'].append(Re)\n                results[name]['Qe'].append(Qe)\n                results[name]['Ae'].append(Ae)\n                results[name]['cum_regret'].append(cum_regret)\n                results[name]['episode_mean_rew'].append(episode_mean_rew)\n    return results\n\nb10_results_s = b10_run_simple_strategies_experiment()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n실행에 대한 결과 그래프 출력\n\nfig, axs = plt.subplots(5, 1, figsize=(28, 28), sharey=False, sharex=False)\n\nlines = [\"-\",\"--\",\":\",\"-.\"]\nlinecycler = cycle(lines)\nmin_reg, max_ret = float('inf'), float('-inf')\nfor label, result in b10_results_s.items():\n    color = next(linecycler)\n\n    # reward\n    episode_mean_rew = np.array(result['episode_mean_rew'])\n    mean_episode_mean_rew = np.mean(episode_mean_rew, axis=0)\n\n    axs[0].plot(mean_episode_mean_rew, color, linewidth=2, label=label)\n\n    axs[1].plot(mean_episode_mean_rew, color, linewidth=2, label=label)\n    axs[1].set_xscale('log')\n    \n    axs[2].plot(mean_episode_mean_rew, color, linewidth=2, label=label)\n    if max_ret &lt; mean_episode_mean_rew[-1]: max_ret = mean_episode_mean_rew[-1]\n    axs[2].axis((mean_episode_mean_rew.shape[0]*0.989,\n                 mean_episode_mean_rew.shape[0],\n                 max_ret-0.06,\n                 max_ret+0.005))\n\n    # regret\n    cum_regret = np.array(result['cum_regret'])\n    mean_cum_regret = np.mean(cum_regret, axis=0)\n\n    axs[3].plot(mean_cum_regret, color, linewidth=2, label=label)\n    \n    axs[4].plot(mean_cum_regret, color, linewidth=2, label=label)\n    if min_reg &gt; mean_cum_regret[-1]: min_reg = mean_cum_regret[-1]\n    plt.axis((mean_cum_regret.shape[0]*0.989,\n              mean_cum_regret.shape[0],\n              min_reg-5,\n              min_reg+45))\n\n    # config plot\n    axs[0].set_title('Mean Episode Reward')\n    axs[1].set_title('Mean Episode Reward (Log scale)')\n    axs[2].set_title('Mean Episode Reward (Zoom on best)')\n    axs[3].set_title('Total Regret')\n    axs[4].set_title('Total Regret (Zoom on best)')\n    plt.xlabel('Episodes')\n    axs[0].legend(loc='upper left')\n\nplt.show()\n\n\n\n\n\n\n열팔 밴딧 환경에서의 개선된 전략 수행\n\ndef b10_run_advanced_strategies_experiment(env_name='BanditTenArmedGaussian-v0'):\n    results = {}\n    experiments = [\n        # baseline strategies \n        lambda env: pure_exploitation(env),\n        lambda env: pure_exploration(env),\n\n        # best from simple strategies\n        lambda env: lin_dec_epsilon_greedy(env, \n                                           init_epsilon=1.0,\n                                           min_epsilon=0.0,\n                                           decay_ratio=0.1),\n        lambda env: exp_dec_epsilon_greedy(env, \n                                           init_epsilon=1.0, \n                                           min_epsilon=0.0,\n                                           decay_ratio=0.1),\n\n        # softmax\n        lambda env: softmax(env, \n                            init_temp=float('inf'), \n                            min_temp=0.0, \n                            decay_ratio=0.005),\n        lambda env: softmax(env, \n                            init_temp=100, \n                            min_temp=0.01, \n                            decay_ratio=0.01),\n\n        # ucb\n        lambda env: upper_confidence_bound(env, c=0.2),\n        lambda env: upper_confidence_bound(env, c=0.5),\n\n        # thompson sampling\n        lambda env: thompson_sampling(env, alpha=1, beta=1),\n        lambda env: thompson_sampling(env, alpha=0.5, beta=0.5),\n    ]\n    for env_seed in tqdm(SEEDS, desc='All experiments'):\n        env = gym.make(env_name, seed=env_seed) ; env.reset()\n        r_dist = np.array(env.env.r_dist)[:,0]\n        true_Q = np.array(env.env.p_dist * r_dist)\n        opt_V = np.max(true_Q)\n        for seed in tqdm(SEEDS, desc='All environments', leave=False):\n            for experiment in tqdm(experiments, \n                                   desc='Experiments with seed {}'.format(seed), \n                                   leave=False):\n                env.seed(seed) ; np.random.seed(seed) ; random.seed(seed)\n                name, Re, Qe, Ae = experiment(env)\n                Ae = np.expand_dims(Ae, -1)\n                episode_mean_rew = np.cumsum(Re) / (np.arange(len(Re)) + 1)\n                Q_selected = np.take_along_axis(\n                    np.tile(true_Q, Ae.shape), Ae, axis=1).squeeze()\n                regret = opt_V - Q_selected\n                cum_regret = np.cumsum(regret)\n                if name not in results.keys(): results[name] = {}\n                if 'Re' not in results[name].keys(): results[name]['Re'] = []\n                if 'Qe' not in results[name].keys(): results[name]['Qe'] = []\n                if 'Ae' not in results[name].keys(): results[name]['Ae'] = []\n                if 'cum_regret' not in results[name].keys(): \n                    results[name]['cum_regret'] = []\n                if 'episode_mean_rew' not in results[name].keys():\n                    results[name]['episode_mean_rew'] = []\n\n                results[name]['Re'].append(Re)\n                results[name]['Qe'].append(Qe)\n                results[name]['Ae'].append(Ae)\n                results[name]['cum_regret'].append(cum_regret)\n                results[name]['episode_mean_rew'].append(episode_mean_rew)\n    return results\n\nb10_results_a = b10_run_advanced_strategies_experiment()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n실행에 대한 결과 그래프 출력\n\nfig, axs = plt.subplots(5, 1, figsize=(28, 28), sharey=False, sharex=False)\n\nlines = [\"-\",\"--\",\":\",\"-.\"]\nlinecycler = cycle(lines)\nmin_reg, max_ret = float('inf'), float('-inf')\nfor label, result in b10_results_a.items():\n    color = next(linecycler)\n\n    # reward\n    episode_mean_rew = np.array(result['episode_mean_rew'])\n    mean_episode_mean_rew = np.mean(episode_mean_rew, axis=0)\n\n    axs[0].plot(mean_episode_mean_rew, color, linewidth=2, label=label)\n\n    axs[1].plot(mean_episode_mean_rew, color, linewidth=2, label=label)\n    axs[1].set_xscale('log')\n    \n    axs[2].plot(mean_episode_mean_rew, color, linewidth=2, label=label)\n    if max_ret &lt; mean_episode_mean_rew[-1]: max_ret = mean_episode_mean_rew[-1]\n    axs[2].axis((mean_episode_mean_rew.shape[0]*0.989,\n                 mean_episode_mean_rew.shape[0],\n                 max_ret-0.01,\n                 max_ret+0.005))\n\n    # regret\n    cum_regret = np.array(result['cum_regret'])\n    mean_cum_regret = np.mean(cum_regret, axis=0)\n\n    axs[3].plot(mean_cum_regret, color, linewidth=2, label=label)\n    \n    axs[4].plot(mean_cum_regret, color, linewidth=2, label=label)\n    if min_reg &gt; mean_cum_regret[-1]: min_reg = mean_cum_regret[-1]\n    plt.axis((mean_cum_regret.shape[0]*0.989,\n              mean_cum_regret.shape[0],\n              min_reg-5,\n              min_reg+12))\n\n    # config plot\n    axs[0].set_title('Mean Episode Reward')\n    axs[1].set_title('Mean Episode Reward (Log scale)')\n    axs[2].set_title('Mean Episode Reward (Zoom on best)')\n    axs[3].set_title('Total Regret')\n    axs[4].set_title('Total Regret (Zoom on best)')\n    plt.xlabel('Episodes')\n    axs[0].legend(loc='upper left')\n\nplt.show()"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-6.html#tldr",
    "href": "publication/GDRL/GDRL-chapter-6.html#tldr",
    "title": "Chapter 6: Improving Agents behaviors",
    "section": "TL;DR",
    "text": "TL;DR\n\n그로킹 심층 강화학습 중 6장 내용인 “에이전트의 행동 개선”에 대한 내용입니다.\n\n\n\nCode\n!pip install tqdm numpy scikit-learn pyglet setuptools && \\\n!pip install gym asciinema pandas tabulate tornado==5.* PyBullet && \\\n!pip install git+https://github.com/pybox2d/pybox2d#egg=Box2D && \\\n!pip install git+https://github.com/mimoralea/gym-bandits#egg=gym-bandits && \\\n!pip install git+https://github.com/mimoralea/gym-walk#egg=gym-walk && \\\n!pip install git+https://github.com/mimoralea/gym-aima#egg=gym-aima && \\\n!pip install gym[atari]"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-6.html#실행에-필요한-helper-function",
    "href": "publication/GDRL/GDRL-chapter-6.html#실행에-필요한-helper-function",
    "title": "Chapter 6: Improving Agents behaviors",
    "section": "실행에 필요한 helper function",
    "text": "실행에 필요한 helper function\n\ndef value_iteration(P, gamma=1.0, theta=1e-10):\n    V = np.zeros(len(P), dtype=np.float64)\n    while True:\n        Q = np.zeros((len(P), len(P[0])), dtype=np.float64)\n        for s in range(len(P)):\n            for a in range(len(P[s])):\n                for prob, next_state, reward, done in P[s][a]:\n                    Q[s][a] += prob * (reward + gamma * V[next_state] * (not done))\n        if np.max(np.abs(V - np.max(Q, axis=1))) &lt; theta:\n            break\n        V = np.max(Q, axis=1)\n    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n    return Q, V, pi\n\n\ndef print_policy(pi, P, action_symbols=('&lt;', 'v', '&gt;', '^'), n_cols=4, title='정책:'):\n    print(title)\n    arrs = {k:v for k,v in enumerate(action_symbols)}\n    for s in range(len(P)):\n        a = pi(s)\n        print(\"| \", end=\"\")\n        if np.all([done for action in P[s].values() for _, _, _, done in action]):\n            print(\"\".rjust(9), end=\" \")\n        else:\n            print(str(s).zfill(2), arrs[a].rjust(6), end=\" \")\n        if (s + 1) % n_cols == 0: print(\"|\")\n\n\ndef print_state_value_function(V, P, n_cols=4, prec=3, title='상태-가치 함수:'):\n    print(title)\n    for s in range(len(P)):\n        v = V[s]\n        print(\"| \", end=\"\")\n        if np.all([done for action in P[s].values() for _, _, _, done in action]):\n            print(\"\".rjust(9), end=\" \")\n        else:\n            print(str(s).zfill(2), '{}'.format(np.round(v, prec)).rjust(6), end=\" \")\n        if (s + 1) % n_cols == 0: print(\"|\")\n\n\ndef print_action_value_function(Q, \n                                optimal_Q=None, \n                                action_symbols=('&lt;', '&gt;'), \n                                prec=3, \n                                title='행동-가치 함수:'):\n    vf_types=('',) if optimal_Q is None else ('', '*', 'er')\n    headers = ['s',] + [' '.join(i) for i in list(itertools.product(vf_types, action_symbols))]\n    print(title)\n    states = np.arange(len(Q))[..., np.newaxis]\n    arr = np.hstack((states, np.round(Q, prec)))\n    if not (optimal_Q is None):\n        arr = np.hstack((arr, np.round(optimal_Q, prec), np.round(optimal_Q-Q, prec)))\n    print(tabulate(arr, headers, tablefmt=\"fancy_grid\"))\n\n\ndef get_policy_metrics(env, gamma, pi, goal_state, optimal_Q, \n                       n_episodes=100, max_steps=200):\n    random.seed(123); np.random.seed(123) ; env.seed(123)\n    reached_goal, episode_reward, episode_regret = [], [], []\n    for _ in range(n_episodes):\n        state, done, steps = env.reset(), False, 0\n        episode_reward.append(0.0)\n        episode_regret.append(0.0)\n        while not done and steps &lt; max_steps:\n            action = pi(state)\n            regret = np.max(optimal_Q[state]) - optimal_Q[state][action]\n            episode_regret[-1] += regret\n            \n            state, reward, done, _ = env.step(action)\n            episode_reward[-1] += (gamma**steps * reward)\n            \n            steps += 1\n\n        reached_goal.append(state == goal_state)\n    results = np.array((np.sum(reached_goal)/len(reached_goal)*100, \n                        np.mean(episode_reward), \n                        np.mean(episode_regret)))\n    return results\n\n\ndef get_metrics_from_tracks(env, gamma, goal_state, optimal_Q, pi_track, coverage=0.1):\n    total_samples = len(pi_track)\n    n_samples = int(total_samples * coverage)\n    samples_e = np.linspace(0, total_samples, n_samples, endpoint=True, dtype=np.int)\n    metrics = []\n    for e, pi in enumerate(tqdm(pi_track)):\n        if e in samples_e:\n            metrics.append(get_policy_metrics(\n                env, \n                gamma=gamma, \n                pi=lambda s: pi[s], \n                goal_state=goal_state, \n                optimal_Q=optimal_Q))\n        else:\n            metrics.append(metrics[-1])\n    metrics = np.array(metrics)\n    success_rate_ma, mean_return_ma, mean_regret_ma = np.apply_along_axis(moving_average, axis=0, arr=metrics).T\n    return success_rate_ma, mean_return_ma, mean_regret_ma \n\n\ndef rmse(x, y, dp=4):\n    return np.round(np.sqrt(np.mean((x - y)**2)), dp)\n\n\ndef moving_average(a, n=100) :\n    ret = np.cumsum(a, dtype=float)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1:] / n\n\n\ndef plot_value_function(title, V_track, V_true=None, log=False, limit_value=0.05, limit_items=5):\n    np.random.seed(123)\n    per_col = 25\n    linecycler = cycle([\"-\",\"--\",\":\",\"-.\"])\n    legends = []\n\n    valid_values = np.argwhere(V_track[-1] &gt; limit_value).squeeze()\n    items_idxs = np.random.choice(valid_values, \n                                  min(len(valid_values), limit_items), \n                                  replace=False)\n    # 첫번째 참값을 뽑아냅니다.\n    if V_true is not None:\n        for i, state in enumerate(V_track.T):\n            if i not in items_idxs:\n                continue\n            if state[-1] &lt; limit_value:\n                continue\n\n            label = 'v*({})'.format(i)\n            plt.axhline(y=V_true[i], color='k', linestyle='-', linewidth=1)\n            plt.text(int(len(V_track)*1.02), V_true[i]+.01, label)\n\n    # 이에 대한 추정치를 계산합니다.\n    for i, state in enumerate(V_track.T):\n        if i not in items_idxs:\n            continue\n        if state[-1] &lt; limit_value:\n            continue\n        line_type = next(linecycler)\n        label = 'V({})'.format(i)\n        p, = plt.plot(state, line_type, label=label, linewidth=3)\n        legends.append(p)\n        \n    legends.reverse()\n\n    ls = []\n    for loc, idx in enumerate(range(0, len(legends), per_col)):\n        subset = legends[idx:idx+per_col]\n        l = plt.legend(subset, [p.get_label() for p in subset], \n                       loc='center right', bbox_to_anchor=(1.25, 0.5))\n        ls.append(l)\n    [plt.gca().add_artist(l) for l in ls[:-1]]\n    if log: plt.xscale('log')\n    plt.title(title)\n    plt.ylabel('State-value function')\n    plt.xlabel('Episodes (log scale)' if log else 'Episodes')\n    plt.show()\n\n\ndef decay_schedule(init_value, min_value, decay_ratio, max_steps, log_start=-2, log_base=10):\n    decay_steps = int(max_steps * decay_ratio)\n    rem_steps = max_steps - decay_steps\n    values = np.logspace(log_start, 0, decay_steps, base=log_base, endpoint=True)[::-1]\n    values = (values - values.min()) / (values.max() - values.min())\n    values = (init_value - min_value) * values + min_value\n    values = np.pad(values, (0, rem_steps), 'edge')\n    return values"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-6.html#미끄러지는-7개의-통로",
    "href": "publication/GDRL/GDRL-chapter-6.html#미끄러지는-7개의-통로",
    "title": "Chapter 6: Improving Agents behaviors",
    "section": "미끄러지는 7개의 통로",
    "text": "미끄러지는 7개의 통로\n\nenv = gym.make('SlipperyWalkSeven-v0')\ninit_state = env.reset()\ngoal_state = 8\ngamma = 0.99\nn_episodes = 3000\nP = env.env.P\nn_cols, svf_prec, err_prec, avf_prec=9, 4, 2, 3\naction_symbols=('&lt;', '&gt;')\nlimit_items, limit_value = 5, 0.0\ncu_limit_items, cu_limit_value, cu_episodes = 10, 0.0, 100\n\n\n알파와 입실론 스케쥴링\n\nplt.plot(decay_schedule(0.5, 0.01, 0.5, n_episodes), \n         '-', linewidth=2, \n         label='Alpha schedule')\nplt.plot(decay_schedule(1.0, 0.1, 0.9, n_episodes), \n         ':', linewidth=2, \n         label='Epsilon schedule')\nplt.legend(loc=1, ncol=1)\n\nplt.title('Alpha and epsilon schedules')\nplt.xlabel('Episodes')\nplt.ylabel('Hyperparameter values')\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\n\n이상적인 가치 함수와 정책\n\noptimal_Q, optimal_V, optimal_pi = value_iteration(P, gamma=gamma)\nprint_state_value_function(optimal_V, P, n_cols=n_cols, prec=svf_prec, title='Optimal state-value function:')\nprint()\n\nprint_action_value_function(optimal_Q, \n                            None, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Optimal action-value function:')\nprint()\nprint_policy(optimal_pi, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_op, mean_return_op, mean_regret_op = get_policy_metrics(\n    env, gamma=gamma, pi=optimal_pi, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_op, mean_return_op, mean_regret_op))\n\nOptimal state-value function:\n|           | 01 0.5637 | 02  0.763 | 03 0.8449 | 04 0.8892 | 05  0.922 | 06 0.9515 | 07 0.9806 |           |\n\nOptimal action-value function:\n╒═════╤═══════╤═══════╕\n│   s │     &lt; │     &gt; │\n╞═════╪═══════╪═══════╡\n│   0 │ 0     │ 0     │\n├─────┼───────┼───────┤\n│   1 │ 0.312 │ 0.564 │\n├─────┼───────┼───────┤\n│   2 │ 0.67  │ 0.763 │\n├─────┼───────┼───────┤\n│   3 │ 0.803 │ 0.845 │\n├─────┼───────┼───────┤\n│   4 │ 0.864 │ 0.889 │\n├─────┼───────┼───────┤\n│   5 │ 0.901 │ 0.922 │\n├─────┼───────┼───────┤\n│   6 │ 0.932 │ 0.952 │\n├─────┼───────┼───────┤\n│   7 │ 0.961 │ 0.981 │\n├─────┼───────┼───────┤\n│   8 │ 0     │ 0     │\n╘═════╧═══════╧═══════╛\n\n정책:\n|           | 01      &gt; | 02      &gt; | 03      &gt; | 04      &gt; | 05      &gt; | 06      &gt; | 07      &gt; |           |\nReaches goal 96.00%. Obtains an average return of 0.8548. Regret of 0.0000\n\n\n\n\n첫방문 몬테카를로 제어 (FVMC)\n\ndef generate_trajectory(select_action, Q, epsilon, env, max_steps=200):\n    done, trajectory = False, []\n    while not done:\n        state = env.reset()\n        for t in count():\n            action = select_action(state, Q, epsilon)\n            next_state, reward, done, _ = env.step(action)\n            experience = (state, action, reward, next_state, done)\n            trajectory.append(experience)\n            if done:\n                break\n            if t &gt;= max_steps - 1:\n                trajectory = []\n                break\n            state = next_state\n    return np.array(trajectory, np.object)\n\n\ndef mc_control(env,\n               gamma=1.0,\n               init_alpha=0.5,\n               min_alpha=0.01,\n               alpha_decay_ratio=0.5,\n               init_epsilon=1.0,\n               min_epsilon=0.1,\n               epsilon_decay_ratio=0.9,\n               n_episodes=3000,\n               max_steps=200,\n               first_visit=True):\n    nS, nA = env.observation_space.n, env.action_space.n\n    discounts = np.logspace(0, \n                            max_steps, \n                            num=max_steps, \n                            base=gamma, \n                            endpoint=False) \n    alphas = decay_schedule(init_alpha, \n                           min_alpha, \n                           alpha_decay_ratio, \n                           n_episodes)\n    epsilons = decay_schedule(init_epsilon, \n                              min_epsilon, \n                              epsilon_decay_ratio, \n                              n_episodes)\n    pi_track = []\n    Q = np.zeros((nS, nA), dtype=np.float64)\n    Q_track = np.zeros((n_episodes, nS, nA), dtype=np.float64)\n    select_action = lambda state, Q, epsilon: np.argmax(Q[state]) \\\n        if np.random.random() &gt; epsilon \\\n        else np.random.randint(len(Q[state]))\n\n    for e in tqdm(range(n_episodes), leave=False):\n        \n        trajectory = generate_trajectory(select_action,\n                                         Q,\n                                         epsilons[e],\n                                         env, \n                                         max_steps)\n        visited = np.zeros((nS, nA), dtype=np.bool)\n        for t, (state, action, reward, _, _) in enumerate(trajectory):\n            if visited[state][action] and first_visit:\n                continue\n            visited[state][action] = True\n            \n            n_steps = len(trajectory[t:])\n            G = np.sum(discounts[:n_steps] * trajectory[t:, 2])\n            Q[state][action] = Q[state][action] + alphas[e] * (G - Q[state][action])\n\n        Q_track[e] = Q\n        pi_track.append(np.argmax(Q, axis=1))\n\n    V = np.max(Q, axis=1)\n    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n    return Q, V, pi, Q_track, pi_track\n\n\nQ_mcs, V_mcs, Q_track_mcs = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_mc, V_mc, pi_mc, Q_track_mc, pi_track_mc = mc_control(env, gamma=gamma, n_episodes=n_episodes)\n    Q_mcs.append(Q_mc) ; V_mcs.append(V_mc) ; Q_track_mcs.append(Q_track_mc)\nQ_mc, V_mc, Q_track_mc = np.mean(Q_mcs, axis=0), np.mean(V_mcs, axis=0), np.mean(Q_track_mcs, axis=0)\ndel Q_mcs ; del V_mcs ; del Q_track_mcs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_mc, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by FVMC:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_mc - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_mc, optimal_V)))\nprint()\nprint_action_value_function(Q_mc, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='FVMC action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_mc, optimal_Q)))\nprint()\nprint_policy(pi_mc, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_mc, mean_return_mc, mean_regret_mc = get_policy_metrics(\n    env, gamma=gamma, pi=pi_mc, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_mc, mean_return_mc, mean_regret_mc))\n\nState-value function found by FVMC:\n|           | 01 0.4895 | 02 0.7209 | 03 0.8311 | 04 0.8766 | 05 0.9137 | 06 0.9463 | 07 0.9788 |           |\nOptimal state-value function:\n|           | 01 0.5637 | 02  0.763 | 03 0.8449 | 04 0.8892 | 05  0.922 | 06 0.9515 | 07 0.9806 |           |\nState-value function errors:\n|           | 01  -0.07 | 02  -0.04 | 03  -0.01 | 04  -0.01 | 05  -0.01 | 06  -0.01 | 07   -0.0 |           |\nState-value function RMSE: 0.0293\n\nFVMC action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╕\n│   s │     &lt; │     &gt; │   * &lt; │   * &gt; │   er &lt; │   er &gt; │\n╞═════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╡\n│   0 │ 0     │ 0     │ 0     │ 0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   1 │ 0.194 │ 0.489 │ 0.312 │ 0.564 │  0.118 │  0.074 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   2 │ 0.549 │ 0.721 │ 0.67  │ 0.763 │  0.121 │  0.042 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   3 │ 0.73  │ 0.831 │ 0.803 │ 0.845 │  0.073 │  0.014 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   4 │ 0.843 │ 0.877 │ 0.864 │ 0.889 │  0.021 │  0.013 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   5 │ 0.883 │ 0.914 │ 0.901 │ 0.922 │  0.019 │  0.008 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   6 │ 0.925 │ 0.946 │ 0.932 │ 0.952 │  0.007 │  0.005 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   7 │ 0.955 │ 0.979 │ 0.961 │ 0.981 │  0.006 │  0.002 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   8 │ 0     │ 0     │ 0     │ 0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╛\nAction-value function RMSE: 0.0486\n\n정책:\n|           | 01      &gt; | 02      &gt; | 03      &gt; | 04      &gt; | 05      &gt; | 06      &gt; | 07      &gt; |           |\nReaches goal 96.00%. Obtains an average return of 0.8548. Regret of 0.0000\n\n\n\n\nSARSA\n\ndef sarsa(env,\n          gamma=1.0,\n          init_alpha=0.5,\n          min_alpha=0.01,\n          alpha_decay_ratio=0.5,\n          init_epsilon=1.0,\n          min_epsilon=0.1,\n          epsilon_decay_ratio=0.9,\n          n_episodes=3000):\n    nS, nA = env.observation_space.n, env.action_space.n\n    pi_track = []\n    Q = np.zeros((nS, nA), dtype=np.float64)\n    Q_track = np.zeros((n_episodes, nS, nA), dtype=np.float64)\n    select_action = lambda state, Q, epsilon: np.argmax(Q[state]) \\\n        if np.random.random() &gt; epsilon \\\n        else np.random.randint(len(Q[state]))\n    alphas = decay_schedule(init_alpha, \n                           min_alpha, \n                           alpha_decay_ratio, \n                           n_episodes)\n    epsilons = decay_schedule(init_epsilon, \n                              min_epsilon, \n                              epsilon_decay_ratio, \n                              n_episodes)\n    \n    for e in tqdm(range(n_episodes), leave=False):\n        state, done = env.reset(), False\n        action = select_action(state, Q, epsilons[e])\n        while not done:\n            next_state, reward, done, _ = env.step(action)\n            next_action = select_action(next_state, Q, epsilons[e])\n            td_target = reward + gamma * Q[next_state][next_action] * (not done)\n            td_error = td_target - Q[state][action]\n            Q[state][action] = Q[state][action] + alphas[e] * td_error\n            state, action = next_state, next_action\n        Q_track[e] = Q\n        pi_track.append(np.argmax(Q, axis=1))\n\n    V = np.max(Q, axis=1)\n    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n    return Q, V, pi, Q_track, pi_track\n\n\nQ_sarsas, V_sarsas, Q_track_sarsas = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_sarsa, V_sarsa, pi_sarsa, Q_track_sarsa, pi_track_sarsa = sarsa(env, gamma=gamma, n_episodes=n_episodes)\n    Q_sarsas.append(Q_sarsa) ; V_sarsas.append(V_sarsa) ; Q_track_sarsas.append(Q_track_sarsa)\nQ_sarsa = np.mean(Q_sarsas, axis=0)\nV_sarsa = np.mean(V_sarsas, axis=0)\nQ_track_sarsa = np.mean(Q_track_sarsas, axis=0)\ndel Q_sarsas ; del V_sarsas ; del Q_track_sarsas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_sarsa, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Sarsa:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_sarsa - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_sarsa, optimal_V)))\nprint()\nprint_action_value_function(Q_sarsa, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Sarsa action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_sarsa, optimal_Q)))\nprint()\nprint_policy(pi_sarsa, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_sarsa, mean_return_sarsa, mean_regret_sarsa = get_policy_metrics(\n    env, gamma=gamma, pi=pi_sarsa, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_sarsa, mean_return_sarsa, mean_regret_sarsa))\n\nState-value function found by Sarsa:\n|           | 01  0.461 | 02 0.6868 | 03  0.797 | 04  0.863 | 05 0.9075 | 06 0.9461 | 07 0.9767 |           |\nOptimal state-value function:\n|           | 01 0.5637 | 02  0.763 | 03 0.8449 | 04 0.8892 | 05  0.922 | 06 0.9515 | 07 0.9806 |           |\nState-value function errors:\n|           | 01   -0.1 | 02  -0.08 | 03  -0.05 | 04  -0.03 | 05  -0.01 | 06  -0.01 | 07   -0.0 |           |\nState-value function RMSE: 0.0467\n\nSarsa action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╕\n│   s │     &lt; │     &gt; │   * &lt; │   * &gt; │   er &lt; │   er &gt; │\n╞═════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╡\n│   0 │ 0     │ 0     │ 0     │ 0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   1 │ 0.163 │ 0.461 │ 0.312 │ 0.564 │  0.149 │  0.103 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   2 │ 0.5   │ 0.687 │ 0.67  │ 0.763 │  0.17  │  0.076 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   3 │ 0.7   │ 0.797 │ 0.803 │ 0.845 │  0.103 │  0.048 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   4 │ 0.817 │ 0.863 │ 0.864 │ 0.889 │  0.047 │  0.026 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   5 │ 0.874 │ 0.908 │ 0.901 │ 0.922 │  0.028 │  0.014 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   6 │ 0.917 │ 0.946 │ 0.932 │ 0.952 │  0.016 │  0.005 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   7 │ 0.951 │ 0.977 │ 0.961 │ 0.981 │  0.01  │  0.004 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   8 │ 0     │ 0     │ 0     │ 0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╛\nAction-value function RMSE: 0.0687\n\n정책:\n|           | 01      &gt; | 02      &gt; | 03      &gt; | 04      &gt; | 05      &gt; | 06      &gt; | 07      &gt; |           |\nReaches goal 96.00%. Obtains an average return of 0.8548. Regret of 0.0000\n\n\n\n\nQ학습\n\ndef q_learning(env, \n               gamma=1.0,\n               init_alpha=0.5,\n               min_alpha=0.01,\n               alpha_decay_ratio=0.5,\n               init_epsilon=1.0,\n               min_epsilon=0.1,\n               epsilon_decay_ratio=0.9,\n               n_episodes=3000):\n    nS, nA = env.observation_space.n, env.action_space.n\n    pi_track = []\n    Q = np.zeros((nS, nA), dtype=np.float64)\n    Q_track = np.zeros((n_episodes, nS, nA), dtype=np.float64)\n    select_action = lambda state, Q, epsilon: np.argmax(Q[state]) \\\n        if np.random.random() &gt; epsilon \\\n        else np.random.randint(len(Q[state]))\n    alphas = decay_schedule(init_alpha, \n                           min_alpha, \n                           alpha_decay_ratio, \n                           n_episodes)\n    epsilons = decay_schedule(init_epsilon, \n                              min_epsilon, \n                              epsilon_decay_ratio, \n                              n_episodes)\n    for e in tqdm(range(n_episodes), leave=False):\n        state, done = env.reset(), False\n        while not done:\n            action = select_action(state, Q, epsilons[e])\n            next_state, reward, done, _ = env.step(action)\n            td_target = reward + gamma * Q[next_state].max() * (not done)\n            td_error = td_target - Q[state][action]\n            Q[state][action] = Q[state][action] + alphas[e] * td_error\n            state = next_state\n\n        Q_track[e] = Q\n        pi_track.append(np.argmax(Q, axis=1))\n\n    V = np.max(Q, axis=1)        \n    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n    return Q, V, pi, Q_track, pi_track\n\n\nQ_qls, V_qls, Q_track_qls = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_ql, V_ql, pi_ql, Q_track_ql, pi_track_ql = q_learning(env, gamma=gamma, n_episodes=n_episodes)\n    Q_qls.append(Q_ql) ; V_qls.append(V_ql) ; Q_track_qls.append(Q_track_ql)\nQ_ql = np.mean(Q_qls, axis=0)\nV_ql = np.mean(V_qls, axis=0)\nQ_track_ql = np.mean(Q_track_qls, axis=0)\ndel Q_qls ; del V_qls ; del Q_track_qls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_ql, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Q-learning:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_ql - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_ql, optimal_V)))\nprint()\nprint_action_value_function(Q_ql, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Q-learning action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_ql, optimal_Q)))\nprint()\nprint_policy(pi_ql, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_ql, mean_return_ql, mean_regret_ql = get_policy_metrics(\n    env, gamma=gamma, pi=pi_ql, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_ql, mean_return_ql, mean_regret_ql))\n\nState-value function found by Q-learning:\n|           | 01 0.5523 | 02  0.754 | 03 0.8432 | 04 0.8893 | 05 0.9215 | 06 0.9509 | 07   0.98 |           |\nOptimal state-value function:\n|           | 01 0.5637 | 02  0.763 | 03 0.8449 | 04 0.8892 | 05  0.922 | 06 0.9515 | 07 0.9806 |           |\nState-value function errors:\n|           | 01  -0.01 | 02  -0.01 | 03   -0.0 | 04    0.0 | 05   -0.0 | 06   -0.0 | 07   -0.0 |           |\nState-value function RMSE: 0.0049\n\nQ-learning action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╕\n│   s │     &lt; │     &gt; │   * &lt; │   * &gt; │   er &lt; │   er &gt; │\n╞═════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╡\n│   0 │ 0     │ 0     │ 0     │ 0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   1 │ 0.303 │ 0.552 │ 0.312 │ 0.564 │  0.009 │  0.011 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   2 │ 0.659 │ 0.754 │ 0.67  │ 0.763 │  0.011 │  0.009 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   3 │ 0.795 │ 0.843 │ 0.803 │ 0.845 │  0.008 │  0.002 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   4 │ 0.864 │ 0.889 │ 0.864 │ 0.889 │ -0.001 │ -0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   5 │ 0.901 │ 0.922 │ 0.901 │ 0.922 │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   6 │ 0.932 │ 0.951 │ 0.932 │ 0.952 │  0     │  0.001 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   7 │ 0.961 │ 0.98  │ 0.961 │ 0.981 │ -0     │  0.001 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   8 │ 0     │ 0     │ 0     │ 0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╛\nAction-value function RMSE: 0.0052\n\n정책:\n|           | 01      &gt; | 02      &gt; | 03      &gt; | 04      &gt; | 05      &gt; | 06      &gt; | 07      &gt; |           |\nReaches goal 96.00%. Obtains an average return of 0.8548. Regret of 0.0000\n\n\n\n\n이중 Q학습\n\ndef double_q_learning(env,\n                      gamma=1.0,\n                      init_alpha=0.5,\n                      min_alpha=0.01,\n                      alpha_decay_ratio=0.5,\n                      init_epsilon=1.0,\n                      min_epsilon=0.1,\n                      epsilon_decay_ratio=0.9,\n                      n_episodes=3000):\n    nS, nA = env.observation_space.n, env.action_space.n\n    pi_track = []\n    Q1 = np.zeros((nS, nA), dtype=np.float64)\n    Q2 = np.zeros((nS, nA), dtype=np.float64)\n    Q_track1 = np.zeros((n_episodes, nS, nA), dtype=np.float64)\n    Q_track2 = np.zeros((n_episodes, nS, nA), dtype=np.float64)\n    select_action = lambda state, Q, epsilon: np.argmax(Q[state]) \\\n        if np.random.random() &gt; epsilon \\\n        else np.random.randint(len(Q[state]))\n    alphas = decay_schedule(init_alpha, \n                           min_alpha, \n                           alpha_decay_ratio, \n                           n_episodes)\n    epsilons = decay_schedule(init_epsilon, \n                              min_epsilon, \n                              epsilon_decay_ratio, \n                              n_episodes)\n    for e in tqdm(range(n_episodes), leave=False):\n        state, done = env.reset(), False\n        while not done:\n            action = select_action(state, (Q1 + Q2)/2, epsilons[e])\n            next_state, reward, done, _ = env.step(action)\n\n            if np.random.randint(2):\n                argmax_Q1 = np.argmax(Q1[next_state])\n                td_target = reward + gamma * Q2[next_state][argmax_Q1] * (not done)\n                td_error = td_target - Q1[state][action]\n                Q1[state][action] = Q1[state][action] + alphas[e] * td_error\n            else:\n                argmax_Q2 = np.argmax(Q2[next_state])\n                td_target = reward + gamma * Q1[next_state][argmax_Q2] * (not done)\n                td_error = td_target - Q2[state][action]\n                Q2[state][action] = Q2[state][action] + alphas[e] * td_error\n            state = next_state\n\n        Q_track1[e] = Q1\n        Q_track2[e] = Q2        \n        pi_track.append(np.argmax((Q1 + Q2)/2, axis=1))\n\n    Q = (Q1 + Q2)/2.\n    V = np.max(Q, axis=1)    \n    pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n    return Q, V, pi, (Q_track1 + Q_track2)/2., pi_track\n\n\nQ_dqls, V_dqls, Q_track_dqls = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_dql, V_dql, pi_dql, Q_track_dql, pi_track_dql = double_q_learning(env, gamma=gamma, n_episodes=n_episodes)\n    Q_dqls.append(Q_dql) ; V_dqls.append(V_dql) ; Q_track_dqls.append(Q_track_dql)\nQ_dql, V_dql, Q_track_dql = np.mean(Q_dqls, axis=0), np.mean(V_dqls, axis=0), np.mean(Q_track_dqls, axis=0)\ndel Q_dqls ; del V_dqls ; del Q_track_dqls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_dql, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Double Q-Learning:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_dql - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_dql, optimal_V)))\nprint()\nprint_action_value_function(Q_dql, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Double Q-Learning action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_dql, optimal_Q)))\nprint()\nprint_policy(pi_dql, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_dql, mean_return_dql, mean_regret_dql = get_policy_metrics(\n    env, gamma=gamma, pi=pi_dql, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_dql, mean_return_dql, mean_regret_dql))\n\nState-value function found by Double Q-Learning:\n|           | 01  0.576 | 02 0.7688 | 03 0.8467 | 04 0.8896 | 05 0.9221 | 06 0.9515 | 07 0.9804 |           |\nOptimal state-value function:\n|           | 01 0.5637 | 02  0.763 | 03 0.8449 | 04 0.8892 | 05  0.922 | 06 0.9515 | 07 0.9806 |           |\nState-value function errors:\n|           | 01   0.01 | 02   0.01 | 03    0.0 | 04    0.0 | 05    0.0 | 06   -0.0 | 07   -0.0 |           |\nState-value function RMSE: 0.0046\n\nDouble Q-Learning action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╕\n│   s │     &lt; │     &gt; │   * &lt; │   * &gt; │   er &lt; │   er &gt; │\n╞═════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╡\n│   0 │ 0     │ 0     │ 0     │ 0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   1 │ 0.292 │ 0.576 │ 0.312 │ 0.564 │  0.02  │ -0.012 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   2 │ 0.692 │ 0.769 │ 0.67  │ 0.763 │ -0.021 │ -0.006 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   3 │ 0.811 │ 0.847 │ 0.803 │ 0.845 │ -0.007 │ -0.002 │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   4 │ 0.866 │ 0.89  │ 0.864 │ 0.889 │ -0.002 │ -0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   5 │ 0.903 │ 0.922 │ 0.901 │ 0.922 │ -0.001 │ -0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   6 │ 0.933 │ 0.951 │ 0.932 │ 0.952 │ -0.001 │  0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   7 │ 0.963 │ 0.98  │ 0.961 │ 0.981 │ -0.001 │  0     │\n├─────┼───────┼───────┼───────┼───────┼────────┼────────┤\n│   8 │ 0     │ 0     │ 0     │ 0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╛\nAction-value function RMSE: 0.0078\n\n정책:\n|           | 01      &gt; | 02      &gt; | 03      &gt; | 04      &gt; | 05      &gt; | 06      &gt; | 07      &gt; |           |\nReaches goal 96.00%. Obtains an average return of 0.8548. Regret of 0.0000\n\n\n\n\n각 에피소드별 max(Q) 비교\n\n첫방문 몬테카를로\n\nplot_value_function(\n    'FVMC estimates through time vs. true values', \n    np.max(Q_track_mc, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'FVMC estimates through time vs. true values (log scale)', \n    np.max(Q_track_mc, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'FVMC estimates through time (close up)', \n    np.max(Q_track_mc, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\nSARSA\n\nplot_value_function(\n    'Sarsa estimates through time vs. true values', \n    np.max(Q_track_sarsa, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Sarsa estimates through time vs. true values (log scale)', \n    np.max(Q_track_sarsa, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Sarsa estimates through time (close up)', \n    np.max(Q_track_sarsa, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\nQ학습\n\nplot_value_function(\n    'Q-Learning estimates through time vs. true values', \n    np.max(Q_track_ql, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Q-Learning estimates through time vs. true values (log scale)', \n    np.max(Q_track_ql, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Q-Learning estimates through time (close up)', \n    np.max(Q_track_ql, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\n이중 Q학습\n\nplot_value_function(\n    'Double Q-Learning estimates through time vs. true values', \n    np.max(Q_track_dql, axis=2), \n    optimal_V,\n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Double Q-Learning estimates through time vs. true values (log scale)', \n    np.max(Q_track_dql, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Double Q-Learning estimates through time (close up)', \n    np.max(Q_track_dql, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\n\n정책 평가 비교\n\nmc_success_rate_ma, mc_mean_return_ma, mc_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_mc)\n\n\n\n\n\nsarsa_success_rate_ma, sarsa_mean_return_ma, sarsa_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_sarsa)\n\n\n\n\n\nql_success_rate_ma, ql_mean_return_ma, ql_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_ql)\n\n\n\n\n\ndql_success_rate_ma, dql_mean_return_ma, dql_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_dql)\n\n\n\n\n\nplt.axhline(y=success_rate_op, color='k', linestyle='-', linewidth=1)\nplt.text(int(len(mc_success_rate_ma)*1.02), success_rate_op*1.01, 'π*')\n\nplt.plot(mc_success_rate_ma, '-', linewidth=2, label='FVMC')\nplt.plot(sarsa_success_rate_ma, '--', linewidth=2, label='Sarsa')\nplt.plot(ql_success_rate_ma, ':', linewidth=2, label='Q-Learning')\nplt.plot(dql_success_rate_ma, '-.', linewidth=2, label='Double Q-Learning')\nplt.legend(loc=4, ncol=1)\n\nplt.title('Policy success rate (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Success rate %')\nplt.ylim(-1, 101)\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.axhline(y=mean_return_op, color='k', linestyle='-', linewidth=1)\nplt.text(int(len(mc_mean_return_ma)*1.02), mean_return_op*1.01, 'π*')\n\nplt.plot(mc_mean_return_ma, '-', linewidth=2, label='FVMC')\nplt.plot(sarsa_mean_return_ma, '--', linewidth=2, label='Sarsa')\nplt.plot(ql_mean_return_ma, ':', linewidth=2, label='Q-Learning')\nplt.plot(dql_mean_return_ma, '-.', linewidth=2, label='Double Q-Learning')\nplt.legend(loc=4, ncol=1)\n\nplt.title('Policy episode return (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Return (Gt:T)')\n\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.plot(mc_mean_regret_ma, '-', linewidth=2, label='FVMC')\nplt.plot(sarsa_mean_regret_ma, '--', linewidth=2, label='Sarsa')\nplt.plot(ql_mean_regret_ma, ':', linewidth=2, label='Q-Learning')\nplt.plot(dql_mean_regret_ma, '-.', linewidth=2, label='Double Q-Learning')\nplt.legend(loc=1, ncol=1)\n\nplt.title('Policy episode regret (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Regret (q* - Q)')\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.axhline(y=optimal_V[init_state], color='k', linestyle='-', linewidth=1)\nplt.text(int(len(Q_track_mc)*1.05), optimal_V[init_state]+.01, 'v*({})'.format(init_state))\n\nplt.plot(moving_average(np.max(Q_track_mc, axis=2).T[init_state]), \n         '-', linewidth=2, label='FVMC')\nplt.plot(moving_average(np.max(Q_track_sarsa, axis=2).T[init_state]), \n         '--', linewidth=2, label='Sarsa')\nplt.plot(moving_average(np.max(Q_track_ql, axis=2).T[init_state]), \n         ':', linewidth=2, label='Q-Learning')\nplt.plot(moving_average(np.max(Q_track_dql, axis=2).T[init_state]), \n         '-.', linewidth=2, label='Double Q-Learning')\nplt.legend(loc=4, ncol=1)\n\nplt.title('Estimated expected return (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Estimated value of initial state V({})'.format(init_state))\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_mc, axis=2) - optimal_V), axis=1)), \n         '-', linewidth=2, label='FVMC')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_sarsa, axis=2) - optimal_V), axis=1)), \n         '--', linewidth=2, label='Sarsa')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_ql, axis=2) - optimal_V), axis=1)), \n         ':', linewidth=2, label='Q-Learning')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_dql, axis=2) - optimal_V), axis=1)), \n         '-.', linewidth=2, label='Double Q-Learning')\nplt.legend(loc=1, ncol=1)\n\nplt.title('State-value function estimation error (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Mean Absolute Error MAE(V, v*)')\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.plot(moving_average(np.mean(np.abs(Q_track_mc - optimal_Q), axis=(1,2))), \n         '-', linewidth=2, label='FVMC')\nplt.plot(moving_average(np.mean(np.abs(Q_track_sarsa - optimal_Q), axis=(1,2))), \n         '--', linewidth=2, label='Sarsa')\nplt.plot(moving_average(np.mean(np.abs(Q_track_ql - optimal_Q), axis=(1,2))), \n         ':', linewidth=2, label='Q-Learning')\nplt.plot(moving_average(np.mean(np.abs(Q_track_dql - optimal_Q), axis=(1,2))), \n         '-.', linewidth=2, label='Double Q-Learning')\nplt.legend(loc=1, ncol=1)\n\nplt.title('Action-value function estimation error (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Mean Absolute Error MAE(Q, q*)')\nplt.xticks(rotation=45)\n\nplt.show()"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-6.html#russell-norvig의-gridworld-환경",
    "href": "publication/GDRL/GDRL-chapter-6.html#russell-norvig의-gridworld-환경",
    "title": "Chapter 6: Improving Agents behaviors",
    "section": "Russell & Norvig의 Gridworld 환경",
    "text": "Russell & Norvig의 Gridworld 환경\n\nenv = gym.make('RussellNorvigGridworld-v0')\ninit_state = env.reset()\ngoal_state = 3\ngamma = 1.0\nn_episodes = 4000\nP = env.env.P\nn_cols, svf_prec, err_prec, avf_prec=4, 4, 2, 3\naction_symbols=('&lt;', 'v', '&gt;', '^')\nlimit_items, limit_value = 5, 0.01\ncu_limit_items, cu_limit_value, cu_episodes = 10, 0.0, 1000\n\n\n알파와 입실론 스케쥴링\n\nplt.plot(decay_schedule(0.5, 0.01, 0.5, n_episodes), \n         '-', linewidth=2, \n         label='Alpha schedule')\nplt.plot(decay_schedule(1.0, 0.1, 0.9, n_episodes), \n         ':', linewidth=2, \n         label='Epsilon schedule')\nplt.legend(loc=1, ncol=1)\n\nplt.title('Alpha and epsilon schedules')\nplt.xlabel('Episodes')\nplt.ylabel('Hyperparameter values')\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\n\n이상적인 가치 함수와 정책\n\noptimal_Q, optimal_V, optimal_pi = value_iteration(P, gamma=gamma)\nprint_state_value_function(optimal_V, P, n_cols=n_cols, prec=svf_prec, title='Optimal state-value function:')\nprint()\n\nprint_action_value_function(optimal_Q, \n                            None, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Optimal action-value function:')\nprint()\nprint_policy(optimal_pi, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_op, mean_return_op, mean_regret_op = get_policy_metrics(\n    env, gamma=gamma, pi=optimal_pi, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_op, mean_return_op, mean_regret_op))\n\nOptimal state-value function:\n| 00 0.8116 | 01 0.8678 | 02 0.9178 |           |\n| 04 0.7616 |           | 06 0.6603 |           |\n| 08 0.7053 | 09 0.6553 | 10 0.6114 | 11 0.3879 |\n\nOptimal action-value function:\n╒═════╤═══════╤═══════╤════════╤════════╕\n│   s │     &lt; │     v │      &gt; │      ^ │\n╞═════╪═══════╪═══════╪════════╪════════╡\n│   0 │ 0.767 │ 0.737 │  0.812 │  0.777 │\n├─────┼───────┼───────┼────────┼────────┤\n│   1 │ 0.783 │ 0.827 │  0.868 │  0.827 │\n├─────┼───────┼───────┼────────┼────────┤\n│   2 │ 0.812 │ 0.675 │  0.918 │  0.881 │\n├─────┼───────┼───────┼────────┼────────┤\n│   3 │ 0     │ 0     │  0     │  0     │\n├─────┼───────┼───────┼────────┼────────┤\n│   4 │ 0.721 │ 0.677 │  0.721 │  0.762 │\n├─────┼───────┼───────┼────────┼────────┤\n│   5 │ 0     │ 0     │  0     │  0     │\n├─────┼───────┼───────┼────────┼────────┤\n│   6 │ 0.641 │ 0.415 │ -0.687 │  0.66  │\n├─────┼───────┼───────┼────────┼────────┤\n│   7 │ 0     │ 0     │  0     │  0     │\n├─────┼───────┼───────┼────────┼────────┤\n│   8 │ 0.671 │ 0.66  │  0.631 │  0.705 │\n├─────┼───────┼───────┼────────┼────────┤\n│   9 │ 0.655 │ 0.616 │  0.58  │  0.616 │\n├─────┼───────┼───────┼────────┼────────┤\n│  10 │ 0.611 │ 0.553 │  0.398 │  0.593 │\n├─────┼───────┼───────┼────────┼────────┤\n│  11 │ 0.388 │ 0.37  │  0.209 │ -0.74  │\n╘═════╧═══════╧═══════╧════════╧════════╛\n\n정책:\n| 00      &gt; | 01      &gt; | 02      &gt; |           |\n| 04      ^ |           | 06      ^ |           |\n| 08      ^ | 09      &lt; | 10      &lt; | 11      &lt; |\nReaches goal 96.00%. Obtains an average return of 0.6424. Regret of 0.0000\n\n\n\n\n몬테카를로 제어\n\nQ_mcs, V_mcs, Q_track_mcs = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_mc, V_mc, pi_mc, Q_track_mc, pi_track_mc = mc_control(env, gamma=gamma, n_episodes=n_episodes)\n    Q_mcs.append(Q_mc) ; V_mcs.append(V_mc) ; Q_track_mcs.append(Q_track_mc)\nQ_mc, V_mc, Q_track_mc = np.mean(Q_mcs, axis=0), np.mean(V_mcs, axis=0), np.mean(Q_track_mcs, axis=0)\ndel Q_mcs ; del V_mcs ; del Q_track_mcs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_mc, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by FVMC:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_mc - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_mc, optimal_V)))\nprint()\nprint_action_value_function(Q_mc, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='FVMC action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_mc, optimal_Q)))\nprint()\nprint_policy(pi_mc, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_mc, mean_return_mc, mean_regret_mc = get_policy_metrics(\n    env, gamma=gamma, pi=pi_mc, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_mc, mean_return_mc, mean_regret_mc))\n\nState-value function found by FVMC:\n| 00 0.7899 | 01 0.8518 | 02 0.9191 |           |\n| 04 0.7349 |           | 06 0.6609 |           |\n| 08 0.6698 | 09 0.5816 | 10 0.2939 | 11 -0.2194 |\nOptimal state-value function:\n| 00 0.8116 | 01 0.8678 | 02 0.9178 |           |\n| 04 0.7616 |           | 06 0.6603 |           |\n| 08 0.7053 | 09 0.6553 | 10 0.6114 | 11 0.3879 |\nState-value function errors:\n| 00  -0.02 | 01  -0.02 | 02    0.0 |           |\n| 04  -0.03 |           | 06    0.0 |           |\n| 08  -0.04 | 09  -0.07 | 10  -0.32 | 11  -0.61 |\nState-value function RMSE: 0.1995\n\nFVMC action-value function:\n╒═════╤════════╤════════╤════════╤════════╤═══════╤═══════╤════════╤════════╤════════╤════════╤════════╤════════╕\n│   s │      &lt; │      v │      &gt; │      ^ │   * &lt; │   * v │    * &gt; │    * ^ │   er &lt; │   er v │   er &gt; │   er ^ │\n╞═════╪════════╪════════╪════════╪════════╪═══════╪═══════╪════════╪════════╪════════╪════════╪════════╪════════╡\n│   0 │  0.66  │  0.618 │  0.79  │  0.668 │ 0.767 │ 0.737 │  0.812 │  0.777 │  0.106 │  0.119 │  0.022 │  0.109 │\n├─────┼────────┼────────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   1 │  0.661 │  0.747 │  0.852 │  0.76  │ 0.783 │ 0.827 │  0.868 │  0.827 │  0.121 │  0.08  │  0.016 │  0.068 │\n├─────┼────────┼────────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   2 │  0.727 │  0.571 │  0.919 │  0.816 │ 0.812 │ 0.675 │  0.918 │  0.881 │  0.085 │  0.104 │ -0.001 │  0.065 │\n├─────┼────────┼────────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   3 │  0     │  0     │  0     │  0     │ 0     │ 0     │  0     │  0     │  0     │  0     │  0     │  0     │\n├─────┼────────┼────────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   4 │  0.587 │  0.542 │  0.582 │  0.735 │ 0.721 │ 0.677 │  0.721 │  0.762 │  0.134 │  0.134 │  0.138 │  0.027 │\n├─────┼────────┼────────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   5 │  0     │  0     │  0     │  0     │ 0     │ 0     │  0     │  0     │  0     │  0     │  0     │  0     │\n├─────┼────────┼────────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   6 │  0.248 │  0.052 │ -0.698 │  0.661 │ 0.641 │ 0.415 │ -0.687 │  0.66  │  0.393 │  0.363 │  0.011 │ -0.001 │\n├─────┼────────┼────────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   7 │  0     │  0     │  0     │  0     │ 0     │ 0     │  0     │  0     │  0     │  0     │  0     │  0     │\n├─────┼────────┼────────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   8 │  0.541 │  0.505 │  0.441 │  0.67  │ 0.671 │ 0.66  │  0.631 │  0.705 │  0.13  │  0.155 │  0.19  │  0.036 │\n├─────┼────────┼────────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   9 │  0.526 │  0.097 │  0.132 │  0.041 │ 0.655 │ 0.616 │  0.58  │  0.616 │  0.13  │  0.519 │  0.448 │  0.575 │\n├─────┼────────┼────────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│  10 │ -0.074 │ -0.218 │ -0.332 │  0.185 │ 0.611 │ 0.553 │  0.398 │  0.593 │  0.685 │  0.771 │  0.73  │  0.408 │\n├─────┼────────┼────────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│  11 │ -0.242 │ -0.75  │ -0.829 │ -0.905 │ 0.388 │ 0.37  │  0.209 │ -0.74  │  0.63  │  1.12  │  1.038 │  0.165 │\n╘═════╧════════╧════════╧════════╧════════╧═══════╧═══════╧════════╧════════╧════════╧════════╧════════╧════════╛\nAction-value function RMSE: 0.3489\n\n정책:\n| 00      &gt; | 01      &gt; | 02      &gt; |           |\n| 04      ^ |           | 06      ^ |           |\n| 08      ^ | 09      &lt; | 10      &lt; | 11      &lt; |\nReaches goal 96.00%. Obtains an average return of 0.6424. Regret of 0.0000\n\n\n\n\nSARSA\n\nQ_sarsas, V_sarsas, Q_track_sarsas = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_sarsa, V_sarsa, pi_sarsa, Q_track_sarsa, pi_track_sarsa = sarsa(env, gamma=gamma, n_episodes=n_episodes)\n    Q_sarsas.append(Q_sarsa) ; V_sarsas.append(V_sarsa) ; Q_track_sarsas.append(Q_track_sarsa)\nQ_sarsa = np.mean(Q_sarsas, axis=0)\nV_sarsa = np.mean(V_sarsas, axis=0)\nQ_track_sarsa = np.mean(Q_track_sarsas, axis=0)\ndel Q_sarsas ; del V_sarsas ; del Q_track_sarsas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_sarsa, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Sarsa:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_sarsa - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_sarsa, optimal_V)))\nprint()\nprint_action_value_function(Q_sarsa, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Sarsa action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_sarsa, optimal_Q)))\nprint()\nprint_policy(pi_sarsa, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_sarsa, mean_return_sarsa, mean_regret_sarsa = get_policy_metrics(\n    env, gamma=gamma, pi=pi_sarsa, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_sarsa, mean_return_sarsa, mean_regret_sarsa))\n\nState-value function found by Sarsa:\n| 00 0.7646 | 01 0.8317 | 02 0.9003 |           |\n| 04 0.7009 |           | 06 0.6164 |           |\n| 08 0.6212 | 09 0.5314 | 10 0.1956 | 11 -0.4743 |\nOptimal state-value function:\n| 00 0.8116 | 01 0.8678 | 02 0.9178 |           |\n| 04 0.7616 |           | 06 0.6603 |           |\n| 08 0.7053 | 09 0.6553 | 10 0.6114 | 11 0.3879 |\nState-value function errors:\n| 00  -0.05 | 01  -0.04 | 02  -0.02 |           |\n| 04  -0.06 |           | 06  -0.04 |           |\n| 08  -0.08 | 09  -0.12 | 10  -0.42 | 11  -0.86 |\nState-value function RMSE: 0.2811\n\nSarsa action-value function:\n╒═════╤════════╤════════╤════════╤════════╤═══════╤═══════╤════════╤════════╤════════╤════════╤════════╤════════╕\n│   s │      &lt; │      v │      &gt; │      ^ │   * &lt; │   * v │    * &gt; │    * ^ │   er &lt; │   er v │   er &gt; │   er ^ │\n╞═════╪════════╪════════╪════════╪════════╪═══════╪═══════╪════════╪════════╪════════╪════════╪════════╪════════╡\n│   0 │  0.645 │  0.603 │  0.765 │  0.667 │ 0.767 │ 0.737 │  0.812 │  0.777 │  0.121 │  0.134 │  0.047 │  0.11  │\n├─────┼────────┼────────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   1 │  0.67  │  0.739 │  0.832 │  0.738 │ 0.783 │ 0.827 │  0.868 │  0.827 │  0.113 │  0.088 │  0.036 │  0.089 │\n├─────┼────────┼────────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   2 │  0.713 │  0.492 │  0.9   │  0.82  │ 0.812 │ 0.675 │  0.918 │  0.881 │  0.1   │  0.183 │  0.018 │  0.061 │\n├─────┼────────┼────────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   3 │  0     │  0     │  0     │  0     │ 0     │ 0     │  0     │  0     │  0     │  0     │  0     │  0     │\n├─────┼────────┼────────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   4 │  0.583 │  0.511 │  0.586 │  0.701 │ 0.721 │ 0.677 │  0.721 │  0.762 │  0.138 │  0.166 │  0.135 │  0.061 │\n├─────┼────────┼────────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   5 │  0     │  0     │  0     │  0     │ 0     │ 0     │  0     │  0     │  0     │  0     │  0     │  0     │\n├─────┼────────┼────────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   6 │  0.221 │ -0.22  │ -0.768 │  0.616 │ 0.641 │ 0.415 │ -0.687 │  0.66  │  0.42  │  0.635 │  0.081 │  0.044 │\n├─────┼────────┼────────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   7 │  0     │  0     │  0     │  0     │ 0     │ 0     │  0     │  0     │  0     │  0     │  0     │  0     │\n├─────┼────────┼────────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   8 │  0.485 │  0.468 │  0.371 │  0.621 │ 0.671 │ 0.66  │  0.631 │  0.705 │  0.186 │  0.192 │  0.26  │  0.084 │\n├─────┼────────┼────────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   9 │  0.531 │  0.103 │ -0.047 │  0.125 │ 0.655 │ 0.616 │  0.58  │  0.616 │  0.124 │  0.513 │  0.627 │  0.491 │\n├─────┼────────┼────────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│  10 │ -0.052 │ -0.341 │ -0.722 │  0.045 │ 0.611 │ 0.553 │  0.398 │  0.593 │  0.663 │  0.894 │  1.12  │  0.548 │\n├─────┼────────┼────────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│  11 │ -0.474 │ -0.828 │ -0.9   │ -0.976 │ 0.388 │ 0.37  │  0.209 │ -0.74  │  0.862 │  1.198 │  1.109 │  0.236 │\n╘═════╧════════╧════════╧════════╧════════╧═══════╧═══════╧════════╧════════╧════════╧════════╧════════╧════════╛\nAction-value function RMSE: 0.4107\n\n정책:\n| 00      &gt; | 01      &gt; | 02      &gt; |           |\n| 04      ^ |           | 06      ^ |           |\n| 08      ^ | 09      &lt; | 10      ^ | 11      &lt; |\nReaches goal 96.00%. Obtains an average return of 0.6424. Regret of 0.0000\n\n\n\n\nQ학습\n\nQ_qls, V_qls, Q_track_qls = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_ql, V_ql, pi_ql, Q_track_ql, pi_track_ql = q_learning(env, gamma=gamma, n_episodes=n_episodes)\n    Q_qls.append(Q_ql) ; V_qls.append(V_ql) ; Q_track_qls.append(Q_track_ql)\nQ_ql = np.mean(Q_qls, axis=0)\nV_ql = np.mean(V_qls, axis=0)\nQ_track_ql = np.mean(Q_track_qls, axis=0)\ndel Q_qls ; del V_qls ; del Q_track_qls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_ql, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Q-learning:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_ql - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_ql, optimal_V)))\nprint()\nprint_action_value_function(Q_ql, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Q-learning action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_ql, optimal_Q)))\nprint()\nprint_policy(pi_ql, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_ql, mean_return_ql, mean_regret_ql = get_policy_metrics(\n    env, gamma=gamma, pi=pi_ql, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_ql, mean_return_ql, mean_regret_ql))\n\nState-value function found by Q-learning:\n| 00 0.8146 | 01 0.8688 | 02 0.9164 |           |\n| 04 0.7645 |           | 06 0.6005 |           |\n| 08 0.7068 | 09 0.6558 | 10 0.6114 | 11  0.438 |\nOptimal state-value function:\n| 00 0.8116 | 01 0.8678 | 02 0.9178 |           |\n| 04 0.7616 |           | 06 0.6603 |           |\n| 08 0.7053 | 09 0.6553 | 10 0.6114 | 11 0.3879 |\nState-value function errors:\n| 00    0.0 | 01    0.0 | 02   -0.0 |           |\n| 04    0.0 |           | 06  -0.06 |           |\n| 08    0.0 | 09    0.0 | 10    0.0 | 11   0.05 |\nState-value function RMSE: 0.0225\n\nQ-learning action-value function:\n╒═════╤═══════╤═══════╤════════╤════════╤═══════╤═══════╤════════╤════════╤════════╤════════╤════════╤════════╕\n│   s │     &lt; │     v │      &gt; │      ^ │   * &lt; │   * v │    * &gt; │    * ^ │   er &lt; │   er v │   er &gt; │   er ^ │\n╞═════╪═══════╪═══════╪════════╪════════╪═══════╪═══════╪════════╪════════╪════════╪════════╪════════╪════════╡\n│   0 │ 0.768 │ 0.741 │  0.815 │  0.779 │ 0.767 │ 0.737 │  0.812 │  0.777 │ -0.002 │ -0.004 │ -0.003 │ -0.002 │\n├─────┼───────┼───────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   1 │ 0.785 │ 0.828 │  0.869 │  0.829 │ 0.783 │ 0.827 │  0.868 │  0.827 │ -0.002 │ -0.001 │ -0.001 │ -0.002 │\n├─────┼───────┼───────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   2 │ 0.812 │ 0.675 │  0.916 │  0.882 │ 0.812 │ 0.675 │  0.918 │  0.881 │ -0     │  0     │  0.001 │ -0.001 │\n├─────┼───────┼───────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   3 │ 0     │ 0     │  0     │  0     │ 0     │ 0     │  0     │  0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   4 │ 0.722 │ 0.679 │  0.722 │  0.764 │ 0.721 │ 0.677 │  0.721 │  0.762 │ -0.001 │ -0.002 │ -0.001 │ -0.003 │\n├─────┼───────┼───────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   5 │ 0     │ 0     │  0     │  0     │ 0     │ 0     │  0     │  0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   6 │ 0.582 │ 0.435 │ -0.716 │  0.593 │ 0.641 │ 0.415 │ -0.687 │  0.66  │  0.059 │ -0.02  │  0.029 │  0.068 │\n├─────┼───────┼───────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   7 │ 0     │ 0     │  0     │  0     │ 0     │ 0     │  0     │  0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   8 │ 0.674 │ 0.664 │  0.635 │  0.707 │ 0.671 │ 0.66  │  0.631 │  0.705 │ -0.003 │ -0.004 │ -0.004 │ -0.002 │\n├─────┼───────┼───────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   9 │ 0.656 │ 0.621 │  0.586 │  0.623 │ 0.655 │ 0.616 │  0.58  │  0.616 │ -0.001 │ -0.005 │ -0.006 │ -0.007 │\n├─────┼───────┼───────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│  10 │ 0.611 │ 0.579 │  0.463 │  0.586 │ 0.611 │ 0.553 │  0.398 │  0.593 │ -0     │ -0.026 │ -0.065 │  0.007 │\n├─────┼───────┼───────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│  11 │ 0.402 │ 0.423 │  0.298 │ -0.795 │ 0.388 │ 0.37  │  0.209 │ -0.74  │ -0.015 │ -0.052 │ -0.089 │  0.055 │\n╘═════╧═══════╧═══════╧════════╧════════╧═══════╧═══════╧════════╧════════╧════════╧════════╧════════╧════════╛\nAction-value function RMSE: 0.0243\n\n정책:\n| 00      &gt; | 01      &gt; | 02      &gt; |           |\n| 04      ^ |           | 06      ^ |           |\n| 08      ^ | 09      &lt; | 10      &lt; | 11      &lt; |\nReaches goal 96.00%. Obtains an average return of 0.6424. Regret of 0.0000\n\n\n\n\n이중 Q학습\n\nQ_dqls, V_dqls, Q_track_dqls = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_dql, V_dql, pi_dql, Q_track_dql, pi_track_dql = double_q_learning(env, gamma=gamma, n_episodes=n_episodes)\n    Q_dqls.append(Q_dql) ; V_dqls.append(V_dql) ; Q_track_dqls.append(Q_track_dql)\nQ_dql, V_dql, Q_track_dql = np.mean(Q_dqls, axis=0), np.mean(V_dqls, axis=0), np.mean(Q_track_dqls, axis=0)\ndel Q_dqls ; del V_dqls ; del Q_track_dqls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_dql, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Double Q-Learning:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_dql - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_dql, optimal_V)))\nprint()\nprint_action_value_function(Q_dql, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Double Q-Learning action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_dql, optimal_Q)))\nprint()\nprint_policy(pi_dql, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_dql, mean_return_dql, mean_regret_dql = get_policy_metrics(\n    env, gamma=gamma, pi=pi_dql, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_dql, mean_return_dql, mean_regret_dql))\n\nState-value function found by Double Q-Learning:\n| 00 0.8098 | 01 0.8667 | 02 0.9186 |           |\n| 04 0.7589 |           | 06 0.6613 |           |\n| 08 0.7035 | 09 0.6532 | 10 0.5885 | 11 0.3222 |\nOptimal state-value function:\n| 00 0.8116 | 01 0.8678 | 02 0.9178 |           |\n| 04 0.7616 |           | 06 0.6603 |           |\n| 08 0.7053 | 09 0.6553 | 10 0.6114 | 11 0.3879 |\nState-value function errors:\n| 00   -0.0 | 01   -0.0 | 02    0.0 |           |\n| 04   -0.0 |           | 06    0.0 |           |\n| 08   -0.0 | 09   -0.0 | 10  -0.02 | 11  -0.07 |\nState-value function RMSE: 0.0201\n\nDouble Q-Learning action-value function:\n╒═════╤═══════╤═══════╤════════╤════════╤═══════╤═══════╤════════╤════════╤════════╤════════╤════════╤════════╕\n│   s │     &lt; │     v │      &gt; │      ^ │   * &lt; │   * v │    * &gt; │    * ^ │   er &lt; │   er v │   er &gt; │   er ^ │\n╞═════╪═══════╪═══════╪════════╪════════╪═══════╪═══════╪════════╪════════╪════════╪════════╪════════╪════════╡\n│   0 │ 0.761 │ 0.734 │  0.81  │  0.772 │ 0.767 │ 0.737 │  0.812 │  0.777 │  0.005 │  0.003 │  0.002 │  0.005 │\n├─────┼───────┼───────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   1 │ 0.778 │ 0.821 │  0.867 │  0.822 │ 0.783 │ 0.827 │  0.868 │  0.827 │  0.005 │  0.006 │  0.001 │  0.006 │\n├─────┼───────┼───────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   2 │ 0.807 │ 0.654 │  0.919 │  0.875 │ 0.812 │ 0.675 │  0.918 │  0.881 │  0.005 │  0.021 │ -0.001 │  0.006 │\n├─────┼───────┼───────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   3 │ 0     │ 0     │  0     │  0     │ 0     │ 0     │  0     │  0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   4 │ 0.716 │ 0.673 │  0.717 │  0.759 │ 0.721 │ 0.677 │  0.721 │  0.762 │  0.005 │  0.004 │  0.004 │  0.003 │\n├─────┼───────┼───────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   5 │ 0     │ 0     │  0     │  0     │ 0     │ 0     │  0     │  0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   6 │ 0.536 │ 0.367 │ -0.689 │  0.661 │ 0.641 │ 0.415 │ -0.687 │  0.66  │  0.105 │  0.048 │  0.002 │ -0.001 │\n├─────┼───────┼───────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   7 │ 0     │ 0     │  0     │  0     │ 0     │ 0     │  0     │  0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   8 │ 0.665 │ 0.656 │  0.622 │  0.704 │ 0.671 │ 0.66  │  0.631 │  0.705 │  0.006 │  0.004 │  0.009 │  0.002 │\n├─────┼───────┼───────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│   9 │ 0.653 │ 0.601 │  0.534 │  0.598 │ 0.655 │ 0.616 │  0.58  │  0.616 │  0.002 │  0.015 │  0.047 │  0.017 │\n├─────┼───────┼───────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│  10 │ 0.588 │ 0.458 │  0.259 │  0.494 │ 0.611 │ 0.553 │  0.398 │  0.593 │  0.023 │  0.096 │  0.138 │  0.099 │\n├─────┼───────┼───────┼────────┼────────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│  11 │ 0.314 │ 0.13  │  0.014 │ -0.741 │ 0.388 │ 0.37  │  0.209 │ -0.74  │  0.074 │  0.241 │  0.196 │  0.001 │\n╘═════╧═══════╧═══════╧════════╧════════╧═══════╧═══════╧════════╧════════╧════════╧════════╧════════╧════════╛\nAction-value function RMSE: 0.0572\n\n정책:\n| 00      &gt; | 01      &gt; | 02      &gt; |           |\n| 04      ^ |           | 06      ^ |           |\n| 08      ^ | 09      &lt; | 10      &lt; | 11      &lt; |\nReaches goal 96.00%. Obtains an average return of 0.6424. Regret of 0.0000\n\n\n\n\n매 에피소드별 max(Q) 비교\n\n첫방문 몬테카를로\n\nplot_value_function(\n    'FVMC estimates through time vs. true values', \n    np.max(Q_track_mc, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'FVMC estimates through time vs. true values (log scale)', \n    np.max(Q_track_mc, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'FVMC estimates through time (close up)', \n    np.max(Q_track_mc, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\nSARSA\n\nplot_value_function(\n    'Sarsa estimates through time vs. true values', \n    np.max(Q_track_sarsa, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Sarsa estimates through time vs. true values (log scale)', \n    np.max(Q_track_sarsa, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Sarsa estimates through time (close up)', \n    np.max(Q_track_sarsa, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\nQ학습\n\nplot_value_function(\n    'Q-Learning estimates through time vs. true values', \n    np.max(Q_track_ql, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Q-Learning estimates through time vs. true values (log scale)', \n    np.max(Q_track_ql, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Q-Learning estimates through time (close up)', \n    np.max(Q_track_ql, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\n이중 Q학습\n\nplot_value_function(\n    'Double Q-Learning estimates through time vs. true values', \n    np.max(Q_track_dql, axis=2), \n    optimal_V,\n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Double Q-Learning estimates through time vs. true values (log scale)', \n    np.max(Q_track_dql, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Double Q-Learning estimates through time (close up)', \n    np.max(Q_track_dql, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\n\n정책 평가 비교\n\nmc_success_rate_ma, mc_mean_return_ma, mc_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_mc)\n\n\n\n\n\nsarsa_success_rate_ma, sarsa_mean_return_ma, sarsa_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_sarsa)\n\n\n\n\n\nql_success_rate_ma, ql_mean_return_ma, ql_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_ql)\n\n\n\n\n\ndql_success_rate_ma, dql_mean_return_ma, dql_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_dql)\n\n\n\n\n\nplt.axhline(y=success_rate_op, color='k', linestyle='-', linewidth=1)\nplt.text(int(len(mc_success_rate_ma)*1.02), success_rate_op*1.01, 'π*')\n\nplt.plot(mc_success_rate_ma, '-', linewidth=2, label='FVMC')\nplt.plot(sarsa_success_rate_ma, '--', linewidth=2, label='Sarsa')\nplt.plot(ql_success_rate_ma, ':', linewidth=2, label='Q-Learning')\nplt.plot(dql_success_rate_ma, '-.', linewidth=2, label='Double Q-Learning')\nplt.legend(loc=4, ncol=1)\n\nplt.title('Policy success rate (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Success rate %')\nplt.ylim(-1, 101)\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.axhline(y=mean_return_op, color='k', linestyle='-', linewidth=1)\nplt.text(int(len(mc_mean_return_ma)*1.02), mean_return_op*1.01, 'π*')\n\nplt.plot(mc_mean_return_ma, '-', linewidth=2, label='FVMC')\nplt.plot(sarsa_mean_return_ma, '--', linewidth=2, label='Sarsa')\nplt.plot(ql_mean_return_ma, ':', linewidth=2, label='Q-Learning')\nplt.plot(dql_mean_return_ma, '-.', linewidth=2, label='Double Q-Learning')\nplt.legend(loc=4, ncol=1)\n\nplt.title('Policy episode return (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Return (Gt:T)')\n\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.plot(mc_mean_regret_ma, '-', linewidth=2, label='FVMC')\nplt.plot(sarsa_mean_regret_ma, '--', linewidth=2, label='Sarsa')\nplt.plot(ql_mean_regret_ma, ':', linewidth=2, label='Q-Learning')\nplt.plot(dql_mean_regret_ma, '-.', linewidth=2, label='Double Q-Learning')\nplt.legend(loc=1, ncol=1)\n\nplt.title('Policy episode regret (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Regret (q* - Q)')\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.axhline(y=optimal_V[init_state], color='k', linestyle='-', linewidth=1)\nplt.text(int(len(Q_track_mc)*1.05), optimal_V[init_state]+.01, 'v*({})'.format(init_state))\n\nplt.plot(moving_average(np.max(Q_track_mc, axis=2).T[init_state]), \n         '-', linewidth=2, label='FVMC')\nplt.plot(moving_average(np.max(Q_track_sarsa, axis=2).T[init_state]), \n         '--', linewidth=2, label='Sarsa')\nplt.plot(moving_average(np.max(Q_track_ql, axis=2).T[init_state]), \n         ':', linewidth=2, label='Q-Learning')\nplt.plot(moving_average(np.max(Q_track_dql, axis=2).T[init_state]), \n         '-.', linewidth=2, label='Double Q-Learning')\nplt.legend(loc=4, ncol=1)\n\nplt.title('Estimated expected return (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Estimated value of initial state V({})'.format(init_state))\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_mc, axis=2) - optimal_V), axis=1)), \n         '-', linewidth=2, label='FVMC')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_sarsa, axis=2) - optimal_V), axis=1)), \n         '--', linewidth=2, label='Sarsa')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_ql, axis=2) - optimal_V), axis=1)), \n         ':', linewidth=2, label='Q-Learning')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_dql, axis=2) - optimal_V), axis=1)), \n         '-.', linewidth=2, label='Double Q-Learning')\nplt.legend(loc=1, ncol=1)\n\nplt.title('State-value function estimation error (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Mean Absolute Error MAE(V, v*)')\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.plot(moving_average(np.mean(np.abs(Q_track_mc - optimal_Q), axis=(1,2))), \n         '-', linewidth=2, label='FVMC')\nplt.plot(moving_average(np.mean(np.abs(Q_track_sarsa - optimal_Q), axis=(1,2))), \n         '--', linewidth=2, label='Sarsa')\nplt.plot(moving_average(np.mean(np.abs(Q_track_ql - optimal_Q), axis=(1,2))), \n         ':', linewidth=2, label='Q-Learning')\nplt.plot(moving_average(np.mean(np.abs(Q_track_dql - optimal_Q), axis=(1,2))), \n         '-.', linewidth=2, label='Double Q-Learning')\nplt.legend(loc=1, ncol=1)\n\nplt.title('Action-value function estimation error (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Mean Absolute Error MAE(Q, q*)')\nplt.xticks(rotation=45)\n\nplt.show()"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-6.html#프로즌레이크-환경",
    "href": "publication/GDRL/GDRL-chapter-6.html#프로즌레이크-환경",
    "title": "Chapter 6: Improving Agents behaviors",
    "section": "프로즌레이크 환경",
    "text": "프로즌레이크 환경\n\nenv = gym.make('FrozenLake-v0')\ninit_state = env.reset()\ngoal_state = 15\ngamma = 0.99\nn_episodes = 10000\nP = env.env.P\nn_cols, svf_prec, err_prec, avf_prec=4, 4, 2, 3\naction_symbols=('&lt;', 'v', '&gt;', '^')\nlimit_items, limit_value = 5, 0.0\ncu_limit_items, cu_limit_value, cu_episodes = 10, 0.01, 2000\n\n\n알파와 입실론 스케쥴링\n\nplt.plot(decay_schedule(0.5, 0.01, 0.5, n_episodes), \n         '-', linewidth=2, \n         label='Alpha schedule')\nplt.plot(decay_schedule(1.0, 0.1, 0.9, n_episodes), \n         ':', linewidth=2, \n         label='Epsilon schedule')\nplt.legend(loc=1, ncol=1)\n\nplt.title('Alpha and epsilon schedules')\nplt.xlabel('Episodes')\nplt.ylabel('Hyperparameter values')\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\n\n이상적인 가치 함수와 정책\n\noptimal_Q, optimal_V, optimal_pi = value_iteration(P, gamma=gamma)\nprint_state_value_function(optimal_V, P, n_cols=n_cols, prec=svf_prec, title='Optimal state-value function:')\nprint()\n\nprint_action_value_function(optimal_Q, \n                            None, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Optimal action-value function:')\nprint()\nprint_policy(optimal_pi, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_op, mean_return_op, mean_regret_op = get_policy_metrics(\n    env, gamma=gamma, pi=optimal_pi, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_op, mean_return_op, mean_regret_op))\n\nOptimal state-value function:\n| 00  0.542 | 01 0.4988 | 02 0.4707 | 03 0.4569 |\n| 04 0.5585 |           | 06 0.3583 |           |\n| 08 0.5918 | 09 0.6431 | 10 0.6152 |           |\n|           | 13 0.7417 | 14 0.8628 |           |\n\nOptimal action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╕\n│   s │     &lt; │     v │     &gt; │     ^ │\n╞═════╪═══════╪═══════╪═══════╪═══════╡\n│   0 │ 0.542 │ 0.528 │ 0.528 │ 0.522 │\n├─────┼───────┼───────┼───────┼───────┤\n│   1 │ 0.343 │ 0.334 │ 0.32  │ 0.499 │\n├─────┼───────┼───────┼───────┼───────┤\n│   2 │ 0.438 │ 0.434 │ 0.424 │ 0.471 │\n├─────┼───────┼───────┼───────┼───────┤\n│   3 │ 0.306 │ 0.306 │ 0.302 │ 0.457 │\n├─────┼───────┼───────┼───────┼───────┤\n│   4 │ 0.558 │ 0.38  │ 0.374 │ 0.363 │\n├─────┼───────┼───────┼───────┼───────┤\n│   5 │ 0     │ 0     │ 0     │ 0     │\n├─────┼───────┼───────┼───────┼───────┤\n│   6 │ 0.358 │ 0.203 │ 0.358 │ 0.155 │\n├─────┼───────┼───────┼───────┼───────┤\n│   7 │ 0     │ 0     │ 0     │ 0     │\n├─────┼───────┼───────┼───────┼───────┤\n│   8 │ 0.38  │ 0.408 │ 0.397 │ 0.592 │\n├─────┼───────┼───────┼───────┼───────┤\n│   9 │ 0.44  │ 0.643 │ 0.448 │ 0.398 │\n├─────┼───────┼───────┼───────┼───────┤\n│  10 │ 0.615 │ 0.497 │ 0.403 │ 0.33  │\n├─────┼───────┼───────┼───────┼───────┤\n│  11 │ 0     │ 0     │ 0     │ 0     │\n├─────┼───────┼───────┼───────┼───────┤\n│  12 │ 0     │ 0     │ 0     │ 0     │\n├─────┼───────┼───────┼───────┼───────┤\n│  13 │ 0.457 │ 0.53  │ 0.742 │ 0.497 │\n├─────┼───────┼───────┼───────┼───────┤\n│  14 │ 0.733 │ 0.863 │ 0.821 │ 0.781 │\n├─────┼───────┼───────┼───────┼───────┤\n│  15 │ 0     │ 0     │ 0     │ 0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╛\n\n정책:\n| 00      &lt; | 01      ^ | 02      ^ | 03      ^ |\n| 04      &lt; |           | 06      &lt; |           |\n| 08      ^ | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      v |           |\nReaches goal 74.00%. Obtains an average return of 0.5116. Regret of 0.0000\n\n\n\n\n몬테카를로 제어\n\nQ_mcs, V_mcs, Q_track_mcs = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_mc, V_mc, pi_mc, Q_track_mc, pi_track_mc = mc_control(env, gamma=gamma, n_episodes=n_episodes)\n    Q_mcs.append(Q_mc) ; V_mcs.append(V_mc) ; Q_track_mcs.append(Q_track_mc)\nQ_mc, V_mc, Q_track_mc = np.mean(Q_mcs, axis=0), np.mean(V_mcs, axis=0), np.mean(Q_track_mcs, axis=0)\ndel Q_mcs ; del V_mcs ; del Q_track_mcs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_mc, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by FVMC:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_mc - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_mc, optimal_V)))\nprint()\nprint_action_value_function(Q_mc, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='FVMC action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_mc, optimal_Q)))\nprint()\nprint_policy(pi_mc, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_mc, mean_return_mc, mean_regret_mc = get_policy_metrics(\n    env, gamma=gamma, pi=pi_mc, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_mc, mean_return_mc, mean_regret_mc))\n\nState-value function found by FVMC:\n| 00 0.2924 | 01 0.1962 | 02 0.1745 | 03 0.0798 |\n| 04 0.3093 |           | 06 0.2139 |           |\n| 08 0.3592 | 09 0.4479 | 10 0.4471 |           |\n|           | 13 0.5975 | 14 0.7784 |           |\nOptimal state-value function:\n| 00  0.542 | 01 0.4988 | 02 0.4707 | 03 0.4569 |\n| 04 0.5585 |           | 06 0.3583 |           |\n| 08 0.5918 | 09 0.6431 | 10 0.6152 |           |\n|           | 13 0.7417 | 14 0.8628 |           |\nState-value function errors:\n| 00  -0.25 | 01   -0.3 | 02   -0.3 | 03  -0.38 |\n| 04  -0.25 |           | 06  -0.14 |           |\n| 08  -0.23 | 09   -0.2 | 10  -0.17 |           |\n|           | 13  -0.14 | 14  -0.08 |           |\nState-value function RMSE: 0.1961\n\nFVMC action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╤════════╤════════╕\n│   s │     &lt; │     v │     &gt; │     ^ │   * &lt; │   * v │   * &gt; │   * ^ │   er &lt; │   er v │   er &gt; │   er ^ │\n╞═════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╪════════╪════════╡\n│   0 │ 0.292 │ 0.232 │ 0.239 │ 0.243 │ 0.542 │ 0.528 │ 0.528 │ 0.522 │  0.25  │  0.296 │  0.289 │  0.279 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   1 │ 0.09  │ 0.108 │ 0.079 │ 0.196 │ 0.343 │ 0.334 │ 0.32  │ 0.499 │  0.253 │  0.226 │  0.241 │  0.303 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   2 │ 0.149 │ 0.134 │ 0.114 │ 0.109 │ 0.438 │ 0.434 │ 0.424 │ 0.471 │  0.289 │  0.3   │  0.31  │  0.362 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   3 │ 0.057 │ 0.049 │ 0.013 │ 0.041 │ 0.306 │ 0.306 │ 0.302 │ 0.457 │  0.25  │  0.257 │  0.289 │  0.416 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   4 │ 0.309 │ 0.203 │ 0.192 │ 0.171 │ 0.558 │ 0.38  │ 0.374 │ 0.363 │  0.249 │  0.177 │  0.182 │  0.192 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   5 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   6 │ 0.152 │ 0.094 │ 0.18  │ 0.03  │ 0.358 │ 0.203 │ 0.358 │ 0.155 │  0.207 │  0.109 │  0.178 │  0.125 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   7 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   8 │ 0.189 │ 0.235 │ 0.232 │ 0.359 │ 0.38  │ 0.408 │ 0.397 │ 0.592 │  0.191 │  0.172 │  0.165 │  0.233 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   9 │ 0.261 │ 0.448 │ 0.317 │ 0.2   │ 0.44  │ 0.643 │ 0.448 │ 0.398 │  0.179 │  0.195 │  0.131 │  0.198 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  10 │ 0.405 │ 0.34  │ 0.269 │ 0.162 │ 0.615 │ 0.497 │ 0.403 │ 0.33  │  0.21  │  0.157 │  0.134 │  0.168 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  11 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  12 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  13 │ 0.27  │ 0.378 │ 0.598 │ 0.356 │ 0.457 │ 0.53  │ 0.742 │ 0.497 │  0.187 │  0.151 │  0.144 │  0.141 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  14 │ 0.543 │ 0.778 │ 0.667 │ 0.607 │ 0.733 │ 0.863 │ 0.821 │ 0.781 │  0.19  │  0.084 │  0.154 │  0.174 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  15 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╧════════╧════════╛\nAction-value function RMSE: 0.1859\n\n정책:\n| 00      &lt; | 01      ^ | 02      &gt; | 03      &lt; |\n| 04      &lt; |           | 06      &lt; |           |\n| 08      ^ | 09      v | 10      v |           |\n|           | 13      &gt; | 14      v |           |\nReaches goal 62.00%. Obtains an average return of 0.4355. Regret of 0.1419\n\n\n\n\nSARSA\n\nQ_sarsas, V_sarsas, Q_track_sarsas = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_sarsa, V_sarsa, pi_sarsa, Q_track_sarsa, pi_track_sarsa = sarsa(env, gamma=gamma, n_episodes=n_episodes)\n    Q_sarsas.append(Q_sarsa) ; V_sarsas.append(V_sarsa) ; Q_track_sarsas.append(Q_track_sarsa)\nQ_sarsa = np.mean(Q_sarsas, axis=0)\nV_sarsa = np.mean(V_sarsas, axis=0)\nQ_track_sarsa = np.mean(Q_track_sarsas, axis=0)\ndel Q_sarsas ; del V_sarsas ; del Q_track_sarsas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_sarsa, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Sarsa:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_sarsa - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_sarsa, optimal_V)))\nprint()\nprint_action_value_function(Q_sarsa, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Sarsa action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_sarsa, optimal_Q)))\nprint()\nprint_policy(pi_sarsa, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_sarsa, mean_return_sarsa, mean_regret_sarsa = get_policy_metrics(\n    env, gamma=gamma, pi=pi_sarsa, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_sarsa, mean_return_sarsa, mean_regret_sarsa))\n\nState-value function found by Sarsa:\n| 00 0.2822 | 01 0.2237 | 02 0.1984 | 03 0.1127 |\n| 04 0.3003 |           | 06 0.2074 |           |\n| 08 0.3473 | 09 0.4417 | 10 0.4533 |           |\n|           | 13 0.5771 | 14 0.7754 |           |\nOptimal state-value function:\n| 00  0.542 | 01 0.4988 | 02 0.4707 | 03 0.4569 |\n| 04 0.5585 |           | 06 0.3583 |           |\n| 08 0.5918 | 09 0.6431 | 10 0.6152 |           |\n|           | 13 0.7417 | 14 0.8628 |           |\nState-value function errors:\n| 00  -0.26 | 01  -0.28 | 02  -0.27 | 03  -0.34 |\n| 04  -0.26 |           | 06  -0.15 |           |\n| 08  -0.24 | 09   -0.2 | 10  -0.16 |           |\n|           | 13  -0.16 | 14  -0.09 |           |\nState-value function RMSE: 0.1915\n\nSarsa action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╤════════╤════════╕\n│   s │     &lt; │     v │     &gt; │     ^ │   * &lt; │   * v │   * &gt; │   * ^ │   er &lt; │   er v │   er &gt; │   er ^ │\n╞═════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╪════════╪════════╡\n│   0 │ 0.282 │ 0.257 │ 0.257 │ 0.253 │ 0.542 │ 0.528 │ 0.528 │ 0.522 │  0.26  │  0.271 │  0.271 │  0.27  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   1 │ 0.118 │ 0.105 │ 0.092 │ 0.224 │ 0.343 │ 0.334 │ 0.32  │ 0.499 │  0.225 │  0.229 │  0.228 │  0.275 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   2 │ 0.198 │ 0.117 │ 0.115 │ 0.113 │ 0.438 │ 0.434 │ 0.424 │ 0.471 │  0.24  │  0.317 │  0.309 │  0.358 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   3 │ 0.035 │ 0.033 │ 0.024 │ 0.113 │ 0.306 │ 0.306 │ 0.302 │ 0.457 │  0.271 │  0.273 │  0.277 │  0.344 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   4 │ 0.3   │ 0.209 │ 0.209 │ 0.188 │ 0.558 │ 0.38  │ 0.374 │ 0.363 │  0.258 │  0.171 │  0.165 │  0.176 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   5 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   6 │ 0.171 │ 0.114 │ 0.184 │ 0.038 │ 0.358 │ 0.203 │ 0.358 │ 0.155 │  0.188 │  0.089 │  0.174 │  0.117 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   7 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   8 │ 0.203 │ 0.254 │ 0.235 │ 0.347 │ 0.38  │ 0.408 │ 0.397 │ 0.592 │  0.177 │  0.153 │  0.162 │  0.244 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   9 │ 0.281 │ 0.442 │ 0.321 │ 0.246 │ 0.44  │ 0.643 │ 0.448 │ 0.398 │  0.159 │  0.201 │  0.127 │  0.152 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  10 │ 0.453 │ 0.362 │ 0.29  │ 0.166 │ 0.615 │ 0.497 │ 0.403 │ 0.33  │  0.162 │  0.135 │  0.113 │  0.165 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  11 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  12 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  13 │ 0.305 │ 0.404 │ 0.577 │ 0.367 │ 0.457 │ 0.53  │ 0.742 │ 0.497 │  0.152 │  0.125 │  0.165 │  0.13  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  14 │ 0.536 │ 0.775 │ 0.703 │ 0.621 │ 0.733 │ 0.863 │ 0.821 │ 0.781 │  0.197 │  0.087 │  0.118 │  0.161 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  15 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╧════════╧════════╛\nAction-value function RMSE: 0.176\n\n정책:\n| 00      &lt; | 01      ^ | 02      &lt; | 03      ^ |\n| 04      &lt; |           | 06      &gt; |           |\n| 08      ^ | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      v |           |\nReaches goal 70.00%. Obtains an average return of 0.4864. Regret of 0.0156\n\n\n\n\nQ학습\n\nQ_qls, V_qls, Q_track_qls = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_ql, V_ql, pi_ql, Q_track_ql, pi_track_ql = q_learning(env, gamma=gamma, n_episodes=n_episodes)\n    Q_qls.append(Q_ql) ; V_qls.append(V_ql) ; Q_track_qls.append(Q_track_ql)\nQ_ql = np.mean(Q_qls, axis=0)\nV_ql = np.mean(V_qls, axis=0)\nQ_track_ql = np.mean(Q_track_qls, axis=0)\ndel Q_qls ; del V_qls ; del Q_track_qls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_ql, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Q-learning:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_ql - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_ql, optimal_V)))\nprint()\nprint_action_value_function(Q_ql, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Q-learning action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_ql, optimal_Q)))\nprint()\nprint_policy(pi_ql, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_ql, mean_return_ql, mean_regret_ql = get_policy_metrics(\n    env, gamma=gamma, pi=pi_ql, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_ql, mean_return_ql, mean_regret_ql))\n\nState-value function found by Q-learning:\n| 00 0.5219 | 01 0.4762 | 02 0.4434 | 03 0.4284 |\n| 04  0.539 |           | 06 0.3521 |           |\n| 08 0.5742 | 09 0.6247 | 10 0.6011 |           |\n|           | 13 0.7321 | 14 0.8545 |           |\nOptimal state-value function:\n| 00  0.542 | 01 0.4988 | 02 0.4707 | 03 0.4569 |\n| 04 0.5585 |           | 06 0.3583 |           |\n| 08 0.5918 | 09 0.6431 | 10 0.6152 |           |\n|           | 13 0.7417 | 14 0.8628 |           |\nState-value function errors:\n| 00  -0.02 | 01  -0.02 | 02  -0.03 | 03  -0.03 |\n| 04  -0.02 |           | 06  -0.01 |           |\n| 08  -0.02 | 09  -0.02 | 10  -0.01 |           |\n|           | 13  -0.01 | 14  -0.01 |           |\nState-value function RMSE: 0.0156\n\nQ-learning action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╤════════╤════════╕\n│   s │     &lt; │     v │     &gt; │     ^ │   * &lt; │   * v │   * &gt; │   * ^ │   er &lt; │   er v │   er &gt; │   er ^ │\n╞═════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╪════════╪════════╡\n│   0 │ 0.522 │ 0.506 │ 0.506 │ 0.501 │ 0.542 │ 0.528 │ 0.528 │ 0.522 │  0.02  │  0.022 │  0.022 │  0.022 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   1 │ 0.338 │ 0.324 │ 0.307 │ 0.476 │ 0.343 │ 0.334 │ 0.32  │ 0.499 │  0.006 │  0.01  │  0.013 │  0.023 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   2 │ 0.429 │ 0.426 │ 0.414 │ 0.443 │ 0.438 │ 0.434 │ 0.424 │ 0.471 │  0.01  │  0.008 │  0.011 │  0.027 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   3 │ 0.297 │ 0.311 │ 0.289 │ 0.428 │ 0.306 │ 0.306 │ 0.302 │ 0.457 │  0.009 │ -0.005 │  0.012 │  0.028 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   4 │ 0.539 │ 0.368 │ 0.353 │ 0.354 │ 0.558 │ 0.38  │ 0.374 │ 0.363 │  0.019 │  0.011 │  0.021 │  0.009 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   5 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   6 │ 0.339 │ 0.222 │ 0.331 │ 0.161 │ 0.358 │ 0.203 │ 0.358 │ 0.155 │  0.02  │ -0.019 │  0.028 │ -0.005 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   7 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   8 │ 0.379 │ 0.399 │ 0.375 │ 0.574 │ 0.38  │ 0.408 │ 0.397 │ 0.592 │  0.001 │  0.009 │  0.021 │  0.018 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   9 │ 0.424 │ 0.625 │ 0.437 │ 0.378 │ 0.44  │ 0.643 │ 0.448 │ 0.398 │  0.016 │  0.018 │  0.01  │  0.021 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  10 │ 0.601 │ 0.484 │ 0.41  │ 0.322 │ 0.615 │ 0.497 │ 0.403 │ 0.33  │  0.014 │  0.013 │ -0.007 │  0.009 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  11 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  12 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  13 │ 0.451 │ 0.534 │ 0.732 │ 0.47  │ 0.457 │ 0.53  │ 0.742 │ 0.497 │  0.006 │ -0.005 │  0.01  │  0.027 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  14 │ 0.724 │ 0.854 │ 0.814 │ 0.774 │ 0.733 │ 0.863 │ 0.821 │ 0.781 │  0.009 │  0.008 │  0.007 │  0.007 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  15 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╧════════╧════════╛\nAction-value function RMSE: 0.013\n\n정책:\n| 00      &lt; | 01      ^ | 02      ^ | 03      ^ |\n| 04      &lt; |           | 06      &lt; |           |\n| 08      ^ | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      v |           |\nReaches goal 74.00%. Obtains an average return of 0.5116. Regret of 0.0000\n\n\n\n\n이중 Q학습\n\nQ_dqls, V_dqls, Q_track_dqls = [], [], []\nfor seed in tqdm(SEEDS, desc='All seeds', leave=True):\n    random.seed(seed); np.random.seed(seed) ; env.seed(seed)\n    Q_dql, V_dql, pi_dql, Q_track_dql, pi_track_dql = double_q_learning(env, gamma=gamma, n_episodes=n_episodes)\n    Q_dqls.append(Q_dql) ; V_dqls.append(V_dql) ; Q_track_dqls.append(Q_track_dql)\nQ_dql, V_dql, Q_track_dql = np.mean(Q_dqls, axis=0), np.mean(V_dqls, axis=0), np.mean(Q_track_dqls, axis=0)\ndel Q_dqls ; del V_dqls ; del Q_track_dqls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprint_state_value_function(V_dql, P, n_cols=n_cols, \n                           prec=svf_prec, title='State-value function found by Double Q-Learning:')\nprint_state_value_function(optimal_V, P, n_cols=n_cols, \n                           prec=svf_prec, title='Optimal state-value function:')\nprint_state_value_function(V_dql - optimal_V, P, n_cols=n_cols, \n                           prec=err_prec, title='State-value function errors:')\nprint('State-value function RMSE: {}'.format(rmse(V_dql, optimal_V)))\nprint()\nprint_action_value_function(Q_dql, \n                            optimal_Q, \n                            action_symbols=action_symbols, \n                            prec=avf_prec, \n                            title='Double Q-Learning action-value function:')\nprint('Action-value function RMSE: {}'.format(rmse(Q_dql, optimal_Q)))\nprint()\nprint_policy(pi_dql, P, action_symbols=action_symbols, n_cols=n_cols)\nsuccess_rate_dql, mean_return_dql, mean_regret_dql = get_policy_metrics(\n    env, gamma=gamma, pi=pi_dql, goal_state=goal_state, optimal_Q=optimal_Q)\nprint('Reaches goal {:.2f}%. Obtains an average return of {:.4f}. Regret of {:.4f}'.format(\n    success_rate_dql, mean_return_dql, mean_regret_dql))\n\nState-value function found by Double Q-Learning:\n| 00 0.5184 | 01 0.4354 | 02 0.3635 | 03 0.1936 |\n| 04  0.535 |           | 06 0.3091 |           |\n| 08 0.5681 | 09 0.6211 | 10 0.5848 |           |\n|           | 13 0.7279 | 14 0.8563 |           |\nOptimal state-value function:\n| 00  0.542 | 01 0.4988 | 02 0.4707 | 03 0.4569 |\n| 04 0.5585 |           | 06 0.3583 |           |\n| 08 0.5918 | 09 0.6431 | 10 0.6152 |           |\n|           | 13 0.7417 | 14 0.8628 |           |\nState-value function errors:\n| 00  -0.02 | 01  -0.06 | 02  -0.11 | 03  -0.26 |\n| 04  -0.02 |           | 06  -0.05 |           |\n| 08  -0.02 | 09  -0.02 | 10  -0.03 |           |\n|           | 13  -0.01 | 14  -0.01 |           |\nState-value function RMSE: 0.0752\n\nDouble Q-Learning action-value function:\n╒═════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤════════╤════════╤════════╤════════╕\n│   s │     &lt; │     v │     &gt; │     ^ │   * &lt; │   * v │   * &gt; │   * ^ │   er &lt; │   er v │   er &gt; │   er ^ │\n╞═════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪═══════╪════════╪════════╪════════╪════════╡\n│   0 │ 0.518 │ 0.486 │ 0.485 │ 0.482 │ 0.542 │ 0.528 │ 0.528 │ 0.522 │  0.024 │  0.042 │  0.043 │  0.04  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   1 │ 0.262 │ 0.245 │ 0.201 │ 0.435 │ 0.343 │ 0.334 │ 0.32  │ 0.499 │  0.081 │  0.089 │  0.119 │  0.063 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   2 │ 0.364 │ 0.226 │ 0.208 │ 0.237 │ 0.438 │ 0.434 │ 0.424 │ 0.471 │  0.075 │  0.208 │  0.216 │  0.234 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   3 │ 0.076 │ 0.063 │ 0.04  │ 0.171 │ 0.306 │ 0.306 │ 0.302 │ 0.457 │  0.231 │  0.243 │  0.261 │  0.286 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   4 │ 0.535 │ 0.371 │ 0.354 │ 0.36  │ 0.558 │ 0.38  │ 0.374 │ 0.363 │  0.023 │  0.009 │  0.02  │  0.003 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   5 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   6 │ 0.273 │ 0.136 │ 0.246 │ 0.069 │ 0.358 │ 0.203 │ 0.358 │ 0.155 │  0.086 │  0.067 │  0.113 │  0.086 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   7 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   8 │ 0.357 │ 0.387 │ 0.385 │ 0.568 │ 0.38  │ 0.408 │ 0.397 │ 0.592 │  0.023 │  0.02  │  0.011 │  0.024 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│   9 │ 0.41  │ 0.621 │ 0.406 │ 0.358 │ 0.44  │ 0.643 │ 0.448 │ 0.398 │  0.03  │  0.022 │  0.042 │  0.04  │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  10 │ 0.585 │ 0.45  │ 0.359 │ 0.255 │ 0.615 │ 0.497 │ 0.403 │ 0.33  │  0.03  │  0.047 │  0.044 │  0.076 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  11 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  12 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  13 │ 0.406 │ 0.493 │ 0.728 │ 0.466 │ 0.457 │ 0.53  │ 0.742 │ 0.497 │  0.051 │  0.037 │  0.014 │  0.031 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  14 │ 0.659 │ 0.856 │ 0.766 │ 0.722 │ 0.733 │ 0.863 │ 0.821 │ 0.781 │  0.073 │  0.007 │  0.055 │  0.059 │\n├─────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┤\n│  15 │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │ 0     │  0     │  0     │  0     │  0     │\n╘═════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧════════╧════════╧════════╧════════╛\nAction-value function RMSE: 0.0899\n\n정책:\n| 00      &lt; | 01      ^ | 02      &lt; | 03      v |\n| 04      &lt; |           | 06      &gt; |           |\n| 08      ^ | 09      v | 10      &lt; |           |\n|           | 13      &gt; | 14      v |           |\nReaches goal 70.00%. Obtains an average return of 0.4864. Regret of 0.0156\n\n\n\n\n매 에피소드별 max(Q) 비교\n\n첫방문 몬테카를로\n\nplot_value_function(\n    'FVMC estimates through time vs. true values', \n    np.max(Q_track_mc, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'FVMC estimates through time vs. true values (log scale)', \n    np.max(Q_track_mc, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'FVMC estimates through time (close up)', \n    np.max(Q_track_mc, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\nSARSA\n\nplot_value_function(\n    'Sarsa estimates through time vs. true values', \n    np.max(Q_track_sarsa, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Sarsa estimates through time vs. true values (log scale)', \n    np.max(Q_track_sarsa, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Sarsa estimates through time (close up)', \n    np.max(Q_track_sarsa, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\nQ학습\n\nplot_value_function(\n    'Q-Learning estimates through time vs. true values', \n    np.max(Q_track_ql, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Q-Learning estimates through time vs. true values (log scale)', \n    np.max(Q_track_ql, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Q-Learning estimates through time (close up)', \n    np.max(Q_track_ql, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\n이중 Q학습\n\nplot_value_function(\n    'Double Q-Learning estimates through time vs. true values', \n    np.max(Q_track_dql, axis=2), \n    optimal_V,\n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=False)\n\n\n\n\n\nplot_value_function(\n    'Double Q-Learning estimates through time vs. true values (log scale)', \n    np.max(Q_track_dql, axis=2), \n    optimal_V, \n    limit_items=limit_items,\n    limit_value=limit_value,\n    log=True)\n\n\n\n\n\nplot_value_function(\n    'Double Q-Learning estimates through time (close up)', \n    np.max(Q_track_dql, axis=2)[:cu_episodes], \n    None,\n    limit_items=cu_limit_items,\n    limit_value=cu_limit_value,\n    log=False)\n\n\n\n\n\n\n\n정책 평가 비교\n\nmc_success_rate_ma, mc_mean_return_ma, mc_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_mc)\n\n\n\n\n\nsarsa_success_rate_ma, sarsa_mean_return_ma, sarsa_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_sarsa)\n\n\n\n\n\nql_success_rate_ma, ql_mean_return_ma, ql_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_ql)\n\n\n\n\n\ndql_success_rate_ma, dql_mean_return_ma, dql_mean_regret_ma = get_metrics_from_tracks(\n    env, gamma, goal_state, optimal_Q, pi_track_dql)\n\n\n\n\n\nplt.axhline(y=success_rate_op, color='k', linestyle='-', linewidth=1)\nplt.text(int(len(mc_success_rate_ma)*1.02), success_rate_op*1.01, 'π*')\n\nplt.plot(mc_success_rate_ma, '-', linewidth=2, label='FVMC')\nplt.plot(sarsa_success_rate_ma, '--', linewidth=2, label='Sarsa')\nplt.plot(ql_success_rate_ma, ':', linewidth=2, label='Q-Learning')\nplt.plot(dql_success_rate_ma, '-.', linewidth=2, label='Double Q-Learning')\nplt.legend(loc=4, ncol=1)\n\nplt.title('Policy success rate (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Success rate %')\nplt.ylim(-1, 101)\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.axhline(y=mean_return_op, color='k', linestyle='-', linewidth=1)\nplt.text(int(len(mc_mean_return_ma)*1.02), mean_return_op*1.01, 'π*')\n\nplt.plot(mc_mean_return_ma, '-', linewidth=2, label='FVMC')\nplt.plot(sarsa_mean_return_ma, '--', linewidth=2, label='Sarsa')\nplt.plot(ql_mean_return_ma, ':', linewidth=2, label='Q-Learning')\nplt.plot(dql_mean_return_ma, '-.', linewidth=2, label='Double Q-Learning')\nplt.legend(loc=4, ncol=1)\n\nplt.title('Policy episode return (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Return (Gt:T)')\n\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.plot(mc_mean_regret_ma, '-', linewidth=2, label='FVMC')\nplt.plot(sarsa_mean_regret_ma, '--', linewidth=2, label='Sarsa')\nplt.plot(ql_mean_regret_ma, ':', linewidth=2, label='Q-Learning')\nplt.plot(dql_mean_regret_ma, '-.', linewidth=2, label='Double Q-Learning')\nplt.legend(loc=1, ncol=1)\n\nplt.title('Policy episode regret (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Regret (q* - Q)')\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.axhline(y=optimal_V[init_state], color='k', linestyle='-', linewidth=1)\nplt.text(int(len(Q_track_mc)*1.05), optimal_V[init_state]+.01, 'v*({})'.format(init_state))\n\nplt.plot(moving_average(np.max(Q_track_mc, axis=2).T[init_state]), \n         '-', linewidth=2, label='FVMC')\nplt.plot(moving_average(np.max(Q_track_sarsa, axis=2).T[init_state]), \n         '--', linewidth=2, label='Sarsa')\nplt.plot(moving_average(np.max(Q_track_ql, axis=2).T[init_state]), \n         ':', linewidth=2, label='Q-Learning')\nplt.plot(moving_average(np.max(Q_track_dql, axis=2).T[init_state]), \n         '-.', linewidth=2, label='Double Q-Learning')\nplt.legend(loc=4, ncol=1)\n\nplt.title('Estimated expected return (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Estimated value of initial state V({})'.format(init_state))\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_mc, axis=2) - optimal_V), axis=1)), \n         '-', linewidth=2, label='FVMC')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_sarsa, axis=2) - optimal_V), axis=1)), \n         '--', linewidth=2, label='Sarsa')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_ql, axis=2) - optimal_V), axis=1)), \n         ':', linewidth=2, label='Q-Learning')\nplt.plot(moving_average(np.mean(np.abs(np.max(Q_track_dql, axis=2) - optimal_V), axis=1)), \n         '-.', linewidth=2, label='Double Q-Learning')\nplt.legend(loc=1, ncol=1)\n\nplt.title('State-value function estimation error (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Mean Absolute Error MAE(V, v*)')\nplt.xticks(rotation=45)\n\nplt.show()\n\n\n\n\n\nplt.plot(moving_average(np.mean(np.abs(Q_track_mc - optimal_Q), axis=(1,2))), \n         '-', linewidth=2, label='FVMC')\nplt.plot(moving_average(np.mean(np.abs(Q_track_sarsa - optimal_Q), axis=(1,2))), \n         '--', linewidth=2, label='Sarsa')\nplt.plot(moving_average(np.mean(np.abs(Q_track_ql - optimal_Q), axis=(1,2))), \n         ':', linewidth=2, label='Q-Learning')\nplt.plot(moving_average(np.mean(np.abs(Q_track_dql - optimal_Q), axis=(1,2))), \n         '-.', linewidth=2, label='Double Q-Learning')\nplt.legend(loc=1, ncol=1)\n\nplt.title('Action-value function estimation error (ma 100)')\nplt.xlabel('Episodes')\nplt.ylabel('Mean Absolute Error MAE(Q, q*)')\nplt.xticks(rotation=45)\n\nplt.show()"
  },
  {
    "objectID": "publication/GDRL/GDRL-chapter-8.html#tldr",
    "href": "publication/GDRL/GDRL-chapter-8.html#tldr",
    "title": "Chapter 8: Introduction to Value based Deep Reinforcement Learning",
    "section": "TL;DR",
    "text": "TL;DR\n\n그로킹 심층 강화학습 중 8장 내용인 “가치기반 심층 강화학습 개요”에 대한 내용입니다.\n\n\n\nCode\n!pip install tqdm numpy scikit-learn pyglet setuptools && \\\n!pip install gym asciinema pandas tabulate tornado==5.* PyBullet && \\\n!pip install git+https://github.com/pybox2d/pybox2d#egg=Box2D && \\\n!pip install git+https://github.com/mimoralea/gym-bandits#egg=gym-bandits && \\\n!pip install git+https://github.com/mimoralea/gym-walk#egg=gym-walk && \\\n!pip install git+https://github.com/mimoralea/gym-aima#egg=gym-aima && \\\n!pip install gym[atari]\n!pip install torch torchvision\n\n\n\nimport warnings ; warnings.filterwarnings('ignore')\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport numpy as np\nfrom IPython.display import display\nfrom collections import namedtuple, deque\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nfrom itertools import cycle, count\nfrom textwrap import wrap\n\nimport matplotlib\nimport subprocess\nimport os.path\nimport tempfile\nimport random\nimport base64\nimport pprint\nimport glob\nimport time\nimport json\nimport sys\nimport gym\nimport io\nimport os\nimport gc\nimport platform\n\nfrom gym import wrappers\nfrom subprocess import check_output\nfrom IPython.display import HTML\n\nLEAVE_PRINT_EVERY_N_SECS = 60\nERASE_LINE = '\\x1b[2K'\nEPS = 1e-6\nRESULTS_DIR = os.path.join('.', 'gym-results')\nSEEDS = (12, 34, 56, 78, 90)\n\n%matplotlib inline\n\n\nplt.style.use('fivethirtyeight')\nparams = {\n    'figure.figsize': (15, 8),\n    'font.size': 24,\n    'legend.fontsize': 20,\n    'axes.titlesize': 28,\n    'axes.labelsize': 24,\n    'xtick.labelsize': 20,\n    'ytick.labelsize': 20\n}\npylab.rcParams.update(params)\nnp.set_printoptions(suppress=True)\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\ndef get_make_env_fn(**kargs):\n    def make_env_fn(env_name, seed=None, render=None, record=False,\n                    unwrapped=False, monitor_mode=None, \n                    inner_wrappers=None, outer_wrappers=None):\n        mdir = tempfile.mkdtemp()\n        env = None\n        if render:\n            try:\n                env = gym.make(env_name, render=render)\n            except:\n                pass\n        if env is None:\n            env = gym.make(env_name)\n        if seed is not None: env.seed(seed)\n        env = env.unwrapped if unwrapped else env\n        if inner_wrappers:\n            for wrapper in inner_wrappers:\n                env = wrapper(env)\n        env = wrappers.Monitor(\n            env, mdir, force=True, \n            mode=monitor_mode, \n            video_callable=lambda e_idx: record) if monitor_mode else env\n        if outer_wrappers:\n            for wrapper in outer_wrappers:\n                env = wrapper(env)\n        return env\n    return make_env_fn, kargs\n\n\ndef get_videos_html(env_videos, title, max_n_videos=5):\n    videos = np.array(env_videos)\n    if len(videos) == 0:\n        return\n    \n    n_videos = max(1, min(max_n_videos, len(videos)))\n    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int) if n_videos &gt; 1 else [-1,]\n    videos = videos[idxs,...]\n\n    strm = '&lt;h2&gt;{}&lt;/h2&gt;'.format(title)\n    for video_path, meta_path in videos:\n        video = io.open(video_path, 'r+b').read()\n        encoded = base64.b64encode(video)\n\n        with open(meta_path) as data_file:    \n            meta = json.load(data_file)\n\n        html_tag = \"\"\"\n        &lt;h3&gt;{0}&lt;/h3&gt;\n        &lt;video width=\"960\" height=\"540\" controls&gt;\n            &lt;source src=\"data:video/mp4;base64,{1}\" type=\"video/mp4\" /&gt;\n        &lt;/video&gt;\"\"\"\n        strm += html_tag.format('Episode ' + str(meta['episode_id']), encoded.decode('ascii'))\n    return strm\n\n\nplatform.system()\n\n'Windows'\n\n\n\ndef get_gif_html(env_videos, title, subtitle_eps=None, max_n_videos=4):\n    videos = np.array(env_videos)\n    if len(videos) == 0:\n        return\n    \n    n_videos = max(1, min(max_n_videos, len(videos)))\n    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int) if n_videos &gt; 1 else [-1,]\n    videos = videos[idxs,...]\n\n    strm = '&lt;h2&gt;{}&lt;/h2&gt;'.format(title)\n    for video_path, meta_path in videos:\n        basename = os.path.splitext(video_path)[0]\n        gif_path = basename + '.gif'\n        if not os.path.exists(gif_path):\n            if platform.system() == 'Linux':\n                ps = subprocess.Popen(\n                    ('ffmpeg', \n                     '-i', video_path, \n                     '-r', '7',\n                     '-f', 'image2pipe', \n                     '-vcodec', 'ppm',\n                     '-crf', '20',\n                     '-vf', 'scale=512:-1',\n                     '-'), \n                    stdout=subprocess.PIPE,\n                    universal_newlines=True)\n                output = subprocess.check_output(\n                    ('convert',\n                     '-coalesce',\n                     '-delay', '7',\n                     '-loop', '0',\n                     '-fuzz', '2%',\n                     '+dither',\n                     '-deconstruct',\n                     '-layers', 'Optimize',\n                     '-', gif_path), \n                    stdin=ps.stdout)\n                ps.wait()\n            else:\n                ps = subprocess.Popen('ffmpeg -i {} -r 7 -f image2pipe \\\n                                      -vcodec ppm -crf 20 -vf scale=512:-1 - | \\\n                                      convert -coalesce -delay 7 -loop 0 -fuzz 2% \\\n                                      +dither -deconstruct -layers Optimize \\\n                                      - {}'.format(video_path, gif_path), \n                                      stdin=subprocess.PIPE, \n                                      shell=True)\n                ps.wait()\n\n        gif = io.open(gif_path, 'r+b').read()\n        encoded = base64.b64encode(gif)\n            \n        with open(meta_path) as data_file:    \n            meta = json.load(data_file)\n\n        html_tag = \"\"\"\n        &lt;h3&gt;{0}&lt;/h3&gt;\n        &lt;img src=\"data:image/gif;base64,{1}\" /&gt;\"\"\"\n        prefix = 'Trial ' if subtitle_eps is None else 'Episode '\n        sufix = str(meta['episode_id'] if subtitle_eps is None \\\n                    else subtitle_eps[meta['episode_id']])\n        strm += html_tag.format(prefix + sufix, encoded.decode('ascii'))\n    return strm\n\n\nclass DiscountedCartPole(gym.Wrapper):\n    def __init__(self, env):\n        gym.Wrapper.__init__(self, env)\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n    def step(self, a):\n        o, r, d, _ = self.env.step(a)\n        (x, x_dot, theta, theta_dot) = o\n        pole_fell =  x &lt; -self.env.unwrapped.x_threshold \\\n                    or x &gt; self.env.unwrapped.x_threshold \\\n                    or theta &lt; -self.env.unwrapped.theta_threshold_radians \\\n                    or theta &gt; self.env.unwrapped.theta_threshold_radians\n        r = -1 if pole_fell else 0\n        return o, r, d, _\n\n\nclass FCQ(nn.Module):\n    def __init__(self, \n                 input_dim, \n                 output_dim, \n                 hidden_dims=(32,32), \n                 activation_fc=F.relu):\n        super(FCQ, self).__init__()\n        self.activation_fc = activation_fc\n\n        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n        self.hidden_layers = nn.ModuleList()\n        for i in range(len(hidden_dims)-1):\n            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n            self.hidden_layers.append(hidden_layer)\n        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n\n        device = \"cpu\"\n        if torch.cuda.is_available():\n            device = \"cuda:0\"\n        self.device = torch.device(device)\n        self.to(self.device)\n        \n    def _format(self, state):\n        x = state\n        if not isinstance(x, torch.Tensor):\n            x = torch.tensor(x, \n                             device=self.device, \n                             dtype=torch.float32)\n            x = x.unsqueeze(0)\n        return x\n\n    def forward(self, state):\n        x = self._format(state)\n        x = self.activation_fc(self.input_layer(x))\n        for hidden_layer in self.hidden_layers:\n            x = self.activation_fc(hidden_layer(x))\n        x = self.output_layer(x)\n        return x\n    \n    def numpy_float_to_device(self, variable):\n        variable = torch.from_numpy(variable).float().to(self.device)\n        return variable\n    \n    def load(self, experiences):\n        states, actions, new_states, rewards, is_terminals = experiences\n        states = torch.from_numpy(states).float().to(self.device)\n        actions = torch.from_numpy(actions).long().to(self.device)\n        new_states = torch.from_numpy(new_states).float().to(self.device)\n        rewards = torch.from_numpy(rewards).float().to(self.device)\n        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n        return states, actions, new_states, rewards, is_terminals\n\n\nclass GreedyStrategy():\n    def __init__(self):\n        self.exploratory_action_taken = False\n\n    def select_action(self, model, state):\n        with torch.no_grad():\n            q_values = model(state).cpu().detach().data.numpy().squeeze()\n            return np.argmax(q_values)\n\n\nclass EGreedyStrategy():\n    def __init__(self, epsilon=0.1):\n        self.epsilon = epsilon\n        self.exploratory_action_taken = None\n\n    def select_action(self, model, state):\n        self.exploratory_action_taken = False\n        with torch.no_grad():\n            q_values = model(state).cpu().detach().data.numpy().squeeze()\n\n        if np.random.rand() &gt; self.epsilon:\n            action = np.argmax(q_values)\n        else: \n            action = np.random.randint(len(q_values))\n\n        self.exploratory_action_taken = action != np.argmax(q_values)\n        return action\n\n\nclass NFQ():\n    def __init__(self, \n                 value_model_fn, \n                 value_optimizer_fn, \n                 value_optimizer_lr,\n                 training_strategy_fn,\n                 evaluation_strategy_fn,\n                 batch_size,\n                 epochs):\n        self.value_model_fn = value_model_fn\n        self.value_optimizer_fn = value_optimizer_fn\n        self.value_optimizer_lr = value_optimizer_lr\n        self.training_strategy_fn = training_strategy_fn\n        self.evaluation_strategy_fn = evaluation_strategy_fn\n        self.batch_size = batch_size\n        self.epochs = epochs\n\n    def optimize_model(self, experiences):\n        states, actions, rewards, next_states, is_terminals = experiences\n        batch_size = len(is_terminals)\n        \n        max_a_q_sp = self.online_model(next_states).detach().max(1)[0].unsqueeze(1)\n        target_q_s = rewards + self.gamma * max_a_q_sp * (1 - is_terminals)\n        q_sa = self.online_model(states).gather(1, actions)\n\n        td_errors = q_sa - target_q_s\n        value_loss = td_errors.pow(2).mul(0.5).mean()\n        self.value_optimizer.zero_grad()\n        value_loss.backward()\n        self.value_optimizer.step()\n\n    def interaction_step(self, state, env):\n        action = self.training_strategy.select_action(self.online_model, state)\n        new_state, reward, is_terminal, info = env.step(action)\n        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n        is_failure = is_terminal and not is_truncated\n        experience = (state, action, reward, new_state, float(is_failure))\n\n        self.experiences.append(experience)\n        self.episode_reward[-1] += reward\n        self.episode_timestep[-1] += 1\n        self.episode_exploration[-1] += int(self.training_strategy.exploratory_action_taken)\n        return new_state, is_terminal\n\n    def train(self, make_env_fn, make_env_kargs, seed, gamma, \n              max_minutes, max_episodes, goal_mean_100_reward):\n        training_start, last_debug_time = time.time(), float('-inf')\n\n        self.checkpoint_dir = tempfile.mkdtemp()\n        self.make_env_fn = make_env_fn\n        self.make_env_kargs = make_env_kargs\n        self.seed = seed\n        self.gamma = gamma\n        \n        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n    \n        nS, nA = env.observation_space.shape[0], env.action_space.n\n        self.episode_timestep = []\n        self.episode_reward = []\n        self.episode_seconds = []\n        self.evaluation_scores = []        \n        self.episode_exploration = []\n        \n        self.online_model = self.value_model_fn(nS, nA)\n        self.value_optimizer = self.value_optimizer_fn(self.online_model, \n                                                       self.value_optimizer_lr)\n\n        self.training_strategy = training_strategy_fn()\n        self.evaluation_strategy = evaluation_strategy_fn() \n        self.experiences = []\n\n        result = np.empty((max_episodes, 5))\n        result[:] = np.nan\n        training_time = 0\n        for episode in range(1, max_episodes + 1):\n            episode_start = time.time()\n            \n            state, is_terminal = env.reset(), False\n            self.episode_reward.append(0.0)\n            self.episode_timestep.append(0.0)\n            self.episode_exploration.append(0.0)\n\n            for step in count():\n                state, is_terminal = self.interaction_step(state, env)\n                \n                if len(self.experiences) &gt;= self.batch_size:\n                    experiences = np.array(self.experiences)\n                    batches = [np.vstack(sars) for sars in experiences.T]\n                    experiences = self.online_model.load(batches)\n                    for _ in range(self.epochs):\n                        self.optimize_model(experiences)\n                    self.experiences.clear()\n                \n                if is_terminal:\n                    gc.collect()\n                    break\n            \n            # stats\n            episode_elapsed = time.time() - episode_start\n            self.episode_seconds.append(episode_elapsed)\n            training_time += episode_elapsed\n            evaluation_score, _ = self.evaluate(self.online_model, env)\n            self.save_checkpoint(episode-1, self.online_model)\n\n            total_step = int(np.sum(self.episode_timestep))\n            self.evaluation_scores.append(evaluation_score)\n            \n            mean_10_reward = np.mean(self.episode_reward[-10:])\n            std_10_reward = np.std(self.episode_reward[-10:])\n            mean_100_reward = np.mean(self.episode_reward[-100:])\n            std_100_reward = np.std(self.episode_reward[-100:])\n            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n            lst_100_exp_rat = np.array(\n                self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n            mean_100_exp_rat = np.mean(lst_100_exp_rat)\n            std_100_exp_rat = np.std(lst_100_exp_rat)\n            \n            wallclock_elapsed = time.time() - training_start\n            result[episode-1] = total_step, mean_100_reward, \\\n                mean_100_eval_score, training_time, wallclock_elapsed\n            \n            reached_debug_time = time.time() - last_debug_time &gt;= LEAVE_PRINT_EVERY_N_SECS\n            reached_max_minutes = wallclock_elapsed &gt;= max_minutes * 60\n            reached_max_episodes = episode &gt;= max_episodes\n            reached_goal_mean_reward = mean_100_eval_score &gt;= goal_mean_100_reward\n            training_is_over = reached_max_minutes or \\\n                               reached_max_episodes or \\\n                               reached_goal_mean_reward\n\n            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n            debug_message = 'el {}, ep {:04}, ts {:06}, '\n            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n            debug_message = debug_message.format(\n                elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n                mean_100_eval_score, std_100_eval_score)\n            print(debug_message, end='\\r', flush=True)\n            if reached_debug_time or training_is_over:\n                print(ERASE_LINE + debug_message, flush=True)\n                last_debug_time = time.time()\n            if training_is_over:\n                if reached_max_minutes: print(u'--&gt; reached_max_minutes \\u2715')\n                if reached_max_episodes: print(u'--&gt; reached_max_episodes \\u2715')\n                if reached_goal_mean_reward: print(u'--&gt; reached_goal_mean_reward \\u2713')\n                break\n                \n        final_eval_score, score_std = self.evaluate(self.online_model, env, n_episodes=100)\n        wallclock_time = time.time() - training_start\n        print('Training complete.')\n        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n              ' {:.2f}s wall-clock time.\\n'.format(\n                  final_eval_score, score_std, training_time, wallclock_time))\n        env.close() ; del env\n        self.get_cleaned_checkpoints()\n        return result, final_eval_score, training_time, wallclock_time\n    \n    def evaluate(self, eval_policy_model, eval_env, n_episodes=1):\n        rs = []\n        for _ in range(n_episodes):\n            s, d = eval_env.reset(), False\n            rs.append(0)\n            for _ in count():\n                a = self.evaluation_strategy.select_action(eval_policy_model, s)\n                s, r, d, _ = eval_env.step(a)\n                rs[-1] += r\n                if d: break\n        return np.mean(rs), np.std(rs)\n\n    def get_cleaned_checkpoints(self, n_checkpoints=5):\n        try: \n            return self.checkpoint_paths\n        except AttributeError:\n            self.checkpoint_paths = {}\n\n        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n        last_ep = max(paths_dic.keys())\n        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n\n        for idx, path in paths_dic.items():\n            if idx in checkpoint_idxs:\n                self.checkpoint_paths[idx] = path\n            else:\n                os.unlink(path)\n\n        return self.checkpoint_paths\n\n    def demo_last(self, title='Fully-trained {} Agent', n_episodes=3, max_n_videos=3):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        last_ep = max(checkpoint_paths.keys())\n        self.online_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n\n        self.evaluate(self.online_model, env, n_episodes=n_episodes)\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def demo_progression(self, title='{} Agent progression', max_n_videos=5):\n        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n\n        checkpoint_paths = self.get_cleaned_checkpoints()\n        for i in sorted(checkpoint_paths.keys()):\n            self.online_model.load_state_dict(torch.load(checkpoint_paths[i]))\n            self.evaluate(self.online_model, env, n_episodes=1)\n\n        env.close()\n        data = get_gif_html(env_videos=env.videos, \n                            title=title.format(self.__class__.__name__),\n                            subtitle_eps=sorted(checkpoint_paths.keys()),\n                            max_n_videos=max_n_videos)\n        del env\n        return HTML(data=data)\n\n    def save_checkpoint(self, episode_idx, model):\n        torch.save(model.state_dict(), \n                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))\n\n\nnfq_results = []\nbest_agent, best_eval_score = None, float('-inf')\nfor seed in SEEDS:\n    environment_settings = {\n        'env_name': 'CartPole-v1',\n        'gamma': 1.00,\n        'max_minutes': 20,\n        'max_episodes': 10000,\n        'goal_mean_100_reward': 475\n    }\n    \n    value_model_fn = lambda nS, nA: FCQ(nS, nA, hidden_dims=(512,128))\n    # value_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n    value_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr=lr)\n    value_optimizer_lr = 0.0005\n\n    training_strategy_fn = lambda: EGreedyStrategy(epsilon=0.5)\n    # evaluation_strategy_fn = lambda: EGreedyStrategy(epsilon=0.05)\n    evaluation_strategy_fn = lambda: GreedyStrategy()\n\n    batch_size = 1024\n    epochs = 40\n\n    env_name, gamma, max_minutes, \\\n    max_episodes, goal_mean_100_reward = environment_settings.values()\n    agent = NFQ(value_model_fn, \n                value_optimizer_fn, \n                value_optimizer_lr,\n                training_strategy_fn,\n                evaluation_strategy_fn,\n                batch_size,\n                epochs)\n\n    # make_env_fn, make_env_kargs = get_make_env_fn(\n    #     env_name=env_name, addon_wrappers=[DiscountedCartPole,])\n    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n    result, final_eval_score, training_time, wallclock_time = agent.train(\n        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n    nfq_results.append(result)\n    if final_eval_score &gt; best_eval_score:\n        best_eval_score = final_eval_score\n        best_agent = agent\nnfq_results = np.array(nfq_results)\n\nel 00:00:00, ep 0000, ts 000015, ar 10 015.0±000.0, 100 015.0±000.0, ex 100 0.3±0.0, ev 009.0±000.0\nel 00:01:00, ep 0899, ts 028293, ar 10 130.4±056.3, 100 056.1±040.1, ex 100 0.3±0.1, ev 080.0±050.5\nel 00:02:00, ep 1342, ts 066205, ar 10 125.7±081.2, 100 132.6±079.2, ex 100 0.3±0.1, ev 233.2±066.6\nel 00:03:00, ep 1631, ts 101018, ar 10 110.9±066.5, 100 126.3±082.1, ex 100 0.3±0.1, ev 312.1±098.1\nel 00:04:00, ep 1871, ts 140058, ar 10 296.5±124.0, 100 165.3±122.1, ex 100 0.3±0.1, ev 337.5±118.2\nel 00:05:00, ep 2087, ts 176833, ar 10 184.5±084.8, 100 162.1±114.2, ex 100 0.3±0.1, ev 428.0±072.0\nel 00:06:00, ep 2299, ts 213879, ar 10 187.4±122.9, 100 181.3±125.7, ex 100 0.3±0.1, ev 456.5±074.0\nel 00:06:34, ep 2419, ts 233086, ar 10 211.8±123.1, 100 160.8±117.1, ex 100 0.3±0.1, ev 475.9±059.9\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 499.31±4.58 in 188.65s training time, 414.18s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000015, ar 10 015.0±000.0, 100 015.0±000.0, ex 100 0.3±0.0, ev 010.0±000.0\nel 00:01:00, ep 0788, ts 028599, ar 10 013.3±004.7, 100 042.2±057.9, ex 100 0.2±0.1, ev 096.0±156.1\nel 00:02:00, ep 1242, ts 062379, ar 10 098.8±047.5, 100 062.8±050.6, ex 100 0.3±0.1, ev 131.6±066.6\nel 00:03:00, ep 1563, ts 102834, ar 10 201.5±156.3, 100 123.4±100.0, ex 100 0.3±0.1, ev 237.4±090.9\nel 00:04:00, ep 1813, ts 139898, ar 10 119.9±058.8, 100 151.4±098.1, ex 100 0.3±0.1, ev 345.9±112.9\nel 00:05:00, ep 2050, ts 177897, ar 10 138.1±088.2, 100 155.7±117.1, ex 100 0.3±0.1, ev 400.7±104.2\nel 00:06:01, ep 2281, ts 214001, ar 10 189.4±139.4, 100 157.8±114.9, ex 100 0.3±0.1, ev 387.0±102.0\nel 00:07:01, ep 2493, ts 250122, ar 10 145.6±124.2, 100 164.3±118.9, ex 100 0.3±0.1, ev 437.7±075.7\nel 00:08:01, ep 2707, ts 287658, ar 10 176.0±128.6, 100 169.2±113.1, ex 100 0.3±0.1, ev 419.1±095.2\nel 00:08:28, ep 2798, ts 304508, ar 10 180.6±141.6, 100 185.6±123.2, ex 100 0.3±0.1, ev 476.4±060.3\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 499.47±2.70 in 233.22s training time, 528.88s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000013, ar 10 013.0±000.0, 100 013.0±000.0, ex 100 0.2±0.0, ev 009.0±000.0\nel 00:01:00, ep 0885, ts 025973, ar 10 105.0±066.9, 100 094.7±067.9, ex 100 0.3±0.1, ev 239.7±134.9\nel 00:02:00, ep 1229, ts 062492, ar 10 066.1±047.1, 100 108.0±066.9, ex 100 0.3±0.1, ev 217.8±075.3\nel 00:03:00, ep 1494, ts 100171, ar 10 116.0±050.0, 100 140.5±102.0, ex 100 0.3±0.1, ev 334.3±091.1\nel 00:04:00, ep 1735, ts 136782, ar 10 161.9±102.5, 100 157.2±101.1, ex 100 0.3±0.1, ev 353.4±106.1\nel 00:05:00, ep 1963, ts 173753, ar 10 135.8±101.4, 100 166.6±124.8, ex 100 0.3±0.1, ev 416.5±100.4\nel 00:06:00, ep 2165, ts 204790, ar 10 113.2±089.1, 100 169.1±105.5, ex 100 0.3±0.1, ev 400.0±100.5\nel 00:07:00, ep 2370, ts 236920, ar 10 108.3±059.7, 100 166.8±104.8, ex 100 0.3±0.1, ev 444.6±074.7\nel 00:08:00, ep 2562, ts 270419, ar 10 151.0±086.8, 100 175.4±120.2, ex 100 0.3±0.1, ev 442.4±087.7\nel 00:09:00, ep 2745, ts 302905, ar 10 201.2±127.1, 100 168.0±112.9, ex 100 0.3±0.1, ev 442.0±089.3\nel 00:09:07, ep 2764, ts 306173, ar 10 185.9±132.7, 100 176.3±118.2, ex 100 0.3±0.1, ev 475.6±058.1\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 500.00±0.00 in 243.08s training time, 567.50s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000052, ar 10 052.0±000.0, 100 052.0±000.0, ex 100 0.2±0.0, ev 035.0±000.0\nel 00:01:00, ep 0749, ts 025988, ar 10 085.8±026.6, 100 095.8±062.5, ex 100 0.3±0.1, ev 195.6±116.2\nel 00:02:00, ep 1120, ts 058853, ar 10 095.9±066.0, 100 087.6±096.0, ex 100 0.3±0.1, ev 213.7±136.2\nel 00:03:00, ep 1416, ts 092345, ar 10 148.5±134.1, 100 142.6±094.1, ex 100 0.3±0.1, ev 323.2±104.9\nel 00:04:00, ep 1685, ts 131111, ar 10 116.1±078.6, 100 134.8±101.0, ex 100 0.3±0.1, ev 282.4±091.9\nel 00:05:00, ep 1914, ts 164830, ar 10 133.1±085.1, 100 136.8±090.8, ex 100 0.3±0.1, ev 410.5±112.3\nel 00:06:00, ep 2142, ts 198735, ar 10 219.3±129.6, 100 154.6±109.0, ex 100 0.3±0.1, ev 412.5±111.1\nel 00:07:00, ep 2355, ts 232257, ar 10 161.4±089.0, 100 164.5±115.5, ex 100 0.3±0.1, ev 430.9±093.2\nel 00:08:00, ep 2562, ts 268801, ar 10 208.2±119.9, 100 178.9±134.9, ex 100 0.3±0.1, ev 415.2±113.1\nel 00:09:01, ep 2756, ts 301532, ar 10 167.6±135.6, 100 161.4±128.9, ex 100 0.3±0.1, ev 465.1±074.3\nel 00:10:01, ep 2956, ts 334390, ar 10 176.4±097.3, 100 168.8±126.0, ex 100 0.3±0.1, ev 433.1±081.4\nel 00:10:30, ep 3046, ts 349198, ar 10 134.2±103.3, 100 165.7±107.4, ex 100 0.3±0.1, ev 475.1±049.9\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 446.17±49.13 in 277.89s training time, 650.60s wall-clock time.\n\nel 00:00:00, ep 0000, ts 000013, ar 10 013.0±000.0, 100 013.0±000.0, ex 100 0.2±0.0, ev 009.0±000.0\nel 00:01:00, ep 0650, ts 028596, ar 10 155.9±114.1, 100 106.3±081.6, ex 100 0.3±0.1, ev 271.9±141.3\nel 00:02:00, ep 0901, ts 061422, ar 10 120.8±037.7, 100 144.1±093.5, ex 100 0.3±0.1, ev 409.3±096.6\nel 00:03:00, ep 1109, ts 095096, ar 10 167.2±144.6, 100 175.2±112.3, ex 100 0.3±0.0, ev 432.9±085.5\nel 00:04:00, ep 1320, ts 127564, ar 10 185.5±124.7, 100 154.4±102.4, ex 100 0.3±0.1, ev 381.7±122.8\nel 00:05:00, ep 1532, ts 161720, ar 10 184.1±136.5, 100 164.9±107.0, ex 100 0.3±0.1, ev 450.7±076.5\nel 00:05:22, ep 1598, ts 173778, ar 10 126.9±078.9, 100 179.2±127.3, ex 100 0.3±0.1, ev 475.8±043.2\n--&gt; reached_goal_mean_reward ✓\nTraining complete.\nFinal evaluation score 473.34±35.27 in 140.87s training time, 342.61s wall-clock time.\n\n\n\n\nbest_agent.demo_progression()\n\nNFQ Agent progression\n        Episode 0\n        \n        Episode 691\n        \n        Episode 1382\n        \n        Episode 2073\n        \n        Episode 2764\n        \n\n\n\nbest_agent.demo_last()\n\nFully-trained NFQ Agent\n        Trial 0\n        \n        Trial 1\n        \n        Trial 2\n        \n\n\n\nnfq_max_t, nfq_max_r, nfq_max_s, \\\n    nfq_max_sec, nfq_max_rt = np.max(nfq_results, axis=0).T\nnfq_min_t, nfq_min_r, nfq_min_s, \\\n    nfq_min_sec, nfq_min_rt = np.min(nfq_results, axis=0).T\nnfq_mean_t, nfq_mean_r, nfq_mean_s, \\\n    nfq_mean_sec, nfq_mean_rt = np.mean(nfq_results, axis=0).T\nnfq_x = np.arange(len(nfq_mean_s))\n\n# nfq_max_t, nfq_max_r, nfq_max_s, \\\n#     nfq_max_sec, nfq_max_rt = np.nanmax(nfq_results, axis=0).T\n# nfq_min_t, nfq_min_r, nfq_min_s, \\\n#     nfq_min_sec, nfq_min_rt = np.nanmin(nfq_results, axis=0).T\n# nfq_mean_t, nfq_mean_r, nfq_mean_s, \\\n#     nfq_mean_sec, nfq_mean_rt = np.nanmean(nfq_results, axis=0).T\n# nfq_x = np.arange(len(nfq_mean_s))\n\n\nfig, axs = plt.subplots(5, 1, figsize=(15,30), sharey=False, sharex=True)\n\n# NFQ\naxs[0].plot(nfq_max_r, 'y', linewidth=1)\naxs[0].plot(nfq_min_r, 'y', linewidth=1)\naxs[0].plot(nfq_mean_r, 'y', label='NFQ', linewidth=2)\naxs[0].fill_between(nfq_x, nfq_min_r, nfq_max_r, facecolor='y', alpha=0.3)\n\naxs[1].plot(nfq_max_s, 'y', linewidth=1)\naxs[1].plot(nfq_min_s, 'y', linewidth=1)\naxs[1].plot(nfq_mean_s, 'y', label='NFQ', linewidth=2)\naxs[1].fill_between(nfq_x, nfq_min_s, nfq_max_s, facecolor='y', alpha=0.3)\n\naxs[2].plot(nfq_max_t, 'y', linewidth=1)\naxs[2].plot(nfq_min_t, 'y', linewidth=1)\naxs[2].plot(nfq_mean_t, 'y', label='NFQ', linewidth=2)\naxs[2].fill_between(nfq_x, nfq_min_t, nfq_max_t, facecolor='y', alpha=0.3)\n\naxs[3].plot(nfq_max_sec, 'y', linewidth=1)\naxs[3].plot(nfq_min_sec, 'y', linewidth=1)\naxs[3].plot(nfq_mean_sec, 'y', label='NFQ', linewidth=2)\naxs[3].fill_between(nfq_x, nfq_min_sec, nfq_max_sec, facecolor='y', alpha=0.3)\n\naxs[4].plot(nfq_max_rt, 'y', linewidth=1)\naxs[4].plot(nfq_min_rt, 'y', linewidth=1)\naxs[4].plot(nfq_mean_rt, 'y', label='NFQ', linewidth=2)\naxs[4].fill_between(nfq_x, nfq_min_rt, nfq_max_rt, facecolor='y', alpha=0.3)\n\n# ALL\naxs[0].set_title('Moving Avg Reward (Training)')\naxs[1].set_title('Moving Avg Reward (Evaluation)')\naxs[2].set_title('Total Steps')\naxs[3].set_title('Training Time')\naxs[4].set_title('Wall-clock Time')\nplt.xlabel('Episodes')\naxs[0].legend(loc='upper left')\nplt.show()\n\n\n\n\n\nnfq_root_dir = os.path.join(RESULTS_DIR, 'nfq')\nnot os.path.exists(nfq_root_dir) and os.makedirs(nfq_root_dir)\n\nnp.save(os.path.join(nfq_root_dir, 'x'), nfq_x)\n\nnp.save(os.path.join(nfq_root_dir, 'max_r'), nfq_max_r)\nnp.save(os.path.join(nfq_root_dir, 'min_r'), nfq_min_r)\nnp.save(os.path.join(nfq_root_dir, 'mean_r'), nfq_mean_r)\n\nnp.save(os.path.join(nfq_root_dir, 'max_s'), nfq_max_s)\nnp.save(os.path.join(nfq_root_dir, 'min_s'), nfq_min_s )\nnp.save(os.path.join(nfq_root_dir, 'mean_s'), nfq_mean_s)\n\nnp.save(os.path.join(nfq_root_dir, 'max_t'), nfq_max_t)\nnp.save(os.path.join(nfq_root_dir, 'min_t'), nfq_min_t)\nnp.save(os.path.join(nfq_root_dir, 'mean_t'), nfq_mean_t)\n\nnp.save(os.path.join(nfq_root_dir, 'max_sec'), nfq_max_sec)\nnp.save(os.path.join(nfq_root_dir, 'min_sec'), nfq_min_sec)\nnp.save(os.path.join(nfq_root_dir, 'mean_sec'), nfq_mean_sec)\n\nnp.save(os.path.join(nfq_root_dir, 'max_rt'), nfq_max_rt)\nnp.save(os.path.join(nfq_root_dir, 'min_rt'), nfq_min_rt)\nnp.save(os.path.join(nfq_root_dir, 'mean_rt'), nfq_mean_rt)"
  },
  {
    "objectID": "publication/GDRL/index.html",
    "href": "publication/GDRL/index.html",
    "title": "그로킹 심층 강화학습",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n\nChapter 9: More Stable Value-based Methods\n\n\nChanseok Kang\n\n\n\n\nChapter 8: Introduction to Value based Deep Reinforcement Learning\n\n\nChanseok Kang\n\n\n\n\nChapter 7: Archieving goals more effectively and efficiently\n\n\nChanseok Kang\n\n\n\n\nChapter 6: Improving Agents behaviors\n\n\nChanseok Kang\n\n\n\n\nChapter 5: Evaluating Agents behaviors\n\n\nChanseok Kang\n\n\n\n\nChapter 4: Balancing the gathering and use of information\n\n\nChanseok Kang\n\n\n\n\nChapter 3: Balancing immediate and long-term goals\n\n\nChanseok Kang\n\n\n\n\nChapter 2: Mathematical foundations of reinforcement learning\n\n\nChanseok Kang\n\n\n\n\nChapter 12: Advanced Actor-Critic Methods\n\n\nChanseok Kang\n\n\n\n\nChapter 11: Policy-Gradient and Actor-Critic Methods\n\n\nChanseok Kang\n\n\n\n\nChapter 10: Sample-Efficient Value-Based Methods\n\n\nChanseok Kang\n\n\n\n\n\n\nNo matching items"
  }
]