<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Chanseok Kang">
<meta name="dcterms.date" content="2023-10-10">
<meta name="description" content="Balanced replay scheme that prioritizes samples encountered online while also encouraging the use of near-on-policy samples from offline dataset.">

<title>Chans Lecture Note - Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-0JHM3R8BEZ"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-0JHM3R8BEZ', { 'anonymize_ip': true});
</script>
<style>html{ scroll-behavior: smooth; }</style>
<script>
MathJax = {
  loader: {
    load: ['[tex]/boldsymbol']
  },
  tex: {
    tags: "all",
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    packages: {
      '[+]': ['boldsymbol']
    }
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Chans Lecture Note</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html" rel="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../publication/index.html" rel="">
 <span class="menu-text">Publication</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-posts" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="">
 <span class="menu-text">Posts</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-posts">    
        <li>
    <a class="dropdown-item" href="../../../index.html" rel="">
 <span class="dropdown-text">English</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../posts/kr.html" rel="">
 <span class="dropdown-text">한글</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/goodboychan" rel=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/chanseokk/" rel=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <h1 class="title">Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble</h1>
            <p class="subtitle lead">Off2OnRL</p>
                  <div>
        <div class="description">
          Balanced replay scheme that prioritizes samples encountered online while also encouraging the use of near-on-policy samples from offline dataset.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">PaperReview</div>
                <div class="quarto-category">ReinforcementLearning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Chanseok Kang </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 10, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#tldr" id="toc-tldr" class="nav-link active" data-scroll-target="#tldr">TL;DR</a></li>
  <li><a href="#내용-정리" id="toc-내용-정리" class="nav-link" data-scroll-target="#내용-정리">내용 정리</a>
  <ul class="collapse">
  <li><a href="#offline-rl-agent-fine-tuning" id="toc-offline-rl-agent-fine-tuning" class="nav-link" data-scroll-target="#offline-rl-agent-fine-tuning">Offline RL Agent Fine-tuning</a></li>
  <li><a href="#method" id="toc-method" class="nav-link" data-scroll-target="#method">Method</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">




<ul>
<li>저자: Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, Jinwoo Shin</li>
<li>발표: CoRL 2021</li>
<li><a href="https://arxiv.org/abs/2107.00591">논문</a></li>
<li><a href="https://openreview.net/forum?id=AlJXhEI6J5W">OpenReview</a></li>
<li><a href="https://github.com/shlee94/Off2OnRL">Code (PyTorch)</a></li>
</ul>
<section id="tldr" class="level2">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>Offline RL에서 아무리 좋은 policy를 학습했다 하더라도, online RL을 통한 fine-tune을 수행하게 되면 state-action distribution shift로 인해서 bootstrap error가 커지는 현상이 발생하게 된다. 이를 방지하기 위해서 online에서 얻은 sample에 대해서 우선순위를 부여하면서, offline dataset에서 뽑은 <strong>near-on-policy</strong> sample을 사용하도록 하는 balanced replay 구조를 제안했다. 추가로 offline상에서 여러개의 Q-function을 pessimistic하게 학습하고 이에 대한 ensemble을 취해서, 초기 학습단계에서 state에 대한 unfamiliar action을 다루는데 overoptimism이 발생하는 것을 방지했다.</p>
</section>
<section id="내용-정리" class="level2">
<h2 class="anchored" data-anchor-id="내용-정리">내용 정리</h2>
<section id="offline-rl-agent-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="offline-rl-agent-fine-tuning">Offline RL Agent Fine-tuning</h3>
<p>이 챕터에서는 Offline RL에서 학습된 Agent를 Online에서 Fine-tuning했을 때 distribution shift에 의해서 성능 저하가 발생하고, 왜 해당 문제가 발생하는지에 대해서 설명했다. 또한 논문에서 제안하는 아이디어의 핵심적인 요소인 sample selection과 Offline Q-function을 선택하는 방법에 대해서 소개했다.</p>
<section id="distribution-shift-in-offline-to-online-rl" class="level4">
<h4 class="anchored" data-anchor-id="distribution-shift-in-offline-to-online-rl">Distribution Shift in Offline-to-Online RL</h4>
<p>Offline data과 Online data을 섞게 되면, offline buffer <span class="math inline">\(\cal{B}^{\text{off}}\)</span> 에서 뽑은 Data인 <span class="math inline">\(d^{\text{off}}(s, a)\)</span> 와 online buffer <span class="math inline">\(\cal{B}^{\text{on}}\)</span> 에서 뽑은 Data인 <span class="math inline">\(d^{\text{on}}(s, a)\)</span> 간의 distribution shift는 존재한다. 논문에서는 D4RL 환경 중 하나인 <em>halfcheetah-random</em> dataset의 state-action pair를 Variational AutoEncoder를 통해서 재생성하고, 여기에서 CQL agent로 돌렸을 때 나온 online sample과 offline sample을 비교했다.</p>
<div id="fig-distribution-shift" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../images/distribution_shift.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Distribution Shift</figcaption><p></p>
</figure>
</div>
<p><a href="#fig-distribution-shift">Figure&nbsp;1</a> 에서 보여지는 것처럼 offline에서의 state-action distribution과 online에서의 state-action distribution 간의 shift가 발생하는 것을 확인할 수 있다. 이렇게 Distribution shift가 발생하게 되면, agent가 만약 unseen state-action을 마주치게 될 경우, 이 때 참고를 하게될 Q-value (이때 값은 bootstrapping에 사용될 값을 위해서 추정된 값이 된다.)가 매우 부정확하게 된다. 이렇게 부정확한 값을 바탕으로 Policy Iteration을 수행하게 되면 Offline RL에서 아무리 좋은 정책을 얻어도 망가지는 현상이 발생한다. 특히 Offline RL agent가 behavior policy, 즉 dataset을 쌓을 때의 policy보다 성능이 좋은 경우에는 Distribution shift에 의한 성능 저하가 심하게 나타난다. 그리고 offline dataset이 어느 한점이 치중해 있는, 소위 narrow distributed 한 case에서는 (보통 single policy로 학습한 경우) 실제 online RL을 수행할 때 기존의 distribution에서 이탈하는 경향이 더 심하게 나타난다.</p>
</section>
<section id="sample-selection" class="level4">
<h4 class="anchored" data-anchor-id="sample-selection">Sample Selection</h4>
<p>Online dataset은 fine-tune에 필요한 요소이긴하지만 distribution shift 문제로 인해서 잠재적으로 위험한 Out-of-Distribution (OoD) sample이기도 하다. 반면 Offline dataset은 in-distribution 이긴 하지만, fine-tune에 영향을 못 미치거나 느리게 영향을 미친다. 이를 실험적으로 보여주기 위해서 CQL로 학습한 offline agent를 SAC로 fine-tune을 수행했을 때, online dataset을 사용하지 않은 경우와 사용한 경우를 비교했다.</p>
<div id="fig-sample-selection" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../images/sample-selection.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: Sample Selection</figcaption><p></p>
</figure>
</div>
<p>Asymptotic하게 얻을 수 있는 Average return 자체는 Online data만을 사용했을 때가 Uniform, 즉 offline data와 online data를 같이 사용한 경우에 비해서보다 높았지만, <a href="#fig-sample-selection">Figure&nbsp;2</a> 에서 학습 초반부를 보면 Average return이 뚝 떨어지는 현상이 나타난다. 이와 같이 online sample만 가지고 fine-tuning하는 것은 unstable하며, 이는 online data 내에 존재하는 OoD sample이 bootstrap error를 야기하기 때문이다.</p>
<p>Offline data와 online data를 같이 사용하는 경우에도 offline data와 online data간의 비율 문제로 인해서 value propagation이 느리게 나타나는 문제는 있지만, 그래도 online data만 썼을 때에 비하면 학습 초반부에는 어느 정도의 stability를 확보할 수 있다. 이때문에 논문의 아이디어도 offline data와 online data를 같이 사용하는 <strong>balanced replay</strong>에 대한 아이디어를 제안하게 되었고, 이는 유용하지만 잠재적으로 위험한 online sample과 안정적이긴 하지만 fine-tune이 느린 offline sample간의 trade-off를 잘 조절하는 목적을 가지고 있다.</p>
</section>
<section id="choice-of-offline-q-function" class="level4">
<h4 class="anchored" data-anchor-id="choice-of-offline-q-function">Choice of Offline Q-function</h4>
<p>Offline RL에서 발생하는 distribution shift 문제를 해결하는데, 대부분의 방법들은 CQL에서 제안하는 것처럼 OoD action에 대해서 최대한 conservative함을 유지하는 것인데, 이 논문에서도 역시 동일하게 <strong>pessimistically trained Q-function</strong>을 사용했다. 이를 보여주기 위한 실험으로 pessimistically trained Q-function을 사용했을 때와, Regularization이 적용되지 않은 Q-function <span class="math inline">\((\approx \text{FQE})\)</span> 을 활용하여 online fine-tune한 결과를 비교했다.</p>
<div id="fig-choice-of-offline-q-function" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../images/CQL-FQE.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3: Choice of Offline Q-function</figcaption><p></p>
</figure>
</div>
<p><a href="#fig-choice-of-offline-q-function">Figure&nbsp;3</a> 의 첫번째 그래프는 <em>halfcheetah-random</em> data에 대한 fine-tune에 대한 결과인데, 이 그래프에서는 두 방법론 모두 비슷한 fine-tune 성능을 보여주고 있다. 문제는 두번째 그래프인데, <em>halfcheetah-medium</em>, 즉 expert data와 random data가 반반씩 섞인 데이터를 가지고 학습한 모델에서 fine-tune을 수행하면 Regularization 이 적용되지 않은 <em>FQE-init</em>의 초반부 성능이 확 저하가 되는 것을 확인할 수 있다. 이는 <em>FQE-init</em>이 novel state에 대한 OoD action에 대해서 overoptimistic한 값을 가지고 있기 때문에 발생하는 문제이다. 다르게 표현하면 policy가 안전하면서 이미 학습때 본 trajectory가 아닌 잠재적으로 나쁜 action 을 더 선호하는 경향이 나오는 것이다. 이런 이유로 인해서 논문에서 제안하고자 했던 두번째 아이디어는 처음에는 offline을 통해서 pessimistic Q function을 학습하고, fine-tune 단계에서 balanced replay를 통해 offline data와 online data를 섞어주면서 점진적으로 pessimism을 줄여나가는 것이다. 여기에 덧붙여서 한 agent에 의한 pessimism을 방지하기 위해서, 여러개의 agent를 동시에 학습한 후 각 agent의 Q-function에 대한 ensemble을 취하는 방식을 사용했다.</p>
</section>
</section>
<section id="method" class="level3">
<h3 class="anchored" data-anchor-id="method">Method</h3>
<p>결과적으로 논문에서 제안하는 방식은 offline data와 online data를 같이 사용하는 balanced replay와 pessimistic Q-function을 사용하는 ensemble을 취하는 것이고, 구조는 <a href="#fig-off2onrl">Figure&nbsp;4</a> 과 같다.</p>
<div id="fig-off2onrl" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="../images/Off2On.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4: Off2OnRL</figcaption><p></p>
</figure>
</div>
<section id="balanced-experience-replay" class="level4">
<h4 class="anchored" data-anchor-id="balanced-experience-replay">Balanced Experience Replay</h4>
<p>물론 balanced replay 내에서 sampling을 하는데 있어 priority가 존재하고, 이를 토대로 sampling하는 형태 자체는 Prioritized Experience Replay (PER)에서 소개하는 방식과 비슷하다. 다만 TD-error를 기반으로 priority를 매기는 PER과는 다르게 Balanced Replay에서는 <strong>online-ness</strong> 라는 것을 계산해서 priority를 계산한다. 이 지수의 목적은 framework내 online RL에 나와있는 것처럼 online data를 우선적으로 사용하면서도, near-on-policy인 offline data를 fine-tune에 활용하고자 하는 것이다. 조금 더 자세하게 설명하자면, offline data와 online data가 섞인 buffer에서 transition <span class="math inline">\((s, a, s')\)</span>을 sampling을 하되, 이 때 sampling 하는 확률을 일종의 density ratio <span class="math inline">\(w(s, a) := d^{\text{on}}(s, a) / d^{\text{off}}(s, a)\)</span> 에 비례하게 정하자는 것이다. 물론 이 density ratio가 1에 가깝다는 것은 online data의 distribution과 offline data의 distribution이 동일하다는 것을 의미하고, 이 비율대로 뽑게 되면 offline buffer <span class="math inline">\(\cal{B}^{\text{off}}\)</span> 에 속하는 데이터 중 최대한 near-on-policy에 해당하는 sample을 얻을 수 있게 되는 것이다. 하지만 위의 <span class="math inline">\(w(s, a)\)</span> 를 계산하는데 필요한 정보인 <span class="math inline">\(d^{\text{on}}(s, a)\)</span> 와 <span class="math inline">\(d^{\text{off}}(s, a)\)</span> 는 실제로는 구하기 어려운 값이기 때문에, 논문에서는 <span class="math inline">\(\psi\)</span> 를 weight으로 가지는 network으로 학습된 <span class="math inline">\(w_{\psi}(s, a)\)</span>로 density ratio를 계산하는 방법을 취했다.</p>
</section>
<section id="pessimistic-q-ensemble" class="level4">
<h4 class="anchored" data-anchor-id="pessimistic-q-ensemble">Pessimistic Q-Ensemble</h4>
<p><a href="#fig-off2onrl">Figure&nbsp;4</a> 에 소개되어 있는 것처럼, 이 논문에서 제안하는 모델은 multiple agent가 pessimistic 하게 (<span class="math inline">\(\approx \text{CQL}\)</span>) 학습한 Q-function 들을 ensemble하는 형태를 가지고 있고, 이는 offline rl에서 발생하는 distribution shift 문제를 대응하기 위함이라고 되어 있다. 구체적으로는 <span class="math inline">\(N\)</span> 개의 CQL agent를 offline data로 학습시켜서 <span class="math inline">\(\{ Q_{\theta_i}, \pi_{\phi_i}\}^N_{i=1}\)</span> 를 얻은 후, 아래와 같은 Q-function과 policy을 가지는 agent들의 ensemble 모델을 만든다.</p>
<p><span class="math display">\[
Q_{\theta} := \frac{1}{N} \sum^N_{i=1} Q_{\theta_i}, \quad \pi_{\phi}(\cdot | s) = \cal{N}(\frac{1}{N} \sum^N_{i=1} \mu_{\phi_i}(s), \frac{1}{N} \sum^N_{i=1} (\sigma^2_{\phi_i}(s) + \mu^2_{\phi_i}(s)) - \mu^2_{\phi}(s))
\]</span></p>
<p>참고로 각 network의 parameter인 <span class="math inline">\(\theta\)</span> 와 <span class="math inline">\(\phi\)</span> 는 fine-tuning 단계에서 업데이트되는데, 이렇게 되면 초기 fine-tuning 단계에서 나타는 unstable한 현상, 즉 해당 state에 취해지는 unseen action에 대한 pessimism을 유지할 수 있게 된다. (아무래도 한개일때보다는 여러개의 ensemble model이 조금더 안정된 형태가 아닐까.. 물론 그만큼 resource를 잡아먹겠지만….)</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="kcsgoodboy/en" data-repo-id="R_kgDOIixJIQ" data-category="General" data-category-id="DIC_kwDOIixJIc4CTiz-" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">Copyright 2023, Chanseok Kang</div>
  </div>
</footer>
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js" type="text/javascript"></script>
<script type="text/javascript">
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    let pseudocodeOptions = {
      indentSize: el.dataset.indentSize || "1.2em",
      commentDelimiter: el.dataset.commentDelimiter || "//",
      lineNumber: el.dataset.lineNumber === "true" ? true : false,
      lineNumberPunc: el.dataset.lineNumberPunc || ":",
      noEnd: el.dataset.noEnd === "true" ? true : false,
      titlePrefix: el.dataset.algTitle || "Algorithm"
    };
    pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
  });
})(document);
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    titleSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
    titlePrefix = el.dataset.algTitle;
    titleIndex = el.dataset.chapterLevel ? el.dataset.chapterLevel + "." + el.dataset.pseudocodeIndex : el.dataset.pseudocodeIndex;
    titleSpan.innerHTML = titlePrefix + " " + titleIndex + " ";
  });
})(document);
</script>



<script src="../../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>